{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bbfe51da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "- 99 percentage of the first 100 images in 'human_files' have a detected human face.\n",
    "- 11 percentage of the first 100 images in 'dog_files have a detected human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of the images hava a detected face in human_files_short: 99.0 %\n",
      "percentage of the images hava a detected face in dog_files_short: 11.0 %\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "def calc_face_detected_proportion(img_paths):\n",
    "    vectorized_detector = np.vectorize(face_detector)\n",
    "    return len(img_paths[vectorized_detector(img_paths)]) / len(img_paths)\n",
    "\n",
    "\n",
    "print(\"percentage of the images hava a detected face in human_files_short: {} %\".format(calc_face_detected_proportion(human_files_short) * 100) )\n",
    "print(\"percentage of the images hava a detected face in dog_files_short: {} %\".format(calc_face_detected_proportion(dog_files_short) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "I think it depends on the business purpose. However, supposed the API is designed for wide usage, it should not pose the limitations on the user because it is possible the service of the user can't get the images with human faces. To detect human without his or her face, CNN can be used if it is trained with the images without human faces.  \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ \n",
    "- 0 percentage of the images in 'human_files_short' have a detected dog.\n",
    "- 100 percentage of the images in 'dog_files_short' have a detected dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of the images hava a detected dog in human_files_short: 0.0 %\n",
      "percentage of the images hava a detected dog in dog_files_short: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "def calc_dog_detected_proportion(img_paths):\n",
    "    vectorized_doc_detector = np.vectorize(dog_detector)\n",
    "    num_dog_detected = len(img_paths[vectorized_doc_detector(img_paths)])\n",
    "    return num_dog_detected / len(img_paths)\n",
    "\n",
    "print(\"percentage of the images hava a detected dog in human_files_short: {} %\".format(calc_dog_detected_proportion(human_files_short) * 100))\n",
    "print(\"percentage of the images hava a detected dog in dog_files_short: {} %\".format(calc_dog_detected_proportion(dog_files_short) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:53<00:00, 124.47it/s]\n",
      "100%|██████████| 835/835 [00:06<00:00, 138.93it/s]\n",
      "100%|██████████| 836/836 [00:05<00:00, 139.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 224, 224, 128)     396       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 128)     512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 112, 112, 256)     33280     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 256)     1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 56, 56, 512)       132096    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 334,865\n",
      "Trainable params: 333,073\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "x = SeparableConv2D(filters=128, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = MaxPooling2D(128, strides=2, padding=\"same\")(x)\n",
    "\n",
    "x = SeparableConv2D(filters=256, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = MaxPooling2D(256, strides=2, padding=\"same\")(x)\n",
    "\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = MaxPooling2D(512, strides=2, padding=\"same\")(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_87 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCon (None, 224, 224, 512) 1563        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 224, 224, 512) 0           separable_conv2d_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 224, 224, 512) 1536        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 224, 224, 512) 2048        dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 224, 224, 512) 0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 224, 224, 512) 2048        dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)   (None, 224, 224, 512) 0           dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 224, 224, 512) 0           max_pooling2d_8[0][0]            \n",
      "                                                                   dropout_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_87 (Glo (None, 512)           0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           global_average_pooling2d_87[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 75,424\n",
      "Trainable params: 73,376\n",
      "Non-trainable params: 2,048\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "residual = Conv2D(filters=512, kernel_size=1, padding='same', use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "residual = Dropout(0.7)(residual)\n",
    "residual = BatchNormalization()(residual)\n",
    "residual = Dropout(0.7)(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=3, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_87 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCon (None, 224, 224, 512) 1563        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 224, 224, 512) 0           separable_conv2d_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 224, 224, 512) 1536        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 224, 224, 512) 2048        dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 224, 224, 512) 0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 224, 224, 512) 2048        dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)   (None, 224, 224, 512) 0           dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 224, 224, 512) 0           max_pooling2d_8[0][0]            \n",
      "                                                                   dropout_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_87 (Glo (None, 512)           0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           global_average_pooling2d_87[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 75,424\n",
      "Trainable params: 73,376\n",
      "Non-trainable params: 2,048\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "residual = Conv2D(filters=512, kernel_size=1, padding='same', use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "residual = Dropout(0.7)(residual)\n",
    "residual = BatchNormalization()(residual)\n",
    "residual = Dropout(0.7)(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=3, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n",
      "1560/6680 [======>.......................] - ETA: 8838s - loss: 4.9013 - acc: 0.0077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6d9c5a199cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m history = model.fit(train_tensors, train_targets, \n\u001b[1;32m     48\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           epochs=epochs, batch_size=20, callbacks=[checkpointer, history_storer], verbose=1)\n\u001b[0m",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History, BaseLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "RANDOM_SEED=10\n",
    "epochs = 5\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "\n",
    "valid_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "train_generator.fit(train_tensors, seed=RANDOM_SEED)\n",
    "valid_generator.fit(valid_tensors, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "history_storer = History()\n",
    "base_logger = BaseLogger()\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size=40\n",
    "#history = model.fit_generator(train_generator.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "#                    steps_per_epoch=train_tensors.shape[0],\n",
    "#                  validation_data=valid_generator.flow(valid_tensors, valid_targets, batch_size=batch_size),\n",
    "#                  epochs=epochs, validation_steps=valid_tensors.shape[0], callbacks=[checkpointer, history_storer], verbose=1)\n",
    "\n",
    "history = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer, history_storer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100 * np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1)) / len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6580/6680 [============================>.] - ETA: 148s - loss: 14.4393 - acc: 0.05 - ETA: 20s - loss: 14.5490 - acc: 0.0188 - ETA: 11s - loss: 14.4552 - acc: 0.023 - ETA: 9s - loss: 14.5233 - acc: 0.023 - ETA: 7s - loss: 14.2980 - acc: 0.03 - ETA: 6s - loss: 14.2529 - acc: 0.03 - ETA: 5s - loss: 14.2113 - acc: 0.03 - ETA: 4s - loss: 14.1709 - acc: 0.03 - ETA: 4s - loss: 14.0686 - acc: 0.04 - ETA: 4s - loss: 13.9183 - acc: 0.04 - ETA: 3s - loss: 13.8582 - acc: 0.04 - ETA: 3s - loss: 13.7835 - acc: 0.05 - ETA: 3s - loss: 13.7094 - acc: 0.05 - ETA: 3s - loss: 13.6460 - acc: 0.05 - ETA: 3s - loss: 13.6196 - acc: 0.05 - ETA: 2s - loss: 13.5635 - acc: 0.05 - ETA: 2s - loss: 13.5402 - acc: 0.05 - ETA: 2s - loss: 13.4613 - acc: 0.06 - ETA: 2s - loss: 13.4469 - acc: 0.06 - ETA: 2s - loss: 13.3886 - acc: 0.06 - ETA: 2s - loss: 13.3197 - acc: 0.06 - ETA: 2s - loss: 13.2838 - acc: 0.07 - ETA: 2s - loss: 13.2448 - acc: 0.07 - ETA: 1s - loss: 13.1960 - acc: 0.07 - ETA: 1s - loss: 13.0754 - acc: 0.07 - ETA: 1s - loss: 13.0476 - acc: 0.07 - ETA: 1s - loss: 12.9930 - acc: 0.08 - ETA: 1s - loss: 12.9255 - acc: 0.08 - ETA: 1s - loss: 12.8542 - acc: 0.08 - ETA: 1s - loss: 12.8286 - acc: 0.08 - ETA: 1s - loss: 12.7661 - acc: 0.09 - ETA: 1s - loss: 12.7339 - acc: 0.09 - ETA: 1s - loss: 12.6976 - acc: 0.09 - ETA: 1s - loss: 12.6459 - acc: 0.09 - ETA: 1s - loss: 12.5538 - acc: 0.10 - ETA: 1s - loss: 12.5098 - acc: 0.10 - ETA: 0s - loss: 12.4834 - acc: 0.10 - ETA: 0s - loss: 12.4200 - acc: 0.10 - ETA: 0s - loss: 12.3607 - acc: 0.10 - ETA: 0s - loss: 12.3212 - acc: 0.11 - ETA: 0s - loss: 12.2731 - acc: 0.11 - ETA: 0s - loss: 12.2582 - acc: 0.11 - ETA: 0s - loss: 12.2195 - acc: 0.11 - ETA: 0s - loss: 12.1752 - acc: 0.11 - ETA: 0s - loss: 12.1475 - acc: 0.12 - ETA: 0s - loss: 12.1003 - acc: 0.12 - ETA: 0s - loss: 12.0741 - acc: 0.12 - ETA: 0s - loss: 12.0473 - acc: 0.12 - ETA: 0s - loss: 11.9993 - acc: 0.12 - ETA: 0s - loss: 11.9650 - acc: 0.13 - ETA: 0s - loss: 11.9343 - acc: 0.1334Epoch 00000: val_loss improved from inf to 10.46817, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 11.9010 - acc: 0.1355 - val_loss: 10.4682 - val_acc: 0.2263\n",
      "Epoch 2/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 11.3439 - acc: 0.15 - ETA: 3s - loss: 10.2317 - acc: 0.28 - ETA: 3s - loss: 10.1631 - acc: 0.26 - ETA: 3s - loss: 10.3988 - acc: 0.25 - ETA: 2s - loss: 10.0310 - acc: 0.26 - ETA: 2s - loss: 10.0438 - acc: 0.27 - ETA: 2s - loss: 10.0230 - acc: 0.27 - ETA: 2s - loss: 9.9733 - acc: 0.2756 - ETA: 2s - loss: 9.9857 - acc: 0.279 - ETA: 2s - loss: 9.9475 - acc: 0.281 - ETA: 2s - loss: 10.0310 - acc: 0.27 - ETA: 2s - loss: 9.9808 - acc: 0.2750 - ETA: 2s - loss: 9.8892 - acc: 0.277 - ETA: 2s - loss: 9.9210 - acc: 0.275 - ETA: 2s - loss: 9.8995 - acc: 0.278 - ETA: 2s - loss: 9.8839 - acc: 0.276 - ETA: 2s - loss: 9.8819 - acc: 0.278 - ETA: 2s - loss: 9.8265 - acc: 0.282 - ETA: 1s - loss: 9.7613 - acc: 0.286 - ETA: 1s - loss: 9.7479 - acc: 0.287 - ETA: 1s - loss: 9.7510 - acc: 0.286 - ETA: 1s - loss: 9.7538 - acc: 0.287 - ETA: 1s - loss: 9.7245 - acc: 0.289 - ETA: 1s - loss: 9.6594 - acc: 0.292 - ETA: 1s - loss: 9.6864 - acc: 0.290 - ETA: 1s - loss: 9.6931 - acc: 0.290 - ETA: 1s - loss: 9.7054 - acc: 0.289 - ETA: 1s - loss: 9.7082 - acc: 0.290 - ETA: 1s - loss: 9.6784 - acc: 0.293 - ETA: 1s - loss: 9.6798 - acc: 0.294 - ETA: 1s - loss: 9.6853 - acc: 0.294 - ETA: 1s - loss: 9.6935 - acc: 0.294 - ETA: 1s - loss: 9.6809 - acc: 0.296 - ETA: 1s - loss: 9.6417 - acc: 0.298 - ETA: 1s - loss: 9.6017 - acc: 0.299 - ETA: 0s - loss: 9.5979 - acc: 0.300 - ETA: 0s - loss: 9.5987 - acc: 0.300 - ETA: 0s - loss: 9.5970 - acc: 0.300 - ETA: 0s - loss: 9.6143 - acc: 0.300 - ETA: 0s - loss: 9.5941 - acc: 0.302 - ETA: 0s - loss: 9.5886 - acc: 0.303 - ETA: 0s - loss: 9.5462 - acc: 0.305 - ETA: 0s - loss: 9.5473 - acc: 0.305 - ETA: 0s - loss: 9.5513 - acc: 0.305 - ETA: 0s - loss: 9.5505 - acc: 0.306 - ETA: 0s - loss: 9.5326 - acc: 0.306 - ETA: 0s - loss: 9.5150 - acc: 0.306 - ETA: 0s - loss: 9.4905 - acc: 0.308 - ETA: 0s - loss: 9.4825 - acc: 0.308 - ETA: 0s - loss: 9.4832 - acc: 0.309 - ETA: 0s - loss: 9.5028 - acc: 0.308 - ETA: 0s - loss: 9.4826 - acc: 0.309 - ETA: 0s - loss: 9.5016 - acc: 0.309 - ETA: 0s - loss: 9.4969 - acc: 0.3103Epoch 00001: val_loss improved from 10.46817 to 9.37262, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4935 - acc: 0.3105 - val_loss: 9.3726 - val_acc: 0.3269\n",
      "Epoch 3/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.9605 - acc: 0.400 - ETA: 3s - loss: 8.8353 - acc: 0.378 - ETA: 2s - loss: 9.4159 - acc: 0.361 - ETA: 2s - loss: 9.1783 - acc: 0.373 - ETA: 2s - loss: 9.0468 - acc: 0.384 - ETA: 2s - loss: 9.2068 - acc: 0.374 - ETA: 2s - loss: 9.1361 - acc: 0.379 - ETA: 2s - loss: 9.1137 - acc: 0.373 - ETA: 2s - loss: 9.0287 - acc: 0.377 - ETA: 2s - loss: 9.1241 - acc: 0.373 - ETA: 2s - loss: 9.0334 - acc: 0.378 - ETA: 2s - loss: 8.9112 - acc: 0.381 - ETA: 2s - loss: 8.9210 - acc: 0.378 - ETA: 2s - loss: 8.9963 - acc: 0.375 - ETA: 2s - loss: 9.0427 - acc: 0.369 - ETA: 2s - loss: 8.9661 - acc: 0.372 - ETA: 2s - loss: 8.9375 - acc: 0.373 - ETA: 2s - loss: 8.8885 - acc: 0.376 - ETA: 1s - loss: 8.8451 - acc: 0.378 - ETA: 1s - loss: 8.8216 - acc: 0.380 - ETA: 1s - loss: 8.8661 - acc: 0.379 - ETA: 1s - loss: 8.8478 - acc: 0.380 - ETA: 1s - loss: 8.8229 - acc: 0.383 - ETA: 1s - loss: 8.8608 - acc: 0.379 - ETA: 1s - loss: 8.8577 - acc: 0.380 - ETA: 1s - loss: 8.8589 - acc: 0.381 - ETA: 1s - loss: 8.8738 - acc: 0.378 - ETA: 1s - loss: 8.8724 - acc: 0.379 - ETA: 1s - loss: 8.8711 - acc: 0.380 - ETA: 1s - loss: 8.9108 - acc: 0.378 - ETA: 1s - loss: 8.8954 - acc: 0.378 - ETA: 1s - loss: 8.8411 - acc: 0.381 - ETA: 1s - loss: 8.8492 - acc: 0.379 - ETA: 1s - loss: 8.8443 - acc: 0.380 - ETA: 1s - loss: 8.8353 - acc: 0.381 - ETA: 1s - loss: 8.8410 - acc: 0.381 - ETA: 0s - loss: 8.8586 - acc: 0.379 - ETA: 0s - loss: 8.8255 - acc: 0.381 - ETA: 0s - loss: 8.8362 - acc: 0.381 - ETA: 0s - loss: 8.8675 - acc: 0.380 - ETA: 0s - loss: 8.8812 - acc: 0.379 - ETA: 0s - loss: 8.8733 - acc: 0.379 - ETA: 0s - loss: 8.8501 - acc: 0.381 - ETA: 0s - loss: 8.8464 - acc: 0.381 - ETA: 0s - loss: 8.8665 - acc: 0.380 - ETA: 0s - loss: 8.8626 - acc: 0.380 - ETA: 0s - loss: 8.8726 - acc: 0.380 - ETA: 0s - loss: 8.8891 - acc: 0.379 - ETA: 0s - loss: 8.8843 - acc: 0.379 - ETA: 0s - loss: 8.8623 - acc: 0.380 - ETA: 0s - loss: 8.8573 - acc: 0.380 - ETA: 0s - loss: 8.8402 - acc: 0.381 - ETA: 0s - loss: 8.8329 - acc: 0.381 - ETA: 0s - loss: 8.8208 - acc: 0.381 - ETA: 0s - loss: 8.8063 - acc: 0.3821Epoch 00002: val_loss improved from 9.37262 to 9.01960, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.8262 - acc: 0.3813 - val_loss: 9.0196 - val_acc: 0.3521\n",
      "Epoch 4/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 5.3237 - acc: 0.600 - ETA: 3s - loss: 7.1220 - acc: 0.507 - ETA: 2s - loss: 7.3717 - acc: 0.496 - ETA: 2s - loss: 8.4736 - acc: 0.431 - ETA: 2s - loss: 8.7189 - acc: 0.414 - ETA: 2s - loss: 8.8358 - acc: 0.408 - ETA: 2s - loss: 8.5364 - acc: 0.423 - ETA: 2s - loss: 8.4849 - acc: 0.421 - ETA: 2s - loss: 8.3415 - acc: 0.431 - ETA: 2s - loss: 8.3891 - acc: 0.428 - ETA: 2s - loss: 8.2625 - acc: 0.438 - ETA: 2s - loss: 8.2255 - acc: 0.439 - ETA: 2s - loss: 8.1427 - acc: 0.443 - ETA: 2s - loss: 8.0838 - acc: 0.444 - ETA: 2s - loss: 8.1855 - acc: 0.438 - ETA: 2s - loss: 8.2372 - acc: 0.436 - ETA: 1s - loss: 8.2657 - acc: 0.435 - ETA: 1s - loss: 8.2471 - acc: 0.435 - ETA: 1s - loss: 8.2455 - acc: 0.434 - ETA: 1s - loss: 8.2371 - acc: 0.432 - ETA: 1s - loss: 8.2651 - acc: 0.430 - ETA: 1s - loss: 8.3193 - acc: 0.426 - ETA: 1s - loss: 8.3409 - acc: 0.424 - ETA: 1s - loss: 8.4076 - acc: 0.421 - ETA: 1s - loss: 8.4390 - acc: 0.419 - ETA: 1s - loss: 8.4817 - acc: 0.416 - ETA: 1s - loss: 8.4930 - acc: 0.416 - ETA: 1s - loss: 8.4874 - acc: 0.417 - ETA: 1s - loss: 8.4706 - acc: 0.418 - ETA: 1s - loss: 8.4739 - acc: 0.417 - ETA: 1s - loss: 8.4571 - acc: 0.419 - ETA: 1s - loss: 8.4532 - acc: 0.419 - ETA: 1s - loss: 8.4446 - acc: 0.420 - ETA: 1s - loss: 8.4694 - acc: 0.419 - ETA: 1s - loss: 8.4866 - acc: 0.418 - ETA: 1s - loss: 8.4713 - acc: 0.419 - ETA: 0s - loss: 8.4670 - acc: 0.420 - ETA: 0s - loss: 8.4651 - acc: 0.419 - ETA: 0s - loss: 8.4688 - acc: 0.419 - ETA: 0s - loss: 8.4627 - acc: 0.419 - ETA: 0s - loss: 8.4608 - acc: 0.419 - ETA: 0s - loss: 8.4735 - acc: 0.418 - ETA: 0s - loss: 8.4998 - acc: 0.416 - ETA: 0s - loss: 8.4985 - acc: 0.416 - ETA: 0s - loss: 8.5191 - acc: 0.415 - ETA: 0s - loss: 8.4988 - acc: 0.416 - ETA: 0s - loss: 8.4695 - acc: 0.418 - ETA: 0s - loss: 8.4595 - acc: 0.418 - ETA: 0s - loss: 8.4962 - acc: 0.416 - ETA: 0s - loss: 8.5077 - acc: 0.415 - ETA: 0s - loss: 8.5033 - acc: 0.415 - ETA: 0s - loss: 8.5056 - acc: 0.415 - ETA: 0s - loss: 8.4939 - acc: 0.416 - ETA: 0s - loss: 8.5064 - acc: 0.4166Epoch 00003: val_loss improved from 9.01960 to 8.87551, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.5072 - acc: 0.4169 - val_loss: 8.8755 - val_acc: 0.3665\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.7305 - acc: 0.350 - ETA: 3s - loss: 8.0499 - acc: 0.466 - ETA: 3s - loss: 8.2876 - acc: 0.445 - ETA: 3s - loss: 8.4324 - acc: 0.429 - ETA: 2s - loss: 8.3660 - acc: 0.439 - ETA: 2s - loss: 8.2938 - acc: 0.446 - ETA: 2s - loss: 8.2847 - acc: 0.445 - ETA: 2s - loss: 8.2611 - acc: 0.447 - ETA: 2s - loss: 8.3435 - acc: 0.442 - ETA: 2s - loss: 8.3644 - acc: 0.438 - ETA: 2s - loss: 8.4946 - acc: 0.432 - ETA: 2s - loss: 8.4310 - acc: 0.437 - ETA: 2s - loss: 8.4968 - acc: 0.434 - ETA: 2s - loss: 8.5064 - acc: 0.431 - ETA: 2s - loss: 8.4344 - acc: 0.436 - ETA: 2s - loss: 8.4922 - acc: 0.432 - ETA: 2s - loss: 8.5164 - acc: 0.431 - ETA: 2s - loss: 8.4297 - acc: 0.436 - ETA: 2s - loss: 8.4066 - acc: 0.437 - ETA: 1s - loss: 8.4033 - acc: 0.438 - ETA: 1s - loss: 8.4564 - acc: 0.436 - ETA: 1s - loss: 8.4379 - acc: 0.437 - ETA: 1s - loss: 8.4295 - acc: 0.437 - ETA: 1s - loss: 8.3866 - acc: 0.440 - ETA: 1s - loss: 8.4043 - acc: 0.440 - ETA: 1s - loss: 8.4229 - acc: 0.440 - ETA: 1s - loss: 8.4047 - acc: 0.441 - ETA: 1s - loss: 8.3823 - acc: 0.442 - ETA: 1s - loss: 8.3570 - acc: 0.444 - ETA: 1s - loss: 8.3383 - acc: 0.445 - ETA: 1s - loss: 8.3757 - acc: 0.443 - ETA: 1s - loss: 8.3778 - acc: 0.443 - ETA: 1s - loss: 8.4256 - acc: 0.441 - ETA: 1s - loss: 8.4048 - acc: 0.442 - ETA: 1s - loss: 8.4423 - acc: 0.440 - ETA: 1s - loss: 8.4278 - acc: 0.440 - ETA: 1s - loss: 8.4177 - acc: 0.441 - ETA: 1s - loss: 8.4707 - acc: 0.439 - ETA: 0s - loss: 8.4689 - acc: 0.438 - ETA: 0s - loss: 8.4752 - acc: 0.437 - ETA: 0s - loss: 8.4786 - acc: 0.437 - ETA: 0s - loss: 8.4920 - acc: 0.436 - ETA: 0s - loss: 8.4651 - acc: 0.437 - ETA: 0s - loss: 8.4421 - acc: 0.439 - ETA: 0s - loss: 8.4364 - acc: 0.439 - ETA: 0s - loss: 8.4233 - acc: 0.439 - ETA: 0s - loss: 8.4089 - acc: 0.440 - ETA: 0s - loss: 8.3937 - acc: 0.441 - ETA: 0s - loss: 8.3927 - acc: 0.442 - ETA: 0s - loss: 8.4207 - acc: 0.440 - ETA: 0s - loss: 8.4118 - acc: 0.441 - ETA: 0s - loss: 8.3988 - acc: 0.441 - ETA: 0s - loss: 8.3896 - acc: 0.442 - ETA: 0s - loss: 8.3831 - acc: 0.442 - ETA: 0s - loss: 8.3800 - acc: 0.442 - ETA: 0s - loss: 8.3790 - acc: 0.4415Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.3737 - acc: 0.4415 - val_loss: 8.9443 - val_acc: 0.3689\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 6.7664 - acc: 0.500 - ETA: 2s - loss: 8.0628 - acc: 0.450 - ETA: 2s - loss: 8.5319 - acc: 0.428 - ETA: 2s - loss: 8.5088 - acc: 0.430 - ETA: 2s - loss: 8.5447 - acc: 0.434 - ETA: 2s - loss: 8.6621 - acc: 0.432 - ETA: 2s - loss: 8.3765 - acc: 0.450 - ETA: 2s - loss: 8.3038 - acc: 0.453 - ETA: 2s - loss: 8.2782 - acc: 0.454 - ETA: 2s - loss: 8.3307 - acc: 0.453 - ETA: 2s - loss: 8.3753 - acc: 0.450 - ETA: 2s - loss: 8.3757 - acc: 0.451 - ETA: 2s - loss: 8.3996 - acc: 0.450 - ETA: 2s - loss: 8.3017 - acc: 0.457 - ETA: 2s - loss: 8.3390 - acc: 0.453 - ETA: 1s - loss: 8.3596 - acc: 0.452 - ETA: 1s - loss: 8.3547 - acc: 0.451 - ETA: 1s - loss: 8.3621 - acc: 0.451 - ETA: 1s - loss: 8.2643 - acc: 0.457 - ETA: 1s - loss: 8.2443 - acc: 0.457 - ETA: 1s - loss: 8.3189 - acc: 0.452 - ETA: 1s - loss: 8.3773 - acc: 0.449 - ETA: 1s - loss: 8.3550 - acc: 0.450 - ETA: 1s - loss: 8.3578 - acc: 0.450 - ETA: 1s - loss: 8.3488 - acc: 0.451 - ETA: 1s - loss: 8.3065 - acc: 0.454 - ETA: 1s - loss: 8.3143 - acc: 0.453 - ETA: 1s - loss: 8.3325 - acc: 0.452 - ETA: 1s - loss: 8.3011 - acc: 0.454 - ETA: 1s - loss: 8.3003 - acc: 0.454 - ETA: 1s - loss: 8.3014 - acc: 0.454 - ETA: 1s - loss: 8.3164 - acc: 0.453 - ETA: 1s - loss: 8.3182 - acc: 0.453 - ETA: 1s - loss: 8.3307 - acc: 0.452 - ETA: 1s - loss: 8.3344 - acc: 0.452 - ETA: 1s - loss: 8.3261 - acc: 0.453 - ETA: 0s - loss: 8.3210 - acc: 0.453 - ETA: 0s - loss: 8.3294 - acc: 0.452 - ETA: 0s - loss: 8.3608 - acc: 0.451 - ETA: 0s - loss: 8.3588 - acc: 0.451 - ETA: 0s - loss: 8.3566 - acc: 0.452 - ETA: 0s - loss: 8.3247 - acc: 0.453 - ETA: 0s - loss: 8.3241 - acc: 0.452 - ETA: 0s - loss: 8.3132 - acc: 0.453 - ETA: 0s - loss: 8.3365 - acc: 0.452 - ETA: 0s - loss: 8.3218 - acc: 0.452 - ETA: 0s - loss: 8.2879 - acc: 0.455 - ETA: 0s - loss: 8.2901 - acc: 0.455 - ETA: 0s - loss: 8.2931 - acc: 0.455 - ETA: 0s - loss: 8.2892 - acc: 0.455 - ETA: 0s - loss: 8.2579 - acc: 0.457 - ETA: 0s - loss: 8.2726 - acc: 0.456 - ETA: 0s - loss: 8.2728 - acc: 0.457 - ETA: 0s - loss: 8.2699 - acc: 0.457 - ETA: 0s - loss: 8.2874 - acc: 0.4566Epoch 00005: val_loss improved from 8.87551 to 8.71168, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.2933 - acc: 0.4564 - val_loss: 8.7117 - val_acc: 0.3916\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 7.3177 - acc: 0.500 - ETA: 3s - loss: 8.7902 - acc: 0.421 - ETA: 2s - loss: 9.2973 - acc: 0.388 - ETA: 2s - loss: 8.7896 - acc: 0.425 - ETA: 2s - loss: 8.6150 - acc: 0.441 - ETA: 2s - loss: 8.4869 - acc: 0.451 - ETA: 2s - loss: 8.2471 - acc: 0.464 - ETA: 2s - loss: 8.3114 - acc: 0.459 - ETA: 2s - loss: 8.2859 - acc: 0.463 - ETA: 2s - loss: 8.2576 - acc: 0.464 - ETA: 2s - loss: 8.2291 - acc: 0.465 - ETA: 2s - loss: 8.1707 - acc: 0.469 - ETA: 2s - loss: 8.1631 - acc: 0.469 - ETA: 2s - loss: 8.1207 - acc: 0.472 - ETA: 2s - loss: 8.1269 - acc: 0.472 - ETA: 2s - loss: 8.2007 - acc: 0.467 - ETA: 2s - loss: 8.1342 - acc: 0.470 - ETA: 1s - loss: 8.1475 - acc: 0.470 - ETA: 1s - loss: 8.1266 - acc: 0.471 - ETA: 1s - loss: 8.1363 - acc: 0.471 - ETA: 1s - loss: 8.0957 - acc: 0.474 - ETA: 1s - loss: 8.1484 - acc: 0.470 - ETA: 1s - loss: 8.1751 - acc: 0.468 - ETA: 1s - loss: 8.1707 - acc: 0.467 - ETA: 1s - loss: 8.1790 - acc: 0.467 - ETA: 1s - loss: 8.1472 - acc: 0.469 - ETA: 1s - loss: 8.0897 - acc: 0.473 - ETA: 1s - loss: 8.1017 - acc: 0.472 - ETA: 1s - loss: 8.1085 - acc: 0.472 - ETA: 1s - loss: 8.1156 - acc: 0.471 - ETA: 1s - loss: 8.1155 - acc: 0.472 - ETA: 1s - loss: 8.1199 - acc: 0.472 - ETA: 1s - loss: 8.1228 - acc: 0.472 - ETA: 1s - loss: 8.0876 - acc: 0.474 - ETA: 1s - loss: 8.0900 - acc: 0.474 - ETA: 1s - loss: 8.1104 - acc: 0.473 - ETA: 0s - loss: 8.1243 - acc: 0.472 - ETA: 0s - loss: 8.0906 - acc: 0.473 - ETA: 0s - loss: 8.1229 - acc: 0.471 - ETA: 0s - loss: 8.1124 - acc: 0.471 - ETA: 0s - loss: 8.1142 - acc: 0.471 - ETA: 0s - loss: 8.1242 - acc: 0.470 - ETA: 0s - loss: 8.1325 - acc: 0.470 - ETA: 0s - loss: 8.1348 - acc: 0.469 - ETA: 0s - loss: 8.1567 - acc: 0.468 - ETA: 0s - loss: 8.1599 - acc: 0.468 - ETA: 0s - loss: 8.1638 - acc: 0.468 - ETA: 0s - loss: 8.1636 - acc: 0.468 - ETA: 0s - loss: 8.1317 - acc: 0.470 - ETA: 0s - loss: 8.1503 - acc: 0.468 - ETA: 0s - loss: 8.1421 - acc: 0.468 - ETA: 0s - loss: 8.1326 - acc: 0.468 - ETA: 0s - loss: 8.1473 - acc: 0.4681Epoch 00006: val_loss improved from 8.71168 to 8.69161, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1615 - acc: 0.4672 - val_loss: 8.6916 - val_acc: 0.3880\n",
      "Epoch 8/20\n",
      "6560/6680 [============================>.] - ETA: 4s - loss: 8.0625 - acc: 0.500 - ETA: 3s - loss: 6.9307 - acc: 0.557 - ETA: 3s - loss: 6.9942 - acc: 0.550 - ETA: 2s - loss: 7.5059 - acc: 0.521 - ETA: 2s - loss: 7.7243 - acc: 0.507 - ETA: 2s - loss: 7.9415 - acc: 0.495 - ETA: 2s - loss: 7.9000 - acc: 0.497 - ETA: 2s - loss: 7.8857 - acc: 0.493 - ETA: 2s - loss: 7.7506 - acc: 0.501 - ETA: 2s - loss: 7.7340 - acc: 0.501 - ETA: 2s - loss: 7.7987 - acc: 0.499 - ETA: 2s - loss: 7.8160 - acc: 0.497 - ETA: 2s - loss: 7.8259 - acc: 0.498 - ETA: 2s - loss: 7.9456 - acc: 0.491 - ETA: 2s - loss: 8.0395 - acc: 0.485 - ETA: 1s - loss: 8.0448 - acc: 0.485 - ETA: 1s - loss: 8.1496 - acc: 0.479 - ETA: 1s - loss: 8.1067 - acc: 0.480 - ETA: 1s - loss: 8.0512 - acc: 0.484 - ETA: 1s - loss: 8.0687 - acc: 0.483 - ETA: 1s - loss: 8.1079 - acc: 0.481 - ETA: 1s - loss: 8.1183 - acc: 0.481 - ETA: 1s - loss: 8.0718 - acc: 0.484 - ETA: 1s - loss: 8.1284 - acc: 0.480 - ETA: 1s - loss: 8.1255 - acc: 0.481 - ETA: 1s - loss: 8.1345 - acc: 0.480 - ETA: 1s - loss: 8.1256 - acc: 0.480 - ETA: 1s - loss: 8.0885 - acc: 0.482 - ETA: 1s - loss: 8.0836 - acc: 0.483 - ETA: 1s - loss: 8.1146 - acc: 0.481 - ETA: 1s - loss: 8.1055 - acc: 0.482 - ETA: 1s - loss: 8.1227 - acc: 0.481 - ETA: 1s - loss: 8.1481 - acc: 0.479 - ETA: 1s - loss: 8.1527 - acc: 0.478 - ETA: 1s - loss: 8.1441 - acc: 0.478 - ETA: 0s - loss: 8.1290 - acc: 0.479 - ETA: 0s - loss: 8.1149 - acc: 0.480 - ETA: 0s - loss: 8.1329 - acc: 0.478 - ETA: 0s - loss: 8.1383 - acc: 0.478 - ETA: 0s - loss: 8.1085 - acc: 0.480 - ETA: 0s - loss: 8.1078 - acc: 0.480 - ETA: 0s - loss: 8.1061 - acc: 0.479 - ETA: 0s - loss: 8.1058 - acc: 0.480 - ETA: 0s - loss: 8.0874 - acc: 0.480 - ETA: 0s - loss: 8.0822 - acc: 0.480 - ETA: 0s - loss: 8.1018 - acc: 0.479 - ETA: 0s - loss: 8.1102 - acc: 0.478 - ETA: 0s - loss: 8.1126 - acc: 0.478 - ETA: 0s - loss: 8.1391 - acc: 0.477 - ETA: 0s - loss: 8.1334 - acc: 0.477 - ETA: 0s - loss: 8.1133 - acc: 0.478 - ETA: 0s - loss: 8.1041 - acc: 0.479 - ETA: 0s - loss: 8.1038 - acc: 0.4791Epoch 00007: val_loss improved from 8.69161 to 8.60822, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0902 - acc: 0.4799 - val_loss: 8.6082 - val_acc: 0.4024\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 4.0661 - acc: 0.700 - ETA: 3s - loss: 7.2257 - acc: 0.535 - ETA: 3s - loss: 7.5119 - acc: 0.525 - ETA: 2s - loss: 7.4731 - acc: 0.530 - ETA: 2s - loss: 7.6414 - acc: 0.516 - ETA: 2s - loss: 7.5592 - acc: 0.520 - ETA: 2s - loss: 7.7839 - acc: 0.506 - ETA: 2s - loss: 7.8440 - acc: 0.502 - ETA: 2s - loss: 7.9425 - acc: 0.495 - ETA: 2s - loss: 7.8985 - acc: 0.498 - ETA: 2s - loss: 7.7897 - acc: 0.505 - ETA: 2s - loss: 7.7552 - acc: 0.508 - ETA: 2s - loss: 7.7702 - acc: 0.506 - ETA: 2s - loss: 7.8182 - acc: 0.502 - ETA: 2s - loss: 7.7519 - acc: 0.506 - ETA: 2s - loss: 7.7147 - acc: 0.507 - ETA: 2s - loss: 7.7126 - acc: 0.506 - ETA: 1s - loss: 7.7690 - acc: 0.502 - ETA: 1s - loss: 7.7842 - acc: 0.502 - ETA: 1s - loss: 7.7606 - acc: 0.504 - ETA: 1s - loss: 7.7861 - acc: 0.502 - ETA: 1s - loss: 7.7662 - acc: 0.504 - ETA: 1s - loss: 7.7111 - acc: 0.506 - ETA: 1s - loss: 7.7150 - acc: 0.507 - ETA: 1s - loss: 7.7148 - acc: 0.507 - ETA: 1s - loss: 7.7530 - acc: 0.504 - ETA: 1s - loss: 7.7746 - acc: 0.504 - ETA: 1s - loss: 7.7976 - acc: 0.502 - ETA: 1s - loss: 7.8645 - acc: 0.498 - ETA: 1s - loss: 7.8706 - acc: 0.497 - ETA: 1s - loss: 7.8980 - acc: 0.495 - ETA: 1s - loss: 7.8786 - acc: 0.496 - ETA: 1s - loss: 7.9065 - acc: 0.494 - ETA: 1s - loss: 7.9359 - acc: 0.492 - ETA: 1s - loss: 7.9589 - acc: 0.491 - ETA: 0s - loss: 7.9287 - acc: 0.494 - ETA: 0s - loss: 7.9187 - acc: 0.494 - ETA: 0s - loss: 7.9169 - acc: 0.494 - ETA: 0s - loss: 7.9008 - acc: 0.495 - ETA: 0s - loss: 7.9155 - acc: 0.494 - ETA: 0s - loss: 7.9045 - acc: 0.494 - ETA: 0s - loss: 7.8947 - acc: 0.495 - ETA: 0s - loss: 7.8603 - acc: 0.497 - ETA: 0s - loss: 7.8929 - acc: 0.495 - ETA: 0s - loss: 7.8665 - acc: 0.497 - ETA: 0s - loss: 7.8477 - acc: 0.498 - ETA: 0s - loss: 7.8540 - acc: 0.498 - ETA: 0s - loss: 7.8417 - acc: 0.499 - ETA: 0s - loss: 7.8549 - acc: 0.498 - ETA: 0s - loss: 7.8628 - acc: 0.497 - ETA: 0s - loss: 7.8908 - acc: 0.495 - ETA: 0s - loss: 7.9032 - acc: 0.495 - ETA: 0s - loss: 7.9311 - acc: 0.493 - ETA: 0s - loss: 7.9516 - acc: 0.492 - ETA: 0s - loss: 7.9685 - acc: 0.4913Epoch 00008: val_loss improved from 8.60822 to 8.60318, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9671 - acc: 0.4913 - val_loss: 8.6032 - val_acc: 0.3940\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.8650 - acc: 0.450 - ETA: 2s - loss: 8.6839 - acc: 0.442 - ETA: 2s - loss: 8.2762 - acc: 0.473 - ETA: 2s - loss: 7.8472 - acc: 0.502 - ETA: 2s - loss: 7.9378 - acc: 0.498 - ETA: 2s - loss: 7.6806 - acc: 0.512 - ETA: 2s - loss: 7.6580 - acc: 0.514 - ETA: 2s - loss: 7.5822 - acc: 0.518 - ETA: 2s - loss: 7.5514 - acc: 0.519 - ETA: 2s - loss: 7.7158 - acc: 0.510 - ETA: 2s - loss: 7.7615 - acc: 0.508 - ETA: 2s - loss: 7.7764 - acc: 0.507 - ETA: 2s - loss: 7.7249 - acc: 0.511 - ETA: 2s - loss: 7.6207 - acc: 0.516 - ETA: 2s - loss: 7.7167 - acc: 0.510 - ETA: 2s - loss: 7.7191 - acc: 0.509 - ETA: 1s - loss: 7.8207 - acc: 0.503 - ETA: 1s - loss: 7.7705 - acc: 0.506 - ETA: 1s - loss: 7.8011 - acc: 0.504 - ETA: 1s - loss: 7.8122 - acc: 0.503 - ETA: 1s - loss: 7.8438 - acc: 0.502 - ETA: 1s - loss: 7.8734 - acc: 0.500 - ETA: 1s - loss: 7.8896 - acc: 0.499 - ETA: 1s - loss: 7.9301 - acc: 0.497 - ETA: 1s - loss: 7.9379 - acc: 0.496 - ETA: 1s - loss: 7.9804 - acc: 0.493 - ETA: 1s - loss: 8.0081 - acc: 0.492 - ETA: 1s - loss: 7.9654 - acc: 0.495 - ETA: 1s - loss: 7.9939 - acc: 0.493 - ETA: 1s - loss: 8.0011 - acc: 0.493 - ETA: 1s - loss: 7.9730 - acc: 0.495 - ETA: 1s - loss: 7.9556 - acc: 0.496 - ETA: 1s - loss: 7.9591 - acc: 0.496 - ETA: 1s - loss: 7.9737 - acc: 0.494 - ETA: 1s - loss: 7.9618 - acc: 0.495 - ETA: 0s - loss: 7.9688 - acc: 0.495 - ETA: 0s - loss: 7.9872 - acc: 0.493 - ETA: 0s - loss: 8.0061 - acc: 0.492 - ETA: 0s - loss: 7.9974 - acc: 0.493 - ETA: 0s - loss: 7.9690 - acc: 0.495 - ETA: 0s - loss: 7.9739 - acc: 0.494 - ETA: 0s - loss: 7.9570 - acc: 0.495 - ETA: 0s - loss: 7.9763 - acc: 0.494 - ETA: 0s - loss: 7.9666 - acc: 0.494 - ETA: 0s - loss: 7.9509 - acc: 0.496 - ETA: 0s - loss: 7.9417 - acc: 0.496 - ETA: 0s - loss: 7.9592 - acc: 0.495 - ETA: 0s - loss: 7.9755 - acc: 0.494 - ETA: 0s - loss: 7.9901 - acc: 0.493 - ETA: 0s - loss: 7.9907 - acc: 0.493 - ETA: 0s - loss: 7.9572 - acc: 0.495 - ETA: 0s - loss: 7.9470 - acc: 0.495 - ETA: 0s - loss: 7.9594 - acc: 0.495 - ETA: 0s - loss: 7.9658 - acc: 0.494 - ETA: 0s - loss: 7.9369 - acc: 0.4961Epoch 00009: val_loss improved from 8.60318 to 8.52814, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9284 - acc: 0.4967 - val_loss: 8.5281 - val_acc: 0.4036\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 6.4753 - acc: 0.600 - ETA: 2s - loss: 8.0408 - acc: 0.492 - ETA: 2s - loss: 7.7725 - acc: 0.510 - ETA: 2s - loss: 7.7966 - acc: 0.509 - ETA: 2s - loss: 7.8259 - acc: 0.509 - ETA: 2s - loss: 7.6773 - acc: 0.518 - ETA: 2s - loss: 7.8284 - acc: 0.507 - ETA: 2s - loss: 7.7973 - acc: 0.507 - ETA: 2s - loss: 7.8276 - acc: 0.506 - ETA: 2s - loss: 7.7553 - acc: 0.511 - ETA: 2s - loss: 7.6622 - acc: 0.515 - ETA: 2s - loss: 7.7079 - acc: 0.513 - ETA: 2s - loss: 7.8032 - acc: 0.507 - ETA: 2s - loss: 7.8985 - acc: 0.501 - ETA: 2s - loss: 7.9279 - acc: 0.500 - ETA: 1s - loss: 7.8880 - acc: 0.502 - ETA: 1s - loss: 7.9313 - acc: 0.500 - ETA: 1s - loss: 7.9165 - acc: 0.500 - ETA: 1s - loss: 7.8733 - acc: 0.503 - ETA: 1s - loss: 7.9109 - acc: 0.501 - ETA: 1s - loss: 7.8660 - acc: 0.504 - ETA: 1s - loss: 7.8285 - acc: 0.506 - ETA: 1s - loss: 7.8437 - acc: 0.504 - ETA: 1s - loss: 7.8375 - acc: 0.504 - ETA: 1s - loss: 7.8467 - acc: 0.504 - ETA: 1s - loss: 7.8241 - acc: 0.506 - ETA: 1s - loss: 7.8252 - acc: 0.505 - ETA: 1s - loss: 7.8310 - acc: 0.504 - ETA: 1s - loss: 7.8590 - acc: 0.502 - ETA: 1s - loss: 7.9053 - acc: 0.499 - ETA: 1s - loss: 7.8959 - acc: 0.499 - ETA: 1s - loss: 7.8829 - acc: 0.500 - ETA: 1s - loss: 7.8671 - acc: 0.501 - ETA: 1s - loss: 7.8786 - acc: 0.500 - ETA: 1s - loss: 7.8638 - acc: 0.500 - ETA: 0s - loss: 7.8558 - acc: 0.500 - ETA: 0s - loss: 7.8634 - acc: 0.500 - ETA: 0s - loss: 7.9031 - acc: 0.497 - ETA: 0s - loss: 7.9386 - acc: 0.495 - ETA: 0s - loss: 7.8997 - acc: 0.497 - ETA: 0s - loss: 7.8795 - acc: 0.498 - ETA: 0s - loss: 7.8634 - acc: 0.499 - ETA: 0s - loss: 7.8535 - acc: 0.500 - ETA: 0s - loss: 7.8629 - acc: 0.499 - ETA: 0s - loss: 7.8572 - acc: 0.499 - ETA: 0s - loss: 7.8186 - acc: 0.502 - ETA: 0s - loss: 7.8203 - acc: 0.501 - ETA: 0s - loss: 7.8057 - acc: 0.502 - ETA: 0s - loss: 7.8350 - acc: 0.500 - ETA: 0s - loss: 7.8526 - acc: 0.499 - ETA: 0s - loss: 7.8618 - acc: 0.499 - ETA: 0s - loss: 7.8685 - acc: 0.498 - ETA: 0s - loss: 7.8611 - acc: 0.498 - ETA: 0s - loss: 7.8550 - acc: 0.499 - ETA: 0s - loss: 7.8588 - acc: 0.4995Epoch 00010: val_loss improved from 8.52814 to 8.50432, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.8737 - acc: 0.4985 - val_loss: 8.5043 - val_acc: 0.4168\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 7.2532 - acc: 0.550 - ETA: 3s - loss: 6.9818 - acc: 0.557 - ETA: 3s - loss: 7.2974 - acc: 0.541 - ETA: 2s - loss: 7.2627 - acc: 0.541 - ETA: 2s - loss: 7.4991 - acc: 0.527 - ETA: 2s - loss: 7.6433 - acc: 0.515 - ETA: 2s - loss: 7.7138 - acc: 0.508 - ETA: 2s - loss: 7.8006 - acc: 0.504 - ETA: 2s - loss: 7.8403 - acc: 0.503 - ETA: 2s - loss: 7.9016 - acc: 0.499 - ETA: 2s - loss: 7.8951 - acc: 0.499 - ETA: 2s - loss: 7.8934 - acc: 0.499 - ETA: 2s - loss: 7.8347 - acc: 0.502 - ETA: 2s - loss: 7.8716 - acc: 0.499 - ETA: 2s - loss: 7.8028 - acc: 0.503 - ETA: 2s - loss: 7.7847 - acc: 0.505 - ETA: 2s - loss: 7.8757 - acc: 0.500 - ETA: 1s - loss: 7.8864 - acc: 0.500 - ETA: 1s - loss: 7.8538 - acc: 0.501 - ETA: 1s - loss: 7.8369 - acc: 0.503 - ETA: 1s - loss: 7.8250 - acc: 0.504 - ETA: 1s - loss: 7.7755 - acc: 0.507 - ETA: 1s - loss: 7.8052 - acc: 0.505 - ETA: 1s - loss: 7.7237 - acc: 0.509 - ETA: 1s - loss: 7.8014 - acc: 0.504 - ETA: 1s - loss: 7.7964 - acc: 0.505 - ETA: 1s - loss: 7.8336 - acc: 0.503 - ETA: 1s - loss: 7.8378 - acc: 0.502 - ETA: 1s - loss: 7.8016 - acc: 0.505 - ETA: 1s - loss: 7.8259 - acc: 0.503 - ETA: 1s - loss: 7.8122 - acc: 0.504 - ETA: 1s - loss: 7.8266 - acc: 0.503 - ETA: 1s - loss: 7.8547 - acc: 0.501 - ETA: 1s - loss: 7.8914 - acc: 0.499 - ETA: 1s - loss: 7.8980 - acc: 0.498 - ETA: 1s - loss: 7.9066 - acc: 0.497 - ETA: 0s - loss: 7.8722 - acc: 0.500 - ETA: 0s - loss: 7.8738 - acc: 0.500 - ETA: 0s - loss: 7.8863 - acc: 0.499 - ETA: 0s - loss: 7.8636 - acc: 0.500 - ETA: 0s - loss: 7.8474 - acc: 0.501 - ETA: 0s - loss: 7.8617 - acc: 0.500 - ETA: 0s - loss: 7.8493 - acc: 0.501 - ETA: 0s - loss: 7.8417 - acc: 0.501 - ETA: 0s - loss: 7.8272 - acc: 0.502 - ETA: 0s - loss: 7.8287 - acc: 0.502 - ETA: 0s - loss: 7.8426 - acc: 0.501 - ETA: 0s - loss: 7.8256 - acc: 0.502 - ETA: 0s - loss: 7.8117 - acc: 0.503 - ETA: 0s - loss: 7.7925 - acc: 0.504 - ETA: 0s - loss: 7.7978 - acc: 0.504 - ETA: 0s - loss: 7.7867 - acc: 0.505 - ETA: 0s - loss: 7.7990 - acc: 0.504 - ETA: 0s - loss: 7.7733 - acc: 0.506 - ETA: 0s - loss: 7.7650 - acc: 0.506 - ETA: 0s - loss: 7.7795 - acc: 0.506 - ETA: 0s - loss: 7.7720 - acc: 0.5065Epoch 00011: val_loss improved from 8.50432 to 8.46542, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.7925 - acc: 0.5054 - val_loss: 8.4654 - val_acc: 0.4096\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6540/6680 [============================>.] - ETA: 2s - loss: 6.4495 - acc: 0.600 - ETA: 2s - loss: 7.8594 - acc: 0.512 - ETA: 2s - loss: 8.1359 - acc: 0.492 - ETA: 2s - loss: 7.3457 - acc: 0.535 - ETA: 2s - loss: 7.3331 - acc: 0.534 - ETA: 2s - loss: 7.5548 - acc: 0.518 - ETA: 2s - loss: 7.5526 - acc: 0.518 - ETA: 2s - loss: 7.6068 - acc: 0.515 - ETA: 2s - loss: 7.5039 - acc: 0.523 - ETA: 2s - loss: 7.4838 - acc: 0.525 - ETA: 2s - loss: 7.5669 - acc: 0.521 - ETA: 2s - loss: 7.4369 - acc: 0.529 - ETA: 2s - loss: 7.5020 - acc: 0.526 - ETA: 2s - loss: 7.5402 - acc: 0.523 - ETA: 2s - loss: 7.4798 - acc: 0.526 - ETA: 2s - loss: 7.5529 - acc: 0.522 - ETA: 2s - loss: 7.5918 - acc: 0.520 - ETA: 2s - loss: 7.6391 - acc: 0.517 - ETA: 2s - loss: 7.6866 - acc: 0.514 - ETA: 2s - loss: 7.6370 - acc: 0.517 - ETA: 2s - loss: 7.6614 - acc: 0.516 - ETA: 2s - loss: 7.6722 - acc: 0.515 - ETA: 2s - loss: 7.6665 - acc: 0.515 - ETA: 2s - loss: 7.6485 - acc: 0.516 - ETA: 1s - loss: 7.5695 - acc: 0.521 - ETA: 1s - loss: 7.5502 - acc: 0.523 - ETA: 1s - loss: 7.5639 - acc: 0.522 - ETA: 1s - loss: 7.6171 - acc: 0.518 - ETA: 1s - loss: 7.6392 - acc: 0.517 - ETA: 1s - loss: 7.6815 - acc: 0.515 - ETA: 1s - loss: 7.6905 - acc: 0.514 - ETA: 1s - loss: 7.6745 - acc: 0.515 - ETA: 1s - loss: 7.6859 - acc: 0.514 - ETA: 1s - loss: 7.6618 - acc: 0.515 - ETA: 1s - loss: 7.6856 - acc: 0.514 - ETA: 1s - loss: 7.6521 - acc: 0.516 - ETA: 1s - loss: 7.6623 - acc: 0.515 - ETA: 1s - loss: 7.6947 - acc: 0.514 - ETA: 1s - loss: 7.7066 - acc: 0.513 - ETA: 1s - loss: 7.7163 - acc: 0.513 - ETA: 1s - loss: 7.7202 - acc: 0.513 - ETA: 0s - loss: 7.7373 - acc: 0.512 - ETA: 0s - loss: 7.7484 - acc: 0.511 - ETA: 0s - loss: 7.7246 - acc: 0.512 - ETA: 0s - loss: 7.7433 - acc: 0.511 - ETA: 0s - loss: 7.7160 - acc: 0.513 - ETA: 0s - loss: 7.7529 - acc: 0.511 - ETA: 0s - loss: 7.7435 - acc: 0.511 - ETA: 0s - loss: 7.7257 - acc: 0.512 - ETA: 0s - loss: 7.7210 - acc: 0.512 - ETA: 0s - loss: 7.7143 - acc: 0.512 - ETA: 0s - loss: 7.7229 - acc: 0.512 - ETA: 0s - loss: 7.7514 - acc: 0.510 - ETA: 0s - loss: 7.7648 - acc: 0.509 - ETA: 0s - loss: 7.7822 - acc: 0.508 - ETA: 0s - loss: 7.7880 - acc: 0.508 - ETA: 0s - loss: 7.7783 - acc: 0.5093Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.7654 - acc: 0.5100 - val_loss: 8.4743 - val_acc: 0.4036\n",
      "Epoch 14/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 7.3460 - acc: 0.5286 - ETA: 2s - loss: 7.6405 - acc: 0.510 - ETA: 2s - loss: 8.0890 - acc: 0.487 - ETA: 2s - loss: 8.0152 - acc: 0.490 - ETA: 2s - loss: 7.8556 - acc: 0.498 - ETA: 2s - loss: 7.7358 - acc: 0.505 - ETA: 2s - loss: 7.6293 - acc: 0.513 - ETA: 2s - loss: 7.7857 - acc: 0.503 - ETA: 2s - loss: 7.9546 - acc: 0.493 - ETA: 2s - loss: 8.0355 - acc: 0.487 - ETA: 2s - loss: 8.0276 - acc: 0.486 - ETA: 2s - loss: 8.0238 - acc: 0.486 - ETA: 2s - loss: 8.0075 - acc: 0.487 - ETA: 2s - loss: 8.0244 - acc: 0.485 - ETA: 2s - loss: 8.0768 - acc: 0.482 - ETA: 1s - loss: 8.0705 - acc: 0.482 - ETA: 1s - loss: 7.9951 - acc: 0.486 - ETA: 1s - loss: 8.0326 - acc: 0.485 - ETA: 1s - loss: 7.9879 - acc: 0.487 - ETA: 1s - loss: 7.9851 - acc: 0.488 - ETA: 1s - loss: 7.9421 - acc: 0.490 - ETA: 1s - loss: 7.8868 - acc: 0.494 - ETA: 1s - loss: 7.9118 - acc: 0.493 - ETA: 1s - loss: 7.9199 - acc: 0.492 - ETA: 1s - loss: 7.8760 - acc: 0.495 - ETA: 1s - loss: 7.9038 - acc: 0.494 - ETA: 1s - loss: 7.8777 - acc: 0.495 - ETA: 1s - loss: 7.8453 - acc: 0.497 - ETA: 1s - loss: 7.8356 - acc: 0.498 - ETA: 1s - loss: 7.8693 - acc: 0.496 - ETA: 1s - loss: 7.8666 - acc: 0.497 - ETA: 1s - loss: 7.8427 - acc: 0.498 - ETA: 1s - loss: 7.8304 - acc: 0.499 - ETA: 1s - loss: 7.7840 - acc: 0.502 - ETA: 1s - loss: 7.7776 - acc: 0.503 - ETA: 0s - loss: 7.7484 - acc: 0.504 - ETA: 0s - loss: 7.7135 - acc: 0.506 - ETA: 0s - loss: 7.7423 - acc: 0.505 - ETA: 0s - loss: 7.7297 - acc: 0.505 - ETA: 0s - loss: 7.7079 - acc: 0.507 - ETA: 0s - loss: 7.6878 - acc: 0.508 - ETA: 0s - loss: 7.6600 - acc: 0.510 - ETA: 0s - loss: 7.6465 - acc: 0.511 - ETA: 0s - loss: 7.6526 - acc: 0.510 - ETA: 0s - loss: 7.6627 - acc: 0.510 - ETA: 0s - loss: 7.6488 - acc: 0.511 - ETA: 0s - loss: 7.6228 - acc: 0.513 - ETA: 0s - loss: 7.6111 - acc: 0.513 - ETA: 0s - loss: 7.6070 - acc: 0.514 - ETA: 0s - loss: 7.6190 - acc: 0.513 - ETA: 0s - loss: 7.6214 - acc: 0.513 - ETA: 0s - loss: 7.6466 - acc: 0.512 - ETA: 0s - loss: 7.6692 - acc: 0.5111Epoch 00013: val_loss improved from 8.46542 to 8.36962, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6757 - acc: 0.5108 - val_loss: 8.3696 - val_acc: 0.4168\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 8.1191 - acc: 0.450 - ETA: 3s - loss: 8.0702 - acc: 0.491 - ETA: 3s - loss: 7.9357 - acc: 0.500 - ETA: 3s - loss: 7.9247 - acc: 0.502 - ETA: 3s - loss: 7.8120 - acc: 0.509 - ETA: 3s - loss: 7.6424 - acc: 0.519 - ETA: 3s - loss: 7.4859 - acc: 0.530 - ETA: 2s - loss: 7.6784 - acc: 0.519 - ETA: 2s - loss: 7.6886 - acc: 0.518 - ETA: 2s - loss: 7.7069 - acc: 0.517 - ETA: 2s - loss: 7.7120 - acc: 0.516 - ETA: 2s - loss: 7.8228 - acc: 0.509 - ETA: 2s - loss: 7.7544 - acc: 0.513 - ETA: 2s - loss: 7.7470 - acc: 0.514 - ETA: 2s - loss: 7.6705 - acc: 0.518 - ETA: 2s - loss: 7.6413 - acc: 0.520 - ETA: 2s - loss: 7.5957 - acc: 0.521 - ETA: 2s - loss: 7.5690 - acc: 0.523 - ETA: 2s - loss: 7.4859 - acc: 0.529 - ETA: 2s - loss: 7.4043 - acc: 0.534 - ETA: 2s - loss: 7.4210 - acc: 0.533 - ETA: 2s - loss: 7.4082 - acc: 0.534 - ETA: 2s - loss: 7.4216 - acc: 0.533 - ETA: 1s - loss: 7.4345 - acc: 0.532 - ETA: 1s - loss: 7.3897 - acc: 0.535 - ETA: 1s - loss: 7.4044 - acc: 0.534 - ETA: 1s - loss: 7.3905 - acc: 0.534 - ETA: 1s - loss: 7.3873 - acc: 0.534 - ETA: 1s - loss: 7.3781 - acc: 0.534 - ETA: 1s - loss: 7.3924 - acc: 0.533 - ETA: 1s - loss: 7.4233 - acc: 0.531 - ETA: 1s - loss: 7.4619 - acc: 0.528 - ETA: 1s - loss: 7.4428 - acc: 0.529 - ETA: 1s - loss: 7.4667 - acc: 0.527 - ETA: 1s - loss: 7.4746 - acc: 0.526 - ETA: 1s - loss: 7.4823 - acc: 0.525 - ETA: 1s - loss: 7.4716 - acc: 0.526 - ETA: 1s - loss: 7.4985 - acc: 0.524 - ETA: 1s - loss: 7.4770 - acc: 0.526 - ETA: 1s - loss: 7.4648 - acc: 0.527 - ETA: 1s - loss: 7.5160 - acc: 0.524 - ETA: 0s - loss: 7.5405 - acc: 0.523 - ETA: 0s - loss: 7.5298 - acc: 0.523 - ETA: 0s - loss: 7.5401 - acc: 0.522 - ETA: 0s - loss: 7.5246 - acc: 0.523 - ETA: 0s - loss: 7.5507 - acc: 0.522 - ETA: 0s - loss: 7.5677 - acc: 0.521 - ETA: 0s - loss: 7.5770 - acc: 0.520 - ETA: 0s - loss: 7.5592 - acc: 0.521 - ETA: 0s - loss: 7.5814 - acc: 0.520 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6050 - acc: 0.519 - ETA: 0s - loss: 7.6259 - acc: 0.518 - ETA: 0s - loss: 7.6211 - acc: 0.518 - ETA: 0s - loss: 7.6039 - acc: 0.519 - ETA: 0s - loss: 7.6143 - acc: 0.518 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6215 - acc: 0.5186Epoch 00014: val_loss improved from 8.36962 to 8.29462, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6171 - acc: 0.5190 - val_loss: 8.2946 - val_acc: 0.4287\n",
      "Epoch 16/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 7.2635 - acc: 0.550 - ETA: 2s - loss: 7.8818 - acc: 0.500 - ETA: 2s - loss: 7.6538 - acc: 0.519 - ETA: 2s - loss: 7.7301 - acc: 0.513 - ETA: 2s - loss: 7.6270 - acc: 0.521 - ETA: 2s - loss: 7.8571 - acc: 0.506 - ETA: 2s - loss: 7.7768 - acc: 0.512 - ETA: 2s - loss: 7.7399 - acc: 0.515 - ETA: 2s - loss: 7.7312 - acc: 0.514 - ETA: 2s - loss: 7.6309 - acc: 0.521 - ETA: 2s - loss: 7.6504 - acc: 0.520 - ETA: 2s - loss: 7.5978 - acc: 0.521 - ETA: 2s - loss: 7.5918 - acc: 0.522 - ETA: 2s - loss: 7.5499 - acc: 0.524 - ETA: 2s - loss: 7.6451 - acc: 0.519 - ETA: 2s - loss: 7.7274 - acc: 0.514 - ETA: 2s - loss: 7.7723 - acc: 0.511 - ETA: 2s - loss: 7.7591 - acc: 0.512 - ETA: 2s - loss: 7.7080 - acc: 0.516 - ETA: 2s - loss: 7.7203 - acc: 0.515 - ETA: 1s - loss: 7.7586 - acc: 0.513 - ETA: 1s - loss: 7.7348 - acc: 0.514 - ETA: 1s - loss: 7.7444 - acc: 0.513 - ETA: 1s - loss: 7.7465 - acc: 0.513 - ETA: 1s - loss: 7.7508 - acc: 0.513 - ETA: 1s - loss: 7.7711 - acc: 0.511 - ETA: 1s - loss: 7.7875 - acc: 0.511 - ETA: 1s - loss: 7.7927 - acc: 0.510 - ETA: 1s - loss: 7.7901 - acc: 0.510 - ETA: 1s - loss: 7.8511 - acc: 0.507 - ETA: 1s - loss: 7.8544 - acc: 0.506 - ETA: 1s - loss: 7.8357 - acc: 0.507 - ETA: 1s - loss: 7.8195 - acc: 0.508 - ETA: 1s - loss: 7.7948 - acc: 0.510 - ETA: 1s - loss: 7.7749 - acc: 0.511 - ETA: 1s - loss: 7.7451 - acc: 0.513 - ETA: 1s - loss: 7.7392 - acc: 0.514 - ETA: 1s - loss: 7.7333 - acc: 0.514 - ETA: 0s - loss: 7.7100 - acc: 0.516 - ETA: 0s - loss: 7.6758 - acc: 0.518 - ETA: 0s - loss: 7.6740 - acc: 0.518 - ETA: 0s - loss: 7.6735 - acc: 0.518 - ETA: 0s - loss: 7.6920 - acc: 0.516 - ETA: 0s - loss: 7.6697 - acc: 0.518 - ETA: 0s - loss: 7.6695 - acc: 0.518 - ETA: 0s - loss: 7.6542 - acc: 0.519 - ETA: 0s - loss: 7.6551 - acc: 0.519 - ETA: 0s - loss: 7.6409 - acc: 0.520 - ETA: 0s - loss: 7.6396 - acc: 0.519 - ETA: 0s - loss: 7.6266 - acc: 0.520 - ETA: 0s - loss: 7.6219 - acc: 0.521 - ETA: 0s - loss: 7.6389 - acc: 0.520 - ETA: 0s - loss: 7.6367 - acc: 0.519 - ETA: 0s - loss: 7.6329 - acc: 0.520 - ETA: 0s - loss: 7.6234 - acc: 0.520 - ETA: 0s - loss: 7.6242 - acc: 0.5207Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.6061 - acc: 0.5217 - val_loss: 8.3678 - val_acc: 0.4180\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 7.2603 - acc: 0.550 - ETA: 3s - loss: 7.3919 - acc: 0.541 - ETA: 3s - loss: 7.2556 - acc: 0.550 - ETA: 3s - loss: 7.4446 - acc: 0.538 - ETA: 3s - loss: 7.7097 - acc: 0.521 - ETA: 2s - loss: 7.4213 - acc: 0.539 - ETA: 2s - loss: 7.6229 - acc: 0.527 - ETA: 2s - loss: 7.6053 - acc: 0.528 - ETA: 2s - loss: 7.5328 - acc: 0.530 - ETA: 2s - loss: 7.5453 - acc: 0.527 - ETA: 2s - loss: 7.4494 - acc: 0.532 - ETA: 2s - loss: 7.4969 - acc: 0.527 - ETA: 2s - loss: 7.4867 - acc: 0.528 - ETA: 2s - loss: 7.5227 - acc: 0.526 - ETA: 2s - loss: 7.4745 - acc: 0.530 - ETA: 2s - loss: 7.4288 - acc: 0.532 - ETA: 2s - loss: 7.4533 - acc: 0.529 - ETA: 2s - loss: 7.4586 - acc: 0.528 - ETA: 2s - loss: 7.4895 - acc: 0.526 - ETA: 1s - loss: 7.4453 - acc: 0.529 - ETA: 1s - loss: 7.3830 - acc: 0.533 - ETA: 1s - loss: 7.3776 - acc: 0.533 - ETA: 1s - loss: 7.3054 - acc: 0.538 - ETA: 1s - loss: 7.3171 - acc: 0.537 - ETA: 1s - loss: 7.3206 - acc: 0.537 - ETA: 1s - loss: 7.3628 - acc: 0.535 - ETA: 1s - loss: 7.3986 - acc: 0.532 - ETA: 1s - loss: 7.3544 - acc: 0.534 - ETA: 1s - loss: 7.3540 - acc: 0.534 - ETA: 1s - loss: 7.3563 - acc: 0.534 - ETA: 1s - loss: 7.3580 - acc: 0.533 - ETA: 1s - loss: 7.4031 - acc: 0.530 - ETA: 1s - loss: 7.4518 - acc: 0.527 - ETA: 1s - loss: 7.4404 - acc: 0.528 - ETA: 1s - loss: 7.4320 - acc: 0.528 - ETA: 1s - loss: 7.4322 - acc: 0.528 - ETA: 1s - loss: 7.4380 - acc: 0.527 - ETA: 1s - loss: 7.4436 - acc: 0.527 - ETA: 0s - loss: 7.4739 - acc: 0.525 - ETA: 0s - loss: 7.5068 - acc: 0.524 - ETA: 0s - loss: 7.4910 - acc: 0.525 - ETA: 0s - loss: 7.4882 - acc: 0.525 - ETA: 0s - loss: 7.4994 - acc: 0.524 - ETA: 0s - loss: 7.4936 - acc: 0.524 - ETA: 0s - loss: 7.4997 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4998 - acc: 0.524 - ETA: 0s - loss: 7.5014 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4958 - acc: 0.524 - ETA: 0s - loss: 7.4885 - acc: 0.524 - ETA: 0s - loss: 7.4867 - acc: 0.524 - ETA: 0s - loss: 7.5016 - acc: 0.523 - ETA: 0s - loss: 7.4819 - acc: 0.524 - ETA: 0s - loss: 7.4759 - acc: 0.525 - ETA: 0s - loss: 7.4787 - acc: 0.524 - ETA: 0s - loss: 7.4916 - acc: 0.5240Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.4977 - acc: 0.5237 - val_loss: 8.3458 - val_acc: 0.4180\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 7.0893 - acc: 0.550 - ETA: 2s - loss: 6.9683 - acc: 0.560 - ETA: 2s - loss: 6.9400 - acc: 0.561 - ETA: 2s - loss: 7.1489 - acc: 0.546 - ETA: 2s - loss: 7.1231 - acc: 0.548 - ETA: 2s - loss: 7.2517 - acc: 0.541 - ETA: 2s - loss: 7.2871 - acc: 0.538 - ETA: 2s - loss: 7.3151 - acc: 0.538 - ETA: 2s - loss: 7.2947 - acc: 0.540 - ETA: 2s - loss: 7.2529 - acc: 0.543 - ETA: 2s - loss: 7.2548 - acc: 0.543 - ETA: 2s - loss: 7.2656 - acc: 0.541 - ETA: 2s - loss: 7.2673 - acc: 0.540 - ETA: 2s - loss: 7.2426 - acc: 0.540 - ETA: 2s - loss: 7.2729 - acc: 0.538 - ETA: 2s - loss: 7.3583 - acc: 0.532 - ETA: 1s - loss: 7.2495 - acc: 0.537 - ETA: 1s - loss: 7.2414 - acc: 0.537 - ETA: 1s - loss: 7.2802 - acc: 0.534 - ETA: 1s - loss: 7.2668 - acc: 0.535 - ETA: 1s - loss: 7.3609 - acc: 0.529 - ETA: 1s - loss: 7.3710 - acc: 0.528 - ETA: 1s - loss: 7.4021 - acc: 0.526 - ETA: 1s - loss: 7.3636 - acc: 0.529 - ETA: 1s - loss: 7.3848 - acc: 0.527 - ETA: 1s - loss: 7.4198 - acc: 0.524 - ETA: 1s - loss: 7.3947 - acc: 0.526 - ETA: 1s - loss: 7.3927 - acc: 0.527 - ETA: 1s - loss: 7.3394 - acc: 0.530 - ETA: 1s - loss: 7.3555 - acc: 0.529 - ETA: 1s - loss: 7.3621 - acc: 0.528 - ETA: 1s - loss: 7.3426 - acc: 0.529 - ETA: 1s - loss: 7.3620 - acc: 0.527 - ETA: 1s - loss: 7.3478 - acc: 0.528 - ETA: 1s - loss: 7.2966 - acc: 0.531 - ETA: 1s - loss: 7.3112 - acc: 0.530 - ETA: 0s - loss: 7.3256 - acc: 0.529 - ETA: 0s - loss: 7.3207 - acc: 0.530 - ETA: 0s - loss: 7.2994 - acc: 0.531 - ETA: 0s - loss: 7.2911 - acc: 0.532 - ETA: 0s - loss: 7.2815 - acc: 0.533 - ETA: 0s - loss: 7.2866 - acc: 0.533 - ETA: 0s - loss: 7.2518 - acc: 0.535 - ETA: 0s - loss: 7.2667 - acc: 0.534 - ETA: 0s - loss: 7.2647 - acc: 0.534 - ETA: 0s - loss: 7.2813 - acc: 0.533 - ETA: 0s - loss: 7.2608 - acc: 0.534 - ETA: 0s - loss: 7.2649 - acc: 0.534 - ETA: 0s - loss: 7.2799 - acc: 0.533 - ETA: 0s - loss: 7.2986 - acc: 0.532 - ETA: 0s - loss: 7.3243 - acc: 0.530 - ETA: 0s - loss: 7.3390 - acc: 0.529 - ETA: 0s - loss: 7.3387 - acc: 0.529 - ETA: 0s - loss: 7.3281 - acc: 0.529 - ETA: 0s - loss: 7.3360 - acc: 0.5292Epoch 00017: val_loss improved from 8.29462 to 8.08558, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.3317 - acc: 0.5295 - val_loss: 8.0856 - val_acc: 0.4335\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 6.8885 - acc: 0.550 - ETA: 2s - loss: 7.2228 - acc: 0.535 - ETA: 2s - loss: 7.3910 - acc: 0.526 - ETA: 2s - loss: 7.4907 - acc: 0.521 - ETA: 2s - loss: 7.3426 - acc: 0.532 - ETA: 2s - loss: 7.3179 - acc: 0.532 - ETA: 2s - loss: 7.2918 - acc: 0.533 - ETA: 2s - loss: 7.3430 - acc: 0.531 - ETA: 2s - loss: 7.4151 - acc: 0.527 - ETA: 2s - loss: 7.3154 - acc: 0.532 - ETA: 2s - loss: 7.2280 - acc: 0.537 - ETA: 2s - loss: 7.2360 - acc: 0.537 - ETA: 2s - loss: 7.2641 - acc: 0.536 - ETA: 2s - loss: 7.2337 - acc: 0.538 - ETA: 2s - loss: 7.2446 - acc: 0.536 - ETA: 2s - loss: 7.2642 - acc: 0.535 - ETA: 2s - loss: 7.2639 - acc: 0.535 - ETA: 2s - loss: 7.2385 - acc: 0.537 - ETA: 1s - loss: 7.2326 - acc: 0.538 - ETA: 1s - loss: 7.2576 - acc: 0.537 - ETA: 1s - loss: 7.2939 - acc: 0.533 - ETA: 1s - loss: 7.2757 - acc: 0.533 - ETA: 1s - loss: 7.3289 - acc: 0.529 - ETA: 1s - loss: 7.2758 - acc: 0.532 - ETA: 1s - loss: 7.2195 - acc: 0.536 - ETA: 1s - loss: 7.1786 - acc: 0.539 - ETA: 1s - loss: 7.1907 - acc: 0.538 - ETA: 1s - loss: 7.1671 - acc: 0.540 - ETA: 1s - loss: 7.1686 - acc: 0.540 - ETA: 1s - loss: 7.1838 - acc: 0.539 - ETA: 1s - loss: 7.1874 - acc: 0.539 - ETA: 1s - loss: 7.1951 - acc: 0.539 - ETA: 1s - loss: 7.2130 - acc: 0.537 - ETA: 1s - loss: 7.1998 - acc: 0.538 - ETA: 1s - loss: 7.2027 - acc: 0.538 - ETA: 1s - loss: 7.2407 - acc: 0.535 - ETA: 1s - loss: 7.2058 - acc: 0.537 - ETA: 1s - loss: 7.2381 - acc: 0.535 - ETA: 0s - loss: 7.2452 - acc: 0.535 - ETA: 0s - loss: 7.2500 - acc: 0.535 - ETA: 0s - loss: 7.2371 - acc: 0.536 - ETA: 0s - loss: 7.2252 - acc: 0.536 - ETA: 0s - loss: 7.2330 - acc: 0.536 - ETA: 0s - loss: 7.2559 - acc: 0.535 - ETA: 0s - loss: 7.2780 - acc: 0.534 - ETA: 0s - loss: 7.2628 - acc: 0.535 - ETA: 0s - loss: 7.2629 - acc: 0.535 - ETA: 0s - loss: 7.2444 - acc: 0.536 - ETA: 0s - loss: 7.2302 - acc: 0.537 - ETA: 0s - loss: 7.2377 - acc: 0.537 - ETA: 0s - loss: 7.2305 - acc: 0.537 - ETA: 0s - loss: 7.2353 - acc: 0.537 - ETA: 0s - loss: 7.2317 - acc: 0.537 - ETA: 0s - loss: 7.2283 - acc: 0.537 - ETA: 0s - loss: 7.2171 - acc: 0.538 - ETA: 0s - loss: 7.2086 - acc: 0.5388Epoch 00018: val_loss improved from 8.08558 to 8.07654, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.2070 - acc: 0.5388 - val_loss: 8.0765 - val_acc: 0.4299\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 8.0597 - acc: 0.500 - ETA: 2s - loss: 8.6445 - acc: 0.450 - ETA: 2s - loss: 7.7636 - acc: 0.510 - ETA: 2s - loss: 7.4556 - acc: 0.530 - ETA: 2s - loss: 7.4778 - acc: 0.528 - ETA: 2s - loss: 7.4375 - acc: 0.531 - ETA: 2s - loss: 7.4656 - acc: 0.530 - ETA: 2s - loss: 7.5119 - acc: 0.527 - ETA: 2s - loss: 7.3846 - acc: 0.536 - ETA: 2s - loss: 7.5269 - acc: 0.525 - ETA: 2s - loss: 7.4774 - acc: 0.528 - ETA: 2s - loss: 7.4726 - acc: 0.528 - ETA: 2s - loss: 7.4169 - acc: 0.531 - ETA: 2s - loss: 7.3852 - acc: 0.533 - ETA: 2s - loss: 7.3618 - acc: 0.534 - ETA: 2s - loss: 7.3575 - acc: 0.534 - ETA: 2s - loss: 7.3638 - acc: 0.533 - ETA: 1s - loss: 7.3835 - acc: 0.531 - ETA: 1s - loss: 7.3778 - acc: 0.531 - ETA: 1s - loss: 7.3376 - acc: 0.534 - ETA: 1s - loss: 7.3365 - acc: 0.534 - ETA: 1s - loss: 7.3058 - acc: 0.535 - ETA: 1s - loss: 7.3157 - acc: 0.535 - ETA: 1s - loss: 7.2926 - acc: 0.536 - ETA: 1s - loss: 7.2798 - acc: 0.538 - ETA: 1s - loss: 7.2591 - acc: 0.539 - ETA: 1s - loss: 7.2075 - acc: 0.542 - ETA: 1s - loss: 7.2032 - acc: 0.542 - ETA: 1s - loss: 7.1955 - acc: 0.543 - ETA: 1s - loss: 7.1500 - acc: 0.545 - ETA: 1s - loss: 7.1603 - acc: 0.544 - ETA: 1s - loss: 7.1489 - acc: 0.545 - ETA: 1s - loss: 7.1580 - acc: 0.544 - ETA: 1s - loss: 7.1693 - acc: 0.544 - ETA: 1s - loss: 7.1396 - acc: 0.546 - ETA: 1s - loss: 7.1629 - acc: 0.545 - ETA: 1s - loss: 7.1425 - acc: 0.546 - ETA: 0s - loss: 7.1432 - acc: 0.545 - ETA: 0s - loss: 7.1497 - acc: 0.545 - ETA: 0s - loss: 7.1651 - acc: 0.544 - ETA: 0s - loss: 7.1317 - acc: 0.546 - ETA: 0s - loss: 7.1152 - acc: 0.547 - ETA: 0s - loss: 7.1272 - acc: 0.546 - ETA: 0s - loss: 7.1364 - acc: 0.546 - ETA: 0s - loss: 7.1539 - acc: 0.545 - ETA: 0s - loss: 7.1809 - acc: 0.543 - ETA: 0s - loss: 7.1697 - acc: 0.544 - ETA: 0s - loss: 7.1756 - acc: 0.543 - ETA: 0s - loss: 7.1549 - acc: 0.545 - ETA: 0s - loss: 7.1626 - acc: 0.544 - ETA: 0s - loss: 7.1839 - acc: 0.543 - ETA: 0s - loss: 7.1940 - acc: 0.542 - ETA: 0s - loss: 7.1900 - acc: 0.542 - ETA: 0s - loss: 7.1768 - acc: 0.543 - ETA: 0s - loss: 7.1672 - acc: 0.543 - ETA: 0s - loss: 7.1493 - acc: 0.5442Epoch 00019: val_loss improved from 8.07654 to 8.01932, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.1519 - acc: 0.5442 - val_loss: 8.0193 - val_acc: 0.4407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b9f5bff98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.5311%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# transfer model search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def with_bottleneck(network):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_VGG16 = bottleneck_features['train']\n",
    "    valid_VGG16 = bottleneck_features['valid']\n",
    "    test_VGG16 = bottleneck_features['test']\n",
    "\n",
    "    VGG16_model = Sequential()\n",
    "    VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "    VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "    VGG16_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(network)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    VGG16_model.fit(train_VGG16, train_targets, \n",
    "              validation_data=(valid_VGG16, valid_targets),\n",
    "              epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    VGG16_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "    return (test_accuracy, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 119s - loss: 15.3378 - acc: 0.0000e+ - ETA: 22s - loss: 14.8794 - acc: 0.0083     - ETA: 12s - loss: 14.8448 - acc: 0.008 - ETA: 9s - loss: 14.8060 - acc: 0.013 - ETA: 7s - loss: 14.7613 - acc: 0.01 - ETA: 6s - loss: 14.5223 - acc: 0.02 - ETA: 5s - loss: 14.4560 - acc: 0.02 - ETA: 5s - loss: 14.3702 - acc: 0.02 - ETA: 4s - loss: 14.2390 - acc: 0.03 - ETA: 4s - loss: 14.1351 - acc: 0.03 - ETA: 4s - loss: 14.0882 - acc: 0.03 - ETA: 3s - loss: 14.1000 - acc: 0.03 - ETA: 3s - loss: 14.0135 - acc: 0.04 - ETA: 3s - loss: 13.9707 - acc: 0.04 - ETA: 3s - loss: 13.9578 - acc: 0.04 - ETA: 3s - loss: 13.8934 - acc: 0.04 - ETA: 2s - loss: 13.8196 - acc: 0.04 - ETA: 2s - loss: 13.7468 - acc: 0.05 - ETA: 2s - loss: 13.7028 - acc: 0.05 - ETA: 2s - loss: 13.6698 - acc: 0.05 - ETA: 2s - loss: 13.5930 - acc: 0.05 - ETA: 2s - loss: 13.5048 - acc: 0.06 - ETA: 2s - loss: 13.4538 - acc: 0.06 - ETA: 2s - loss: 13.3880 - acc: 0.06 - ETA: 1s - loss: 13.3324 - acc: 0.07 - ETA: 1s - loss: 13.2950 - acc: 0.07 - ETA: 1s - loss: 13.2452 - acc: 0.07 - ETA: 1s - loss: 13.1816 - acc: 0.08 - ETA: 1s - loss: 13.1186 - acc: 0.08 - ETA: 1s - loss: 13.0426 - acc: 0.08 - ETA: 1s - loss: 13.0197 - acc: 0.08 - ETA: 1s - loss: 12.9883 - acc: 0.09 - ETA: 1s - loss: 12.9821 - acc: 0.09 - ETA: 1s - loss: 12.9661 - acc: 0.09 - ETA: 1s - loss: 12.9151 - acc: 0.09 - ETA: 1s - loss: 12.8892 - acc: 0.09 - ETA: 1s - loss: 12.8532 - acc: 0.09 - ETA: 1s - loss: 12.8240 - acc: 0.09 - ETA: 0s - loss: 12.7832 - acc: 0.10 - ETA: 0s - loss: 12.7569 - acc: 0.10 - ETA: 0s - loss: 12.7169 - acc: 0.10 - ETA: 0s - loss: 12.6837 - acc: 0.10 - ETA: 0s - loss: 12.6612 - acc: 0.10 - ETA: 0s - loss: 12.6482 - acc: 0.11 - ETA: 0s - loss: 12.6152 - acc: 0.11 - ETA: 0s - loss: 12.5806 - acc: 0.11 - ETA: 0s - loss: 12.5312 - acc: 0.11 - ETA: 0s - loss: 12.5114 - acc: 0.11 - ETA: 0s - loss: 12.4869 - acc: 0.12 - ETA: 0s - loss: 12.4891 - acc: 0.12 - ETA: 0s - loss: 12.4839 - acc: 0.12 - ETA: 0s - loss: 12.4433 - acc: 0.12 - ETA: 0s - loss: 12.4455 - acc: 0.12 - ETA: 0s - loss: 12.4001 - acc: 0.1264Epoch 00000: val_loss improved from inf to 11.20366, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 12.3916 - acc: 0.1268 - val_loss: 11.2037 - val_acc: 0.1880\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 12.6304 - acc: 0.05 - ETA: 2s - loss: 11.1897 - acc: 0.19 - ETA: 3s - loss: 10.7371 - acc: 0.21 - ETA: 3s - loss: 11.4056 - acc: 0.18 - ETA: 2s - loss: 11.1842 - acc: 0.20 - ETA: 2s - loss: 11.1154 - acc: 0.21 - ETA: 2s - loss: 11.1633 - acc: 0.21 - ETA: 2s - loss: 11.1499 - acc: 0.21 - ETA: 2s - loss: 11.1169 - acc: 0.22 - ETA: 2s - loss: 10.9961 - acc: 0.22 - ETA: 2s - loss: 10.8755 - acc: 0.23 - ETA: 2s - loss: 10.7873 - acc: 0.24 - ETA: 2s - loss: 10.7824 - acc: 0.24 - ETA: 2s - loss: 10.7082 - acc: 0.24 - ETA: 2s - loss: 10.6844 - acc: 0.24 - ETA: 2s - loss: 10.7100 - acc: 0.24 - ETA: 2s - loss: 10.6580 - acc: 0.24 - ETA: 2s - loss: 10.6291 - acc: 0.25 - ETA: 1s - loss: 10.6975 - acc: 0.24 - ETA: 1s - loss: 10.7312 - acc: 0.24 - ETA: 1s - loss: 10.7471 - acc: 0.24 - ETA: 1s - loss: 10.6925 - acc: 0.25 - ETA: 1s - loss: 10.6359 - acc: 0.25 - ETA: 1s - loss: 10.6257 - acc: 0.25 - ETA: 1s - loss: 10.6420 - acc: 0.25 - ETA: 1s - loss: 10.6545 - acc: 0.25 - ETA: 1s - loss: 10.6545 - acc: 0.25 - ETA: 1s - loss: 10.6358 - acc: 0.25 - ETA: 1s - loss: 10.6749 - acc: 0.25 - ETA: 1s - loss: 10.6797 - acc: 0.25 - ETA: 1s - loss: 10.6594 - acc: 0.25 - ETA: 1s - loss: 10.6743 - acc: 0.25 - ETA: 1s - loss: 10.6683 - acc: 0.25 - ETA: 1s - loss: 10.6808 - acc: 0.25 - ETA: 1s - loss: 10.6933 - acc: 0.25 - ETA: 1s - loss: 10.7075 - acc: 0.25 - ETA: 0s - loss: 10.6865 - acc: 0.25 - ETA: 0s - loss: 10.6908 - acc: 0.25 - ETA: 0s - loss: 10.7035 - acc: 0.25 - ETA: 0s - loss: 10.6864 - acc: 0.25 - ETA: 0s - loss: 10.7060 - acc: 0.25 - ETA: 0s - loss: 10.7131 - acc: 0.25 - ETA: 0s - loss: 10.7153 - acc: 0.25 - ETA: 0s - loss: 10.7105 - acc: 0.25 - ETA: 0s - loss: 10.7011 - acc: 0.25 - ETA: 0s - loss: 10.6917 - acc: 0.25 - ETA: 0s - loss: 10.6854 - acc: 0.26 - ETA: 0s - loss: 10.6827 - acc: 0.26 - ETA: 0s - loss: 10.6839 - acc: 0.26 - ETA: 0s - loss: 10.6830 - acc: 0.26 - ETA: 0s - loss: 10.7021 - acc: 0.25 - ETA: 0s - loss: 10.6870 - acc: 0.26 - ETA: 0s - loss: 10.6843 - acc: 0.26 - ETA: 0s - loss: 10.6635 - acc: 0.26 - ETA: 0s - loss: 10.6646 - acc: 0.26 - ETA: 0s - loss: 10.6620 - acc: 0.2639Epoch 00001: val_loss improved from 11.20366 to 10.60583, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.6619 - acc: 0.2638 - val_loss: 10.6058 - val_acc: 0.2575\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 9.9278 - acc: 0.300 - ETA: 3s - loss: 9.8332 - acc: 0.342 - ETA: 2s - loss: 9.5782 - acc: 0.353 - ETA: 2s - loss: 9.7026 - acc: 0.337 - ETA: 2s - loss: 9.9625 - acc: 0.326 - ETA: 2s - loss: 10.1623 - acc: 0.31 - ETA: 2s - loss: 10.2761 - acc: 0.30 - ETA: 2s - loss: 10.3732 - acc: 0.30 - ETA: 2s - loss: 10.3296 - acc: 0.30 - ETA: 2s - loss: 10.3033 - acc: 0.30 - ETA: 2s - loss: 10.2733 - acc: 0.30 - ETA: 2s - loss: 10.3185 - acc: 0.30 - ETA: 2s - loss: 10.2494 - acc: 0.31 - ETA: 2s - loss: 10.2330 - acc: 0.31 - ETA: 2s - loss: 10.2637 - acc: 0.31 - ETA: 2s - loss: 10.2983 - acc: 0.31 - ETA: 2s - loss: 10.3812 - acc: 0.30 - ETA: 2s - loss: 10.3730 - acc: 0.30 - ETA: 1s - loss: 10.3610 - acc: 0.30 - ETA: 1s - loss: 10.3566 - acc: 0.30 - ETA: 1s - loss: 10.3887 - acc: 0.30 - ETA: 1s - loss: 10.3422 - acc: 0.30 - ETA: 1s - loss: 10.3461 - acc: 0.30 - ETA: 1s - loss: 10.3165 - acc: 0.30 - ETA: 1s - loss: 10.3125 - acc: 0.30 - ETA: 1s - loss: 10.3254 - acc: 0.30 - ETA: 1s - loss: 10.3242 - acc: 0.30 - ETA: 1s - loss: 10.3000 - acc: 0.31 - ETA: 1s - loss: 10.3263 - acc: 0.30 - ETA: 1s - loss: 10.3142 - acc: 0.30 - ETA: 1s - loss: 10.3370 - acc: 0.30 - ETA: 1s - loss: 10.3661 - acc: 0.30 - ETA: 1s - loss: 10.3747 - acc: 0.30 - ETA: 1s - loss: 10.4048 - acc: 0.30 - ETA: 1s - loss: 10.4185 - acc: 0.30 - ETA: 1s - loss: 10.4025 - acc: 0.30 - ETA: 1s - loss: 10.3829 - acc: 0.30 - ETA: 0s - loss: 10.3721 - acc: 0.30 - ETA: 0s - loss: 10.3585 - acc: 0.30 - ETA: 0s - loss: 10.3217 - acc: 0.30 - ETA: 0s - loss: 10.3444 - acc: 0.30 - ETA: 0s - loss: 10.3524 - acc: 0.30 - ETA: 0s - loss: 10.3569 - acc: 0.30 - ETA: 0s - loss: 10.3663 - acc: 0.30 - ETA: 0s - loss: 10.3660 - acc: 0.30 - ETA: 0s - loss: 10.3768 - acc: 0.30 - ETA: 0s - loss: 10.3806 - acc: 0.30 - ETA: 0s - loss: 10.4042 - acc: 0.30 - ETA: 0s - loss: 10.3798 - acc: 0.30 - ETA: 0s - loss: 10.3903 - acc: 0.30 - ETA: 0s - loss: 10.3871 - acc: 0.30 - ETA: 0s - loss: 10.3606 - acc: 0.30 - ETA: 0s - loss: 10.3563 - acc: 0.30 - ETA: 0s - loss: 10.3461 - acc: 0.31 - ETA: 0s - loss: 10.3243 - acc: 0.31 - ETA: 0s - loss: 10.3138 - acc: 0.3130Epoch 00002: val_loss improved from 10.60583 to 10.55068, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.3108 - acc: 0.3130 - val_loss: 10.5507 - val_acc: 0.2790\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 11.2897 - acc: 0.30 - ETA: 2s - loss: 10.4424 - acc: 0.31 - ETA: 2s - loss: 10.3723 - acc: 0.31 - ETA: 2s - loss: 10.3209 - acc: 0.32 - ETA: 2s - loss: 10.1619 - acc: 0.33 - ETA: 2s - loss: 10.2264 - acc: 0.32 - ETA: 2s - loss: 10.1690 - acc: 0.33 - ETA: 2s - loss: 10.2242 - acc: 0.32 - ETA: 2s - loss: 10.2998 - acc: 0.32 - ETA: 2s - loss: 10.4210 - acc: 0.31 - ETA: 2s - loss: 10.4479 - acc: 0.31 - ETA: 2s - loss: 10.4183 - acc: 0.31 - ETA: 2s - loss: 10.4434 - acc: 0.31 - ETA: 2s - loss: 10.3837 - acc: 0.31 - ETA: 2s - loss: 10.2909 - acc: 0.32 - ETA: 2s - loss: 10.2704 - acc: 0.32 - ETA: 2s - loss: 10.2554 - acc: 0.32 - ETA: 2s - loss: 10.2601 - acc: 0.32 - ETA: 2s - loss: 10.2778 - acc: 0.32 - ETA: 2s - loss: 10.3072 - acc: 0.32 - ETA: 1s - loss: 10.3053 - acc: 0.32 - ETA: 1s - loss: 10.2451 - acc: 0.32 - ETA: 1s - loss: 10.1917 - acc: 0.33 - ETA: 1s - loss: 10.2108 - acc: 0.33 - ETA: 1s - loss: 10.1918 - acc: 0.33 - ETA: 1s - loss: 10.0906 - acc: 0.33 - ETA: 1s - loss: 10.0931 - acc: 0.33 - ETA: 1s - loss: 10.1006 - acc: 0.33 - ETA: 1s - loss: 10.0801 - acc: 0.33 - ETA: 1s - loss: 10.0279 - acc: 0.34 - ETA: 1s - loss: 10.0680 - acc: 0.33 - ETA: 1s - loss: 10.0951 - acc: 0.33 - ETA: 1s - loss: 10.1069 - acc: 0.33 - ETA: 1s - loss: 10.1102 - acc: 0.33 - ETA: 1s - loss: 10.1094 - acc: 0.33 - ETA: 1s - loss: 10.1253 - acc: 0.33 - ETA: 1s - loss: 10.1460 - acc: 0.33 - ETA: 0s - loss: 10.1751 - acc: 0.33 - ETA: 0s - loss: 10.1822 - acc: 0.33 - ETA: 0s - loss: 10.1569 - acc: 0.33 - ETA: 0s - loss: 10.1238 - acc: 0.33 - ETA: 0s - loss: 10.1362 - acc: 0.33 - ETA: 0s - loss: 10.1135 - acc: 0.33 - ETA: 0s - loss: 10.1101 - acc: 0.33 - ETA: 0s - loss: 10.0935 - acc: 0.33 - ETA: 0s - loss: 10.1049 - acc: 0.33 - ETA: 0s - loss: 10.1110 - acc: 0.33 - ETA: 0s - loss: 10.1079 - acc: 0.33 - ETA: 0s - loss: 10.0990 - acc: 0.33 - ETA: 0s - loss: 10.0894 - acc: 0.33 - ETA: 0s - loss: 10.0872 - acc: 0.33 - ETA: 0s - loss: 10.0702 - acc: 0.33 - ETA: 0s - loss: 10.0827 - acc: 0.33 - ETA: 0s - loss: 10.0901 - acc: 0.33 - ETA: 0s - loss: 10.0625 - acc: 0.3389Epoch 00003: val_loss improved from 10.55068 to 10.24227, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.0572 - acc: 0.3392 - val_loss: 10.2423 - val_acc: 0.2946\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 9.6709 - acc: 0.400 - ETA: 2s - loss: 9.5695 - acc: 0.371 - ETA: 3s - loss: 9.6627 - acc: 0.366 - ETA: 3s - loss: 9.7529 - acc: 0.361 - ETA: 2s - loss: 9.4875 - acc: 0.383 - ETA: 2s - loss: 9.5860 - acc: 0.381 - ETA: 2s - loss: 9.5463 - acc: 0.383 - ETA: 2s - loss: 9.4656 - acc: 0.384 - ETA: 2s - loss: 9.5628 - acc: 0.376 - ETA: 2s - loss: 9.4249 - acc: 0.385 - ETA: 2s - loss: 9.6042 - acc: 0.374 - ETA: 2s - loss: 9.7478 - acc: 0.364 - ETA: 2s - loss: 9.6892 - acc: 0.369 - ETA: 2s - loss: 9.7913 - acc: 0.363 - ETA: 2s - loss: 9.7640 - acc: 0.365 - ETA: 2s - loss: 9.7146 - acc: 0.369 - ETA: 2s - loss: 9.7380 - acc: 0.366 - ETA: 2s - loss: 9.6818 - acc: 0.369 - ETA: 2s - loss: 9.6397 - acc: 0.372 - ETA: 2s - loss: 9.7075 - acc: 0.369 - ETA: 1s - loss: 9.7295 - acc: 0.368 - ETA: 1s - loss: 9.7461 - acc: 0.367 - ETA: 1s - loss: 9.7559 - acc: 0.367 - ETA: 1s - loss: 9.8179 - acc: 0.364 - ETA: 1s - loss: 9.8121 - acc: 0.364 - ETA: 1s - loss: 9.7812 - acc: 0.365 - ETA: 1s - loss: 9.7889 - acc: 0.364 - ETA: 1s - loss: 9.7882 - acc: 0.364 - ETA: 1s - loss: 9.7894 - acc: 0.365 - ETA: 1s - loss: 9.7647 - acc: 0.366 - ETA: 1s - loss: 9.7775 - acc: 0.365 - ETA: 1s - loss: 9.7815 - acc: 0.365 - ETA: 1s - loss: 9.7634 - acc: 0.366 - ETA: 1s - loss: 9.7957 - acc: 0.364 - ETA: 1s - loss: 9.8072 - acc: 0.364 - ETA: 1s - loss: 9.8121 - acc: 0.362 - ETA: 1s - loss: 9.8241 - acc: 0.361 - ETA: 0s - loss: 9.8158 - acc: 0.362 - ETA: 0s - loss: 9.8351 - acc: 0.361 - ETA: 0s - loss: 9.8442 - acc: 0.360 - ETA: 0s - loss: 9.8467 - acc: 0.360 - ETA: 0s - loss: 9.8426 - acc: 0.360 - ETA: 0s - loss: 9.8311 - acc: 0.360 - ETA: 0s - loss: 9.8317 - acc: 0.361 - ETA: 0s - loss: 9.8293 - acc: 0.361 - ETA: 0s - loss: 9.8140 - acc: 0.362 - ETA: 0s - loss: 9.8209 - acc: 0.362 - ETA: 0s - loss: 9.8090 - acc: 0.362 - ETA: 0s - loss: 9.7979 - acc: 0.362 - ETA: 0s - loss: 9.7925 - acc: 0.362 - ETA: 0s - loss: 9.7922 - acc: 0.361 - ETA: 0s - loss: 9.8030 - acc: 0.360 - ETA: 0s - loss: 9.8245 - acc: 0.359 - ETA: 0s - loss: 9.8285 - acc: 0.358 - ETA: 0s - loss: 9.8005 - acc: 0.360 - ETA: 0s - loss: 9.8162 - acc: 0.3591Epoch 00004: val_loss improved from 10.24227 to 10.01084, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.8157 - acc: 0.3590 - val_loss: 10.0108 - val_acc: 0.3078\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.8974 - acc: 0.350 - ETA: 2s - loss: 9.3096 - acc: 0.381 - ETA: 2s - loss: 9.6258 - acc: 0.373 - ETA: 2s - loss: 9.7100 - acc: 0.365 - ETA: 2s - loss: 9.5335 - acc: 0.373 - ETA: 2s - loss: 9.5676 - acc: 0.367 - ETA: 2s - loss: 9.6860 - acc: 0.363 - ETA: 2s - loss: 9.4614 - acc: 0.378 - ETA: 2s - loss: 9.5849 - acc: 0.374 - ETA: 2s - loss: 9.5137 - acc: 0.380 - ETA: 2s - loss: 9.5565 - acc: 0.379 - ETA: 2s - loss: 9.5174 - acc: 0.381 - ETA: 2s - loss: 9.4171 - acc: 0.388 - ETA: 2s - loss: 9.4501 - acc: 0.385 - ETA: 2s - loss: 9.4404 - acc: 0.387 - ETA: 2s - loss: 9.4612 - acc: 0.385 - ETA: 2s - loss: 9.4803 - acc: 0.384 - ETA: 1s - loss: 9.4852 - acc: 0.384 - ETA: 1s - loss: 9.4500 - acc: 0.386 - ETA: 1s - loss: 9.4613 - acc: 0.386 - ETA: 1s - loss: 9.5312 - acc: 0.382 - ETA: 1s - loss: 9.5936 - acc: 0.378 - ETA: 1s - loss: 9.5697 - acc: 0.380 - ETA: 1s - loss: 9.5948 - acc: 0.378 - ETA: 1s - loss: 9.5634 - acc: 0.380 - ETA: 1s - loss: 9.5650 - acc: 0.380 - ETA: 1s - loss: 9.5769 - acc: 0.378 - ETA: 1s - loss: 9.5647 - acc: 0.378 - ETA: 1s - loss: 9.6021 - acc: 0.376 - ETA: 1s - loss: 9.5811 - acc: 0.377 - ETA: 1s - loss: 9.5880 - acc: 0.376 - ETA: 1s - loss: 9.5478 - acc: 0.379 - ETA: 1s - loss: 9.5425 - acc: 0.380 - ETA: 1s - loss: 9.5057 - acc: 0.381 - ETA: 1s - loss: 9.5174 - acc: 0.382 - ETA: 1s - loss: 9.5119 - acc: 0.382 - ETA: 0s - loss: 9.5188 - acc: 0.382 - ETA: 0s - loss: 9.5260 - acc: 0.382 - ETA: 0s - loss: 9.5177 - acc: 0.382 - ETA: 0s - loss: 9.5032 - acc: 0.383 - ETA: 0s - loss: 9.4906 - acc: 0.384 - ETA: 0s - loss: 9.4690 - acc: 0.385 - ETA: 0s - loss: 9.4791 - acc: 0.385 - ETA: 0s - loss: 9.4787 - acc: 0.385 - ETA: 0s - loss: 9.4715 - acc: 0.385 - ETA: 0s - loss: 9.4508 - acc: 0.386 - ETA: 0s - loss: 9.4302 - acc: 0.387 - ETA: 0s - loss: 9.4396 - acc: 0.386 - ETA: 0s - loss: 9.4447 - acc: 0.386 - ETA: 0s - loss: 9.4679 - acc: 0.385 - ETA: 0s - loss: 9.4851 - acc: 0.385 - ETA: 0s - loss: 9.5050 - acc: 0.383 - ETA: 0s - loss: 9.5252 - acc: 0.382 - ETA: 0s - loss: 9.5314 - acc: 0.381 - ETA: 0s - loss: 9.5270 - acc: 0.3820Epoch 00005: val_loss improved from 10.01084 to 9.90151, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.5435 - acc: 0.3810 - val_loss: 9.9015 - val_acc: 0.3198\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 2s - loss: 9.6719 - acc: 0.400 - ETA: 2s - loss: 9.2036 - acc: 0.425 - ETA: 2s - loss: 9.6489 - acc: 0.396 - ETA: 2s - loss: 9.5534 - acc: 0.395 - ETA: 2s - loss: 9.3719 - acc: 0.405 - ETA: 2s - loss: 9.4437 - acc: 0.400 - ETA: 2s - loss: 9.5243 - acc: 0.396 - ETA: 2s - loss: 9.4389 - acc: 0.401 - ETA: 2s - loss: 9.3461 - acc: 0.405 - ETA: 2s - loss: 9.3702 - acc: 0.403 - ETA: 2s - loss: 9.4574 - acc: 0.399 - ETA: 2s - loss: 9.4674 - acc: 0.398 - ETA: 2s - loss: 9.5198 - acc: 0.395 - ETA: 2s - loss: 9.4318 - acc: 0.400 - ETA: 2s - loss: 9.4213 - acc: 0.400 - ETA: 2s - loss: 9.4033 - acc: 0.401 - ETA: 1s - loss: 9.4310 - acc: 0.399 - ETA: 1s - loss: 9.3726 - acc: 0.403 - ETA: 1s - loss: 9.3732 - acc: 0.402 - ETA: 1s - loss: 9.3981 - acc: 0.401 - ETA: 1s - loss: 9.3757 - acc: 0.402 - ETA: 1s - loss: 9.4459 - acc: 0.397 - ETA: 1s - loss: 9.4671 - acc: 0.395 - ETA: 1s - loss: 9.4657 - acc: 0.394 - ETA: 1s - loss: 9.4800 - acc: 0.393 - ETA: 1s - loss: 9.4894 - acc: 0.392 - ETA: 1s - loss: 9.4578 - acc: 0.395 - ETA: 1s - loss: 9.4667 - acc: 0.395 - ETA: 1s - loss: 9.4787 - acc: 0.394 - ETA: 1s - loss: 9.4830 - acc: 0.394 - ETA: 1s - loss: 9.4850 - acc: 0.394 - ETA: 1s - loss: 9.5133 - acc: 0.393 - ETA: 1s - loss: 9.5070 - acc: 0.393 - ETA: 1s - loss: 9.4932 - acc: 0.394 - ETA: 1s - loss: 9.4904 - acc: 0.394 - ETA: 0s - loss: 9.4956 - acc: 0.393 - ETA: 0s - loss: 9.5125 - acc: 0.392 - ETA: 0s - loss: 9.5172 - acc: 0.392 - ETA: 0s - loss: 9.4943 - acc: 0.394 - ETA: 0s - loss: 9.5255 - acc: 0.391 - ETA: 0s - loss: 9.4925 - acc: 0.393 - ETA: 0s - loss: 9.4931 - acc: 0.393 - ETA: 0s - loss: 9.4537 - acc: 0.396 - ETA: 0s - loss: 9.4308 - acc: 0.397 - ETA: 0s - loss: 9.4483 - acc: 0.396 - ETA: 0s - loss: 9.4578 - acc: 0.396 - ETA: 0s - loss: 9.4653 - acc: 0.396 - ETA: 0s - loss: 9.4808 - acc: 0.395 - ETA: 0s - loss: 9.4879 - acc: 0.394 - ETA: 0s - loss: 9.4663 - acc: 0.396 - ETA: 0s - loss: 9.4710 - acc: 0.395 - ETA: 0s - loss: 9.4606 - acc: 0.395 - ETA: 0s - loss: 9.4389 - acc: 0.396 - ETA: 0s - loss: 9.4587 - acc: 0.3956Epoch 00006: val_loss improved from 9.90151 to 9.81552, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4580 - acc: 0.3957 - val_loss: 9.8155 - val_acc: 0.3317\n",
      "Epoch 8/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 10.4831 - acc: 0.35 - ETA: 2s - loss: 9.7451 - acc: 0.3786 - ETA: 2s - loss: 10.0430 - acc: 0.36 - ETA: 2s - loss: 10.3393 - acc: 0.34 - ETA: 2s - loss: 10.5632 - acc: 0.33 - ETA: 2s - loss: 10.4290 - acc: 0.34 - ETA: 2s - loss: 10.3791 - acc: 0.34 - ETA: 2s - loss: 10.2534 - acc: 0.35 - ETA: 2s - loss: 10.0805 - acc: 0.36 - ETA: 2s - loss: 10.1713 - acc: 0.35 - ETA: 2s - loss: 9.9523 - acc: 0.3706 - ETA: 2s - loss: 9.8392 - acc: 0.378 - ETA: 2s - loss: 9.7432 - acc: 0.384 - ETA: 2s - loss: 9.7054 - acc: 0.387 - ETA: 2s - loss: 9.6764 - acc: 0.389 - ETA: 2s - loss: 9.6603 - acc: 0.390 - ETA: 2s - loss: 9.5997 - acc: 0.394 - ETA: 1s - loss: 9.5584 - acc: 0.397 - ETA: 1s - loss: 9.5723 - acc: 0.396 - ETA: 1s - loss: 9.5794 - acc: 0.396 - ETA: 1s - loss: 9.5261 - acc: 0.399 - ETA: 1s - loss: 9.4900 - acc: 0.401 - ETA: 1s - loss: 9.5113 - acc: 0.400 - ETA: 1s - loss: 9.4370 - acc: 0.404 - ETA: 1s - loss: 9.4413 - acc: 0.404 - ETA: 1s - loss: 9.4623 - acc: 0.403 - ETA: 1s - loss: 9.4673 - acc: 0.402 - ETA: 1s - loss: 9.4614 - acc: 0.402 - ETA: 1s - loss: 9.4988 - acc: 0.400 - ETA: 1s - loss: 9.4856 - acc: 0.400 - ETA: 1s - loss: 9.4741 - acc: 0.401 - ETA: 1s - loss: 9.4387 - acc: 0.404 - ETA: 1s - loss: 9.4344 - acc: 0.404 - ETA: 1s - loss: 9.4849 - acc: 0.401 - ETA: 1s - loss: 9.4694 - acc: 0.402 - ETA: 1s - loss: 9.4629 - acc: 0.401 - ETA: 0s - loss: 9.4677 - acc: 0.401 - ETA: 0s - loss: 9.4635 - acc: 0.401 - ETA: 0s - loss: 9.4841 - acc: 0.400 - ETA: 0s - loss: 9.4629 - acc: 0.400 - ETA: 0s - loss: 9.4551 - acc: 0.401 - ETA: 0s - loss: 9.4873 - acc: 0.399 - ETA: 0s - loss: 9.4883 - acc: 0.398 - ETA: 0s - loss: 9.4828 - acc: 0.398 - ETA: 0s - loss: 9.4883 - acc: 0.398 - ETA: 0s - loss: 9.4771 - acc: 0.399 - ETA: 0s - loss: 9.4623 - acc: 0.400 - ETA: 0s - loss: 9.4637 - acc: 0.399 - ETA: 0s - loss: 9.4570 - acc: 0.400 - ETA: 0s - loss: 9.4598 - acc: 0.400 - ETA: 0s - loss: 9.4498 - acc: 0.400 - ETA: 0s - loss: 9.4205 - acc: 0.402 - ETA: 0s - loss: 9.4459 - acc: 0.401 - ETA: 0s - loss: 9.4351 - acc: 0.402 - ETA: 0s - loss: 9.4256 - acc: 0.4027Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.4166 - acc: 0.4033 - val_loss: 9.8341 - val_acc: 0.3246\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 9.6709 - acc: 0.400 - ETA: 3s - loss: 9.2475 - acc: 0.407 - ETA: 2s - loss: 9.1155 - acc: 0.425 - ETA: 2s - loss: 9.2934 - acc: 0.415 - ETA: 2s - loss: 9.3192 - acc: 0.415 - ETA: 2s - loss: 9.0744 - acc: 0.428 - ETA: 2s - loss: 8.9575 - acc: 0.436 - ETA: 2s - loss: 9.0370 - acc: 0.433 - ETA: 2s - loss: 9.0517 - acc: 0.432 - ETA: 2s - loss: 9.0452 - acc: 0.430 - ETA: 2s - loss: 9.1085 - acc: 0.426 - ETA: 2s - loss: 9.0728 - acc: 0.429 - ETA: 2s - loss: 9.1218 - acc: 0.427 - ETA: 2s - loss: 9.1927 - acc: 0.423 - ETA: 2s - loss: 9.1147 - acc: 0.427 - ETA: 2s - loss: 9.1079 - acc: 0.428 - ETA: 2s - loss: 9.0878 - acc: 0.429 - ETA: 1s - loss: 9.1180 - acc: 0.427 - ETA: 1s - loss: 9.1429 - acc: 0.426 - ETA: 1s - loss: 9.2086 - acc: 0.421 - ETA: 1s - loss: 9.2318 - acc: 0.420 - ETA: 1s - loss: 9.2090 - acc: 0.421 - ETA: 1s - loss: 9.2472 - acc: 0.419 - ETA: 1s - loss: 9.3128 - acc: 0.414 - ETA: 1s - loss: 9.3219 - acc: 0.414 - ETA: 1s - loss: 9.3461 - acc: 0.413 - ETA: 1s - loss: 9.3159 - acc: 0.414 - ETA: 1s - loss: 9.3103 - acc: 0.414 - ETA: 1s - loss: 9.3418 - acc: 0.413 - ETA: 1s - loss: 9.3989 - acc: 0.409 - ETA: 1s - loss: 9.4052 - acc: 0.408 - ETA: 1s - loss: 9.3844 - acc: 0.409 - ETA: 1s - loss: 9.4272 - acc: 0.407 - ETA: 1s - loss: 9.4588 - acc: 0.405 - ETA: 1s - loss: 9.4232 - acc: 0.407 - ETA: 1s - loss: 9.4004 - acc: 0.409 - ETA: 0s - loss: 9.3929 - acc: 0.410 - ETA: 0s - loss: 9.3935 - acc: 0.410 - ETA: 0s - loss: 9.3966 - acc: 0.409 - ETA: 0s - loss: 9.3635 - acc: 0.411 - ETA: 0s - loss: 9.3973 - acc: 0.410 - ETA: 0s - loss: 9.4106 - acc: 0.409 - ETA: 0s - loss: 9.4150 - acc: 0.408 - ETA: 0s - loss: 9.4258 - acc: 0.408 - ETA: 0s - loss: 9.4073 - acc: 0.409 - ETA: 0s - loss: 9.4174 - acc: 0.408 - ETA: 0s - loss: 9.4331 - acc: 0.407 - ETA: 0s - loss: 9.4229 - acc: 0.408 - ETA: 0s - loss: 9.3980 - acc: 0.410 - ETA: 0s - loss: 9.3836 - acc: 0.411 - ETA: 0s - loss: 9.3727 - acc: 0.411 - ETA: 0s - loss: 9.3615 - acc: 0.412 - ETA: 0s - loss: 9.3639 - acc: 0.411 - ETA: 0s - loss: 9.4020 - acc: 0.409 - ETA: 0s - loss: 9.3944 - acc: 0.4094Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.3906 - acc: 0.4097 - val_loss: 9.8160 - val_acc: 0.3269\n",
      "Epoch 10/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 15.3122 - acc: 0.05 - ETA: 2s - loss: 10.3707 - acc: 0.35 - ETA: 2s - loss: 9.7129 - acc: 0.3962 - ETA: 2s - loss: 9.5146 - acc: 0.405 - ETA: 2s - loss: 9.2000 - acc: 0.416 - ETA: 2s - loss: 9.3234 - acc: 0.407 - ETA: 2s - loss: 9.2947 - acc: 0.411 - ETA: 2s - loss: 9.2923 - acc: 0.412 - ETA: 2s - loss: 9.4415 - acc: 0.402 - ETA: 2s - loss: 9.5099 - acc: 0.398 - ETA: 2s - loss: 9.4031 - acc: 0.401 - ETA: 2s - loss: 9.3220 - acc: 0.406 - ETA: 2s - loss: 9.3410 - acc: 0.405 - ETA: 2s - loss: 9.3577 - acc: 0.404 - ETA: 2s - loss: 9.2284 - acc: 0.411 - ETA: 2s - loss: 9.2859 - acc: 0.408 - ETA: 2s - loss: 9.3541 - acc: 0.402 - ETA: 1s - loss: 9.3763 - acc: 0.401 - ETA: 1s - loss: 9.3593 - acc: 0.402 - ETA: 1s - loss: 9.3600 - acc: 0.403 - ETA: 1s - loss: 9.3516 - acc: 0.403 - ETA: 1s - loss: 9.3139 - acc: 0.405 - ETA: 1s - loss: 9.2931 - acc: 0.406 - ETA: 1s - loss: 9.2814 - acc: 0.406 - ETA: 1s - loss: 9.2576 - acc: 0.406 - ETA: 1s - loss: 9.2242 - acc: 0.408 - ETA: 1s - loss: 9.2374 - acc: 0.407 - ETA: 1s - loss: 9.2303 - acc: 0.408 - ETA: 1s - loss: 9.2163 - acc: 0.408 - ETA: 1s - loss: 9.2366 - acc: 0.407 - ETA: 1s - loss: 9.2419 - acc: 0.407 - ETA: 1s - loss: 9.2543 - acc: 0.407 - ETA: 1s - loss: 9.2782 - acc: 0.405 - ETA: 1s - loss: 9.2767 - acc: 0.405 - ETA: 1s - loss: 9.2484 - acc: 0.408 - ETA: 1s - loss: 9.2227 - acc: 0.409 - ETA: 0s - loss: 9.2125 - acc: 0.409 - ETA: 0s - loss: 9.2101 - acc: 0.409 - ETA: 0s - loss: 9.2061 - acc: 0.409 - ETA: 0s - loss: 9.1956 - acc: 0.410 - ETA: 0s - loss: 9.1805 - acc: 0.410 - ETA: 0s - loss: 9.1705 - acc: 0.410 - ETA: 0s - loss: 9.1891 - acc: 0.409 - ETA: 0s - loss: 9.1887 - acc: 0.408 - ETA: 0s - loss: 9.1910 - acc: 0.408 - ETA: 0s - loss: 9.1905 - acc: 0.408 - ETA: 0s - loss: 9.1922 - acc: 0.407 - ETA: 0s - loss: 9.1677 - acc: 0.408 - ETA: 0s - loss: 9.1672 - acc: 0.408 - ETA: 0s - loss: 9.1698 - acc: 0.408 - ETA: 0s - loss: 9.1637 - acc: 0.408 - ETA: 0s - loss: 9.1811 - acc: 0.408 - ETA: 0s - loss: 9.1989 - acc: 0.406 - ETA: 0s - loss: 9.2113 - acc: 0.405 - ETA: 0s - loss: 9.2134 - acc: 0.4062Epoch 00009: val_loss improved from 9.81552 to 9.59339, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.2159 - acc: 0.4060 - val_loss: 9.5934 - val_acc: 0.3377\n",
      "Epoch 11/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.6729 - acc: 0.400 - ETA: 2s - loss: 8.1780 - acc: 0.475 - ETA: 2s - loss: 8.5988 - acc: 0.450 - ETA: 2s - loss: 8.9805 - acc: 0.423 - ETA: 2s - loss: 9.1092 - acc: 0.414 - ETA: 2s - loss: 8.9469 - acc: 0.427 - ETA: 2s - loss: 8.9880 - acc: 0.425 - ETA: 2s - loss: 9.0795 - acc: 0.422 - ETA: 2s - loss: 9.1501 - acc: 0.419 - ETA: 2s - loss: 9.0962 - acc: 0.424 - ETA: 2s - loss: 9.0242 - acc: 0.427 - ETA: 2s - loss: 8.9904 - acc: 0.429 - ETA: 2s - loss: 9.0679 - acc: 0.424 - ETA: 2s - loss: 9.0963 - acc: 0.422 - ETA: 2s - loss: 9.0754 - acc: 0.423 - ETA: 2s - loss: 9.1418 - acc: 0.420 - ETA: 2s - loss: 9.1269 - acc: 0.421 - ETA: 1s - loss: 9.1435 - acc: 0.420 - ETA: 1s - loss: 9.1375 - acc: 0.420 - ETA: 1s - loss: 9.0908 - acc: 0.422 - ETA: 1s - loss: 9.0493 - acc: 0.425 - ETA: 1s - loss: 9.0284 - acc: 0.426 - ETA: 1s - loss: 8.9866 - acc: 0.428 - ETA: 1s - loss: 8.9987 - acc: 0.428 - ETA: 1s - loss: 8.9823 - acc: 0.429 - ETA: 1s - loss: 8.9499 - acc: 0.431 - ETA: 1s - loss: 8.9476 - acc: 0.431 - ETA: 1s - loss: 8.9677 - acc: 0.429 - ETA: 1s - loss: 9.0217 - acc: 0.426 - ETA: 1s - loss: 9.0001 - acc: 0.428 - ETA: 1s - loss: 9.0482 - acc: 0.425 - ETA: 1s - loss: 9.0517 - acc: 0.424 - ETA: 1s - loss: 9.0659 - acc: 0.423 - ETA: 1s - loss: 9.0588 - acc: 0.424 - ETA: 1s - loss: 9.0503 - acc: 0.425 - ETA: 1s - loss: 9.0341 - acc: 0.426 - ETA: 1s - loss: 9.0184 - acc: 0.427 - ETA: 0s - loss: 9.0181 - acc: 0.428 - ETA: 0s - loss: 9.0825 - acc: 0.424 - ETA: 0s - loss: 9.0672 - acc: 0.425 - ETA: 0s - loss: 9.0594 - acc: 0.425 - ETA: 0s - loss: 9.0626 - acc: 0.425 - ETA: 0s - loss: 9.0365 - acc: 0.427 - ETA: 0s - loss: 9.0339 - acc: 0.427 - ETA: 0s - loss: 9.0465 - acc: 0.426 - ETA: 0s - loss: 9.0560 - acc: 0.426 - ETA: 0s - loss: 9.0667 - acc: 0.425 - ETA: 0s - loss: 9.0653 - acc: 0.425 - ETA: 0s - loss: 9.0722 - acc: 0.425 - ETA: 0s - loss: 9.0602 - acc: 0.426 - ETA: 0s - loss: 9.0502 - acc: 0.426 - ETA: 0s - loss: 9.0526 - acc: 0.426 - ETA: 0s - loss: 9.0477 - acc: 0.426 - ETA: 0s - loss: 9.0589 - acc: 0.426 - ETA: 0s - loss: 9.0559 - acc: 0.426 - ETA: 0s - loss: 9.0707 - acc: 0.4252Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.0726 - acc: 0.4251 - val_loss: 9.6434 - val_acc: 0.3353\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 11.2832 - acc: 0.30 - ETA: 2s - loss: 9.3679 - acc: 0.4000 - ETA: 2s - loss: 9.8187 - acc: 0.380 - ETA: 2s - loss: 9.3336 - acc: 0.413 - ETA: 2s - loss: 9.1478 - acc: 0.424 - ETA: 2s - loss: 8.8391 - acc: 0.443 - ETA: 2s - loss: 8.9523 - acc: 0.437 - ETA: 2s - loss: 9.0529 - acc: 0.432 - ETA: 2s - loss: 9.0654 - acc: 0.431 - ETA: 2s - loss: 9.0440 - acc: 0.433 - ETA: 2s - loss: 9.0128 - acc: 0.435 - ETA: 2s - loss: 9.0042 - acc: 0.435 - ETA: 2s - loss: 9.0319 - acc: 0.433 - ETA: 2s - loss: 9.0873 - acc: 0.430 - ETA: 2s - loss: 9.0719 - acc: 0.430 - ETA: 2s - loss: 9.0274 - acc: 0.431 - ETA: 1s - loss: 8.9770 - acc: 0.432 - ETA: 1s - loss: 9.0558 - acc: 0.428 - ETA: 1s - loss: 9.0622 - acc: 0.427 - ETA: 1s - loss: 9.0565 - acc: 0.428 - ETA: 1s - loss: 9.0480 - acc: 0.428 - ETA: 1s - loss: 9.0038 - acc: 0.431 - ETA: 1s - loss: 9.0131 - acc: 0.430 - ETA: 1s - loss: 8.9807 - acc: 0.432 - ETA: 1s - loss: 9.0034 - acc: 0.431 - ETA: 1s - loss: 9.0191 - acc: 0.430 - ETA: 1s - loss: 9.0417 - acc: 0.429 - ETA: 1s - loss: 9.0723 - acc: 0.427 - ETA: 1s - loss: 9.0534 - acc: 0.428 - ETA: 1s - loss: 9.0625 - acc: 0.427 - ETA: 1s - loss: 9.0324 - acc: 0.429 - ETA: 1s - loss: 9.0188 - acc: 0.430 - ETA: 1s - loss: 8.9914 - acc: 0.431 - ETA: 1s - loss: 9.0026 - acc: 0.430 - ETA: 1s - loss: 9.0055 - acc: 0.429 - ETA: 0s - loss: 8.9669 - acc: 0.432 - ETA: 0s - loss: 8.9433 - acc: 0.433 - ETA: 0s - loss: 8.9942 - acc: 0.430 - ETA: 0s - loss: 8.9981 - acc: 0.430 - ETA: 0s - loss: 8.9958 - acc: 0.430 - ETA: 0s - loss: 8.9993 - acc: 0.430 - ETA: 0s - loss: 9.0152 - acc: 0.429 - ETA: 0s - loss: 9.0232 - acc: 0.428 - ETA: 0s - loss: 9.0130 - acc: 0.429 - ETA: 0s - loss: 9.0189 - acc: 0.429 - ETA: 0s - loss: 9.0074 - acc: 0.429 - ETA: 0s - loss: 8.9879 - acc: 0.431 - ETA: 0s - loss: 8.9977 - acc: 0.430 - ETA: 0s - loss: 9.0062 - acc: 0.429 - ETA: 0s - loss: 8.9882 - acc: 0.431 - ETA: 0s - loss: 8.9784 - acc: 0.431 - ETA: 0s - loss: 8.9739 - acc: 0.432 - ETA: 0s - loss: 8.9550 - acc: 0.433 - ETA: 0s - loss: 8.9641 - acc: 0.432 - ETA: 0s - loss: 8.9633 - acc: 0.4324Epoch 00011: val_loss improved from 9.59339 to 9.37930, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.9660 - acc: 0.4322 - val_loss: 9.3793 - val_acc: 0.3605\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 8.5643 - acc: 0.468 - ETA: 2s - loss: 8.8176 - acc: 0.450 - ETA: 2s - loss: 8.6339 - acc: 0.460 - ETA: 2s - loss: 8.6205 - acc: 0.459 - ETA: 2s - loss: 8.5849 - acc: 0.457 - ETA: 2s - loss: 8.7978 - acc: 0.444 - ETA: 2s - loss: 8.8261 - acc: 0.444 - ETA: 2s - loss: 8.9265 - acc: 0.439 - ETA: 2s - loss: 9.0919 - acc: 0.428 - ETA: 2s - loss: 9.0181 - acc: 0.432 - ETA: 2s - loss: 9.0580 - acc: 0.428 - ETA: 2s - loss: 9.0194 - acc: 0.430 - ETA: 2s - loss: 9.1042 - acc: 0.424 - ETA: 2s - loss: 9.1194 - acc: 0.422 - ETA: 2s - loss: 9.0962 - acc: 0.424 - ETA: 2s - loss: 9.1048 - acc: 0.424 - ETA: 2s - loss: 9.0497 - acc: 0.427 - ETA: 1s - loss: 9.0213 - acc: 0.430 - ETA: 1s - loss: 9.0873 - acc: 0.426 - ETA: 1s - loss: 9.0284 - acc: 0.429 - ETA: 1s - loss: 8.9423 - acc: 0.434 - ETA: 1s - loss: 8.9717 - acc: 0.431 - ETA: 1s - loss: 8.9499 - acc: 0.433 - ETA: 1s - loss: 8.9358 - acc: 0.433 - ETA: 1s - loss: 8.9311 - acc: 0.434 - ETA: 1s - loss: 8.9061 - acc: 0.435 - ETA: 1s - loss: 8.8573 - acc: 0.437 - ETA: 1s - loss: 8.8834 - acc: 0.435 - ETA: 1s - loss: 8.8919 - acc: 0.434 - ETA: 1s - loss: 8.8643 - acc: 0.436 - ETA: 1s - loss: 8.8410 - acc: 0.437 - ETA: 1s - loss: 8.8602 - acc: 0.436 - ETA: 1s - loss: 8.8636 - acc: 0.435 - ETA: 1s - loss: 8.9027 - acc: 0.433 - ETA: 0s - loss: 8.9023 - acc: 0.432 - ETA: 0s - loss: 8.8992 - acc: 0.433 - ETA: 0s - loss: 8.9080 - acc: 0.432 - ETA: 0s - loss: 8.8913 - acc: 0.433 - ETA: 0s - loss: 8.8892 - acc: 0.433 - ETA: 0s - loss: 8.8688 - acc: 0.434 - ETA: 0s - loss: 8.8697 - acc: 0.434 - ETA: 0s - loss: 8.8497 - acc: 0.435 - ETA: 0s - loss: 8.8513 - acc: 0.435 - ETA: 0s - loss: 8.8428 - acc: 0.435 - ETA: 0s - loss: 8.8711 - acc: 0.433 - ETA: 0s - loss: 8.8629 - acc: 0.433 - ETA: 0s - loss: 8.8420 - acc: 0.435 - ETA: 0s - loss: 8.8444 - acc: 0.434 - ETA: 0s - loss: 8.8419 - acc: 0.434 - ETA: 0s - loss: 8.8400 - acc: 0.434 - ETA: 0s - loss: 8.8251 - acc: 0.434 - ETA: 0s - loss: 8.8137 - acc: 0.435 - ETA: 0s - loss: 8.8080 - acc: 0.4358Epoch 00012: val_loss improved from 9.37930 to 9.25013, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.7967 - acc: 0.4367 - val_loss: 9.2501 - val_acc: 0.3509\n",
      "Epoch 14/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 10.6388 - acc: 0.30 - ETA: 2s - loss: 9.3647 - acc: 0.3857 - ETA: 2s - loss: 8.4519 - acc: 0.446 - ETA: 2s - loss: 7.9125 - acc: 0.478 - ETA: 2s - loss: 8.3519 - acc: 0.454 - ETA: 2s - loss: 8.1343 - acc: 0.464 - ETA: 2s - loss: 8.2461 - acc: 0.462 - ETA: 2s - loss: 8.1726 - acc: 0.466 - ETA: 2s - loss: 8.2201 - acc: 0.464 - ETA: 2s - loss: 8.2886 - acc: 0.461 - ETA: 2s - loss: 8.4000 - acc: 0.453 - ETA: 2s - loss: 8.4134 - acc: 0.453 - ETA: 2s - loss: 8.4772 - acc: 0.450 - ETA: 2s - loss: 8.4880 - acc: 0.450 - ETA: 2s - loss: 8.5087 - acc: 0.448 - ETA: 1s - loss: 8.5076 - acc: 0.447 - ETA: 1s - loss: 8.4709 - acc: 0.451 - ETA: 1s - loss: 8.4954 - acc: 0.450 - ETA: 1s - loss: 8.4752 - acc: 0.452 - ETA: 1s - loss: 8.4297 - acc: 0.455 - ETA: 1s - loss: 8.4916 - acc: 0.451 - ETA: 1s - loss: 8.4371 - acc: 0.455 - ETA: 1s - loss: 8.4735 - acc: 0.453 - ETA: 1s - loss: 8.4795 - acc: 0.452 - ETA: 1s - loss: 8.4852 - acc: 0.452 - ETA: 1s - loss: 8.4813 - acc: 0.453 - ETA: 1s - loss: 8.4900 - acc: 0.453 - ETA: 1s - loss: 8.5187 - acc: 0.451 - ETA: 1s - loss: 8.5075 - acc: 0.452 - ETA: 1s - loss: 8.4614 - acc: 0.454 - ETA: 1s - loss: 8.4567 - acc: 0.455 - ETA: 1s - loss: 8.4668 - acc: 0.455 - ETA: 1s - loss: 8.4762 - acc: 0.454 - ETA: 1s - loss: 8.4900 - acc: 0.453 - ETA: 1s - loss: 8.4913 - acc: 0.454 - ETA: 0s - loss: 8.5143 - acc: 0.452 - ETA: 0s - loss: 8.4867 - acc: 0.454 - ETA: 0s - loss: 8.4906 - acc: 0.454 - ETA: 0s - loss: 8.4679 - acc: 0.456 - ETA: 0s - loss: 8.4863 - acc: 0.454 - ETA: 0s - loss: 8.4844 - acc: 0.454 - ETA: 0s - loss: 8.4863 - acc: 0.454 - ETA: 0s - loss: 8.4574 - acc: 0.456 - ETA: 0s - loss: 8.4644 - acc: 0.455 - ETA: 0s - loss: 8.4504 - acc: 0.456 - ETA: 0s - loss: 8.4760 - acc: 0.454 - ETA: 0s - loss: 8.4796 - acc: 0.454 - ETA: 0s - loss: 8.4769 - acc: 0.455 - ETA: 0s - loss: 8.4983 - acc: 0.454 - ETA: 0s - loss: 8.5217 - acc: 0.452 - ETA: 0s - loss: 8.5176 - acc: 0.453 - ETA: 0s - loss: 8.5410 - acc: 0.451 - ETA: 0s - loss: 8.5700 - acc: 0.450 - ETA: 0s - loss: 8.5690 - acc: 0.4503Epoch 00013: val_loss improved from 9.25013 to 9.05533, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 8.5607 - acc: 0.4507 - val_loss: 9.0553 - val_acc: 0.3784\n",
      "Epoch 15/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 8.8666 - acc: 0.450 - ETA: 2s - loss: 8.4875 - acc: 0.471 - ETA: 2s - loss: 8.3549 - acc: 0.480 - ETA: 2s - loss: 8.4903 - acc: 0.463 - ETA: 2s - loss: 8.3206 - acc: 0.470 - ETA: 2s - loss: 8.2671 - acc: 0.472 - ETA: 2s - loss: 8.3220 - acc: 0.468 - ETA: 2s - loss: 8.4224 - acc: 0.462 - ETA: 2s - loss: 8.4279 - acc: 0.464 - ETA: 2s - loss: 8.4319 - acc: 0.463 - ETA: 2s - loss: 8.5666 - acc: 0.452 - ETA: 2s - loss: 8.5099 - acc: 0.457 - ETA: 2s - loss: 8.4957 - acc: 0.459 - ETA: 2s - loss: 8.4451 - acc: 0.461 - ETA: 2s - loss: 8.3282 - acc: 0.469 - ETA: 2s - loss: 8.3322 - acc: 0.469 - ETA: 2s - loss: 8.3915 - acc: 0.466 - ETA: 1s - loss: 8.3013 - acc: 0.471 - ETA: 1s - loss: 8.3407 - acc: 0.469 - ETA: 1s - loss: 8.4236 - acc: 0.465 - ETA: 1s - loss: 8.4167 - acc: 0.465 - ETA: 1s - loss: 8.3695 - acc: 0.468 - ETA: 1s - loss: 8.3713 - acc: 0.468 - ETA: 1s - loss: 8.4282 - acc: 0.464 - ETA: 1s - loss: 8.4745 - acc: 0.462 - ETA: 1s - loss: 8.5276 - acc: 0.458 - ETA: 1s - loss: 8.5166 - acc: 0.459 - ETA: 1s - loss: 8.5066 - acc: 0.459 - ETA: 1s - loss: 8.4483 - acc: 0.463 - ETA: 1s - loss: 8.4576 - acc: 0.463 - ETA: 1s - loss: 8.4494 - acc: 0.464 - ETA: 1s - loss: 8.4417 - acc: 0.464 - ETA: 1s - loss: 8.4276 - acc: 0.465 - ETA: 1s - loss: 8.4050 - acc: 0.466 - ETA: 1s - loss: 8.4207 - acc: 0.465 - ETA: 1s - loss: 8.3956 - acc: 0.467 - ETA: 0s - loss: 8.3714 - acc: 0.469 - ETA: 0s - loss: 8.3582 - acc: 0.469 - ETA: 0s - loss: 8.3685 - acc: 0.469 - ETA: 0s - loss: 8.3892 - acc: 0.467 - ETA: 0s - loss: 8.4001 - acc: 0.466 - ETA: 0s - loss: 8.3884 - acc: 0.467 - ETA: 0s - loss: 8.3875 - acc: 0.467 - ETA: 0s - loss: 8.3997 - acc: 0.466 - ETA: 0s - loss: 8.4223 - acc: 0.465 - ETA: 0s - loss: 8.4342 - acc: 0.465 - ETA: 0s - loss: 8.4467 - acc: 0.464 - ETA: 0s - loss: 8.4416 - acc: 0.465 - ETA: 0s - loss: 8.4651 - acc: 0.463 - ETA: 0s - loss: 8.4725 - acc: 0.462 - ETA: 0s - loss: 8.4840 - acc: 0.462 - ETA: 0s - loss: 8.4904 - acc: 0.461 - ETA: 0s - loss: 8.5210 - acc: 0.459 - ETA: 0s - loss: 8.5167 - acc: 0.459 - ETA: 0s - loss: 8.4982 - acc: 0.4606Epoch 00014: val_loss improved from 9.05533 to 9.02061, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.4799 - acc: 0.4618 - val_loss: 9.0206 - val_acc: 0.3796\n",
      "Epoch 16/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 9.6958 - acc: 0.400 - ETA: 2s - loss: 9.5613 - acc: 0.407 - ETA: 2s - loss: 9.1863 - acc: 0.426 - ETA: 2s - loss: 8.9119 - acc: 0.444 - ETA: 2s - loss: 8.8640 - acc: 0.448 - ETA: 2s - loss: 8.7904 - acc: 0.453 - ETA: 2s - loss: 8.8554 - acc: 0.447 - ETA: 2s - loss: 8.8401 - acc: 0.447 - ETA: 2s - loss: 8.7201 - acc: 0.454 - ETA: 2s - loss: 8.6949 - acc: 0.455 - ETA: 2s - loss: 8.6860 - acc: 0.456 - ETA: 2s - loss: 8.6443 - acc: 0.458 - ETA: 2s - loss: 8.5118 - acc: 0.465 - ETA: 2s - loss: 8.4431 - acc: 0.470 - ETA: 2s - loss: 8.4524 - acc: 0.469 - ETA: 2s - loss: 8.4624 - acc: 0.469 - ETA: 2s - loss: 8.5378 - acc: 0.464 - ETA: 1s - loss: 8.5494 - acc: 0.463 - ETA: 1s - loss: 8.5374 - acc: 0.464 - ETA: 1s - loss: 8.5066 - acc: 0.466 - ETA: 1s - loss: 8.5865 - acc: 0.461 - ETA: 1s - loss: 8.6431 - acc: 0.457 - ETA: 1s - loss: 8.5753 - acc: 0.460 - ETA: 1s - loss: 8.5564 - acc: 0.461 - ETA: 1s - loss: 8.5201 - acc: 0.462 - ETA: 1s - loss: 8.5394 - acc: 0.460 - ETA: 1s - loss: 8.5831 - acc: 0.458 - ETA: 1s - loss: 8.5658 - acc: 0.458 - ETA: 1s - loss: 8.5609 - acc: 0.458 - ETA: 1s - loss: 8.5251 - acc: 0.459 - ETA: 1s - loss: 8.5187 - acc: 0.459 - ETA: 1s - loss: 8.4930 - acc: 0.461 - ETA: 1s - loss: 8.4598 - acc: 0.463 - ETA: 1s - loss: 8.4196 - acc: 0.465 - ETA: 1s - loss: 8.4059 - acc: 0.466 - ETA: 0s - loss: 8.3915 - acc: 0.467 - ETA: 0s - loss: 8.3472 - acc: 0.469 - ETA: 0s - loss: 8.3575 - acc: 0.469 - ETA: 0s - loss: 8.3420 - acc: 0.470 - ETA: 0s - loss: 8.3458 - acc: 0.470 - ETA: 0s - loss: 8.3567 - acc: 0.469 - ETA: 0s - loss: 8.3675 - acc: 0.467 - ETA: 0s - loss: 8.3624 - acc: 0.467 - ETA: 0s - loss: 8.3866 - acc: 0.466 - ETA: 0s - loss: 8.3670 - acc: 0.466 - ETA: 0s - loss: 8.3524 - acc: 0.467 - ETA: 0s - loss: 8.3549 - acc: 0.467 - ETA: 0s - loss: 8.3265 - acc: 0.469 - ETA: 0s - loss: 8.3096 - acc: 0.469 - ETA: 0s - loss: 8.3025 - acc: 0.470 - ETA: 0s - loss: 8.2876 - acc: 0.471 - ETA: 0s - loss: 8.3094 - acc: 0.469 - ETA: 0s - loss: 8.3322 - acc: 0.468 - ETA: 0s - loss: 8.3507 - acc: 0.4674Epoch 00015: val_loss improved from 9.02061 to 8.86007, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.3547 - acc: 0.4668 - val_loss: 8.8601 - val_acc: 0.3880\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 10.4867 - acc: 0.35 - ETA: 2s - loss: 8.3029 - acc: 0.4750 - ETA: 2s - loss: 8.3235 - acc: 0.475 - ETA: 2s - loss: 8.3367 - acc: 0.472 - ETA: 2s - loss: 8.6455 - acc: 0.455 - ETA: 2s - loss: 8.4866 - acc: 0.467 - ETA: 2s - loss: 8.6238 - acc: 0.457 - ETA: 2s - loss: 8.4091 - acc: 0.471 - ETA: 2s - loss: 8.2916 - acc: 0.477 - ETA: 2s - loss: 8.1941 - acc: 0.482 - ETA: 2s - loss: 8.1651 - acc: 0.482 - ETA: 2s - loss: 8.0815 - acc: 0.487 - ETA: 2s - loss: 8.0454 - acc: 0.490 - ETA: 2s - loss: 8.0983 - acc: 0.486 - ETA: 2s - loss: 8.1509 - acc: 0.484 - ETA: 2s - loss: 8.1012 - acc: 0.486 - ETA: 1s - loss: 8.0910 - acc: 0.487 - ETA: 1s - loss: 8.0321 - acc: 0.491 - ETA: 1s - loss: 8.1058 - acc: 0.487 - ETA: 1s - loss: 8.1807 - acc: 0.482 - ETA: 1s - loss: 8.2062 - acc: 0.480 - ETA: 1s - loss: 8.1815 - acc: 0.481 - ETA: 1s - loss: 8.1528 - acc: 0.483 - ETA: 1s - loss: 8.1723 - acc: 0.481 - ETA: 1s - loss: 8.2141 - acc: 0.479 - ETA: 1s - loss: 8.2149 - acc: 0.479 - ETA: 1s - loss: 8.2043 - acc: 0.480 - ETA: 1s - loss: 8.1619 - acc: 0.482 - ETA: 1s - loss: 8.2102 - acc: 0.480 - ETA: 1s - loss: 8.1921 - acc: 0.481 - ETA: 1s - loss: 8.1656 - acc: 0.483 - ETA: 1s - loss: 8.1591 - acc: 0.483 - ETA: 1s - loss: 8.1277 - acc: 0.485 - ETA: 1s - loss: 8.1273 - acc: 0.485 - ETA: 1s - loss: 8.1641 - acc: 0.483 - ETA: 0s - loss: 8.1402 - acc: 0.484 - ETA: 0s - loss: 8.1332 - acc: 0.484 - ETA: 0s - loss: 8.1296 - acc: 0.484 - ETA: 0s - loss: 8.1256 - acc: 0.483 - ETA: 0s - loss: 8.1413 - acc: 0.482 - ETA: 0s - loss: 8.1348 - acc: 0.483 - ETA: 0s - loss: 8.1477 - acc: 0.482 - ETA: 0s - loss: 8.1205 - acc: 0.483 - ETA: 0s - loss: 8.1595 - acc: 0.480 - ETA: 0s - loss: 8.1932 - acc: 0.479 - ETA: 0s - loss: 8.2057 - acc: 0.478 - ETA: 0s - loss: 8.2023 - acc: 0.478 - ETA: 0s - loss: 8.1947 - acc: 0.479 - ETA: 0s - loss: 8.1983 - acc: 0.478 - ETA: 0s - loss: 8.2016 - acc: 0.478 - ETA: 0s - loss: 8.2302 - acc: 0.477 - ETA: 0s - loss: 8.2371 - acc: 0.476 - ETA: 0s - loss: 8.2211 - acc: 0.477 - ETA: 0s - loss: 8.1909 - acc: 0.479 - ETA: 0s - loss: 8.1894 - acc: 0.4796Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1880 - acc: 0.4795 - val_loss: 8.8805 - val_acc: 0.3784\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 9.0137 - acc: 0.4286 - ETA: 3s - loss: 8.9559 - acc: 0.433 - ETA: 2s - loss: 8.4061 - acc: 0.466 - ETA: 2s - loss: 8.3709 - acc: 0.468 - ETA: 2s - loss: 8.4160 - acc: 0.468 - ETA: 2s - loss: 8.4494 - acc: 0.465 - ETA: 2s - loss: 8.3571 - acc: 0.472 - ETA: 2s - loss: 8.4544 - acc: 0.467 - ETA: 2s - loss: 8.4704 - acc: 0.467 - ETA: 2s - loss: 8.4563 - acc: 0.469 - ETA: 2s - loss: 8.5063 - acc: 0.466 - ETA: 2s - loss: 8.4804 - acc: 0.468 - ETA: 2s - loss: 8.4842 - acc: 0.467 - ETA: 2s - loss: 8.4721 - acc: 0.467 - ETA: 2s - loss: 8.4760 - acc: 0.468 - ETA: 2s - loss: 8.4421 - acc: 0.470 - ETA: 2s - loss: 8.3599 - acc: 0.475 - ETA: 1s - loss: 8.3795 - acc: 0.473 - ETA: 1s - loss: 8.3338 - acc: 0.476 - ETA: 1s - loss: 8.3321 - acc: 0.475 - ETA: 1s - loss: 8.3737 - acc: 0.472 - ETA: 1s - loss: 8.3414 - acc: 0.475 - ETA: 1s - loss: 8.3552 - acc: 0.474 - ETA: 1s - loss: 8.3100 - acc: 0.476 - ETA: 1s - loss: 8.3375 - acc: 0.475 - ETA: 1s - loss: 8.3078 - acc: 0.477 - ETA: 1s - loss: 8.3006 - acc: 0.477 - ETA: 1s - loss: 8.2544 - acc: 0.480 - ETA: 1s - loss: 8.2569 - acc: 0.480 - ETA: 1s - loss: 8.2742 - acc: 0.479 - ETA: 1s - loss: 8.2367 - acc: 0.482 - ETA: 1s - loss: 8.2452 - acc: 0.481 - ETA: 1s - loss: 8.2595 - acc: 0.480 - ETA: 1s - loss: 8.2565 - acc: 0.480 - ETA: 1s - loss: 8.2584 - acc: 0.480 - ETA: 1s - loss: 8.2550 - acc: 0.480 - ETA: 0s - loss: 8.2390 - acc: 0.481 - ETA: 0s - loss: 8.2522 - acc: 0.480 - ETA: 0s - loss: 8.2103 - acc: 0.483 - ETA: 0s - loss: 8.1609 - acc: 0.486 - ETA: 0s - loss: 8.1867 - acc: 0.484 - ETA: 0s - loss: 8.1750 - acc: 0.485 - ETA: 0s - loss: 8.1962 - acc: 0.484 - ETA: 0s - loss: 8.1681 - acc: 0.485 - ETA: 0s - loss: 8.1544 - acc: 0.486 - ETA: 0s - loss: 8.1496 - acc: 0.486 - ETA: 0s - loss: 8.1540 - acc: 0.486 - ETA: 0s - loss: 8.1566 - acc: 0.486 - ETA: 0s - loss: 8.1575 - acc: 0.486 - ETA: 0s - loss: 8.1670 - acc: 0.485 - ETA: 0s - loss: 8.1571 - acc: 0.486 - ETA: 0s - loss: 8.1681 - acc: 0.485 - ETA: 0s - loss: 8.1692 - acc: 0.485 - ETA: 0s - loss: 8.1565 - acc: 0.486 - ETA: 0s - loss: 8.1402 - acc: 0.4872Epoch 00017: val_loss improved from 8.86007 to 8.78440, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1470 - acc: 0.4868 - val_loss: 8.7844 - val_acc: 0.3820\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 2s - loss: 9.6757 - acc: 0.400 - ETA: 3s - loss: 9.7202 - acc: 0.383 - ETA: 3s - loss: 9.4795 - acc: 0.404 - ETA: 3s - loss: 9.4059 - acc: 0.411 - ETA: 2s - loss: 8.8094 - acc: 0.450 - ETA: 2s - loss: 8.7746 - acc: 0.450 - ETA: 2s - loss: 8.8171 - acc: 0.445 - ETA: 2s - loss: 8.5293 - acc: 0.464 - ETA: 2s - loss: 8.5895 - acc: 0.461 - ETA: 2s - loss: 8.4699 - acc: 0.468 - ETA: 2s - loss: 8.4027 - acc: 0.472 - ETA: 2s - loss: 8.4619 - acc: 0.468 - ETA: 2s - loss: 8.3753 - acc: 0.473 - ETA: 2s - loss: 8.2609 - acc: 0.479 - ETA: 2s - loss: 8.1046 - acc: 0.488 - ETA: 2s - loss: 8.1485 - acc: 0.485 - ETA: 2s - loss: 8.2412 - acc: 0.480 - ETA: 2s - loss: 8.2948 - acc: 0.477 - ETA: 1s - loss: 8.2617 - acc: 0.479 - ETA: 1s - loss: 8.2513 - acc: 0.480 - ETA: 1s - loss: 8.2154 - acc: 0.482 - ETA: 1s - loss: 8.1697 - acc: 0.485 - ETA: 1s - loss: 8.1344 - acc: 0.488 - ETA: 1s - loss: 8.1609 - acc: 0.486 - ETA: 1s - loss: 8.1627 - acc: 0.486 - ETA: 1s - loss: 8.1587 - acc: 0.487 - ETA: 1s - loss: 8.1344 - acc: 0.488 - ETA: 1s - loss: 8.1965 - acc: 0.485 - ETA: 1s - loss: 8.1744 - acc: 0.486 - ETA: 1s - loss: 8.1347 - acc: 0.488 - ETA: 1s - loss: 8.1648 - acc: 0.486 - ETA: 1s - loss: 8.1652 - acc: 0.486 - ETA: 1s - loss: 8.1333 - acc: 0.488 - ETA: 1s - loss: 8.1314 - acc: 0.488 - ETA: 1s - loss: 8.1531 - acc: 0.487 - ETA: 1s - loss: 8.1773 - acc: 0.486 - ETA: 1s - loss: 8.1562 - acc: 0.487 - ETA: 0s - loss: 8.1502 - acc: 0.487 - ETA: 0s - loss: 8.1381 - acc: 0.488 - ETA: 0s - loss: 8.1370 - acc: 0.488 - ETA: 0s - loss: 8.1403 - acc: 0.488 - ETA: 0s - loss: 8.1396 - acc: 0.488 - ETA: 0s - loss: 8.1233 - acc: 0.489 - ETA: 0s - loss: 8.1141 - acc: 0.489 - ETA: 0s - loss: 8.0801 - acc: 0.491 - ETA: 0s - loss: 8.0702 - acc: 0.492 - ETA: 0s - loss: 8.0644 - acc: 0.492 - ETA: 0s - loss: 8.0730 - acc: 0.492 - ETA: 0s - loss: 8.1063 - acc: 0.490 - ETA: 0s - loss: 8.0945 - acc: 0.490 - ETA: 0s - loss: 8.0779 - acc: 0.491 - ETA: 0s - loss: 8.1020 - acc: 0.490 - ETA: 0s - loss: 8.1071 - acc: 0.490 - ETA: 0s - loss: 8.1086 - acc: 0.490 - ETA: 0s - loss: 8.1202 - acc: 0.489 - ETA: 0s - loss: 8.1341 - acc: 0.4886Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1356 - acc: 0.4885 - val_loss: 8.8578 - val_acc: 0.3868\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 4.0343 - acc: 0.750 - ETA: 3s - loss: 7.1817 - acc: 0.542 - ETA: 2s - loss: 7.8353 - acc: 0.507 - ETA: 2s - loss: 8.0645 - acc: 0.492 - ETA: 2s - loss: 7.9990 - acc: 0.498 - ETA: 2s - loss: 7.9084 - acc: 0.503 - ETA: 2s - loss: 8.0428 - acc: 0.495 - ETA: 2s - loss: 8.0451 - acc: 0.496 - ETA: 2s - loss: 7.9977 - acc: 0.500 - ETA: 2s - loss: 7.9157 - acc: 0.503 - ETA: 2s - loss: 7.9689 - acc: 0.500 - ETA: 2s - loss: 8.0068 - acc: 0.497 - ETA: 2s - loss: 8.0433 - acc: 0.496 - ETA: 2s - loss: 8.0573 - acc: 0.494 - ETA: 2s - loss: 8.0196 - acc: 0.497 - ETA: 2s - loss: 8.0709 - acc: 0.493 - ETA: 2s - loss: 8.0981 - acc: 0.491 - ETA: 1s - loss: 8.0977 - acc: 0.491 - ETA: 1s - loss: 8.0816 - acc: 0.492 - ETA: 1s - loss: 8.0859 - acc: 0.492 - ETA: 1s - loss: 8.0911 - acc: 0.491 - ETA: 1s - loss: 8.0859 - acc: 0.491 - ETA: 1s - loss: 8.1023 - acc: 0.490 - ETA: 1s - loss: 8.1246 - acc: 0.488 - ETA: 1s - loss: 8.1450 - acc: 0.487 - ETA: 1s - loss: 8.0880 - acc: 0.490 - ETA: 1s - loss: 8.0688 - acc: 0.490 - ETA: 1s - loss: 8.0833 - acc: 0.489 - ETA: 1s - loss: 8.0594 - acc: 0.491 - ETA: 1s - loss: 8.0735 - acc: 0.490 - ETA: 1s - loss: 8.0868 - acc: 0.489 - ETA: 1s - loss: 8.0350 - acc: 0.493 - ETA: 1s - loss: 8.0424 - acc: 0.493 - ETA: 1s - loss: 8.0742 - acc: 0.491 - ETA: 1s - loss: 8.0860 - acc: 0.490 - ETA: 1s - loss: 8.0796 - acc: 0.490 - ETA: 0s - loss: 8.0469 - acc: 0.492 - ETA: 0s - loss: 8.0688 - acc: 0.491 - ETA: 0s - loss: 8.0719 - acc: 0.490 - ETA: 0s - loss: 8.0693 - acc: 0.490 - ETA: 0s - loss: 8.0889 - acc: 0.489 - ETA: 0s - loss: 8.0927 - acc: 0.489 - ETA: 0s - loss: 8.1016 - acc: 0.489 - ETA: 0s - loss: 8.1072 - acc: 0.488 - ETA: 0s - loss: 8.1106 - acc: 0.488 - ETA: 0s - loss: 8.0805 - acc: 0.490 - ETA: 0s - loss: 8.0762 - acc: 0.490 - ETA: 0s - loss: 8.1069 - acc: 0.488 - ETA: 0s - loss: 8.1146 - acc: 0.487 - ETA: 0s - loss: 8.0974 - acc: 0.488 - ETA: 0s - loss: 8.0840 - acc: 0.489 - ETA: 0s - loss: 8.0629 - acc: 0.490 - ETA: 0s - loss: 8.0656 - acc: 0.490 - ETA: 0s - loss: 8.0865 - acc: 0.489 - ETA: 0s - loss: 8.0848 - acc: 0.4889Epoch 00019: val_loss improved from 8.78440 to 8.66561, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0812 - acc: 0.4889 - val_loss: 8.6656 - val_acc: 0.3928\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 122s - loss: 5.8515 - acc: 0.0000e+0 - ETA: 27s - loss: 6.1988 - acc: 0.0000e+0 - ETA: 17s - loss: 5.8245 - acc: 0.0222   - ETA: 12s - loss: 5.6034 - acc: 0.03 - ETA: 10s - loss: 5.4419 - acc: 0.05 - ETA: 9s - loss: 5.1984 - acc: 0.0714 - ETA: 8s - loss: 4.9812 - acc: 0.092 - ETA: 7s - loss: 4.7915 - acc: 0.126 - ETA: 7s - loss: 4.6228 - acc: 0.136 - ETA: 6s - loss: 4.4714 - acc: 0.153 - ETA: 6s - loss: 4.3205 - acc: 0.167 - ETA: 5s - loss: 4.1549 - acc: 0.189 - ETA: 5s - loss: 4.0175 - acc: 0.209 - ETA: 5s - loss: 3.9111 - acc: 0.221 - ETA: 4s - loss: 3.7832 - acc: 0.240 - ETA: 4s - loss: 3.6722 - acc: 0.259 - ETA: 4s - loss: 3.5549 - acc: 0.278 - ETA: 4s - loss: 3.4681 - acc: 0.288 - ETA: 4s - loss: 3.3967 - acc: 0.301 - ETA: 4s - loss: 3.3168 - acc: 0.312 - ETA: 3s - loss: 3.2410 - acc: 0.324 - ETA: 3s - loss: 3.1849 - acc: 0.333 - ETA: 3s - loss: 3.1267 - acc: 0.339 - ETA: 3s - loss: 3.0504 - acc: 0.351 - ETA: 3s - loss: 3.0005 - acc: 0.358 - ETA: 3s - loss: 2.9558 - acc: 0.364 - ETA: 3s - loss: 2.8869 - acc: 0.378 - ETA: 3s - loss: 2.8367 - acc: 0.387 - ETA: 3s - loss: 2.7853 - acc: 0.396 - ETA: 3s - loss: 2.7355 - acc: 0.405 - ETA: 3s - loss: 2.6888 - acc: 0.411 - ETA: 2s - loss: 2.6485 - acc: 0.416 - ETA: 2s - loss: 2.6080 - acc: 0.421 - ETA: 2s - loss: 2.5546 - acc: 0.432 - ETA: 2s - loss: 2.5275 - acc: 0.437 - ETA: 2s - loss: 2.4978 - acc: 0.442 - ETA: 2s - loss: 2.4680 - acc: 0.447 - ETA: 2s - loss: 2.4331 - acc: 0.455 - ETA: 2s - loss: 2.4000 - acc: 0.461 - ETA: 2s - loss: 2.3703 - acc: 0.464 - ETA: 2s - loss: 2.3410 - acc: 0.468 - ETA: 2s - loss: 2.3180 - acc: 0.471 - ETA: 2s - loss: 2.2929 - acc: 0.476 - ETA: 2s - loss: 2.2640 - acc: 0.481 - ETA: 2s - loss: 2.2412 - acc: 0.485 - ETA: 2s - loss: 2.2150 - acc: 0.488 - ETA: 1s - loss: 2.1906 - acc: 0.493 - ETA: 1s - loss: 2.1654 - acc: 0.498 - ETA: 1s - loss: 2.1443 - acc: 0.500 - ETA: 1s - loss: 2.1227 - acc: 0.504 - ETA: 1s - loss: 2.0975 - acc: 0.509 - ETA: 1s - loss: 2.0776 - acc: 0.512 - ETA: 1s - loss: 2.0587 - acc: 0.516 - ETA: 1s - loss: 2.0396 - acc: 0.519 - ETA: 1s - loss: 2.0205 - acc: 0.522 - ETA: 1s - loss: 2.0037 - acc: 0.525 - ETA: 1s - loss: 1.9840 - acc: 0.529 - ETA: 1s - loss: 1.9682 - acc: 0.531 - ETA: 1s - loss: 1.9470 - acc: 0.536 - ETA: 1s - loss: 1.9307 - acc: 0.538 - ETA: 1s - loss: 1.9089 - acc: 0.542 - ETA: 1s - loss: 1.8948 - acc: 0.545 - ETA: 1s - loss: 1.8803 - acc: 0.548 - ETA: 0s - loss: 1.8658 - acc: 0.550 - ETA: 0s - loss: 1.8472 - acc: 0.554 - ETA: 0s - loss: 1.8333 - acc: 0.556 - ETA: 0s - loss: 1.8220 - acc: 0.558 - ETA: 0s - loss: 1.8045 - acc: 0.562 - ETA: 0s - loss: 1.7884 - acc: 0.566 - ETA: 0s - loss: 1.7788 - acc: 0.568 - ETA: 0s - loss: 1.7642 - acc: 0.570 - ETA: 0s - loss: 1.7517 - acc: 0.572 - ETA: 0s - loss: 1.7349 - acc: 0.575 - ETA: 0s - loss: 1.7192 - acc: 0.578 - ETA: 0s - loss: 1.7115 - acc: 0.580 - ETA: 0s - loss: 1.6991 - acc: 0.583 - ETA: 0s - loss: 1.6930 - acc: 0.583 - ETA: 0s - loss: 1.6830 - acc: 0.585 - ETA: 0s - loss: 1.6723 - acc: 0.587 - ETA: 0s - loss: 1.6632 - acc: 0.5891Epoch 00000: val_loss improved from inf to 0.83818, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.6494 - acc: 0.5918 - val_loss: 0.8382 - val_acc: 0.7485\n",
      "Epoch 2/20\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.2988 - acc: 0.900 - ETA: 3s - loss: 0.3833 - acc: 0.883 - ETA: 3s - loss: 0.3547 - acc: 0.890 - ETA: 3s - loss: 0.3911 - acc: 0.881 - ETA: 3s - loss: 0.4145 - acc: 0.876 - ETA: 3s - loss: 0.3961 - acc: 0.880 - ETA: 3s - loss: 0.4207 - acc: 0.869 - ETA: 3s - loss: 0.4147 - acc: 0.872 - ETA: 3s - loss: 0.4198 - acc: 0.868 - ETA: 3s - loss: 0.4213 - acc: 0.864 - ETA: 3s - loss: 0.4302 - acc: 0.862 - ETA: 3s - loss: 0.4372 - acc: 0.861 - ETA: 3s - loss: 0.4333 - acc: 0.862 - ETA: 3s - loss: 0.4379 - acc: 0.862 - ETA: 3s - loss: 0.4316 - acc: 0.864 - ETA: 3s - loss: 0.4280 - acc: 0.866 - ETA: 2s - loss: 0.4300 - acc: 0.865 - ETA: 2s - loss: 0.4341 - acc: 0.864 - ETA: 2s - loss: 0.4324 - acc: 0.866 - ETA: 2s - loss: 0.4362 - acc: 0.865 - ETA: 2s - loss: 0.4391 - acc: 0.864 - ETA: 2s - loss: 0.4367 - acc: 0.864 - ETA: 2s - loss: 0.4421 - acc: 0.863 - ETA: 2s - loss: 0.4433 - acc: 0.862 - ETA: 2s - loss: 0.4409 - acc: 0.862 - ETA: 2s - loss: 0.4383 - acc: 0.863 - ETA: 2s - loss: 0.4376 - acc: 0.863 - ETA: 2s - loss: 0.4345 - acc: 0.864 - ETA: 2s - loss: 0.4374 - acc: 0.863 - ETA: 2s - loss: 0.4371 - acc: 0.865 - ETA: 2s - loss: 0.4428 - acc: 0.863 - ETA: 2s - loss: 0.4421 - acc: 0.861 - ETA: 2s - loss: 0.4457 - acc: 0.860 - ETA: 2s - loss: 0.4434 - acc: 0.861 - ETA: 1s - loss: 0.4434 - acc: 0.862 - ETA: 1s - loss: 0.4432 - acc: 0.862 - ETA: 1s - loss: 0.4430 - acc: 0.862 - ETA: 1s - loss: 0.4455 - acc: 0.861 - ETA: 1s - loss: 0.4477 - acc: 0.861 - ETA: 1s - loss: 0.4463 - acc: 0.861 - ETA: 1s - loss: 0.4453 - acc: 0.863 - ETA: 1s - loss: 0.4431 - acc: 0.864 - ETA: 1s - loss: 0.4428 - acc: 0.865 - ETA: 1s - loss: 0.4455 - acc: 0.863 - ETA: 1s - loss: 0.4476 - acc: 0.862 - ETA: 1s - loss: 0.4492 - acc: 0.862 - ETA: 1s - loss: 0.4458 - acc: 0.863 - ETA: 1s - loss: 0.4438 - acc: 0.864 - ETA: 1s - loss: 0.4462 - acc: 0.863 - ETA: 1s - loss: 0.4452 - acc: 0.863 - ETA: 1s - loss: 0.4464 - acc: 0.863 - ETA: 1s - loss: 0.4475 - acc: 0.862 - ETA: 0s - loss: 0.4513 - acc: 0.862 - ETA: 0s - loss: 0.4488 - acc: 0.862 - ETA: 0s - loss: 0.4485 - acc: 0.862 - ETA: 0s - loss: 0.4481 - acc: 0.863 - ETA: 0s - loss: 0.4479 - acc: 0.864 - ETA: 0s - loss: 0.4475 - acc: 0.863 - ETA: 0s - loss: 0.4448 - acc: 0.864 - ETA: 0s - loss: 0.4439 - acc: 0.864 - ETA: 0s - loss: 0.4447 - acc: 0.864 - ETA: 0s - loss: 0.4433 - acc: 0.864 - ETA: 0s - loss: 0.4442 - acc: 0.863 - ETA: 0s - loss: 0.4435 - acc: 0.863 - ETA: 0s - loss: 0.4426 - acc: 0.863 - ETA: 0s - loss: 0.4413 - acc: 0.864 - ETA: 0s - loss: 0.4417 - acc: 0.863 - ETA: 0s - loss: 0.4424 - acc: 0.863 - ETA: 0s - loss: 0.4425 - acc: 0.8641Epoch 00001: val_loss improved from 0.83818 to 0.74855, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.4435 - acc: 0.8639 - val_loss: 0.7485 - val_acc: 0.7701\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.2466 - acc: 0.950 - ETA: 4s - loss: 0.2049 - acc: 0.940 - ETA: 4s - loss: 0.1993 - acc: 0.945 - ETA: 4s - loss: 0.1883 - acc: 0.946 - ETA: 3s - loss: 0.1889 - acc: 0.944 - ETA: 3s - loss: 0.1787 - acc: 0.950 - ETA: 3s - loss: 0.1909 - acc: 0.946 - ETA: 3s - loss: 0.2062 - acc: 0.939 - ETA: 3s - loss: 0.2167 - acc: 0.935 - ETA: 3s - loss: 0.2234 - acc: 0.934 - ETA: 3s - loss: 0.2292 - acc: 0.932 - ETA: 3s - loss: 0.2277 - acc: 0.934 - ETA: 3s - loss: 0.2232 - acc: 0.935 - ETA: 3s - loss: 0.2263 - acc: 0.935 - ETA: 3s - loss: 0.2300 - acc: 0.931 - ETA: 3s - loss: 0.2268 - acc: 0.931 - ETA: 3s - loss: 0.2315 - acc: 0.930 - ETA: 3s - loss: 0.2327 - acc: 0.929 - ETA: 3s - loss: 0.2330 - acc: 0.928 - ETA: 3s - loss: 0.2307 - acc: 0.929 - ETA: 2s - loss: 0.2354 - acc: 0.927 - ETA: 2s - loss: 0.2353 - acc: 0.926 - ETA: 2s - loss: 0.2366 - acc: 0.926 - ETA: 2s - loss: 0.2390 - acc: 0.922 - ETA: 2s - loss: 0.2387 - acc: 0.922 - ETA: 2s - loss: 0.2426 - acc: 0.920 - ETA: 2s - loss: 0.2407 - acc: 0.922 - ETA: 2s - loss: 0.2466 - acc: 0.920 - ETA: 2s - loss: 0.2464 - acc: 0.920 - ETA: 2s - loss: 0.2426 - acc: 0.921 - ETA: 2s - loss: 0.2414 - acc: 0.921 - ETA: 2s - loss: 0.2412 - acc: 0.922 - ETA: 2s - loss: 0.2414 - acc: 0.921 - ETA: 2s - loss: 0.2465 - acc: 0.920 - ETA: 2s - loss: 0.2459 - acc: 0.920 - ETA: 2s - loss: 0.2452 - acc: 0.919 - ETA: 2s - loss: 0.2449 - acc: 0.919 - ETA: 1s - loss: 0.2473 - acc: 0.918 - ETA: 1s - loss: 0.2451 - acc: 0.919 - ETA: 1s - loss: 0.2475 - acc: 0.919 - ETA: 1s - loss: 0.2472 - acc: 0.919 - ETA: 1s - loss: 0.2467 - acc: 0.919 - ETA: 1s - loss: 0.2450 - acc: 0.921 - ETA: 1s - loss: 0.2463 - acc: 0.920 - ETA: 1s - loss: 0.2447 - acc: 0.920 - ETA: 1s - loss: 0.2479 - acc: 0.920 - ETA: 1s - loss: 0.2483 - acc: 0.920 - ETA: 1s - loss: 0.2492 - acc: 0.920 - ETA: 1s - loss: 0.2504 - acc: 0.920 - ETA: 1s - loss: 0.2495 - acc: 0.920 - ETA: 1s - loss: 0.2512 - acc: 0.919 - ETA: 1s - loss: 0.2514 - acc: 0.919 - ETA: 1s - loss: 0.2540 - acc: 0.919 - ETA: 1s - loss: 0.2523 - acc: 0.919 - ETA: 1s - loss: 0.2542 - acc: 0.918 - ETA: 0s - loss: 0.2595 - acc: 0.917 - ETA: 0s - loss: 0.2582 - acc: 0.917 - ETA: 0s - loss: 0.2604 - acc: 0.916 - ETA: 0s - loss: 0.2606 - acc: 0.917 - ETA: 0s - loss: 0.2621 - acc: 0.916 - ETA: 0s - loss: 0.2636 - acc: 0.916 - ETA: 0s - loss: 0.2632 - acc: 0.916 - ETA: 0s - loss: 0.2625 - acc: 0.916 - ETA: 0s - loss: 0.2617 - acc: 0.916 - ETA: 0s - loss: 0.2655 - acc: 0.915 - ETA: 0s - loss: 0.2675 - acc: 0.914 - ETA: 0s - loss: 0.2670 - acc: 0.915 - ETA: 0s - loss: 0.2671 - acc: 0.914 - ETA: 0s - loss: 0.2686 - acc: 0.914 - ETA: 0s - loss: 0.2690 - acc: 0.914 - ETA: 0s - loss: 0.2687 - acc: 0.914 - ETA: 0s - loss: 0.2699 - acc: 0.9139Epoch 00002: val_loss improved from 0.74855 to 0.66068, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.2690 - acc: 0.9142 - val_loss: 0.6607 - val_acc: 0.7976\n",
      "Epoch 4/20\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.1637 - acc: 0.900 - ETA: 3s - loss: 0.2387 - acc: 0.925 - ETA: 3s - loss: 0.1835 - acc: 0.945 - ETA: 3s - loss: 0.1493 - acc: 0.956 - ETA: 3s - loss: 0.1524 - acc: 0.960 - ETA: 3s - loss: 0.1506 - acc: 0.958 - ETA: 3s - loss: 0.1391 - acc: 0.958 - ETA: 3s - loss: 0.1324 - acc: 0.960 - ETA: 3s - loss: 0.1330 - acc: 0.957 - ETA: 3s - loss: 0.1364 - acc: 0.955 - ETA: 3s - loss: 0.1383 - acc: 0.956 - ETA: 3s - loss: 0.1394 - acc: 0.955 - ETA: 3s - loss: 0.1372 - acc: 0.957 - ETA: 3s - loss: 0.1375 - acc: 0.956 - ETA: 3s - loss: 0.1406 - acc: 0.956 - ETA: 3s - loss: 0.1434 - acc: 0.955 - ETA: 3s - loss: 0.1411 - acc: 0.956 - ETA: 3s - loss: 0.1392 - acc: 0.956 - ETA: 2s - loss: 0.1458 - acc: 0.953 - ETA: 2s - loss: 0.1540 - acc: 0.951 - ETA: 2s - loss: 0.1545 - acc: 0.950 - ETA: 2s - loss: 0.1582 - acc: 0.948 - ETA: 2s - loss: 0.1556 - acc: 0.949 - ETA: 2s - loss: 0.1607 - acc: 0.947 - ETA: 2s - loss: 0.1634 - acc: 0.945 - ETA: 2s - loss: 0.1649 - acc: 0.944 - ETA: 2s - loss: 0.1662 - acc: 0.944 - ETA: 2s - loss: 0.1659 - acc: 0.945 - ETA: 2s - loss: 0.1652 - acc: 0.945 - ETA: 2s - loss: 0.1657 - acc: 0.945 - ETA: 2s - loss: 0.1645 - acc: 0.945 - ETA: 2s - loss: 0.1629 - acc: 0.947 - ETA: 2s - loss: 0.1614 - acc: 0.947 - ETA: 2s - loss: 0.1588 - acc: 0.948 - ETA: 2s - loss: 0.1580 - acc: 0.948 - ETA: 1s - loss: 0.1598 - acc: 0.948 - ETA: 1s - loss: 0.1615 - acc: 0.947 - ETA: 1s - loss: 0.1607 - acc: 0.947 - ETA: 1s - loss: 0.1588 - acc: 0.948 - ETA: 1s - loss: 0.1608 - acc: 0.947 - ETA: 1s - loss: 0.1610 - acc: 0.947 - ETA: 1s - loss: 0.1616 - acc: 0.947 - ETA: 1s - loss: 0.1618 - acc: 0.947 - ETA: 1s - loss: 0.1618 - acc: 0.947 - ETA: 1s - loss: 0.1637 - acc: 0.947 - ETA: 1s - loss: 0.1645 - acc: 0.947 - ETA: 1s - loss: 0.1635 - acc: 0.947 - ETA: 1s - loss: 0.1619 - acc: 0.947 - ETA: 1s - loss: 0.1671 - acc: 0.946 - ETA: 1s - loss: 0.1679 - acc: 0.947 - ETA: 1s - loss: 0.1696 - acc: 0.946 - ETA: 1s - loss: 0.1723 - acc: 0.945 - ETA: 1s - loss: 0.1717 - acc: 0.946 - ETA: 0s - loss: 0.1726 - acc: 0.945 - ETA: 0s - loss: 0.1719 - acc: 0.945 - ETA: 0s - loss: 0.1718 - acc: 0.945 - ETA: 0s - loss: 0.1757 - acc: 0.944 - ETA: 0s - loss: 0.1751 - acc: 0.945 - ETA: 0s - loss: 0.1746 - acc: 0.945 - ETA: 0s - loss: 0.1741 - acc: 0.945 - ETA: 0s - loss: 0.1748 - acc: 0.945 - ETA: 0s - loss: 0.1745 - acc: 0.945 - ETA: 0s - loss: 0.1758 - acc: 0.944 - ETA: 0s - loss: 0.1785 - acc: 0.943 - ETA: 0s - loss: 0.1785 - acc: 0.942 - ETA: 0s - loss: 0.1802 - acc: 0.942 - ETA: 0s - loss: 0.1810 - acc: 0.941 - ETA: 0s - loss: 0.1810 - acc: 0.941 - ETA: 0s - loss: 0.1826 - acc: 0.941 - ETA: 0s - loss: 0.1820 - acc: 0.941 - ETA: 0s - loss: 0.1814 - acc: 0.9413Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1821 - acc: 0.9410 - val_loss: 0.7072 - val_acc: 0.7952\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 5s - loss: 0.0548 - acc: 1.000 - ETA: 4s - loss: 0.0791 - acc: 0.983 - ETA: 4s - loss: 0.0861 - acc: 0.975 - ETA: 3s - loss: 0.1004 - acc: 0.970 - ETA: 3s - loss: 0.1015 - acc: 0.971 - ETA: 3s - loss: 0.0937 - acc: 0.972 - ETA: 3s - loss: 0.0956 - acc: 0.969 - ETA: 3s - loss: 0.0955 - acc: 0.969 - ETA: 3s - loss: 0.0956 - acc: 0.968 - ETA: 3s - loss: 0.0917 - acc: 0.970 - ETA: 3s - loss: 0.0983 - acc: 0.967 - ETA: 3s - loss: 0.1020 - acc: 0.964 - ETA: 3s - loss: 0.1015 - acc: 0.964 - ETA: 3s - loss: 0.1032 - acc: 0.963 - ETA: 3s - loss: 0.1021 - acc: 0.965 - ETA: 3s - loss: 0.1002 - acc: 0.965 - ETA: 3s - loss: 0.1051 - acc: 0.964 - ETA: 3s - loss: 0.1057 - acc: 0.964 - ETA: 3s - loss: 0.1040 - acc: 0.965 - ETA: 3s - loss: 0.1034 - acc: 0.965 - ETA: 3s - loss: 0.1058 - acc: 0.965 - ETA: 3s - loss: 0.1055 - acc: 0.964 - ETA: 2s - loss: 0.1065 - acc: 0.964 - ETA: 2s - loss: 0.1103 - acc: 0.965 - ETA: 2s - loss: 0.1140 - acc: 0.963 - ETA: 2s - loss: 0.1154 - acc: 0.962 - ETA: 2s - loss: 0.1177 - acc: 0.960 - ETA: 2s - loss: 0.1231 - acc: 0.959 - ETA: 2s - loss: 0.1217 - acc: 0.959 - ETA: 2s - loss: 0.1205 - acc: 0.959 - ETA: 2s - loss: 0.1196 - acc: 0.959 - ETA: 2s - loss: 0.1183 - acc: 0.960 - ETA: 2s - loss: 0.1211 - acc: 0.960 - ETA: 2s - loss: 0.1207 - acc: 0.959 - ETA: 2s - loss: 0.1192 - acc: 0.960 - ETA: 2s - loss: 0.1178 - acc: 0.960 - ETA: 2s - loss: 0.1176 - acc: 0.960 - ETA: 2s - loss: 0.1166 - acc: 0.960 - ETA: 2s - loss: 0.1151 - acc: 0.961 - ETA: 2s - loss: 0.1181 - acc: 0.961 - ETA: 2s - loss: 0.1183 - acc: 0.961 - ETA: 2s - loss: 0.1185 - acc: 0.961 - ETA: 2s - loss: 0.1171 - acc: 0.962 - ETA: 1s - loss: 0.1178 - acc: 0.961 - ETA: 1s - loss: 0.1162 - acc: 0.962 - ETA: 1s - loss: 0.1155 - acc: 0.962 - ETA: 1s - loss: 0.1153 - acc: 0.962 - ETA: 1s - loss: 0.1152 - acc: 0.962 - ETA: 1s - loss: 0.1143 - acc: 0.963 - ETA: 1s - loss: 0.1143 - acc: 0.963 - ETA: 1s - loss: 0.1144 - acc: 0.962 - ETA: 1s - loss: 0.1133 - acc: 0.963 - ETA: 1s - loss: 0.1153 - acc: 0.962 - ETA: 1s - loss: 0.1141 - acc: 0.963 - ETA: 1s - loss: 0.1130 - acc: 0.963 - ETA: 1s - loss: 0.1125 - acc: 0.963 - ETA: 1s - loss: 0.1124 - acc: 0.963 - ETA: 1s - loss: 0.1128 - acc: 0.963 - ETA: 1s - loss: 0.1154 - acc: 0.963 - ETA: 1s - loss: 0.1162 - acc: 0.962 - ETA: 1s - loss: 0.1168 - acc: 0.962 - ETA: 1s - loss: 0.1165 - acc: 0.962 - ETA: 1s - loss: 0.1192 - acc: 0.962 - ETA: 1s - loss: 0.1192 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1204 - acc: 0.961 - ETA: 0s - loss: 0.1201 - acc: 0.962 - ETA: 0s - loss: 0.1199 - acc: 0.962 - ETA: 0s - loss: 0.1226 - acc: 0.961 - ETA: 0s - loss: 0.1221 - acc: 0.961 - ETA: 0s - loss: 0.1239 - acc: 0.960 - ETA: 0s - loss: 0.1239 - acc: 0.960 - ETA: 0s - loss: 0.1242 - acc: 0.960 - ETA: 0s - loss: 0.1252 - acc: 0.960 - ETA: 0s - loss: 0.1245 - acc: 0.960 - ETA: 0s - loss: 0.1238 - acc: 0.961 - ETA: 0s - loss: 0.1254 - acc: 0.960 - ETA: 0s - loss: 0.1259 - acc: 0.960 - ETA: 0s - loss: 0.1251 - acc: 0.961 - ETA: 0s - loss: 0.1247 - acc: 0.9614Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1240 - acc: 0.9615 - val_loss: 0.6658 - val_acc: 0.8192\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0396 - acc: 1.000 - ETA: 5s - loss: 0.0387 - acc: 0.990 - ETA: 4s - loss: 0.0483 - acc: 0.985 - ETA: 4s - loss: 0.0687 - acc: 0.983 - ETA: 3s - loss: 0.0688 - acc: 0.977 - ETA: 3s - loss: 0.0688 - acc: 0.978 - ETA: 3s - loss: 0.0684 - acc: 0.980 - ETA: 3s - loss: 0.0639 - acc: 0.982 - ETA: 3s - loss: 0.0704 - acc: 0.979 - ETA: 3s - loss: 0.0734 - acc: 0.979 - ETA: 3s - loss: 0.0776 - acc: 0.979 - ETA: 3s - loss: 0.0810 - acc: 0.978 - ETA: 3s - loss: 0.0864 - acc: 0.976 - ETA: 3s - loss: 0.0867 - acc: 0.975 - ETA: 3s - loss: 0.0869 - acc: 0.974 - ETA: 3s - loss: 0.0863 - acc: 0.974 - ETA: 3s - loss: 0.0863 - acc: 0.974 - ETA: 2s - loss: 0.0835 - acc: 0.975 - ETA: 2s - loss: 0.0830 - acc: 0.975 - ETA: 2s - loss: 0.0815 - acc: 0.975 - ETA: 2s - loss: 0.0804 - acc: 0.975 - ETA: 2s - loss: 0.0788 - acc: 0.976 - ETA: 2s - loss: 0.0796 - acc: 0.975 - ETA: 2s - loss: 0.0800 - acc: 0.975 - ETA: 2s - loss: 0.0794 - acc: 0.976 - ETA: 2s - loss: 0.0785 - acc: 0.976 - ETA: 2s - loss: 0.0831 - acc: 0.975 - ETA: 2s - loss: 0.0819 - acc: 0.976 - ETA: 2s - loss: 0.0820 - acc: 0.976 - ETA: 2s - loss: 0.0841 - acc: 0.976 - ETA: 2s - loss: 0.0829 - acc: 0.976 - ETA: 2s - loss: 0.0823 - acc: 0.976 - ETA: 2s - loss: 0.0817 - acc: 0.976 - ETA: 2s - loss: 0.0809 - acc: 0.977 - ETA: 2s - loss: 0.0808 - acc: 0.976 - ETA: 2s - loss: 0.0813 - acc: 0.976 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.978 - ETA: 1s - loss: 0.0794 - acc: 0.977 - ETA: 1s - loss: 0.0781 - acc: 0.978 - ETA: 1s - loss: 0.0800 - acc: 0.978 - ETA: 1s - loss: 0.0795 - acc: 0.978 - ETA: 1s - loss: 0.0810 - acc: 0.977 - ETA: 1s - loss: 0.0803 - acc: 0.977 - ETA: 1s - loss: 0.0816 - acc: 0.977 - ETA: 1s - loss: 0.0808 - acc: 0.977 - ETA: 1s - loss: 0.0801 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.977 - ETA: 1s - loss: 0.0794 - acc: 0.977 - ETA: 1s - loss: 0.0789 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.976 - ETA: 1s - loss: 0.0806 - acc: 0.976 - ETA: 1s - loss: 0.0811 - acc: 0.976 - ETA: 1s - loss: 0.0803 - acc: 0.976 - ETA: 1s - loss: 0.0822 - acc: 0.976 - ETA: 1s - loss: 0.0832 - acc: 0.975 - ETA: 0s - loss: 0.0838 - acc: 0.975 - ETA: 0s - loss: 0.0842 - acc: 0.975 - ETA: 0s - loss: 0.0872 - acc: 0.975 - ETA: 0s - loss: 0.0868 - acc: 0.975 - ETA: 0s - loss: 0.0874 - acc: 0.974 - ETA: 0s - loss: 0.0884 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0887 - acc: 0.974 - ETA: 0s - loss: 0.0895 - acc: 0.973 - ETA: 0s - loss: 0.0896 - acc: 0.973 - ETA: 0s - loss: 0.0893 - acc: 0.973 - ETA: 0s - loss: 0.0900 - acc: 0.973 - ETA: 0s - loss: 0.0892 - acc: 0.973 - ETA: 0s - loss: 0.0890 - acc: 0.973 - ETA: 0s - loss: 0.0896 - acc: 0.973 - ETA: 0s - loss: 0.0898 - acc: 0.973 - ETA: 0s - loss: 0.0892 - acc: 0.9734Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0897 - acc: 0.9734 - val_loss: 0.6728 - val_acc: 0.8156\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0064 - acc: 1.000 - ETA: 4s - loss: 0.0284 - acc: 0.991 - ETA: 4s - loss: 0.0268 - acc: 0.995 - ETA: 3s - loss: 0.0402 - acc: 0.986 - ETA: 3s - loss: 0.0437 - acc: 0.982 - ETA: 3s - loss: 0.0422 - acc: 0.984 - ETA: 3s - loss: 0.0379 - acc: 0.986 - ETA: 3s - loss: 0.0421 - acc: 0.987 - ETA: 3s - loss: 0.0393 - acc: 0.988 - ETA: 3s - loss: 0.0376 - acc: 0.990 - ETA: 3s - loss: 0.0373 - acc: 0.990 - ETA: 3s - loss: 0.0358 - acc: 0.990 - ETA: 3s - loss: 0.0413 - acc: 0.989 - ETA: 3s - loss: 0.0412 - acc: 0.990 - ETA: 3s - loss: 0.0438 - acc: 0.989 - ETA: 3s - loss: 0.0451 - acc: 0.989 - ETA: 3s - loss: 0.0453 - acc: 0.989 - ETA: 3s - loss: 0.0456 - acc: 0.988 - ETA: 3s - loss: 0.0451 - acc: 0.988 - ETA: 2s - loss: 0.0476 - acc: 0.988 - ETA: 2s - loss: 0.0469 - acc: 0.988 - ETA: 2s - loss: 0.0460 - acc: 0.988 - ETA: 2s - loss: 0.0475 - acc: 0.987 - ETA: 2s - loss: 0.0503 - acc: 0.986 - ETA: 2s - loss: 0.0507 - acc: 0.986 - ETA: 2s - loss: 0.0516 - acc: 0.985 - ETA: 2s - loss: 0.0512 - acc: 0.985 - ETA: 2s - loss: 0.0502 - acc: 0.986 - ETA: 2s - loss: 0.0506 - acc: 0.986 - ETA: 2s - loss: 0.0502 - acc: 0.986 - ETA: 2s - loss: 0.0537 - acc: 0.985 - ETA: 2s - loss: 0.0557 - acc: 0.985 - ETA: 2s - loss: 0.0569 - acc: 0.985 - ETA: 2s - loss: 0.0571 - acc: 0.984 - ETA: 2s - loss: 0.0568 - acc: 0.984 - ETA: 2s - loss: 0.0571 - acc: 0.984 - ETA: 2s - loss: 0.0564 - acc: 0.984 - ETA: 2s - loss: 0.0558 - acc: 0.985 - ETA: 1s - loss: 0.0557 - acc: 0.985 - ETA: 1s - loss: 0.0549 - acc: 0.985 - ETA: 1s - loss: 0.0546 - acc: 0.985 - ETA: 1s - loss: 0.0560 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0555 - acc: 0.984 - ETA: 1s - loss: 0.0572 - acc: 0.984 - ETA: 1s - loss: 0.0576 - acc: 0.984 - ETA: 1s - loss: 0.0573 - acc: 0.984 - ETA: 1s - loss: 0.0574 - acc: 0.983 - ETA: 1s - loss: 0.0581 - acc: 0.983 - ETA: 1s - loss: 0.0586 - acc: 0.983 - ETA: 1s - loss: 0.0608 - acc: 0.983 - ETA: 1s - loss: 0.0627 - acc: 0.982 - ETA: 1s - loss: 0.0637 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0635 - acc: 0.982 - ETA: 1s - loss: 0.0631 - acc: 0.982 - ETA: 0s - loss: 0.0626 - acc: 0.982 - ETA: 0s - loss: 0.0626 - acc: 0.982 - ETA: 0s - loss: 0.0628 - acc: 0.982 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0636 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.981 - ETA: 0s - loss: 0.0624 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0627 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.9818Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0644 - acc: 0.9817 - val_loss: 0.7585 - val_acc: 0.8132\n",
      "Epoch 8/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0064 - acc: 1.000 - ETA: 3s - loss: 0.0121 - acc: 1.000 - ETA: 3s - loss: 0.0127 - acc: 1.000 - ETA: 3s - loss: 0.0150 - acc: 0.996 - ETA: 3s - loss: 0.0173 - acc: 0.997 - ETA: 3s - loss: 0.0207 - acc: 0.994 - ETA: 3s - loss: 0.0218 - acc: 0.995 - ETA: 3s - loss: 0.0218 - acc: 0.995 - ETA: 3s - loss: 0.0207 - acc: 0.996 - ETA: 3s - loss: 0.0206 - acc: 0.996 - ETA: 3s - loss: 0.0238 - acc: 0.995 - ETA: 3s - loss: 0.0263 - acc: 0.994 - ETA: 3s - loss: 0.0266 - acc: 0.995 - ETA: 3s - loss: 0.0272 - acc: 0.994 - ETA: 3s - loss: 0.0300 - acc: 0.993 - ETA: 2s - loss: 0.0333 - acc: 0.992 - ETA: 2s - loss: 0.0337 - acc: 0.992 - ETA: 2s - loss: 0.0330 - acc: 0.992 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0350 - acc: 0.991 - ETA: 2s - loss: 0.0357 - acc: 0.990 - ETA: 2s - loss: 0.0351 - acc: 0.991 - ETA: 2s - loss: 0.0368 - acc: 0.989 - ETA: 2s - loss: 0.0361 - acc: 0.990 - ETA: 2s - loss: 0.0365 - acc: 0.990 - ETA: 2s - loss: 0.0358 - acc: 0.990 - ETA: 2s - loss: 0.0358 - acc: 0.990 - ETA: 2s - loss: 0.0354 - acc: 0.990 - ETA: 2s - loss: 0.0355 - acc: 0.990 - ETA: 2s - loss: 0.0372 - acc: 0.989 - ETA: 2s - loss: 0.0362 - acc: 0.989 - ETA: 2s - loss: 0.0356 - acc: 0.989 - ETA: 2s - loss: 0.0359 - acc: 0.989 - ETA: 2s - loss: 0.0353 - acc: 0.990 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0344 - acc: 0.990 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0365 - acc: 0.990 - ETA: 1s - loss: 0.0369 - acc: 0.989 - ETA: 1s - loss: 0.0391 - acc: 0.989 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0394 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.988 - ETA: 1s - loss: 0.0415 - acc: 0.988 - ETA: 1s - loss: 0.0409 - acc: 0.988 - ETA: 1s - loss: 0.0411 - acc: 0.988 - ETA: 1s - loss: 0.0420 - acc: 0.988 - ETA: 1s - loss: 0.0423 - acc: 0.988 - ETA: 1s - loss: 0.0422 - acc: 0.988 - ETA: 1s - loss: 0.0417 - acc: 0.988 - ETA: 1s - loss: 0.0417 - acc: 0.988 - ETA: 0s - loss: 0.0426 - acc: 0.988 - ETA: 0s - loss: 0.0422 - acc: 0.988 - ETA: 0s - loss: 0.0418 - acc: 0.988 - ETA: 0s - loss: 0.0418 - acc: 0.988 - ETA: 0s - loss: 0.0416 - acc: 0.988 - ETA: 0s - loss: 0.0415 - acc: 0.988 - ETA: 0s - loss: 0.0416 - acc: 0.988 - ETA: 0s - loss: 0.0434 - acc: 0.988 - ETA: 0s - loss: 0.0442 - acc: 0.987 - ETA: 0s - loss: 0.0438 - acc: 0.987 - ETA: 0s - loss: 0.0446 - acc: 0.987 - ETA: 0s - loss: 0.0447 - acc: 0.987 - ETA: 0s - loss: 0.0443 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0471 - acc: 0.986 - ETA: 0s - loss: 0.0476 - acc: 0.986 - ETA: 0s - loss: 0.0473 - acc: 0.986 - ETA: 0s - loss: 0.0470 - acc: 0.986 - ETA: 0s - loss: 0.0470 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0469 - acc: 0.9868 - val_loss: 0.7041 - val_acc: 0.8072\n",
      "Epoch 9/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0036 - acc: 1.000 - ETA: 3s - loss: 0.0173 - acc: 1.000 - ETA: 3s - loss: 0.0217 - acc: 0.995 - ETA: 3s - loss: 0.0253 - acc: 0.996 - ETA: 3s - loss: 0.0226 - acc: 0.997 - ETA: 3s - loss: 0.0347 - acc: 0.995 - ETA: 3s - loss: 0.0313 - acc: 0.996 - ETA: 3s - loss: 0.0331 - acc: 0.994 - ETA: 3s - loss: 0.0308 - acc: 0.994 - ETA: 3s - loss: 0.0298 - acc: 0.994 - ETA: 3s - loss: 0.0315 - acc: 0.993 - ETA: 3s - loss: 0.0299 - acc: 0.994 - ETA: 3s - loss: 0.0312 - acc: 0.994 - ETA: 3s - loss: 0.0303 - acc: 0.994 - ETA: 3s - loss: 0.0298 - acc: 0.994 - ETA: 3s - loss: 0.0317 - acc: 0.993 - ETA: 2s - loss: 0.0313 - acc: 0.994 - ETA: 2s - loss: 0.0304 - acc: 0.994 - ETA: 2s - loss: 0.0293 - acc: 0.994 - ETA: 2s - loss: 0.0283 - acc: 0.995 - ETA: 2s - loss: 0.0285 - acc: 0.994 - ETA: 2s - loss: 0.0282 - acc: 0.995 - ETA: 2s - loss: 0.0278 - acc: 0.994 - ETA: 2s - loss: 0.0273 - acc: 0.995 - ETA: 2s - loss: 0.0274 - acc: 0.994 - ETA: 2s - loss: 0.0273 - acc: 0.994 - ETA: 2s - loss: 0.0284 - acc: 0.994 - ETA: 2s - loss: 0.0280 - acc: 0.994 - ETA: 2s - loss: 0.0276 - acc: 0.994 - ETA: 2s - loss: 0.0269 - acc: 0.994 - ETA: 2s - loss: 0.0272 - acc: 0.994 - ETA: 2s - loss: 0.0281 - acc: 0.993 - ETA: 2s - loss: 0.0286 - acc: 0.993 - ETA: 2s - loss: 0.0285 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.992 - ETA: 1s - loss: 0.0282 - acc: 0.993 - ETA: 1s - loss: 0.0287 - acc: 0.992 - ETA: 1s - loss: 0.0286 - acc: 0.993 - ETA: 1s - loss: 0.0283 - acc: 0.993 - ETA: 1s - loss: 0.0288 - acc: 0.992 - ETA: 1s - loss: 0.0285 - acc: 0.993 - ETA: 1s - loss: 0.0282 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0280 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0280 - acc: 0.993 - ETA: 1s - loss: 0.0277 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0279 - acc: 0.993 - ETA: 1s - loss: 0.0279 - acc: 0.993 - ETA: 0s - loss: 0.0285 - acc: 0.993 - ETA: 0s - loss: 0.0283 - acc: 0.993 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0294 - acc: 0.992 - ETA: 0s - loss: 0.0305 - acc: 0.992 - ETA: 0s - loss: 0.0305 - acc: 0.992 - ETA: 0s - loss: 0.0304 - acc: 0.992 - ETA: 0s - loss: 0.0310 - acc: 0.992 - ETA: 0s - loss: 0.0315 - acc: 0.992 - ETA: 0s - loss: 0.0321 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0322 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0326 - acc: 0.991 - ETA: 0s - loss: 0.0327 - acc: 0.991 - ETA: 0s - loss: 0.0330 - acc: 0.991 - ETA: 0s - loss: 0.0329 - acc: 0.991 - ETA: 0s - loss: 0.0336 - acc: 0.9917Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7821 - val_acc: 0.8096\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0033 - acc: 1.000 - ETA: 3s - loss: 0.0183 - acc: 0.991 - ETA: 3s - loss: 0.0136 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.993 - ETA: 3s - loss: 0.0179 - acc: 0.992 - ETA: 3s - loss: 0.0174 - acc: 0.994 - ETA: 3s - loss: 0.0172 - acc: 0.995 - ETA: 3s - loss: 0.0162 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 3s - loss: 0.0167 - acc: 0.995 - ETA: 3s - loss: 0.0174 - acc: 0.995 - ETA: 3s - loss: 0.0170 - acc: 0.994 - ETA: 3s - loss: 0.0160 - acc: 0.995 - ETA: 3s - loss: 0.0160 - acc: 0.994 - ETA: 3s - loss: 0.0172 - acc: 0.994 - ETA: 2s - loss: 0.0175 - acc: 0.994 - ETA: 2s - loss: 0.0179 - acc: 0.994 - ETA: 2s - loss: 0.0174 - acc: 0.994 - ETA: 2s - loss: 0.0177 - acc: 0.994 - ETA: 2s - loss: 0.0171 - acc: 0.994 - ETA: 2s - loss: 0.0183 - acc: 0.994 - ETA: 2s - loss: 0.0216 - acc: 0.994 - ETA: 2s - loss: 0.0213 - acc: 0.994 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0243 - acc: 0.993 - ETA: 2s - loss: 0.0238 - acc: 0.994 - ETA: 2s - loss: 0.0232 - acc: 0.994 - ETA: 2s - loss: 0.0237 - acc: 0.994 - ETA: 2s - loss: 0.0239 - acc: 0.993 - ETA: 2s - loss: 0.0234 - acc: 0.994 - ETA: 2s - loss: 0.0233 - acc: 0.993 - ETA: 2s - loss: 0.0233 - acc: 0.993 - ETA: 2s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0229 - acc: 0.994 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0246 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.994 - ETA: 1s - loss: 0.0241 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0240 - acc: 0.994 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0246 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0241 - acc: 0.993 - ETA: 1s - loss: 0.0238 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0242 - acc: 0.993 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0247 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0244 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.9929Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0259 - acc: 0.9927 - val_loss: 0.7659 - val_acc: 0.8192\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0168 - acc: 1.000 - ETA: 3s - loss: 0.0128 - acc: 0.991 - ETA: 3s - loss: 0.0143 - acc: 0.995 - ETA: 3s - loss: 0.0107 - acc: 0.996 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0117 - acc: 0.996 - ETA: 3s - loss: 0.0111 - acc: 0.996 - ETA: 3s - loss: 0.0100 - acc: 0.997 - ETA: 3s - loss: 0.0121 - acc: 0.996 - ETA: 3s - loss: 0.0118 - acc: 0.996 - ETA: 3s - loss: 0.0115 - acc: 0.997 - ETA: 3s - loss: 0.0132 - acc: 0.995 - ETA: 3s - loss: 0.0190 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.994 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.994 - ETA: 2s - loss: 0.0176 - acc: 0.994 - ETA: 2s - loss: 0.0193 - acc: 0.994 - ETA: 2s - loss: 0.0194 - acc: 0.994 - ETA: 2s - loss: 0.0186 - acc: 0.994 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0191 - acc: 0.994 - ETA: 2s - loss: 0.0189 - acc: 0.994 - ETA: 2s - loss: 0.0185 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0195 - acc: 0.994 - ETA: 2s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0205 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.9938Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0222 - acc: 0.9937 - val_loss: 0.7574 - val_acc: 0.8251\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0038 - acc: 1.000 - ETA: 3s - loss: 0.0093 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0060 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0077 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.999 - ETA: 3s - loss: 0.0066 - acc: 0.999 - ETA: 3s - loss: 0.0064 - acc: 0.999 - ETA: 3s - loss: 0.0070 - acc: 0.999 - ETA: 3s - loss: 0.0071 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 3s - loss: 0.0076 - acc: 0.999 - ETA: 3s - loss: 0.0078 - acc: 0.999 - ETA: 3s - loss: 0.0078 - acc: 0.999 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.999 - ETA: 2s - loss: 0.0100 - acc: 0.999 - ETA: 2s - loss: 0.0099 - acc: 0.999 - ETA: 2s - loss: 0.0096 - acc: 0.999 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 2s - loss: 0.0130 - acc: 0.997 - ETA: 2s - loss: 0.0129 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 2s - loss: 0.0124 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0142 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0165 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.9956Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0175 - acc: 0.9955 - val_loss: 0.7644 - val_acc: 0.8240\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0094 - acc: 0.995 - ETA: 3s - loss: 0.0085 - acc: 0.996 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0102 - acc: 0.996 - ETA: 3s - loss: 0.0093 - acc: 0.996 - ETA: 3s - loss: 0.0082 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0080 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.997 - ETA: 3s - loss: 0.0078 - acc: 0.997 - ETA: 3s - loss: 0.0073 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 2s - loss: 0.0081 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.996 - ETA: 2s - loss: 0.0090 - acc: 0.996 - ETA: 2s - loss: 0.0088 - acc: 0.996 - ETA: 2s - loss: 0.0087 - acc: 0.996 - ETA: 2s - loss: 0.0103 - acc: 0.995 - ETA: 2s - loss: 0.0101 - acc: 0.995 - ETA: 2s - loss: 0.0102 - acc: 0.995 - ETA: 2s - loss: 0.0105 - acc: 0.995 - ETA: 2s - loss: 0.0104 - acc: 0.995 - ETA: 2s - loss: 0.0103 - acc: 0.995 - ETA: 2s - loss: 0.0101 - acc: 0.995 - ETA: 2s - loss: 0.0114 - acc: 0.995 - ETA: 2s - loss: 0.0113 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.995 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.9958Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0152 - acc: 0.9958 - val_loss: 0.8223 - val_acc: 0.8156\n",
      "Epoch 14/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0290 - acc: 0.991 - ETA: 3s - loss: 0.0162 - acc: 0.995 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0105 - acc: 0.997 - ETA: 3s - loss: 0.0113 - acc: 0.998 - ETA: 3s - loss: 0.0106 - acc: 0.998 - ETA: 3s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.997 - ETA: 3s - loss: 0.0123 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0103 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0093 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 2s - loss: 0.0102 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0118 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.997 - ETA: 1s - loss: 0.0116 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0102 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.9973Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0111 - acc: 0.9973 - val_loss: 0.8462 - val_acc: 0.8144\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 0.996 - ETA: 3s - loss: 0.0052 - acc: 0.997 - ETA: 3s - loss: 0.0044 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0036 - acc: 0.998 - ETA: 3s - loss: 0.0034 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0034 - acc: 0.999 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.9982Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0102 - acc: 0.9981 - val_loss: 0.8691 - val_acc: 0.8096\n",
      "Epoch 16/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.1485 - acc: 0.950 - ETA: 4s - loss: 0.0405 - acc: 0.990 - ETA: 4s - loss: 0.0227 - acc: 0.994 - ETA: 4s - loss: 0.0160 - acc: 0.996 - ETA: 4s - loss: 0.0128 - acc: 0.997 - ETA: 4s - loss: 0.0105 - acc: 0.997 - ETA: 4s - loss: 0.0089 - acc: 0.998 - ETA: 3s - loss: 0.0081 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 3s - loss: 0.0065 - acc: 0.998 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0072 - acc: 0.997 - ETA: 3s - loss: 0.0068 - acc: 0.997 - ETA: 3s - loss: 0.0065 - acc: 0.997 - ETA: 3s - loss: 0.0062 - acc: 0.997 - ETA: 3s - loss: 0.0058 - acc: 0.997 - ETA: 3s - loss: 0.0056 - acc: 0.998 - ETA: 3s - loss: 0.0055 - acc: 0.998 - ETA: 3s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0046 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.9983Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0069 - acc: 0.9984 - val_loss: 0.9158 - val_acc: 0.8180\n",
      "Epoch 17/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0150 - acc: 1.000 - ETA: 3s - loss: 0.0374 - acc: 0.991 - ETA: 3s - loss: 0.0205 - acc: 0.995 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0122 - acc: 0.997 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 3s - loss: 0.0064 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.999 - ETA: 3s - loss: 0.0058 - acc: 0.999 - ETA: 3s - loss: 0.0087 - acc: 0.998 - ETA: 3s - loss: 0.0082 - acc: 0.998 - ETA: 3s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.9977Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.9053 - val_acc: 0.8156\n",
      "Epoch 18/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 1.3594e-04 - acc: 1.000 - ETA: 4s - loss: 5.2051e-04 - acc: 1.000 - ETA: 4s - loss: 0.0125 - acc: 0.9944    - ETA: 4s - loss: 0.0089 - acc: 0.996 - ETA: 4s - loss: 0.0068 - acc: 0.997 - ETA: 4s - loss: 0.0179 - acc: 0.995 - ETA: 3s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0115 - acc: 0.997 - ETA: 3s - loss: 0.0104 - acc: 0.997 - ETA: 3s - loss: 0.0115 - acc: 0.996 - ETA: 3s - loss: 0.0103 - acc: 0.996 - ETA: 3s - loss: 0.0097 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0088 - acc: 0.997 - ETA: 3s - loss: 0.0083 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0075 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0068 - acc: 0.997 - ETA: 3s - loss: 0.0091 - acc: 0.997 - ETA: 3s - loss: 0.0088 - acc: 0.997 - ETA: 2s - loss: 0.0085 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.998 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 2s - loss: 0.0080 - acc: 0.997 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0095 - acc: 0.997 - ETA: 2s - loss: 0.0093 - acc: 0.997 - ETA: 2s - loss: 0.0091 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.9982Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0067 - acc: 0.9982 - val_loss: 0.9179 - val_acc: 0.8096\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 4s - loss: 2.2950e-04 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.0000    - ETA: 4s - loss: 9.4197e-04 - acc: 1.000 - ETA: 4s - loss: 8.0986e-04 - acc: 1.000 - ETA: 4s - loss: 8.4522e-04 - acc: 1.000 - ETA: 4s - loss: 7.2525e-04 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.0000    - ETA: 4s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 9.8181e-04 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.0000    - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 9.3605e-04 - acc: 1.000 - ETA: 3s - loss: 8.9276e-04 - acc: 1.000 - ETA: 3s - loss: 8.7360e-04 - acc: 1.000 - ETA: 3s - loss: 8.3012e-04 - acc: 1.000 - ETA: 3s - loss: 8.0113e-04 - acc: 1.000 - ETA: 3s - loss: 7.6481e-04 - acc: 1.000 - ETA: 3s - loss: 7.5926e-04 - acc: 1.000 - ETA: 3s - loss: 7.7357e-04 - acc: 1.000 - ETA: 3s - loss: 7.6125e-04 - acc: 1.000 - ETA: 3s - loss: 7.7173e-04 - acc: 1.000 - ETA: 3s - loss: 7.9504e-04 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 0.9994    - ETA: 3s - loss: 0.0029 - acc: 0.999 - ETA: 3s - loss: 0.0028 - acc: 0.999 - ETA: 3s - loss: 0.0028 - acc: 0.999 - ETA: 3s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0047 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0038 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.9985Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0064 - acc: 0.9985 - val_loss: 0.9253 - val_acc: 0.8192\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 7.9375e-05 - acc: 1.000 - ETA: 3s - loss: 0.0245 - acc: 0.9917    - ETA: 4s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0110 - acc: 0.996 - ETA: 4s - loss: 0.0087 - acc: 0.997 - ETA: 4s - loss: 0.0122 - acc: 0.995 - ETA: 3s - loss: 0.0105 - acc: 0.996 - ETA: 3s - loss: 0.0091 - acc: 0.996 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 3s - loss: 0.0149 - acc: 0.996 - ETA: 3s - loss: 0.0140 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0130 - acc: 0.996 - ETA: 3s - loss: 0.0123 - acc: 0.996 - ETA: 3s - loss: 0.0131 - acc: 0.995 - ETA: 3s - loss: 0.0124 - acc: 0.996 - ETA: 3s - loss: 0.0118 - acc: 0.996 - ETA: 3s - loss: 0.0113 - acc: 0.996 - ETA: 3s - loss: 0.0108 - acc: 0.996 - ETA: 3s - loss: 0.0104 - acc: 0.996 - ETA: 3s - loss: 0.0100 - acc: 0.996 - ETA: 3s - loss: 0.0096 - acc: 0.997 - ETA: 3s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0084 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0081 - acc: 0.997 - ETA: 2s - loss: 0.0078 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.997 - ETA: 2s - loss: 0.0072 - acc: 0.997 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0072 - acc: 0.997 - ETA: 2s - loss: 0.0070 - acc: 0.997 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.9985Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.9970 - val_acc: 0.8096\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_5 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 131s - loss: 5.0593 - acc: 0.050 - ETA: 38s - loss: 5.8533 - acc: 0.075 - ETA: 24s - loss: 5.7769 - acc: 0.06 - ETA: 19s - loss: 5.4237 - acc: 0.08 - ETA: 16s - loss: 5.1456 - acc: 0.11 - ETA: 14s - loss: 4.7881 - acc: 0.15 - ETA: 12s - loss: 4.5018 - acc: 0.20 - ETA: 11s - loss: 4.3151 - acc: 0.23 - ETA: 10s - loss: 4.0944 - acc: 0.27 - ETA: 10s - loss: 3.9208 - acc: 0.29 - ETA: 9s - loss: 3.7574 - acc: 0.3274 - ETA: 9s - loss: 3.6136 - acc: 0.338 - ETA: 8s - loss: 3.5159 - acc: 0.351 - ETA: 8s - loss: 3.4118 - acc: 0.358 - ETA: 8s - loss: 3.2685 - acc: 0.382 - ETA: 7s - loss: 3.1548 - acc: 0.398 - ETA: 7s - loss: 3.0538 - acc: 0.417 - ETA: 7s - loss: 2.9727 - acc: 0.426 - ETA: 7s - loss: 2.8786 - acc: 0.440 - ETA: 7s - loss: 2.8054 - acc: 0.448 - ETA: 6s - loss: 2.7337 - acc: 0.459 - ETA: 6s - loss: 2.6503 - acc: 0.471 - ETA: 6s - loss: 2.5782 - acc: 0.482 - ETA: 6s - loss: 2.5258 - acc: 0.490 - ETA: 6s - loss: 2.4660 - acc: 0.502 - ETA: 6s - loss: 2.4159 - acc: 0.507 - ETA: 6s - loss: 2.3813 - acc: 0.514 - ETA: 5s - loss: 2.3319 - acc: 0.519 - ETA: 5s - loss: 2.2833 - acc: 0.526 - ETA: 5s - loss: 2.2362 - acc: 0.533 - ETA: 5s - loss: 2.1978 - acc: 0.538 - ETA: 5s - loss: 2.1556 - acc: 0.546 - ETA: 5s - loss: 2.1132 - acc: 0.552 - ETA: 5s - loss: 2.0791 - acc: 0.555 - ETA: 5s - loss: 2.0423 - acc: 0.560 - ETA: 5s - loss: 2.0071 - acc: 0.565 - ETA: 5s - loss: 1.9725 - acc: 0.570 - ETA: 4s - loss: 1.9408 - acc: 0.576 - ETA: 4s - loss: 1.9081 - acc: 0.583 - ETA: 4s - loss: 1.8847 - acc: 0.585 - ETA: 4s - loss: 1.8630 - acc: 0.588 - ETA: 4s - loss: 1.8331 - acc: 0.592 - ETA: 4s - loss: 1.8016 - acc: 0.598 - ETA: 4s - loss: 1.7967 - acc: 0.598 - ETA: 4s - loss: 1.7745 - acc: 0.603 - ETA: 4s - loss: 1.7519 - acc: 0.606 - ETA: 4s - loss: 1.7263 - acc: 0.611 - ETA: 4s - loss: 1.7180 - acc: 0.611 - ETA: 4s - loss: 1.7056 - acc: 0.613 - ETA: 3s - loss: 1.6878 - acc: 0.616 - ETA: 3s - loss: 1.6663 - acc: 0.620 - ETA: 3s - loss: 1.6461 - acc: 0.622 - ETA: 3s - loss: 1.6318 - acc: 0.625 - ETA: 3s - loss: 1.6145 - acc: 0.628 - ETA: 3s - loss: 1.6003 - acc: 0.629 - ETA: 3s - loss: 1.5865 - acc: 0.631 - ETA: 3s - loss: 1.5727 - acc: 0.634 - ETA: 3s - loss: 1.5561 - acc: 0.638 - ETA: 3s - loss: 1.5454 - acc: 0.639 - ETA: 3s - loss: 1.5271 - acc: 0.643 - ETA: 3s - loss: 1.5077 - acc: 0.646 - ETA: 3s - loss: 1.4978 - acc: 0.648 - ETA: 3s - loss: 1.4904 - acc: 0.649 - ETA: 2s - loss: 1.4815 - acc: 0.651 - ETA: 2s - loss: 1.4673 - acc: 0.652 - ETA: 2s - loss: 1.4527 - acc: 0.656 - ETA: 2s - loss: 1.4424 - acc: 0.657 - ETA: 2s - loss: 1.4368 - acc: 0.658 - ETA: 2s - loss: 1.4262 - acc: 0.660 - ETA: 2s - loss: 1.4153 - acc: 0.661 - ETA: 2s - loss: 1.4104 - acc: 0.662 - ETA: 2s - loss: 1.4027 - acc: 0.664 - ETA: 2s - loss: 1.3912 - acc: 0.667 - ETA: 2s - loss: 1.3833 - acc: 0.668 - ETA: 2s - loss: 1.3734 - acc: 0.670 - ETA: 2s - loss: 1.3678 - acc: 0.671 - ETA: 2s - loss: 1.3628 - acc: 0.671 - ETA: 2s - loss: 1.3548 - acc: 0.673 - ETA: 2s - loss: 1.3421 - acc: 0.676 - ETA: 1s - loss: 1.3334 - acc: 0.677 - ETA: 1s - loss: 1.3229 - acc: 0.679 - ETA: 1s - loss: 1.3170 - acc: 0.680 - ETA: 1s - loss: 1.3084 - acc: 0.681 - ETA: 1s - loss: 1.3004 - acc: 0.682 - ETA: 1s - loss: 1.2945 - acc: 0.682 - ETA: 1s - loss: 1.2868 - acc: 0.683 - ETA: 1s - loss: 1.2793 - acc: 0.685 - ETA: 1s - loss: 1.2765 - acc: 0.685 - ETA: 1s - loss: 1.2699 - acc: 0.687 - ETA: 1s - loss: 1.2619 - acc: 0.688 - ETA: 1s - loss: 1.2579 - acc: 0.689 - ETA: 1s - loss: 1.2512 - acc: 0.690 - ETA: 1s - loss: 1.2440 - acc: 0.691 - ETA: 1s - loss: 1.2380 - acc: 0.692 - ETA: 1s - loss: 1.2315 - acc: 0.694 - ETA: 0s - loss: 1.2241 - acc: 0.695 - ETA: 0s - loss: 1.2195 - acc: 0.696 - ETA: 0s - loss: 1.2098 - acc: 0.698 - ETA: 0s - loss: 1.2052 - acc: 0.699 - ETA: 0s - loss: 1.1981 - acc: 0.701 - ETA: 0s - loss: 1.1934 - acc: 0.702 - ETA: 0s - loss: 1.1909 - acc: 0.702 - ETA: 0s - loss: 1.1828 - acc: 0.704 - ETA: 0s - loss: 1.1758 - acc: 0.706 - ETA: 0s - loss: 1.1691 - acc: 0.707 - ETA: 0s - loss: 1.1660 - acc: 0.707 - ETA: 0s - loss: 1.1608 - acc: 0.708 - ETA: 0s - loss: 1.1554 - acc: 0.709 - ETA: 0s - loss: 1.1525 - acc: 0.710 - ETA: 0s - loss: 1.1490 - acc: 0.710 - ETA: 0s - loss: 1.1461 - acc: 0.711 - ETA: 0s - loss: 1.1402 - acc: 0.7130Epoch 00000: val_loss improved from inf to 0.66297, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.1385 - acc: 0.7129 - val_loss: 0.6630 - val_acc: 0.8060\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.3795 - acc: 0.800 - ETA: 6s - loss: 0.4965 - acc: 0.812 - ETA: 5s - loss: 0.5998 - acc: 0.814 - ETA: 5s - loss: 0.4872 - acc: 0.845 - ETA: 5s - loss: 0.5233 - acc: 0.838 - ETA: 5s - loss: 0.5207 - acc: 0.840 - ETA: 5s - loss: 0.5079 - acc: 0.847 - ETA: 5s - loss: 0.5053 - acc: 0.847 - ETA: 5s - loss: 0.4895 - acc: 0.850 - ETA: 5s - loss: 0.4719 - acc: 0.857 - ETA: 5s - loss: 0.4616 - acc: 0.861 - ETA: 5s - loss: 0.4443 - acc: 0.866 - ETA: 5s - loss: 0.4365 - acc: 0.868 - ETA: 5s - loss: 0.4337 - acc: 0.870 - ETA: 5s - loss: 0.4442 - acc: 0.867 - ETA: 5s - loss: 0.4644 - acc: 0.860 - ETA: 5s - loss: 0.4595 - acc: 0.864 - ETA: 5s - loss: 0.4541 - acc: 0.862 - ETA: 5s - loss: 0.4453 - acc: 0.867 - ETA: 5s - loss: 0.4389 - acc: 0.867 - ETA: 5s - loss: 0.4460 - acc: 0.864 - ETA: 5s - loss: 0.4427 - acc: 0.866 - ETA: 4s - loss: 0.4566 - acc: 0.862 - ETA: 4s - loss: 0.4685 - acc: 0.858 - ETA: 4s - loss: 0.4667 - acc: 0.858 - ETA: 4s - loss: 0.4668 - acc: 0.858 - ETA: 4s - loss: 0.4714 - acc: 0.858 - ETA: 4s - loss: 0.4804 - acc: 0.854 - ETA: 4s - loss: 0.4753 - acc: 0.855 - ETA: 4s - loss: 0.4702 - acc: 0.856 - ETA: 4s - loss: 0.4706 - acc: 0.857 - ETA: 4s - loss: 0.4738 - acc: 0.856 - ETA: 4s - loss: 0.4708 - acc: 0.858 - ETA: 4s - loss: 0.4702 - acc: 0.858 - ETA: 4s - loss: 0.4723 - acc: 0.858 - ETA: 4s - loss: 0.4704 - acc: 0.859 - ETA: 4s - loss: 0.4721 - acc: 0.858 - ETA: 4s - loss: 0.4754 - acc: 0.858 - ETA: 4s - loss: 0.4712 - acc: 0.859 - ETA: 4s - loss: 0.4764 - acc: 0.858 - ETA: 4s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4878 - acc: 0.856 - ETA: 3s - loss: 0.4843 - acc: 0.857 - ETA: 3s - loss: 0.4776 - acc: 0.859 - ETA: 3s - loss: 0.4757 - acc: 0.858 - ETA: 3s - loss: 0.4789 - acc: 0.858 - ETA: 3s - loss: 0.4786 - acc: 0.858 - ETA: 3s - loss: 0.4811 - acc: 0.857 - ETA: 3s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4808 - acc: 0.859 - ETA: 3s - loss: 0.4815 - acc: 0.858 - ETA: 3s - loss: 0.4820 - acc: 0.857 - ETA: 3s - loss: 0.4806 - acc: 0.857 - ETA: 3s - loss: 0.4775 - acc: 0.859 - ETA: 3s - loss: 0.4821 - acc: 0.856 - ETA: 3s - loss: 0.4850 - acc: 0.856 - ETA: 3s - loss: 0.4896 - acc: 0.855 - ETA: 3s - loss: 0.4839 - acc: 0.855 - ETA: 2s - loss: 0.4796 - acc: 0.856 - ETA: 2s - loss: 0.4784 - acc: 0.856 - ETA: 2s - loss: 0.4794 - acc: 0.856 - ETA: 2s - loss: 0.4820 - acc: 0.855 - ETA: 2s - loss: 0.4863 - acc: 0.855 - ETA: 2s - loss: 0.4846 - acc: 0.856 - ETA: 2s - loss: 0.4835 - acc: 0.856 - ETA: 2s - loss: 0.4847 - acc: 0.857 - ETA: 2s - loss: 0.4834 - acc: 0.857 - ETA: 2s - loss: 0.4827 - acc: 0.857 - ETA: 2s - loss: 0.4839 - acc: 0.857 - ETA: 2s - loss: 0.4831 - acc: 0.856 - ETA: 2s - loss: 0.4834 - acc: 0.856 - ETA: 2s - loss: 0.4790 - acc: 0.857 - ETA: 2s - loss: 0.4811 - acc: 0.857 - ETA: 2s - loss: 0.4792 - acc: 0.857 - ETA: 2s - loss: 0.4805 - acc: 0.857 - ETA: 2s - loss: 0.4784 - acc: 0.858 - ETA: 1s - loss: 0.4812 - acc: 0.857 - ETA: 1s - loss: 0.4809 - acc: 0.857 - ETA: 1s - loss: 0.4780 - acc: 0.857 - ETA: 1s - loss: 0.4781 - acc: 0.857 - ETA: 1s - loss: 0.4792 - acc: 0.857 - ETA: 1s - loss: 0.4808 - acc: 0.857 - ETA: 1s - loss: 0.4818 - acc: 0.856 - ETA: 1s - loss: 0.4815 - acc: 0.856 - ETA: 1s - loss: 0.4836 - acc: 0.855 - ETA: 1s - loss: 0.4811 - acc: 0.856 - ETA: 1s - loss: 0.4837 - acc: 0.855 - ETA: 1s - loss: 0.4850 - acc: 0.855 - ETA: 1s - loss: 0.4826 - acc: 0.856 - ETA: 1s - loss: 0.4798 - acc: 0.856 - ETA: 1s - loss: 0.4798 - acc: 0.857 - ETA: 1s - loss: 0.4786 - acc: 0.857 - ETA: 1s - loss: 0.4763 - acc: 0.858 - ETA: 0s - loss: 0.4767 - acc: 0.858 - ETA: 0s - loss: 0.4744 - acc: 0.858 - ETA: 0s - loss: 0.4715 - acc: 0.859 - ETA: 0s - loss: 0.4719 - acc: 0.859 - ETA: 0s - loss: 0.4728 - acc: 0.859 - ETA: 0s - loss: 0.4737 - acc: 0.858 - ETA: 0s - loss: 0.4750 - acc: 0.858 - ETA: 0s - loss: 0.4723 - acc: 0.859 - ETA: 0s - loss: 0.4731 - acc: 0.858 - ETA: 0s - loss: 0.4725 - acc: 0.858 - ETA: 0s - loss: 0.4728 - acc: 0.858 - ETA: 0s - loss: 0.4729 - acc: 0.858 - ETA: 0s - loss: 0.4734 - acc: 0.858 - ETA: 0s - loss: 0.4732 - acc: 0.858 - ETA: 0s - loss: 0.4746 - acc: 0.858 - ETA: 0s - loss: 0.4759 - acc: 0.857 - ETA: 0s - loss: 0.4742 - acc: 0.858 - ETA: 0s - loss: 0.4742 - acc: 0.857 - ETA: 0s - loss: 0.4743 - acc: 0.8571Epoch 00001: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.4749 - acc: 0.8569 - val_loss: 0.7624 - val_acc: 0.8000\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.2205 - acc: 0.950 - ETA: 7s - loss: 0.2452 - acc: 0.937 - ETA: 7s - loss: 0.2715 - acc: 0.933 - ETA: 7s - loss: 0.2429 - acc: 0.927 - ETA: 7s - loss: 0.2949 - acc: 0.920 - ETA: 6s - loss: 0.2964 - acc: 0.920 - ETA: 6s - loss: 0.3023 - acc: 0.916 - ETA: 6s - loss: 0.2794 - acc: 0.919 - ETA: 6s - loss: 0.2947 - acc: 0.908 - ETA: 6s - loss: 0.3240 - acc: 0.900 - ETA: 6s - loss: 0.3127 - acc: 0.901 - ETA: 5s - loss: 0.3121 - acc: 0.900 - ETA: 5s - loss: 0.3070 - acc: 0.902 - ETA: 5s - loss: 0.3228 - acc: 0.898 - ETA: 5s - loss: 0.3378 - acc: 0.900 - ETA: 5s - loss: 0.3497 - acc: 0.897 - ETA: 5s - loss: 0.3503 - acc: 0.895 - ETA: 5s - loss: 0.3397 - acc: 0.899 - ETA: 5s - loss: 0.3409 - acc: 0.899 - ETA: 5s - loss: 0.3327 - acc: 0.900 - ETA: 5s - loss: 0.3328 - acc: 0.899 - ETA: 5s - loss: 0.3370 - acc: 0.897 - ETA: 5s - loss: 0.3308 - acc: 0.899 - ETA: 5s - loss: 0.3263 - acc: 0.900 - ETA: 4s - loss: 0.3177 - acc: 0.902 - ETA: 4s - loss: 0.3154 - acc: 0.900 - ETA: 4s - loss: 0.3175 - acc: 0.898 - ETA: 4s - loss: 0.3177 - acc: 0.897 - ETA: 4s - loss: 0.3246 - acc: 0.895 - ETA: 4s - loss: 0.3278 - acc: 0.896 - ETA: 4s - loss: 0.3302 - acc: 0.895 - ETA: 4s - loss: 0.3370 - acc: 0.893 - ETA: 4s - loss: 0.3378 - acc: 0.893 - ETA: 4s - loss: 0.3359 - acc: 0.892 - ETA: 4s - loss: 0.3355 - acc: 0.893 - ETA: 4s - loss: 0.3322 - acc: 0.894 - ETA: 4s - loss: 0.3335 - acc: 0.895 - ETA: 4s - loss: 0.3305 - acc: 0.895 - ETA: 4s - loss: 0.3267 - acc: 0.896 - ETA: 4s - loss: 0.3254 - acc: 0.895 - ETA: 3s - loss: 0.3214 - acc: 0.896 - ETA: 3s - loss: 0.3197 - acc: 0.897 - ETA: 3s - loss: 0.3220 - acc: 0.896 - ETA: 3s - loss: 0.3257 - acc: 0.895 - ETA: 3s - loss: 0.3290 - acc: 0.893 - ETA: 3s - loss: 0.3280 - acc: 0.894 - ETA: 3s - loss: 0.3305 - acc: 0.894 - ETA: 3s - loss: 0.3348 - acc: 0.892 - ETA: 3s - loss: 0.3365 - acc: 0.893 - ETA: 3s - loss: 0.3348 - acc: 0.893 - ETA: 3s - loss: 0.3353 - acc: 0.894 - ETA: 3s - loss: 0.3410 - acc: 0.893 - ETA: 3s - loss: 0.3376 - acc: 0.893 - ETA: 3s - loss: 0.3430 - acc: 0.892 - ETA: 3s - loss: 0.3426 - acc: 0.892 - ETA: 3s - loss: 0.3431 - acc: 0.892 - ETA: 3s - loss: 0.3461 - acc: 0.891 - ETA: 3s - loss: 0.3518 - acc: 0.890 - ETA: 2s - loss: 0.3526 - acc: 0.890 - ETA: 2s - loss: 0.3478 - acc: 0.891 - ETA: 2s - loss: 0.3488 - acc: 0.892 - ETA: 2s - loss: 0.3445 - acc: 0.893 - ETA: 2s - loss: 0.3475 - acc: 0.892 - ETA: 2s - loss: 0.3444 - acc: 0.893 - ETA: 2s - loss: 0.3437 - acc: 0.893 - ETA: 2s - loss: 0.3457 - acc: 0.892 - ETA: 2s - loss: 0.3446 - acc: 0.892 - ETA: 2s - loss: 0.3482 - acc: 0.891 - ETA: 2s - loss: 0.3472 - acc: 0.891 - ETA: 2s - loss: 0.3505 - acc: 0.891 - ETA: 2s - loss: 0.3515 - acc: 0.890 - ETA: 2s - loss: 0.3537 - acc: 0.889 - ETA: 2s - loss: 0.3544 - acc: 0.889 - ETA: 2s - loss: 0.3528 - acc: 0.889 - ETA: 2s - loss: 0.3512 - acc: 0.890 - ETA: 2s - loss: 0.3506 - acc: 0.890 - ETA: 2s - loss: 0.3508 - acc: 0.891 - ETA: 1s - loss: 0.3522 - acc: 0.891 - ETA: 1s - loss: 0.3515 - acc: 0.891 - ETA: 1s - loss: 0.3508 - acc: 0.891 - ETA: 1s - loss: 0.3517 - acc: 0.891 - ETA: 1s - loss: 0.3511 - acc: 0.891 - ETA: 1s - loss: 0.3528 - acc: 0.892 - ETA: 1s - loss: 0.3564 - acc: 0.891 - ETA: 1s - loss: 0.3572 - acc: 0.890 - ETA: 1s - loss: 0.3587 - acc: 0.890 - ETA: 1s - loss: 0.3564 - acc: 0.891 - ETA: 1s - loss: 0.3553 - acc: 0.891 - ETA: 1s - loss: 0.3553 - acc: 0.891 - ETA: 1s - loss: 0.3580 - acc: 0.891 - ETA: 1s - loss: 0.3580 - acc: 0.890 - ETA: 1s - loss: 0.3591 - acc: 0.890 - ETA: 1s - loss: 0.3592 - acc: 0.890 - ETA: 1s - loss: 0.3595 - acc: 0.890 - ETA: 0s - loss: 0.3600 - acc: 0.890 - ETA: 0s - loss: 0.3586 - acc: 0.890 - ETA: 0s - loss: 0.3596 - acc: 0.889 - ETA: 0s - loss: 0.3604 - acc: 0.889 - ETA: 0s - loss: 0.3585 - acc: 0.890 - ETA: 0s - loss: 0.3577 - acc: 0.890 - ETA: 0s - loss: 0.3581 - acc: 0.890 - ETA: 0s - loss: 0.3597 - acc: 0.889 - ETA: 0s - loss: 0.3619 - acc: 0.889 - ETA: 0s - loss: 0.3618 - acc: 0.889 - ETA: 0s - loss: 0.3631 - acc: 0.889 - ETA: 0s - loss: 0.3616 - acc: 0.889 - ETA: 0s - loss: 0.3612 - acc: 0.889 - ETA: 0s - loss: 0.3624 - acc: 0.889 - ETA: 0s - loss: 0.3622 - acc: 0.889 - ETA: 0s - loss: 0.3636 - acc: 0.889 - ETA: 0s - loss: 0.3626 - acc: 0.890 - ETA: 0s - loss: 0.3625 - acc: 0.8901Epoch 00002: val_loss improved from 0.66297 to 0.64605, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.3618 - acc: 0.8901 - val_loss: 0.6461 - val_acc: 0.8539\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.4420 - acc: 0.900 - ETA: 6s - loss: 0.3406 - acc: 0.887 - ETA: 6s - loss: 0.2695 - acc: 0.900 - ETA: 6s - loss: 0.3057 - acc: 0.885 - ETA: 6s - loss: 0.2815 - acc: 0.884 - ETA: 6s - loss: 0.2758 - acc: 0.893 - ETA: 6s - loss: 0.2677 - acc: 0.900 - ETA: 5s - loss: 0.2514 - acc: 0.904 - ETA: 5s - loss: 0.2734 - acc: 0.904 - ETA: 5s - loss: 0.2700 - acc: 0.907 - ETA: 5s - loss: 0.2570 - acc: 0.911 - ETA: 5s - loss: 0.2507 - acc: 0.913 - ETA: 5s - loss: 0.2461 - acc: 0.912 - ETA: 5s - loss: 0.2421 - acc: 0.912 - ETA: 5s - loss: 0.2413 - acc: 0.914 - ETA: 5s - loss: 0.2383 - acc: 0.914 - ETA: 5s - loss: 0.2347 - acc: 0.915 - ETA: 5s - loss: 0.2402 - acc: 0.915 - ETA: 5s - loss: 0.2519 - acc: 0.912 - ETA: 5s - loss: 0.2464 - acc: 0.914 - ETA: 5s - loss: 0.2500 - acc: 0.913 - ETA: 5s - loss: 0.2505 - acc: 0.914 - ETA: 4s - loss: 0.2527 - acc: 0.914 - ETA: 4s - loss: 0.2514 - acc: 0.915 - ETA: 4s - loss: 0.2587 - acc: 0.914 - ETA: 4s - loss: 0.2612 - acc: 0.912 - ETA: 4s - loss: 0.2569 - acc: 0.913 - ETA: 4s - loss: 0.2595 - acc: 0.912 - ETA: 4s - loss: 0.2638 - acc: 0.912 - ETA: 4s - loss: 0.2688 - acc: 0.911 - ETA: 4s - loss: 0.2629 - acc: 0.913 - ETA: 4s - loss: 0.2665 - acc: 0.912 - ETA: 4s - loss: 0.2724 - acc: 0.911 - ETA: 4s - loss: 0.2747 - acc: 0.911 - ETA: 4s - loss: 0.2744 - acc: 0.910 - ETA: 4s - loss: 0.2743 - acc: 0.911 - ETA: 4s - loss: 0.2744 - acc: 0.911 - ETA: 4s - loss: 0.2874 - acc: 0.908 - ETA: 4s - loss: 0.2837 - acc: 0.909 - ETA: 4s - loss: 0.2827 - acc: 0.908 - ETA: 3s - loss: 0.2895 - acc: 0.907 - ETA: 3s - loss: 0.2877 - acc: 0.907 - ETA: 3s - loss: 0.2838 - acc: 0.908 - ETA: 3s - loss: 0.2840 - acc: 0.908 - ETA: 3s - loss: 0.2824 - acc: 0.909 - ETA: 3s - loss: 0.2825 - acc: 0.908 - ETA: 3s - loss: 0.2818 - acc: 0.909 - ETA: 3s - loss: 0.2822 - acc: 0.908 - ETA: 3s - loss: 0.2811 - acc: 0.908 - ETA: 3s - loss: 0.2879 - acc: 0.906 - ETA: 3s - loss: 0.2943 - acc: 0.906 - ETA: 3s - loss: 0.2975 - acc: 0.906 - ETA: 3s - loss: 0.3000 - acc: 0.906 - ETA: 3s - loss: 0.2972 - acc: 0.907 - ETA: 3s - loss: 0.2969 - acc: 0.906 - ETA: 3s - loss: 0.2985 - acc: 0.905 - ETA: 3s - loss: 0.2996 - acc: 0.905 - ETA: 3s - loss: 0.3019 - acc: 0.903 - ETA: 2s - loss: 0.3049 - acc: 0.903 - ETA: 2s - loss: 0.3064 - acc: 0.903 - ETA: 2s - loss: 0.3061 - acc: 0.903 - ETA: 2s - loss: 0.3070 - acc: 0.903 - ETA: 2s - loss: 0.3068 - acc: 0.903 - ETA: 2s - loss: 0.3049 - acc: 0.903 - ETA: 2s - loss: 0.3062 - acc: 0.902 - ETA: 2s - loss: 0.3084 - acc: 0.902 - ETA: 2s - loss: 0.3073 - acc: 0.903 - ETA: 2s - loss: 0.3084 - acc: 0.902 - ETA: 2s - loss: 0.3069 - acc: 0.902 - ETA: 2s - loss: 0.3044 - acc: 0.903 - ETA: 2s - loss: 0.3038 - acc: 0.903 - ETA: 2s - loss: 0.3056 - acc: 0.903 - ETA: 2s - loss: 0.3042 - acc: 0.903 - ETA: 2s - loss: 0.3012 - acc: 0.904 - ETA: 2s - loss: 0.3015 - acc: 0.903 - ETA: 2s - loss: 0.2997 - acc: 0.904 - ETA: 1s - loss: 0.2975 - acc: 0.904 - ETA: 1s - loss: 0.2979 - acc: 0.904 - ETA: 1s - loss: 0.2958 - acc: 0.905 - ETA: 1s - loss: 0.2948 - acc: 0.905 - ETA: 1s - loss: 0.2940 - acc: 0.905 - ETA: 1s - loss: 0.2943 - acc: 0.906 - ETA: 1s - loss: 0.2964 - acc: 0.905 - ETA: 1s - loss: 0.2947 - acc: 0.906 - ETA: 1s - loss: 0.2948 - acc: 0.906 - ETA: 1s - loss: 0.2949 - acc: 0.906 - ETA: 1s - loss: 0.2964 - acc: 0.906 - ETA: 1s - loss: 0.2950 - acc: 0.906 - ETA: 1s - loss: 0.2943 - acc: 0.906 - ETA: 1s - loss: 0.2940 - acc: 0.906 - ETA: 1s - loss: 0.2936 - acc: 0.906 - ETA: 1s - loss: 0.2924 - acc: 0.906 - ETA: 1s - loss: 0.2937 - acc: 0.906 - ETA: 1s - loss: 0.2955 - acc: 0.905 - ETA: 0s - loss: 0.2943 - acc: 0.905 - ETA: 0s - loss: 0.2940 - acc: 0.906 - ETA: 0s - loss: 0.2938 - acc: 0.906 - ETA: 0s - loss: 0.2927 - acc: 0.906 - ETA: 0s - loss: 0.2917 - acc: 0.906 - ETA: 0s - loss: 0.2946 - acc: 0.905 - ETA: 0s - loss: 0.2962 - acc: 0.904 - ETA: 0s - loss: 0.2962 - acc: 0.904 - ETA: 0s - loss: 0.2956 - acc: 0.904 - ETA: 0s - loss: 0.2949 - acc: 0.905 - ETA: 0s - loss: 0.2934 - acc: 0.905 - ETA: 0s - loss: 0.2937 - acc: 0.905 - ETA: 0s - loss: 0.2923 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.905 - ETA: 0s - loss: 0.2920 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.905 - ETA: 0s - loss: 0.2911 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.9056Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2907 - acc: 0.9055 - val_loss: 0.6553 - val_acc: 0.8491\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0627 - acc: 1.000 - ETA: 7s - loss: 0.1806 - acc: 0.937 - ETA: 7s - loss: 0.1449 - acc: 0.950 - ETA: 7s - loss: 0.1139 - acc: 0.961 - ETA: 6s - loss: 0.1074 - acc: 0.966 - ETA: 6s - loss: 0.1166 - acc: 0.960 - ETA: 6s - loss: 0.1225 - acc: 0.952 - ETA: 6s - loss: 0.1325 - acc: 0.954 - ETA: 6s - loss: 0.1496 - acc: 0.952 - ETA: 6s - loss: 0.1520 - acc: 0.951 - ETA: 6s - loss: 0.1477 - acc: 0.955 - ETA: 5s - loss: 0.1666 - acc: 0.948 - ETA: 5s - loss: 0.1701 - acc: 0.947 - ETA: 5s - loss: 0.1695 - acc: 0.946 - ETA: 5s - loss: 0.1678 - acc: 0.946 - ETA: 5s - loss: 0.1613 - acc: 0.948 - ETA: 5s - loss: 0.1678 - acc: 0.945 - ETA: 5s - loss: 0.1783 - acc: 0.946 - ETA: 5s - loss: 0.1805 - acc: 0.943 - ETA: 5s - loss: 0.1853 - acc: 0.943 - ETA: 5s - loss: 0.1857 - acc: 0.941 - ETA: 5s - loss: 0.1882 - acc: 0.942 - ETA: 5s - loss: 0.1835 - acc: 0.942 - ETA: 5s - loss: 0.1843 - acc: 0.942 - ETA: 4s - loss: 0.1860 - acc: 0.941 - ETA: 4s - loss: 0.1829 - acc: 0.942 - ETA: 4s - loss: 0.1865 - acc: 0.940 - ETA: 4s - loss: 0.1895 - acc: 0.940 - ETA: 4s - loss: 0.1980 - acc: 0.936 - ETA: 4s - loss: 0.1960 - acc: 0.936 - ETA: 4s - loss: 0.1968 - acc: 0.936 - ETA: 4s - loss: 0.1998 - acc: 0.935 - ETA: 4s - loss: 0.2025 - acc: 0.935 - ETA: 4s - loss: 0.2031 - acc: 0.935 - ETA: 4s - loss: 0.2025 - acc: 0.934 - ETA: 4s - loss: 0.2026 - acc: 0.934 - ETA: 4s - loss: 0.2054 - acc: 0.933 - ETA: 4s - loss: 0.2086 - acc: 0.931 - ETA: 4s - loss: 0.2055 - acc: 0.932 - ETA: 4s - loss: 0.2084 - acc: 0.932 - ETA: 4s - loss: 0.2075 - acc: 0.931 - ETA: 3s - loss: 0.2105 - acc: 0.931 - ETA: 3s - loss: 0.2095 - acc: 0.931 - ETA: 3s - loss: 0.2135 - acc: 0.930 - ETA: 3s - loss: 0.2119 - acc: 0.931 - ETA: 3s - loss: 0.2142 - acc: 0.932 - ETA: 3s - loss: 0.2175 - acc: 0.931 - ETA: 3s - loss: 0.2165 - acc: 0.931 - ETA: 3s - loss: 0.2196 - acc: 0.931 - ETA: 3s - loss: 0.2193 - acc: 0.931 - ETA: 3s - loss: 0.2160 - acc: 0.932 - ETA: 3s - loss: 0.2136 - acc: 0.933 - ETA: 3s - loss: 0.2163 - acc: 0.932 - ETA: 3s - loss: 0.2146 - acc: 0.933 - ETA: 3s - loss: 0.2122 - acc: 0.933 - ETA: 3s - loss: 0.2133 - acc: 0.933 - ETA: 3s - loss: 0.2119 - acc: 0.934 - ETA: 3s - loss: 0.2103 - acc: 0.934 - ETA: 2s - loss: 0.2101 - acc: 0.934 - ETA: 2s - loss: 0.2096 - acc: 0.935 - ETA: 2s - loss: 0.2150 - acc: 0.933 - ETA: 2s - loss: 0.2141 - acc: 0.933 - ETA: 2s - loss: 0.2126 - acc: 0.933 - ETA: 2s - loss: 0.2153 - acc: 0.933 - ETA: 2s - loss: 0.2185 - acc: 0.932 - ETA: 2s - loss: 0.2170 - acc: 0.932 - ETA: 2s - loss: 0.2153 - acc: 0.933 - ETA: 2s - loss: 0.2200 - acc: 0.931 - ETA: 2s - loss: 0.2200 - acc: 0.931 - ETA: 2s - loss: 0.2220 - acc: 0.930 - ETA: 2s - loss: 0.2239 - acc: 0.929 - ETA: 2s - loss: 0.2252 - acc: 0.929 - ETA: 2s - loss: 0.2286 - acc: 0.928 - ETA: 2s - loss: 0.2300 - acc: 0.928 - ETA: 2s - loss: 0.2316 - acc: 0.928 - ETA: 2s - loss: 0.2340 - acc: 0.927 - ETA: 2s - loss: 0.2332 - acc: 0.928 - ETA: 1s - loss: 0.2351 - acc: 0.927 - ETA: 1s - loss: 0.2361 - acc: 0.926 - ETA: 1s - loss: 0.2382 - acc: 0.925 - ETA: 1s - loss: 0.2376 - acc: 0.925 - ETA: 1s - loss: 0.2385 - acc: 0.925 - ETA: 1s - loss: 0.2413 - acc: 0.925 - ETA: 1s - loss: 0.2410 - acc: 0.925 - ETA: 1s - loss: 0.2398 - acc: 0.925 - ETA: 1s - loss: 0.2393 - acc: 0.925 - ETA: 1s - loss: 0.2409 - acc: 0.925 - ETA: 1s - loss: 0.2419 - acc: 0.925 - ETA: 1s - loss: 0.2414 - acc: 0.925 - ETA: 1s - loss: 0.2419 - acc: 0.925 - ETA: 1s - loss: 0.2421 - acc: 0.924 - ETA: 1s - loss: 0.2430 - acc: 0.924 - ETA: 1s - loss: 0.2415 - acc: 0.924 - ETA: 1s - loss: 0.2408 - acc: 0.924 - ETA: 1s - loss: 0.2419 - acc: 0.924 - ETA: 0s - loss: 0.2419 - acc: 0.925 - ETA: 0s - loss: 0.2423 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.923 - ETA: 0s - loss: 0.2428 - acc: 0.924 - ETA: 0s - loss: 0.2424 - acc: 0.924 - ETA: 0s - loss: 0.2428 - acc: 0.924 - ETA: 0s - loss: 0.2416 - acc: 0.924 - ETA: 0s - loss: 0.2414 - acc: 0.924 - ETA: 0s - loss: 0.2421 - acc: 0.925 - ETA: 0s - loss: 0.2451 - acc: 0.923 - ETA: 0s - loss: 0.2438 - acc: 0.924 - ETA: 0s - loss: 0.2460 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2430 - acc: 0.925 - ETA: 0s - loss: 0.2417 - acc: 0.925 - ETA: 0s - loss: 0.2418 - acc: 0.925 - ETA: 0s - loss: 0.2413 - acc: 0.9257Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2407 - acc: 0.9260 - val_loss: 0.7276 - val_acc: 0.8311\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0258 - acc: 1.000 - ETA: 7s - loss: 0.1264 - acc: 0.962 - ETA: 6s - loss: 0.2514 - acc: 0.928 - ETA: 7s - loss: 0.2308 - acc: 0.927 - ETA: 6s - loss: 0.2065 - acc: 0.933 - ETA: 6s - loss: 0.2444 - acc: 0.930 - ETA: 6s - loss: 0.2217 - acc: 0.938 - ETA: 6s - loss: 0.2135 - acc: 0.940 - ETA: 6s - loss: 0.2245 - acc: 0.939 - ETA: 6s - loss: 0.2070 - acc: 0.944 - ETA: 5s - loss: 0.1907 - acc: 0.950 - ETA: 5s - loss: 0.1795 - acc: 0.953 - ETA: 5s - loss: 0.1797 - acc: 0.952 - ETA: 5s - loss: 0.1688 - acc: 0.956 - ETA: 5s - loss: 0.1770 - acc: 0.952 - ETA: 5s - loss: 0.1775 - acc: 0.950 - ETA: 5s - loss: 0.1828 - acc: 0.949 - ETA: 5s - loss: 0.1835 - acc: 0.948 - ETA: 5s - loss: 0.1854 - acc: 0.948 - ETA: 5s - loss: 0.1829 - acc: 0.948 - ETA: 5s - loss: 0.1932 - acc: 0.945 - ETA: 5s - loss: 0.1969 - acc: 0.947 - ETA: 5s - loss: 0.1973 - acc: 0.947 - ETA: 5s - loss: 0.1943 - acc: 0.947 - ETA: 4s - loss: 0.1938 - acc: 0.947 - ETA: 4s - loss: 0.1879 - acc: 0.948 - ETA: 4s - loss: 0.1907 - acc: 0.947 - ETA: 4s - loss: 0.1875 - acc: 0.948 - ETA: 4s - loss: 0.1843 - acc: 0.949 - ETA: 4s - loss: 0.1847 - acc: 0.950 - ETA: 4s - loss: 0.1865 - acc: 0.948 - ETA: 4s - loss: 0.1848 - acc: 0.948 - ETA: 4s - loss: 0.1824 - acc: 0.949 - ETA: 4s - loss: 0.1827 - acc: 0.948 - ETA: 4s - loss: 0.1831 - acc: 0.948 - ETA: 4s - loss: 0.1825 - acc: 0.948 - ETA: 4s - loss: 0.1791 - acc: 0.949 - ETA: 4s - loss: 0.1805 - acc: 0.949 - ETA: 4s - loss: 0.1798 - acc: 0.948 - ETA: 4s - loss: 0.1783 - acc: 0.949 - ETA: 4s - loss: 0.1793 - acc: 0.948 - ETA: 3s - loss: 0.1793 - acc: 0.948 - ETA: 3s - loss: 0.1813 - acc: 0.948 - ETA: 3s - loss: 0.1811 - acc: 0.947 - ETA: 3s - loss: 0.1823 - acc: 0.946 - ETA: 3s - loss: 0.1795 - acc: 0.947 - ETA: 3s - loss: 0.1809 - acc: 0.946 - ETA: 3s - loss: 0.1781 - acc: 0.947 - ETA: 3s - loss: 0.1794 - acc: 0.946 - ETA: 3s - loss: 0.1771 - acc: 0.946 - ETA: 3s - loss: 0.1790 - acc: 0.946 - ETA: 3s - loss: 0.1768 - acc: 0.947 - ETA: 3s - loss: 0.1825 - acc: 0.945 - ETA: 3s - loss: 0.1816 - acc: 0.946 - ETA: 3s - loss: 0.1828 - acc: 0.945 - ETA: 3s - loss: 0.1842 - acc: 0.944 - ETA: 3s - loss: 0.1837 - acc: 0.944 - ETA: 3s - loss: 0.1836 - acc: 0.944 - ETA: 3s - loss: 0.1829 - acc: 0.944 - ETA: 2s - loss: 0.1819 - acc: 0.944 - ETA: 2s - loss: 0.1828 - acc: 0.943 - ETA: 2s - loss: 0.1806 - acc: 0.944 - ETA: 2s - loss: 0.1802 - acc: 0.944 - ETA: 2s - loss: 0.1797 - acc: 0.944 - ETA: 2s - loss: 0.1794 - acc: 0.945 - ETA: 2s - loss: 0.1780 - acc: 0.945 - ETA: 2s - loss: 0.1789 - acc: 0.945 - ETA: 2s - loss: 0.1783 - acc: 0.945 - ETA: 2s - loss: 0.1798 - acc: 0.944 - ETA: 2s - loss: 0.1792 - acc: 0.944 - ETA: 2s - loss: 0.1786 - acc: 0.945 - ETA: 2s - loss: 0.1781 - acc: 0.945 - ETA: 2s - loss: 0.1777 - acc: 0.944 - ETA: 2s - loss: 0.1761 - acc: 0.945 - ETA: 2s - loss: 0.1763 - acc: 0.945 - ETA: 2s - loss: 0.1795 - acc: 0.944 - ETA: 1s - loss: 0.1812 - acc: 0.943 - ETA: 1s - loss: 0.1821 - acc: 0.942 - ETA: 1s - loss: 0.1829 - acc: 0.942 - ETA: 1s - loss: 0.1830 - acc: 0.942 - ETA: 1s - loss: 0.1817 - acc: 0.942 - ETA: 1s - loss: 0.1855 - acc: 0.941 - ETA: 1s - loss: 0.1844 - acc: 0.941 - ETA: 1s - loss: 0.1850 - acc: 0.941 - ETA: 1s - loss: 0.1882 - acc: 0.940 - ETA: 1s - loss: 0.1885 - acc: 0.940 - ETA: 1s - loss: 0.1894 - acc: 0.940 - ETA: 1s - loss: 0.1900 - acc: 0.940 - ETA: 1s - loss: 0.1890 - acc: 0.940 - ETA: 1s - loss: 0.1883 - acc: 0.940 - ETA: 1s - loss: 0.1879 - acc: 0.940 - ETA: 1s - loss: 0.1874 - acc: 0.941 - ETA: 1s - loss: 0.1867 - acc: 0.941 - ETA: 1s - loss: 0.1874 - acc: 0.941 - ETA: 0s - loss: 0.1890 - acc: 0.940 - ETA: 0s - loss: 0.1888 - acc: 0.940 - ETA: 0s - loss: 0.1893 - acc: 0.940 - ETA: 0s - loss: 0.1910 - acc: 0.940 - ETA: 0s - loss: 0.1921 - acc: 0.940 - ETA: 0s - loss: 0.1920 - acc: 0.940 - ETA: 0s - loss: 0.1946 - acc: 0.940 - ETA: 0s - loss: 0.1948 - acc: 0.940 - ETA: 0s - loss: 0.1948 - acc: 0.939 - ETA: 0s - loss: 0.1947 - acc: 0.939 - ETA: 0s - loss: 0.1952 - acc: 0.939 - ETA: 0s - loss: 0.1958 - acc: 0.939 - ETA: 0s - loss: 0.1957 - acc: 0.939 - ETA: 0s - loss: 0.1943 - acc: 0.940 - ETA: 0s - loss: 0.1949 - acc: 0.940 - ETA: 0s - loss: 0.1953 - acc: 0.939 - ETA: 0s - loss: 0.1941 - acc: 0.940 - ETA: 0s - loss: 0.1985 - acc: 0.9392Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1995 - acc: 0.9391 - val_loss: 0.6971 - val_acc: 0.8443\n",
      "Epoch 7/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.1057 - acc: 0.900 - ETA: 6s - loss: 0.1497 - acc: 0.937 - ETA: 6s - loss: 0.1308 - acc: 0.942 - ETA: 6s - loss: 0.1089 - acc: 0.950 - ETA: 6s - loss: 0.1133 - acc: 0.946 - ETA: 6s - loss: 0.1390 - acc: 0.943 - ETA: 6s - loss: 0.1502 - acc: 0.942 - ETA: 6s - loss: 0.1469 - acc: 0.945 - ETA: 6s - loss: 0.1428 - acc: 0.942 - ETA: 5s - loss: 0.1374 - acc: 0.944 - ETA: 5s - loss: 0.1376 - acc: 0.946 - ETA: 5s - loss: 0.1303 - acc: 0.950 - ETA: 5s - loss: 0.1359 - acc: 0.950 - ETA: 5s - loss: 0.1572 - acc: 0.946 - ETA: 5s - loss: 0.1484 - acc: 0.950 - ETA: 5s - loss: 0.1535 - acc: 0.951 - ETA: 5s - loss: 0.1479 - acc: 0.953 - ETA: 5s - loss: 0.1514 - acc: 0.952 - ETA: 5s - loss: 0.1477 - acc: 0.954 - ETA: 5s - loss: 0.1477 - acc: 0.955 - ETA: 5s - loss: 0.1455 - acc: 0.954 - ETA: 5s - loss: 0.1418 - acc: 0.955 - ETA: 5s - loss: 0.1449 - acc: 0.955 - ETA: 4s - loss: 0.1438 - acc: 0.955 - ETA: 4s - loss: 0.1400 - acc: 0.956 - ETA: 4s - loss: 0.1384 - acc: 0.957 - ETA: 4s - loss: 0.1397 - acc: 0.957 - ETA: 4s - loss: 0.1475 - acc: 0.955 - ETA: 4s - loss: 0.1473 - acc: 0.954 - ETA: 4s - loss: 0.1433 - acc: 0.956 - ETA: 4s - loss: 0.1400 - acc: 0.957 - ETA: 4s - loss: 0.1373 - acc: 0.958 - ETA: 4s - loss: 0.1342 - acc: 0.959 - ETA: 4s - loss: 0.1334 - acc: 0.959 - ETA: 4s - loss: 0.1325 - acc: 0.958 - ETA: 4s - loss: 0.1319 - acc: 0.959 - ETA: 4s - loss: 0.1285 - acc: 0.960 - ETA: 4s - loss: 0.1279 - acc: 0.960 - ETA: 4s - loss: 0.1273 - acc: 0.960 - ETA: 4s - loss: 0.1259 - acc: 0.960 - ETA: 4s - loss: 0.1279 - acc: 0.959 - ETA: 3s - loss: 0.1288 - acc: 0.959 - ETA: 3s - loss: 0.1289 - acc: 0.960 - ETA: 3s - loss: 0.1295 - acc: 0.959 - ETA: 3s - loss: 0.1346 - acc: 0.958 - ETA: 3s - loss: 0.1322 - acc: 0.959 - ETA: 3s - loss: 0.1321 - acc: 0.959 - ETA: 3s - loss: 0.1308 - acc: 0.959 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1345 - acc: 0.958 - ETA: 3s - loss: 0.1356 - acc: 0.957 - ETA: 3s - loss: 0.1344 - acc: 0.958 - ETA: 3s - loss: 0.1415 - acc: 0.957 - ETA: 3s - loss: 0.1432 - acc: 0.956 - ETA: 3s - loss: 0.1462 - acc: 0.955 - ETA: 3s - loss: 0.1487 - acc: 0.955 - ETA: 3s - loss: 0.1483 - acc: 0.955 - ETA: 3s - loss: 0.1487 - acc: 0.954 - ETA: 2s - loss: 0.1483 - acc: 0.954 - ETA: 2s - loss: 0.1479 - acc: 0.955 - ETA: 2s - loss: 0.1478 - acc: 0.955 - ETA: 2s - loss: 0.1481 - acc: 0.954 - ETA: 2s - loss: 0.1500 - acc: 0.954 - ETA: 2s - loss: 0.1543 - acc: 0.953 - ETA: 2s - loss: 0.1561 - acc: 0.952 - ETA: 2s - loss: 0.1555 - acc: 0.952 - ETA: 2s - loss: 0.1567 - acc: 0.952 - ETA: 2s - loss: 0.1563 - acc: 0.952 - ETA: 2s - loss: 0.1573 - acc: 0.952 - ETA: 2s - loss: 0.1592 - acc: 0.951 - ETA: 2s - loss: 0.1584 - acc: 0.951 - ETA: 2s - loss: 0.1580 - acc: 0.952 - ETA: 2s - loss: 0.1582 - acc: 0.952 - ETA: 2s - loss: 0.1617 - acc: 0.951 - ETA: 2s - loss: 0.1610 - acc: 0.951 - ETA: 2s - loss: 0.1601 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.951 - ETA: 1s - loss: 0.1581 - acc: 0.951 - ETA: 1s - loss: 0.1580 - acc: 0.951 - ETA: 1s - loss: 0.1572 - acc: 0.951 - ETA: 1s - loss: 0.1574 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.950 - ETA: 1s - loss: 0.1606 - acc: 0.950 - ETA: 1s - loss: 0.1616 - acc: 0.950 - ETA: 1s - loss: 0.1602 - acc: 0.950 - ETA: 1s - loss: 0.1593 - acc: 0.950 - ETA: 1s - loss: 0.1594 - acc: 0.951 - ETA: 1s - loss: 0.1619 - acc: 0.950 - ETA: 1s - loss: 0.1607 - acc: 0.950 - ETA: 1s - loss: 0.1615 - acc: 0.950 - ETA: 1s - loss: 0.1612 - acc: 0.950 - ETA: 1s - loss: 0.1607 - acc: 0.950 - ETA: 1s - loss: 0.1605 - acc: 0.950 - ETA: 0s - loss: 0.1599 - acc: 0.950 - ETA: 0s - loss: 0.1603 - acc: 0.949 - ETA: 0s - loss: 0.1625 - acc: 0.949 - ETA: 0s - loss: 0.1630 - acc: 0.949 - ETA: 0s - loss: 0.1635 - acc: 0.949 - ETA: 0s - loss: 0.1625 - acc: 0.949 - ETA: 0s - loss: 0.1627 - acc: 0.949 - ETA: 0s - loss: 0.1634 - acc: 0.949 - ETA: 0s - loss: 0.1628 - acc: 0.949 - ETA: 0s - loss: 0.1677 - acc: 0.948 - ETA: 0s - loss: 0.1679 - acc: 0.948 - ETA: 0s - loss: 0.1676 - acc: 0.948 - ETA: 0s - loss: 0.1670 - acc: 0.948 - ETA: 0s - loss: 0.1677 - acc: 0.948 - ETA: 0s - loss: 0.1678 - acc: 0.948 - ETA: 0s - loss: 0.1683 - acc: 0.947 - ETA: 0s - loss: 0.1675 - acc: 0.948 - ETA: 0s - loss: 0.1672 - acc: 0.9485Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1670 - acc: 0.9484 - val_loss: 0.6871 - val_acc: 0.8611\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0203 - acc: 1.000 - ETA: 7s - loss: 0.0616 - acc: 0.987 - ETA: 7s - loss: 0.0816 - acc: 0.985 - ETA: 6s - loss: 0.0848 - acc: 0.985 - ETA: 6s - loss: 0.0827 - acc: 0.980 - ETA: 6s - loss: 0.0689 - acc: 0.984 - ETA: 6s - loss: 0.0679 - acc: 0.981 - ETA: 6s - loss: 0.0675 - acc: 0.981 - ETA: 6s - loss: 0.0832 - acc: 0.974 - ETA: 6s - loss: 0.0871 - acc: 0.973 - ETA: 5s - loss: 0.1165 - acc: 0.969 - ETA: 5s - loss: 0.1177 - acc: 0.966 - ETA: 5s - loss: 0.1104 - acc: 0.967 - ETA: 5s - loss: 0.1212 - acc: 0.965 - ETA: 5s - loss: 0.1155 - acc: 0.966 - ETA: 5s - loss: 0.1120 - acc: 0.967 - ETA: 5s - loss: 0.1075 - acc: 0.969 - ETA: 5s - loss: 0.1053 - acc: 0.970 - ETA: 5s - loss: 0.1052 - acc: 0.970 - ETA: 5s - loss: 0.1098 - acc: 0.969 - ETA: 5s - loss: 0.1067 - acc: 0.970 - ETA: 5s - loss: 0.1220 - acc: 0.968 - ETA: 5s - loss: 0.1311 - acc: 0.966 - ETA: 5s - loss: 0.1327 - acc: 0.964 - ETA: 5s - loss: 0.1276 - acc: 0.966 - ETA: 5s - loss: 0.1351 - acc: 0.964 - ETA: 4s - loss: 0.1341 - acc: 0.963 - ETA: 4s - loss: 0.1393 - acc: 0.961 - ETA: 4s - loss: 0.1366 - acc: 0.962 - ETA: 4s - loss: 0.1381 - acc: 0.962 - ETA: 4s - loss: 0.1349 - acc: 0.963 - ETA: 4s - loss: 0.1322 - acc: 0.964 - ETA: 4s - loss: 0.1299 - acc: 0.964 - ETA: 4s - loss: 0.1272 - acc: 0.964 - ETA: 4s - loss: 0.1283 - acc: 0.964 - ETA: 4s - loss: 0.1263 - acc: 0.965 - ETA: 4s - loss: 0.1257 - acc: 0.964 - ETA: 4s - loss: 0.1248 - acc: 0.964 - ETA: 4s - loss: 0.1233 - acc: 0.964 - ETA: 4s - loss: 0.1249 - acc: 0.964 - ETA: 4s - loss: 0.1250 - acc: 0.964 - ETA: 4s - loss: 0.1263 - acc: 0.963 - ETA: 3s - loss: 0.1261 - acc: 0.963 - ETA: 3s - loss: 0.1251 - acc: 0.963 - ETA: 3s - loss: 0.1234 - acc: 0.963 - ETA: 3s - loss: 0.1212 - acc: 0.964 - ETA: 3s - loss: 0.1222 - acc: 0.963 - ETA: 3s - loss: 0.1251 - acc: 0.962 - ETA: 3s - loss: 0.1235 - acc: 0.962 - ETA: 3s - loss: 0.1232 - acc: 0.962 - ETA: 3s - loss: 0.1232 - acc: 0.962 - ETA: 3s - loss: 0.1222 - acc: 0.963 - ETA: 3s - loss: 0.1212 - acc: 0.963 - ETA: 3s - loss: 0.1198 - acc: 0.963 - ETA: 3s - loss: 0.1201 - acc: 0.963 - ETA: 3s - loss: 0.1182 - acc: 0.964 - ETA: 3s - loss: 0.1165 - acc: 0.964 - ETA: 3s - loss: 0.1168 - acc: 0.964 - ETA: 3s - loss: 0.1177 - acc: 0.964 - ETA: 2s - loss: 0.1170 - acc: 0.965 - ETA: 2s - loss: 0.1161 - acc: 0.965 - ETA: 2s - loss: 0.1180 - acc: 0.964 - ETA: 2s - loss: 0.1208 - acc: 0.964 - ETA: 2s - loss: 0.1240 - acc: 0.963 - ETA: 2s - loss: 0.1242 - acc: 0.962 - ETA: 2s - loss: 0.1254 - acc: 0.961 - ETA: 2s - loss: 0.1241 - acc: 0.962 - ETA: 2s - loss: 0.1231 - acc: 0.961 - ETA: 2s - loss: 0.1248 - acc: 0.961 - ETA: 2s - loss: 0.1261 - acc: 0.960 - ETA: 2s - loss: 0.1270 - acc: 0.960 - ETA: 2s - loss: 0.1277 - acc: 0.960 - ETA: 2s - loss: 0.1277 - acc: 0.960 - ETA: 2s - loss: 0.1278 - acc: 0.959 - ETA: 2s - loss: 0.1299 - acc: 0.959 - ETA: 2s - loss: 0.1290 - acc: 0.959 - ETA: 2s - loss: 0.1286 - acc: 0.959 - ETA: 1s - loss: 0.1288 - acc: 0.959 - ETA: 1s - loss: 0.1281 - acc: 0.959 - ETA: 1s - loss: 0.1300 - acc: 0.958 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 1s - loss: 0.1308 - acc: 0.959 - ETA: 1s - loss: 0.1313 - acc: 0.959 - ETA: 1s - loss: 0.1313 - acc: 0.958 - ETA: 1s - loss: 0.1331 - acc: 0.958 - ETA: 1s - loss: 0.1327 - acc: 0.958 - ETA: 1s - loss: 0.1324 - acc: 0.958 - ETA: 1s - loss: 0.1314 - acc: 0.958 - ETA: 1s - loss: 0.1313 - acc: 0.958 - ETA: 1s - loss: 0.1309 - acc: 0.958 - ETA: 1s - loss: 0.1318 - acc: 0.958 - ETA: 1s - loss: 0.1323 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 0s - loss: 0.1315 - acc: 0.958 - ETA: 0s - loss: 0.1317 - acc: 0.958 - ETA: 0s - loss: 0.1325 - acc: 0.958 - ETA: 0s - loss: 0.1335 - acc: 0.957 - ETA: 0s - loss: 0.1346 - acc: 0.957 - ETA: 0s - loss: 0.1345 - acc: 0.956 - ETA: 0s - loss: 0.1341 - acc: 0.956 - ETA: 0s - loss: 0.1333 - acc: 0.957 - ETA: 0s - loss: 0.1330 - acc: 0.957 - ETA: 0s - loss: 0.1359 - acc: 0.957 - ETA: 0s - loss: 0.1354 - acc: 0.957 - ETA: 0s - loss: 0.1360 - acc: 0.957 - ETA: 0s - loss: 0.1364 - acc: 0.956 - ETA: 0s - loss: 0.1360 - acc: 0.956 - ETA: 0s - loss: 0.1376 - acc: 0.956 - ETA: 0s - loss: 0.1401 - acc: 0.955 - ETA: 0s - loss: 0.1396 - acc: 0.955 - ETA: 0s - loss: 0.1392 - acc: 0.9557Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1408 - acc: 0.9552 - val_loss: 0.7830 - val_acc: 0.8491\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.1602 - acc: 0.900 - ETA: 5s - loss: 0.1880 - acc: 0.912 - ETA: 5s - loss: 0.1562 - acc: 0.921 - ETA: 5s - loss: 0.1632 - acc: 0.925 - ETA: 5s - loss: 0.1357 - acc: 0.934 - ETA: 5s - loss: 0.1534 - acc: 0.937 - ETA: 5s - loss: 0.1615 - acc: 0.934 - ETA: 5s - loss: 0.1467 - acc: 0.940 - ETA: 5s - loss: 0.1361 - acc: 0.944 - ETA: 5s - loss: 0.1375 - acc: 0.944 - ETA: 5s - loss: 0.1287 - acc: 0.946 - ETA: 5s - loss: 0.1218 - acc: 0.950 - ETA: 5s - loss: 0.1172 - acc: 0.952 - ETA: 5s - loss: 0.1170 - acc: 0.952 - ETA: 5s - loss: 0.1114 - acc: 0.954 - ETA: 5s - loss: 0.1105 - acc: 0.955 - ETA: 5s - loss: 0.1114 - acc: 0.954 - ETA: 5s - loss: 0.1243 - acc: 0.952 - ETA: 5s - loss: 0.1209 - acc: 0.953 - ETA: 5s - loss: 0.1157 - acc: 0.956 - ETA: 5s - loss: 0.1124 - acc: 0.957 - ETA: 4s - loss: 0.1156 - acc: 0.954 - ETA: 4s - loss: 0.1164 - acc: 0.956 - ETA: 4s - loss: 0.1142 - acc: 0.956 - ETA: 4s - loss: 0.1153 - acc: 0.955 - ETA: 4s - loss: 0.1204 - acc: 0.954 - ETA: 4s - loss: 0.1217 - acc: 0.953 - ETA: 4s - loss: 0.1205 - acc: 0.954 - ETA: 4s - loss: 0.1229 - acc: 0.954 - ETA: 4s - loss: 0.1225 - acc: 0.954 - ETA: 4s - loss: 0.1258 - acc: 0.954 - ETA: 4s - loss: 0.1245 - acc: 0.955 - ETA: 4s - loss: 0.1230 - acc: 0.955 - ETA: 4s - loss: 0.1240 - acc: 0.954 - ETA: 4s - loss: 0.1212 - acc: 0.955 - ETA: 4s - loss: 0.1197 - acc: 0.956 - ETA: 4s - loss: 0.1207 - acc: 0.956 - ETA: 4s - loss: 0.1237 - acc: 0.955 - ETA: 4s - loss: 0.1236 - acc: 0.955 - ETA: 4s - loss: 0.1235 - acc: 0.955 - ETA: 4s - loss: 0.1246 - acc: 0.955 - ETA: 3s - loss: 0.1220 - acc: 0.956 - ETA: 3s - loss: 0.1209 - acc: 0.957 - ETA: 3s - loss: 0.1257 - acc: 0.956 - ETA: 3s - loss: 0.1267 - acc: 0.956 - ETA: 3s - loss: 0.1274 - acc: 0.955 - ETA: 3s - loss: 0.1251 - acc: 0.956 - ETA: 3s - loss: 0.1233 - acc: 0.957 - ETA: 3s - loss: 0.1225 - acc: 0.956 - ETA: 3s - loss: 0.1209 - acc: 0.957 - ETA: 3s - loss: 0.1194 - acc: 0.958 - ETA: 3s - loss: 0.1204 - acc: 0.957 - ETA: 3s - loss: 0.1188 - acc: 0.958 - ETA: 3s - loss: 0.1171 - acc: 0.959 - ETA: 3s - loss: 0.1158 - acc: 0.959 - ETA: 3s - loss: 0.1148 - acc: 0.960 - ETA: 3s - loss: 0.1161 - acc: 0.959 - ETA: 3s - loss: 0.1172 - acc: 0.959 - ETA: 2s - loss: 0.1170 - acc: 0.960 - ETA: 2s - loss: 0.1171 - acc: 0.960 - ETA: 2s - loss: 0.1157 - acc: 0.960 - ETA: 2s - loss: 0.1168 - acc: 0.961 - ETA: 2s - loss: 0.1154 - acc: 0.961 - ETA: 2s - loss: 0.1150 - acc: 0.961 - ETA: 2s - loss: 0.1144 - acc: 0.961 - ETA: 2s - loss: 0.1150 - acc: 0.961 - ETA: 2s - loss: 0.1147 - acc: 0.961 - ETA: 2s - loss: 0.1138 - acc: 0.961 - ETA: 2s - loss: 0.1127 - acc: 0.962 - ETA: 2s - loss: 0.1114 - acc: 0.962 - ETA: 2s - loss: 0.1145 - acc: 0.962 - ETA: 2s - loss: 0.1161 - acc: 0.962 - ETA: 2s - loss: 0.1158 - acc: 0.962 - ETA: 2s - loss: 0.1163 - acc: 0.962 - ETA: 2s - loss: 0.1157 - acc: 0.962 - ETA: 2s - loss: 0.1171 - acc: 0.962 - ETA: 1s - loss: 0.1183 - acc: 0.962 - ETA: 1s - loss: 0.1188 - acc: 0.961 - ETA: 1s - loss: 0.1194 - acc: 0.961 - ETA: 1s - loss: 0.1192 - acc: 0.961 - ETA: 1s - loss: 0.1199 - acc: 0.961 - ETA: 1s - loss: 0.1200 - acc: 0.961 - ETA: 1s - loss: 0.1220 - acc: 0.960 - ETA: 1s - loss: 0.1255 - acc: 0.960 - ETA: 1s - loss: 0.1246 - acc: 0.960 - ETA: 1s - loss: 0.1239 - acc: 0.960 - ETA: 1s - loss: 0.1251 - acc: 0.960 - ETA: 1s - loss: 0.1246 - acc: 0.960 - ETA: 1s - loss: 0.1238 - acc: 0.960 - ETA: 1s - loss: 0.1245 - acc: 0.960 - ETA: 1s - loss: 0.1268 - acc: 0.960 - ETA: 1s - loss: 0.1262 - acc: 0.960 - ETA: 1s - loss: 0.1253 - acc: 0.960 - ETA: 1s - loss: 0.1242 - acc: 0.961 - ETA: 0s - loss: 0.1235 - acc: 0.961 - ETA: 0s - loss: 0.1224 - acc: 0.961 - ETA: 0s - loss: 0.1219 - acc: 0.961 - ETA: 0s - loss: 0.1218 - acc: 0.961 - ETA: 0s - loss: 0.1257 - acc: 0.960 - ETA: 0s - loss: 0.1266 - acc: 0.960 - ETA: 0s - loss: 0.1267 - acc: 0.960 - ETA: 0s - loss: 0.1263 - acc: 0.960 - ETA: 0s - loss: 0.1278 - acc: 0.960 - ETA: 0s - loss: 0.1272 - acc: 0.960 - ETA: 0s - loss: 0.1262 - acc: 0.961 - ETA: 0s - loss: 0.1260 - acc: 0.960 - ETA: 0s - loss: 0.1256 - acc: 0.960 - ETA: 0s - loss: 0.1249 - acc: 0.960 - ETA: 0s - loss: 0.1247 - acc: 0.961 - ETA: 0s - loss: 0.1259 - acc: 0.960 - ETA: 0s - loss: 0.1253 - acc: 0.960 - ETA: 0s - loss: 0.1249 - acc: 0.9605Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1253 - acc: 0.9605 - val_loss: 0.7596 - val_acc: 0.8503\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0931 - acc: 0.950 - ETA: 6s - loss: 0.0472 - acc: 0.975 - ETA: 6s - loss: 0.0523 - acc: 0.978 - ETA: 6s - loss: 0.0480 - acc: 0.980 - ETA: 6s - loss: 0.0650 - acc: 0.973 - ETA: 6s - loss: 0.0559 - acc: 0.978 - ETA: 6s - loss: 0.0613 - acc: 0.978 - ETA: 6s - loss: 0.0608 - acc: 0.977 - ETA: 5s - loss: 0.0641 - acc: 0.976 - ETA: 5s - loss: 0.0674 - acc: 0.975 - ETA: 5s - loss: 0.0650 - acc: 0.975 - ETA: 5s - loss: 0.0704 - acc: 0.975 - ETA: 5s - loss: 0.0680 - acc: 0.975 - ETA: 5s - loss: 0.0691 - acc: 0.975 - ETA: 5s - loss: 0.0656 - acc: 0.976 - ETA: 5s - loss: 0.0667 - acc: 0.976 - ETA: 5s - loss: 0.0729 - acc: 0.972 - ETA: 5s - loss: 0.0776 - acc: 0.970 - ETA: 5s - loss: 0.0803 - acc: 0.968 - ETA: 5s - loss: 0.0803 - acc: 0.969 - ETA: 5s - loss: 0.0781 - acc: 0.970 - ETA: 5s - loss: 0.0754 - acc: 0.971 - ETA: 5s - loss: 0.0724 - acc: 0.973 - ETA: 4s - loss: 0.0746 - acc: 0.972 - ETA: 4s - loss: 0.0720 - acc: 0.974 - ETA: 4s - loss: 0.0712 - acc: 0.974 - ETA: 4s - loss: 0.0717 - acc: 0.974 - ETA: 4s - loss: 0.0720 - acc: 0.973 - ETA: 4s - loss: 0.0753 - acc: 0.973 - ETA: 4s - loss: 0.0746 - acc: 0.973 - ETA: 4s - loss: 0.0732 - acc: 0.974 - ETA: 4s - loss: 0.0799 - acc: 0.973 - ETA: 4s - loss: 0.0786 - acc: 0.973 - ETA: 4s - loss: 0.0776 - acc: 0.974 - ETA: 4s - loss: 0.0826 - acc: 0.973 - ETA: 4s - loss: 0.0833 - acc: 0.974 - ETA: 4s - loss: 0.0825 - acc: 0.973 - ETA: 4s - loss: 0.0865 - acc: 0.972 - ETA: 4s - loss: 0.0860 - acc: 0.973 - ETA: 4s - loss: 0.0844 - acc: 0.973 - ETA: 3s - loss: 0.0830 - acc: 0.974 - ETA: 3s - loss: 0.0851 - acc: 0.972 - ETA: 3s - loss: 0.0843 - acc: 0.972 - ETA: 3s - loss: 0.0841 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.971 - ETA: 3s - loss: 0.0843 - acc: 0.971 - ETA: 3s - loss: 0.0831 - acc: 0.971 - ETA: 3s - loss: 0.0851 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.971 - ETA: 3s - loss: 0.0836 - acc: 0.972 - ETA: 3s - loss: 0.0864 - acc: 0.971 - ETA: 3s - loss: 0.0857 - acc: 0.971 - ETA: 3s - loss: 0.0873 - acc: 0.971 - ETA: 3s - loss: 0.0897 - acc: 0.970 - ETA: 3s - loss: 0.0883 - acc: 0.970 - ETA: 3s - loss: 0.0878 - acc: 0.971 - ETA: 3s - loss: 0.0902 - acc: 0.971 - ETA: 2s - loss: 0.0906 - acc: 0.970 - ETA: 2s - loss: 0.0904 - acc: 0.970 - ETA: 2s - loss: 0.0939 - acc: 0.970 - ETA: 2s - loss: 0.0970 - acc: 0.969 - ETA: 2s - loss: 0.0964 - acc: 0.969 - ETA: 2s - loss: 0.0983 - acc: 0.969 - ETA: 2s - loss: 0.0981 - acc: 0.969 - ETA: 2s - loss: 0.0973 - acc: 0.969 - ETA: 2s - loss: 0.0966 - acc: 0.969 - ETA: 2s - loss: 0.0960 - acc: 0.969 - ETA: 2s - loss: 0.0955 - acc: 0.969 - ETA: 2s - loss: 0.0972 - acc: 0.968 - ETA: 2s - loss: 0.0970 - acc: 0.969 - ETA: 2s - loss: 0.0978 - acc: 0.968 - ETA: 2s - loss: 0.0981 - acc: 0.968 - ETA: 2s - loss: 0.0977 - acc: 0.968 - ETA: 2s - loss: 0.0966 - acc: 0.968 - ETA: 2s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0962 - acc: 0.968 - ETA: 1s - loss: 0.0970 - acc: 0.968 - ETA: 1s - loss: 0.0971 - acc: 0.968 - ETA: 1s - loss: 0.0965 - acc: 0.968 - ETA: 1s - loss: 0.0955 - acc: 0.969 - ETA: 1s - loss: 0.0945 - acc: 0.969 - ETA: 1s - loss: 0.0940 - acc: 0.969 - ETA: 1s - loss: 0.0951 - acc: 0.969 - ETA: 1s - loss: 0.0951 - acc: 0.969 - ETA: 1s - loss: 0.0946 - acc: 0.970 - ETA: 1s - loss: 0.0956 - acc: 0.969 - ETA: 1s - loss: 0.0979 - acc: 0.969 - ETA: 1s - loss: 0.0986 - acc: 0.969 - ETA: 1s - loss: 0.0988 - acc: 0.969 - ETA: 1s - loss: 0.0980 - acc: 0.969 - ETA: 1s - loss: 0.0978 - acc: 0.969 - ETA: 1s - loss: 0.0973 - acc: 0.969 - ETA: 1s - loss: 0.0982 - acc: 0.969 - ETA: 1s - loss: 0.0980 - acc: 0.969 - ETA: 0s - loss: 0.0998 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0988 - acc: 0.969 - ETA: 0s - loss: 0.0981 - acc: 0.969 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.1007 - acc: 0.968 - ETA: 0s - loss: 0.0999 - acc: 0.968 - ETA: 0s - loss: 0.1030 - acc: 0.967 - ETA: 0s - loss: 0.1022 - acc: 0.967 - ETA: 0s - loss: 0.1015 - acc: 0.968 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1019 - acc: 0.967 - ETA: 0s - loss: 0.1017 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.968 - ETA: 0s - loss: 0.1017 - acc: 0.9678Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1017 - acc: 0.9677 - val_loss: 0.8152 - val_acc: 0.8419\n",
      "Epoch 11/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0886 - acc: 0.950 - ETA: 6s - loss: 0.0577 - acc: 0.962 - ETA: 6s - loss: 0.0709 - acc: 0.957 - ETA: 6s - loss: 0.0561 - acc: 0.970 - ETA: 6s - loss: 0.0452 - acc: 0.976 - ETA: 6s - loss: 0.0398 - acc: 0.981 - ETA: 6s - loss: 0.0394 - acc: 0.981 - ETA: 6s - loss: 0.0490 - acc: 0.977 - ETA: 5s - loss: 0.0536 - acc: 0.978 - ETA: 5s - loss: 0.0749 - acc: 0.976 - ETA: 5s - loss: 0.0705 - acc: 0.979 - ETA: 5s - loss: 0.0699 - acc: 0.977 - ETA: 5s - loss: 0.0676 - acc: 0.978 - ETA: 5s - loss: 0.0692 - acc: 0.977 - ETA: 5s - loss: 0.0682 - acc: 0.977 - ETA: 5s - loss: 0.0680 - acc: 0.978 - ETA: 5s - loss: 0.0670 - acc: 0.978 - ETA: 5s - loss: 0.0657 - acc: 0.977 - ETA: 5s - loss: 0.0646 - acc: 0.977 - ETA: 5s - loss: 0.0639 - acc: 0.977 - ETA: 5s - loss: 0.0616 - acc: 0.978 - ETA: 5s - loss: 0.0608 - acc: 0.978 - ETA: 5s - loss: 0.0611 - acc: 0.977 - ETA: 4s - loss: 0.0593 - acc: 0.978 - ETA: 4s - loss: 0.0637 - acc: 0.976 - ETA: 4s - loss: 0.0630 - acc: 0.977 - ETA: 4s - loss: 0.0619 - acc: 0.977 - ETA: 4s - loss: 0.0604 - acc: 0.978 - ETA: 4s - loss: 0.0590 - acc: 0.978 - ETA: 4s - loss: 0.0578 - acc: 0.978 - ETA: 4s - loss: 0.0561 - acc: 0.979 - ETA: 4s - loss: 0.0557 - acc: 0.979 - ETA: 4s - loss: 0.0561 - acc: 0.978 - ETA: 4s - loss: 0.0547 - acc: 0.979 - ETA: 4s - loss: 0.0553 - acc: 0.979 - ETA: 4s - loss: 0.0576 - acc: 0.979 - ETA: 4s - loss: 0.0589 - acc: 0.978 - ETA: 4s - loss: 0.0641 - acc: 0.977 - ETA: 4s - loss: 0.0627 - acc: 0.978 - ETA: 4s - loss: 0.0633 - acc: 0.977 - ETA: 4s - loss: 0.0627 - acc: 0.977 - ETA: 4s - loss: 0.0616 - acc: 0.977 - ETA: 3s - loss: 0.0613 - acc: 0.977 - ETA: 3s - loss: 0.0632 - acc: 0.976 - ETA: 3s - loss: 0.0623 - acc: 0.976 - ETA: 3s - loss: 0.0635 - acc: 0.976 - ETA: 3s - loss: 0.0630 - acc: 0.976 - ETA: 3s - loss: 0.0653 - acc: 0.975 - ETA: 3s - loss: 0.0664 - acc: 0.975 - ETA: 3s - loss: 0.0651 - acc: 0.975 - ETA: 3s - loss: 0.0651 - acc: 0.976 - ETA: 3s - loss: 0.0653 - acc: 0.975 - ETA: 3s - loss: 0.0650 - acc: 0.976 - ETA: 3s - loss: 0.0666 - acc: 0.975 - ETA: 3s - loss: 0.0662 - acc: 0.975 - ETA: 3s - loss: 0.0671 - acc: 0.974 - ETA: 3s - loss: 0.0669 - acc: 0.975 - ETA: 3s - loss: 0.0672 - acc: 0.974 - ETA: 3s - loss: 0.0666 - acc: 0.975 - ETA: 2s - loss: 0.0675 - acc: 0.974 - ETA: 2s - loss: 0.0695 - acc: 0.974 - ETA: 2s - loss: 0.0717 - acc: 0.973 - ETA: 2s - loss: 0.0741 - acc: 0.973 - ETA: 2s - loss: 0.0744 - acc: 0.973 - ETA: 2s - loss: 0.0737 - acc: 0.973 - ETA: 2s - loss: 0.0735 - acc: 0.973 - ETA: 2s - loss: 0.0736 - acc: 0.973 - ETA: 2s - loss: 0.0739 - acc: 0.972 - ETA: 2s - loss: 0.0738 - acc: 0.972 - ETA: 2s - loss: 0.0740 - acc: 0.972 - ETA: 2s - loss: 0.0740 - acc: 0.972 - ETA: 2s - loss: 0.0751 - acc: 0.972 - ETA: 2s - loss: 0.0766 - acc: 0.971 - ETA: 2s - loss: 0.0788 - acc: 0.971 - ETA: 2s - loss: 0.0804 - acc: 0.970 - ETA: 2s - loss: 0.0809 - acc: 0.970 - ETA: 2s - loss: 0.0806 - acc: 0.970 - ETA: 2s - loss: 0.0798 - acc: 0.970 - ETA: 1s - loss: 0.0813 - acc: 0.970 - ETA: 1s - loss: 0.0813 - acc: 0.971 - ETA: 1s - loss: 0.0807 - acc: 0.971 - ETA: 1s - loss: 0.0798 - acc: 0.971 - ETA: 1s - loss: 0.0795 - acc: 0.971 - ETA: 1s - loss: 0.0804 - acc: 0.971 - ETA: 1s - loss: 0.0822 - acc: 0.970 - ETA: 1s - loss: 0.0815 - acc: 0.970 - ETA: 1s - loss: 0.0811 - acc: 0.971 - ETA: 1s - loss: 0.0803 - acc: 0.971 - ETA: 1s - loss: 0.0826 - acc: 0.971 - ETA: 1s - loss: 0.0824 - acc: 0.971 - ETA: 1s - loss: 0.0841 - acc: 0.970 - ETA: 1s - loss: 0.0843 - acc: 0.970 - ETA: 1s - loss: 0.0838 - acc: 0.970 - ETA: 1s - loss: 0.0849 - acc: 0.970 - ETA: 1s - loss: 0.0849 - acc: 0.970 - ETA: 1s - loss: 0.0847 - acc: 0.970 - ETA: 0s - loss: 0.0845 - acc: 0.970 - ETA: 0s - loss: 0.0844 - acc: 0.970 - ETA: 0s - loss: 0.0842 - acc: 0.970 - ETA: 0s - loss: 0.0841 - acc: 0.970 - ETA: 0s - loss: 0.0847 - acc: 0.970 - ETA: 0s - loss: 0.0855 - acc: 0.970 - ETA: 0s - loss: 0.0849 - acc: 0.970 - ETA: 0s - loss: 0.0848 - acc: 0.970 - ETA: 0s - loss: 0.0857 - acc: 0.970 - ETA: 0s - loss: 0.0883 - acc: 0.969 - ETA: 0s - loss: 0.0903 - acc: 0.969 - ETA: 0s - loss: 0.0899 - acc: 0.969 - ETA: 0s - loss: 0.0900 - acc: 0.969 - ETA: 0s - loss: 0.0903 - acc: 0.969 - ETA: 0s - loss: 0.0897 - acc: 0.969 - ETA: 0s - loss: 0.0897 - acc: 0.969 - ETA: 0s - loss: 0.0895 - acc: 0.9695Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0890 - acc: 0.9698 - val_loss: 0.7739 - val_acc: 0.8479\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0764 - acc: 0.950 - ETA: 5s - loss: 0.0404 - acc: 0.987 - ETA: 5s - loss: 0.0294 - acc: 0.992 - ETA: 5s - loss: 0.0209 - acc: 0.995 - ETA: 5s - loss: 0.0224 - acc: 0.992 - ETA: 5s - loss: 0.0234 - acc: 0.990 - ETA: 5s - loss: 0.0343 - acc: 0.986 - ETA: 5s - loss: 0.0400 - acc: 0.984 - ETA: 5s - loss: 0.0418 - acc: 0.984 - ETA: 5s - loss: 0.0442 - acc: 0.983 - ETA: 5s - loss: 0.0404 - acc: 0.985 - ETA: 5s - loss: 0.0384 - acc: 0.986 - ETA: 5s - loss: 0.0379 - acc: 0.986 - ETA: 5s - loss: 0.0370 - acc: 0.986 - ETA: 5s - loss: 0.0390 - acc: 0.984 - ETA: 5s - loss: 0.0390 - acc: 0.984 - ETA: 5s - loss: 0.0388 - acc: 0.984 - ETA: 5s - loss: 0.0435 - acc: 0.983 - ETA: 5s - loss: 0.0441 - acc: 0.983 - ETA: 5s - loss: 0.0435 - acc: 0.983 - ETA: 5s - loss: 0.0464 - acc: 0.982 - ETA: 4s - loss: 0.0452 - acc: 0.983 - ETA: 4s - loss: 0.0443 - acc: 0.984 - ETA: 4s - loss: 0.0436 - acc: 0.984 - ETA: 4s - loss: 0.0463 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.983 - ETA: 4s - loss: 0.0491 - acc: 0.983 - ETA: 4s - loss: 0.0483 - acc: 0.983 - ETA: 4s - loss: 0.0499 - acc: 0.983 - ETA: 4s - loss: 0.0560 - acc: 0.982 - ETA: 4s - loss: 0.0557 - acc: 0.982 - ETA: 4s - loss: 0.0541 - acc: 0.983 - ETA: 4s - loss: 0.0541 - acc: 0.982 - ETA: 4s - loss: 0.0535 - acc: 0.983 - ETA: 4s - loss: 0.0537 - acc: 0.982 - ETA: 4s - loss: 0.0554 - acc: 0.982 - ETA: 4s - loss: 0.0571 - acc: 0.982 - ETA: 4s - loss: 0.0565 - acc: 0.982 - ETA: 4s - loss: 0.0590 - acc: 0.980 - ETA: 4s - loss: 0.0625 - acc: 0.980 - ETA: 4s - loss: 0.0621 - acc: 0.980 - ETA: 3s - loss: 0.0627 - acc: 0.980 - ETA: 3s - loss: 0.0620 - acc: 0.980 - ETA: 3s - loss: 0.0622 - acc: 0.980 - ETA: 3s - loss: 0.0624 - acc: 0.980 - ETA: 3s - loss: 0.0645 - acc: 0.979 - ETA: 3s - loss: 0.0635 - acc: 0.980 - ETA: 3s - loss: 0.0653 - acc: 0.979 - ETA: 3s - loss: 0.0658 - acc: 0.979 - ETA: 3s - loss: 0.0659 - acc: 0.979 - ETA: 3s - loss: 0.0650 - acc: 0.979 - ETA: 3s - loss: 0.0683 - acc: 0.979 - ETA: 3s - loss: 0.0684 - acc: 0.979 - ETA: 3s - loss: 0.0679 - acc: 0.979 - ETA: 3s - loss: 0.0691 - acc: 0.978 - ETA: 3s - loss: 0.0695 - acc: 0.978 - ETA: 3s - loss: 0.0697 - acc: 0.978 - ETA: 3s - loss: 0.0707 - acc: 0.978 - ETA: 2s - loss: 0.0717 - acc: 0.977 - ETA: 2s - loss: 0.0709 - acc: 0.977 - ETA: 2s - loss: 0.0717 - acc: 0.977 - ETA: 2s - loss: 0.0709 - acc: 0.977 - ETA: 2s - loss: 0.0704 - acc: 0.978 - ETA: 2s - loss: 0.0707 - acc: 0.977 - ETA: 2s - loss: 0.0702 - acc: 0.977 - ETA: 2s - loss: 0.0700 - acc: 0.977 - ETA: 2s - loss: 0.0694 - acc: 0.978 - ETA: 2s - loss: 0.0704 - acc: 0.977 - ETA: 2s - loss: 0.0701 - acc: 0.977 - ETA: 2s - loss: 0.0693 - acc: 0.978 - ETA: 2s - loss: 0.0693 - acc: 0.977 - ETA: 2s - loss: 0.0689 - acc: 0.977 - ETA: 2s - loss: 0.0692 - acc: 0.977 - ETA: 2s - loss: 0.0712 - acc: 0.976 - ETA: 2s - loss: 0.0706 - acc: 0.977 - ETA: 2s - loss: 0.0712 - acc: 0.977 - ETA: 1s - loss: 0.0721 - acc: 0.977 - ETA: 1s - loss: 0.0717 - acc: 0.977 - ETA: 1s - loss: 0.0709 - acc: 0.977 - ETA: 1s - loss: 0.0703 - acc: 0.977 - ETA: 1s - loss: 0.0698 - acc: 0.977 - ETA: 1s - loss: 0.0697 - acc: 0.977 - ETA: 1s - loss: 0.0713 - acc: 0.977 - ETA: 1s - loss: 0.0718 - acc: 0.977 - ETA: 1s - loss: 0.0721 - acc: 0.977 - ETA: 1s - loss: 0.0716 - acc: 0.977 - ETA: 1s - loss: 0.0731 - acc: 0.976 - ETA: 1s - loss: 0.0728 - acc: 0.976 - ETA: 1s - loss: 0.0744 - acc: 0.976 - ETA: 1s - loss: 0.0744 - acc: 0.976 - ETA: 1s - loss: 0.0741 - acc: 0.976 - ETA: 1s - loss: 0.0749 - acc: 0.976 - ETA: 1s - loss: 0.0746 - acc: 0.976 - ETA: 1s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0743 - acc: 0.976 - ETA: 0s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0745 - acc: 0.976 - ETA: 0s - loss: 0.0744 - acc: 0.976 - ETA: 0s - loss: 0.0741 - acc: 0.976 - ETA: 0s - loss: 0.0743 - acc: 0.976 - ETA: 0s - loss: 0.0753 - acc: 0.975 - ETA: 0s - loss: 0.0756 - acc: 0.975 - ETA: 0s - loss: 0.0759 - acc: 0.975 - ETA: 0s - loss: 0.0770 - acc: 0.974 - ETA: 0s - loss: 0.0770 - acc: 0.974 - ETA: 0s - loss: 0.0768 - acc: 0.974 - ETA: 0s - loss: 0.0765 - acc: 0.974 - ETA: 0s - loss: 0.0767 - acc: 0.974 - ETA: 0s - loss: 0.0761 - acc: 0.974 - ETA: 0s - loss: 0.0773 - acc: 0.974 - ETA: 0s - loss: 0.0787 - acc: 0.9745Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0786 - acc: 0.9744 - val_loss: 0.8104 - val_acc: 0.8635\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 5s - loss: 0.0423 - acc: 0.950 - ETA: 5s - loss: 0.0221 - acc: 0.987 - ETA: 5s - loss: 0.1039 - acc: 0.964 - ETA: 5s - loss: 0.0802 - acc: 0.975 - ETA: 5s - loss: 0.0725 - acc: 0.976 - ETA: 5s - loss: 0.0694 - acc: 0.978 - ETA: 5s - loss: 0.0640 - acc: 0.978 - ETA: 5s - loss: 0.0775 - acc: 0.975 - ETA: 5s - loss: 0.0760 - acc: 0.976 - ETA: 5s - loss: 0.0702 - acc: 0.978 - ETA: 5s - loss: 0.0652 - acc: 0.980 - ETA: 5s - loss: 0.0646 - acc: 0.979 - ETA: 5s - loss: 0.0639 - acc: 0.979 - ETA: 5s - loss: 0.0624 - acc: 0.980 - ETA: 5s - loss: 0.0600 - acc: 0.980 - ETA: 5s - loss: 0.0578 - acc: 0.981 - ETA: 5s - loss: 0.0564 - acc: 0.982 - ETA: 5s - loss: 0.0543 - acc: 0.982 - ETA: 5s - loss: 0.0529 - acc: 0.982 - ETA: 5s - loss: 0.0520 - acc: 0.982 - ETA: 5s - loss: 0.0516 - acc: 0.982 - ETA: 5s - loss: 0.0498 - acc: 0.983 - ETA: 5s - loss: 0.0516 - acc: 0.982 - ETA: 5s - loss: 0.0513 - acc: 0.982 - ETA: 5s - loss: 0.0500 - acc: 0.982 - ETA: 4s - loss: 0.0488 - acc: 0.983 - ETA: 4s - loss: 0.0512 - acc: 0.982 - ETA: 4s - loss: 0.0521 - acc: 0.982 - ETA: 4s - loss: 0.0548 - acc: 0.981 - ETA: 4s - loss: 0.0536 - acc: 0.981 - ETA: 4s - loss: 0.0565 - acc: 0.981 - ETA: 4s - loss: 0.0556 - acc: 0.981 - ETA: 4s - loss: 0.0545 - acc: 0.982 - ETA: 4s - loss: 0.0535 - acc: 0.982 - ETA: 4s - loss: 0.0541 - acc: 0.982 - ETA: 4s - loss: 0.0547 - acc: 0.982 - ETA: 4s - loss: 0.0544 - acc: 0.982 - ETA: 4s - loss: 0.0577 - acc: 0.982 - ETA: 4s - loss: 0.0565 - acc: 0.982 - ETA: 4s - loss: 0.0555 - acc: 0.982 - ETA: 4s - loss: 0.0554 - acc: 0.982 - ETA: 4s - loss: 0.0551 - acc: 0.982 - ETA: 4s - loss: 0.0564 - acc: 0.982 - ETA: 3s - loss: 0.0588 - acc: 0.982 - ETA: 3s - loss: 0.0589 - acc: 0.982 - ETA: 3s - loss: 0.0588 - acc: 0.982 - ETA: 3s - loss: 0.0581 - acc: 0.982 - ETA: 3s - loss: 0.0607 - acc: 0.980 - ETA: 3s - loss: 0.0598 - acc: 0.981 - ETA: 3s - loss: 0.0602 - acc: 0.980 - ETA: 3s - loss: 0.0602 - acc: 0.980 - ETA: 3s - loss: 0.0616 - acc: 0.980 - ETA: 3s - loss: 0.0606 - acc: 0.980 - ETA: 3s - loss: 0.0599 - acc: 0.980 - ETA: 3s - loss: 0.0606 - acc: 0.980 - ETA: 3s - loss: 0.0615 - acc: 0.980 - ETA: 3s - loss: 0.0626 - acc: 0.979 - ETA: 3s - loss: 0.0618 - acc: 0.980 - ETA: 3s - loss: 0.0617 - acc: 0.979 - ETA: 3s - loss: 0.0614 - acc: 0.979 - ETA: 2s - loss: 0.0611 - acc: 0.979 - ETA: 2s - loss: 0.0607 - acc: 0.979 - ETA: 2s - loss: 0.0616 - acc: 0.980 - ETA: 2s - loss: 0.0609 - acc: 0.980 - ETA: 2s - loss: 0.0603 - acc: 0.980 - ETA: 2s - loss: 0.0607 - acc: 0.980 - ETA: 2s - loss: 0.0618 - acc: 0.980 - ETA: 2s - loss: 0.0625 - acc: 0.980 - ETA: 2s - loss: 0.0626 - acc: 0.980 - ETA: 2s - loss: 0.0620 - acc: 0.980 - ETA: 2s - loss: 0.0621 - acc: 0.980 - ETA: 2s - loss: 0.0621 - acc: 0.980 - ETA: 2s - loss: 0.0616 - acc: 0.980 - ETA: 2s - loss: 0.0611 - acc: 0.980 - ETA: 2s - loss: 0.0630 - acc: 0.980 - ETA: 2s - loss: 0.0634 - acc: 0.980 - ETA: 2s - loss: 0.0630 - acc: 0.980 - ETA: 1s - loss: 0.0633 - acc: 0.980 - ETA: 1s - loss: 0.0626 - acc: 0.980 - ETA: 1s - loss: 0.0625 - acc: 0.980 - ETA: 1s - loss: 0.0622 - acc: 0.980 - ETA: 1s - loss: 0.0630 - acc: 0.980 - ETA: 1s - loss: 0.0636 - acc: 0.980 - ETA: 1s - loss: 0.0642 - acc: 0.980 - ETA: 1s - loss: 0.0643 - acc: 0.980 - ETA: 1s - loss: 0.0641 - acc: 0.980 - ETA: 1s - loss: 0.0645 - acc: 0.980 - ETA: 1s - loss: 0.0658 - acc: 0.980 - ETA: 1s - loss: 0.0652 - acc: 0.980 - ETA: 1s - loss: 0.0661 - acc: 0.979 - ETA: 1s - loss: 0.0663 - acc: 0.979 - ETA: 1s - loss: 0.0674 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0664 - acc: 0.979 - ETA: 0s - loss: 0.0666 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0672 - acc: 0.979 - ETA: 0s - loss: 0.0667 - acc: 0.979 - ETA: 0s - loss: 0.0667 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0668 - acc: 0.979 - ETA: 0s - loss: 0.0678 - acc: 0.979 - ETA: 0s - loss: 0.0674 - acc: 0.979 - ETA: 0s - loss: 0.0673 - acc: 0.979 - ETA: 0s - loss: 0.0673 - acc: 0.979 - ETA: 0s - loss: 0.0671 - acc: 0.979 - ETA: 0s - loss: 0.0671 - acc: 0.979 - ETA: 0s - loss: 0.0683 - acc: 0.979 - ETA: 0s - loss: 0.0695 - acc: 0.979 - ETA: 0s - loss: 0.0694 - acc: 0.9791Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0692 - acc: 0.9792 - val_loss: 0.8291 - val_acc: 0.8623\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0258 - acc: 1.000 - ETA: 6s - loss: 0.0255 - acc: 0.987 - ETA: 6s - loss: 0.0213 - acc: 0.985 - ETA: 5s - loss: 0.0183 - acc: 0.990 - ETA: 5s - loss: 0.0355 - acc: 0.984 - ETA: 5s - loss: 0.0412 - acc: 0.981 - ETA: 5s - loss: 0.0466 - acc: 0.978 - ETA: 5s - loss: 0.0415 - acc: 0.981 - ETA: 5s - loss: 0.0396 - acc: 0.982 - ETA: 5s - loss: 0.0370 - acc: 0.983 - ETA: 5s - loss: 0.0348 - acc: 0.985 - ETA: 5s - loss: 0.0366 - acc: 0.985 - ETA: 5s - loss: 0.0371 - acc: 0.985 - ETA: 5s - loss: 0.0353 - acc: 0.986 - ETA: 5s - loss: 0.0332 - acc: 0.987 - ETA: 5s - loss: 0.0428 - acc: 0.985 - ETA: 5s - loss: 0.0438 - acc: 0.985 - ETA: 5s - loss: 0.0422 - acc: 0.986 - ETA: 5s - loss: 0.0485 - acc: 0.985 - ETA: 5s - loss: 0.0507 - acc: 0.984 - ETA: 5s - loss: 0.0512 - acc: 0.983 - ETA: 5s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0483 - acc: 0.985 - ETA: 4s - loss: 0.0505 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0476 - acc: 0.985 - ETA: 4s - loss: 0.0465 - acc: 0.986 - ETA: 4s - loss: 0.0454 - acc: 0.986 - ETA: 4s - loss: 0.0457 - acc: 0.985 - ETA: 4s - loss: 0.0443 - acc: 0.986 - ETA: 4s - loss: 0.0451 - acc: 0.985 - ETA: 4s - loss: 0.0458 - acc: 0.985 - ETA: 4s - loss: 0.0446 - acc: 0.986 - ETA: 4s - loss: 0.0476 - acc: 0.986 - ETA: 4s - loss: 0.0466 - acc: 0.986 - ETA: 4s - loss: 0.0464 - acc: 0.986 - ETA: 4s - loss: 0.0467 - acc: 0.986 - ETA: 4s - loss: 0.0460 - acc: 0.986 - ETA: 4s - loss: 0.0453 - acc: 0.987 - ETA: 4s - loss: 0.0467 - acc: 0.986 - ETA: 3s - loss: 0.0457 - acc: 0.987 - ETA: 3s - loss: 0.0496 - acc: 0.986 - ETA: 3s - loss: 0.0491 - acc: 0.986 - ETA: 3s - loss: 0.0484 - acc: 0.986 - ETA: 3s - loss: 0.0489 - acc: 0.986 - ETA: 3s - loss: 0.0518 - acc: 0.985 - ETA: 3s - loss: 0.0513 - acc: 0.985 - ETA: 3s - loss: 0.0536 - acc: 0.984 - ETA: 3s - loss: 0.0532 - acc: 0.984 - ETA: 3s - loss: 0.0526 - acc: 0.984 - ETA: 3s - loss: 0.0523 - acc: 0.985 - ETA: 3s - loss: 0.0523 - acc: 0.985 - ETA: 3s - loss: 0.0543 - acc: 0.984 - ETA: 3s - loss: 0.0533 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0565 - acc: 0.984 - ETA: 3s - loss: 0.0593 - acc: 0.983 - ETA: 3s - loss: 0.0587 - acc: 0.983 - ETA: 3s - loss: 0.0583 - acc: 0.983 - ETA: 2s - loss: 0.0575 - acc: 0.984 - ETA: 2s - loss: 0.0601 - acc: 0.983 - ETA: 2s - loss: 0.0597 - acc: 0.984 - ETA: 2s - loss: 0.0591 - acc: 0.984 - ETA: 2s - loss: 0.0594 - acc: 0.984 - ETA: 2s - loss: 0.0596 - acc: 0.983 - ETA: 2s - loss: 0.0594 - acc: 0.983 - ETA: 2s - loss: 0.0588 - acc: 0.983 - ETA: 2s - loss: 0.0600 - acc: 0.983 - ETA: 2s - loss: 0.0592 - acc: 0.983 - ETA: 2s - loss: 0.0587 - acc: 0.983 - ETA: 2s - loss: 0.0590 - acc: 0.983 - ETA: 2s - loss: 0.0587 - acc: 0.983 - ETA: 2s - loss: 0.0585 - acc: 0.983 - ETA: 2s - loss: 0.0580 - acc: 0.983 - ETA: 2s - loss: 0.0583 - acc: 0.983 - ETA: 2s - loss: 0.0581 - acc: 0.983 - ETA: 1s - loss: 0.0575 - acc: 0.983 - ETA: 1s - loss: 0.0588 - acc: 0.983 - ETA: 1s - loss: 0.0589 - acc: 0.983 - ETA: 1s - loss: 0.0598 - acc: 0.982 - ETA: 1s - loss: 0.0601 - acc: 0.982 - ETA: 1s - loss: 0.0600 - acc: 0.982 - ETA: 1s - loss: 0.0619 - acc: 0.982 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 1s - loss: 0.0610 - acc: 0.982 - ETA: 1s - loss: 0.0610 - acc: 0.982 - ETA: 1s - loss: 0.0631 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0649 - acc: 0.981 - ETA: 1s - loss: 0.0647 - acc: 0.981 - ETA: 1s - loss: 0.0641 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0635 - acc: 0.981 - ETA: 1s - loss: 0.0628 - acc: 0.982 - ETA: 0s - loss: 0.0638 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0632 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0624 - acc: 0.981 - ETA: 0s - loss: 0.0620 - acc: 0.981 - ETA: 0s - loss: 0.0617 - acc: 0.981 - ETA: 0s - loss: 0.0613 - acc: 0.981 - ETA: 0s - loss: 0.0620 - acc: 0.981 - ETA: 0s - loss: 0.0617 - acc: 0.981 - ETA: 0s - loss: 0.0622 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0623 - acc: 0.981 - ETA: 0s - loss: 0.0618 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0627 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.9817Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0626 - acc: 0.9817 - val_loss: 0.8523 - val_acc: 0.8479\n",
      "Epoch 15/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0046 - acc: 1.000 - ETA: 5s - loss: 0.0417 - acc: 0.975 - ETA: 5s - loss: 0.0360 - acc: 0.978 - ETA: 5s - loss: 0.0319 - acc: 0.980 - ETA: 5s - loss: 0.0396 - acc: 0.980 - ETA: 5s - loss: 0.0328 - acc: 0.984 - ETA: 5s - loss: 0.0353 - acc: 0.984 - ETA: 5s - loss: 0.0483 - acc: 0.981 - ETA: 5s - loss: 0.0455 - acc: 0.982 - ETA: 5s - loss: 0.0421 - acc: 0.983 - ETA: 5s - loss: 0.0384 - acc: 0.985 - ETA: 5s - loss: 0.0445 - acc: 0.985 - ETA: 5s - loss: 0.0499 - acc: 0.983 - ETA: 5s - loss: 0.0528 - acc: 0.983 - ETA: 5s - loss: 0.0500 - acc: 0.984 - ETA: 5s - loss: 0.0477 - acc: 0.985 - ETA: 5s - loss: 0.0495 - acc: 0.983 - ETA: 5s - loss: 0.0509 - acc: 0.983 - ETA: 5s - loss: 0.0495 - acc: 0.984 - ETA: 5s - loss: 0.0527 - acc: 0.984 - ETA: 5s - loss: 0.0512 - acc: 0.984 - ETA: 5s - loss: 0.0506 - acc: 0.985 - ETA: 4s - loss: 0.0485 - acc: 0.985 - ETA: 4s - loss: 0.0514 - acc: 0.984 - ETA: 4s - loss: 0.0498 - acc: 0.984 - ETA: 4s - loss: 0.0536 - acc: 0.982 - ETA: 4s - loss: 0.0527 - acc: 0.983 - ETA: 4s - loss: 0.0508 - acc: 0.984 - ETA: 4s - loss: 0.0504 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0506 - acc: 0.983 - ETA: 4s - loss: 0.0507 - acc: 0.983 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0486 - acc: 0.983 - ETA: 4s - loss: 0.0519 - acc: 0.982 - ETA: 4s - loss: 0.0508 - acc: 0.983 - ETA: 3s - loss: 0.0502 - acc: 0.983 - ETA: 3s - loss: 0.0498 - acc: 0.983 - ETA: 3s - loss: 0.0487 - acc: 0.983 - ETA: 3s - loss: 0.0479 - acc: 0.983 - ETA: 3s - loss: 0.0475 - acc: 0.983 - ETA: 3s - loss: 0.0469 - acc: 0.984 - ETA: 3s - loss: 0.0463 - acc: 0.984 - ETA: 3s - loss: 0.0458 - acc: 0.984 - ETA: 3s - loss: 0.0451 - acc: 0.985 - ETA: 3s - loss: 0.0449 - acc: 0.985 - ETA: 3s - loss: 0.0445 - acc: 0.985 - ETA: 3s - loss: 0.0438 - acc: 0.985 - ETA: 3s - loss: 0.0431 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.985 - ETA: 3s - loss: 0.0425 - acc: 0.986 - ETA: 3s - loss: 0.0421 - acc: 0.986 - ETA: 3s - loss: 0.0414 - acc: 0.986 - ETA: 3s - loss: 0.0430 - acc: 0.986 - ETA: 2s - loss: 0.0424 - acc: 0.986 - ETA: 2s - loss: 0.0431 - acc: 0.986 - ETA: 2s - loss: 0.0427 - acc: 0.986 - ETA: 2s - loss: 0.0421 - acc: 0.987 - ETA: 2s - loss: 0.0429 - acc: 0.986 - ETA: 2s - loss: 0.0433 - acc: 0.986 - ETA: 2s - loss: 0.0434 - acc: 0.986 - ETA: 2s - loss: 0.0433 - acc: 0.986 - ETA: 2s - loss: 0.0430 - acc: 0.986 - ETA: 2s - loss: 0.0425 - acc: 0.986 - ETA: 2s - loss: 0.0451 - acc: 0.985 - ETA: 2s - loss: 0.0448 - acc: 0.986 - ETA: 2s - loss: 0.0445 - acc: 0.986 - ETA: 2s - loss: 0.0450 - acc: 0.986 - ETA: 2s - loss: 0.0451 - acc: 0.985 - ETA: 2s - loss: 0.0455 - acc: 0.985 - ETA: 2s - loss: 0.0452 - acc: 0.985 - ETA: 2s - loss: 0.0448 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0458 - acc: 0.985 - ETA: 1s - loss: 0.0463 - acc: 0.985 - ETA: 1s - loss: 0.0458 - acc: 0.985 - ETA: 1s - loss: 0.0453 - acc: 0.985 - ETA: 1s - loss: 0.0457 - acc: 0.985 - ETA: 1s - loss: 0.0457 - acc: 0.985 - ETA: 1s - loss: 0.0455 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0480 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0487 - acc: 0.985 - ETA: 1s - loss: 0.0489 - acc: 0.985 - ETA: 1s - loss: 0.0493 - acc: 0.984 - ETA: 1s - loss: 0.0492 - acc: 0.984 - ETA: 1s - loss: 0.0496 - acc: 0.984 - ETA: 1s - loss: 0.0494 - acc: 0.984 - ETA: 0s - loss: 0.0503 - acc: 0.984 - ETA: 0s - loss: 0.0510 - acc: 0.984 - ETA: 0s - loss: 0.0509 - acc: 0.984 - ETA: 0s - loss: 0.0514 - acc: 0.984 - ETA: 0s - loss: 0.0511 - acc: 0.984 - ETA: 0s - loss: 0.0511 - acc: 0.984 - ETA: 0s - loss: 0.0509 - acc: 0.984 - ETA: 0s - loss: 0.0516 - acc: 0.984 - ETA: 0s - loss: 0.0516 - acc: 0.984 - ETA: 0s - loss: 0.0514 - acc: 0.984 - ETA: 0s - loss: 0.0518 - acc: 0.984 - ETA: 0s - loss: 0.0525 - acc: 0.983 - ETA: 0s - loss: 0.0526 - acc: 0.983 - ETA: 0s - loss: 0.0524 - acc: 0.983 - ETA: 0s - loss: 0.0523 - acc: 0.983 - ETA: 0s - loss: 0.0519 - acc: 0.983 - ETA: 0s - loss: 0.0517 - acc: 0.9838Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0530 - acc: 0.9837 - val_loss: 0.8981 - val_acc: 0.8467\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0989 - acc: 0.950 - ETA: 6s - loss: 0.0273 - acc: 0.987 - ETA: 6s - loss: 0.0496 - acc: 0.978 - ETA: 6s - loss: 0.0381 - acc: 0.985 - ETA: 6s - loss: 0.0318 - acc: 0.988 - ETA: 6s - loss: 0.0261 - acc: 0.990 - ETA: 6s - loss: 0.0239 - acc: 0.992 - ETA: 6s - loss: 0.0220 - acc: 0.993 - ETA: 5s - loss: 0.0197 - acc: 0.994 - ETA: 5s - loss: 0.0186 - acc: 0.994 - ETA: 5s - loss: 0.0219 - acc: 0.993 - ETA: 5s - loss: 0.0278 - acc: 0.991 - ETA: 5s - loss: 0.0265 - acc: 0.991 - ETA: 5s - loss: 0.0267 - acc: 0.992 - ETA: 5s - loss: 0.0287 - acc: 0.991 - ETA: 5s - loss: 0.0305 - acc: 0.991 - ETA: 5s - loss: 0.0303 - acc: 0.991 - ETA: 5s - loss: 0.0316 - acc: 0.991 - ETA: 5s - loss: 0.0312 - acc: 0.990 - ETA: 5s - loss: 0.0304 - acc: 0.991 - ETA: 5s - loss: 0.0318 - acc: 0.990 - ETA: 5s - loss: 0.0318 - acc: 0.989 - ETA: 5s - loss: 0.0308 - acc: 0.990 - ETA: 5s - loss: 0.0365 - acc: 0.987 - ETA: 4s - loss: 0.0368 - acc: 0.987 - ETA: 4s - loss: 0.0364 - acc: 0.988 - ETA: 4s - loss: 0.0368 - acc: 0.987 - ETA: 4s - loss: 0.0380 - acc: 0.987 - ETA: 4s - loss: 0.0392 - acc: 0.986 - ETA: 4s - loss: 0.0387 - acc: 0.986 - ETA: 4s - loss: 0.0406 - acc: 0.986 - ETA: 4s - loss: 0.0400 - acc: 0.986 - ETA: 4s - loss: 0.0391 - acc: 0.986 - ETA: 4s - loss: 0.0383 - acc: 0.987 - ETA: 4s - loss: 0.0386 - acc: 0.986 - ETA: 4s - loss: 0.0380 - acc: 0.986 - ETA: 4s - loss: 0.0371 - acc: 0.987 - ETA: 4s - loss: 0.0372 - acc: 0.987 - ETA: 4s - loss: 0.0375 - acc: 0.987 - ETA: 4s - loss: 0.0397 - acc: 0.986 - ETA: 4s - loss: 0.0388 - acc: 0.986 - ETA: 3s - loss: 0.0382 - acc: 0.986 - ETA: 3s - loss: 0.0375 - acc: 0.987 - ETA: 3s - loss: 0.0381 - acc: 0.986 - ETA: 3s - loss: 0.0384 - acc: 0.986 - ETA: 3s - loss: 0.0380 - acc: 0.986 - ETA: 3s - loss: 0.0375 - acc: 0.987 - ETA: 3s - loss: 0.0372 - acc: 0.987 - ETA: 3s - loss: 0.0403 - acc: 0.986 - ETA: 3s - loss: 0.0431 - acc: 0.986 - ETA: 3s - loss: 0.0426 - acc: 0.986 - ETA: 3s - loss: 0.0440 - acc: 0.986 - ETA: 3s - loss: 0.0434 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.986 - ETA: 3s - loss: 0.0428 - acc: 0.986 - ETA: 3s - loss: 0.0460 - acc: 0.986 - ETA: 3s - loss: 0.0463 - acc: 0.985 - ETA: 3s - loss: 0.0456 - acc: 0.986 - ETA: 2s - loss: 0.0472 - acc: 0.985 - ETA: 2s - loss: 0.0465 - acc: 0.986 - ETA: 2s - loss: 0.0460 - acc: 0.986 - ETA: 2s - loss: 0.0460 - acc: 0.985 - ETA: 2s - loss: 0.0482 - acc: 0.985 - ETA: 2s - loss: 0.0484 - acc: 0.985 - ETA: 2s - loss: 0.0483 - acc: 0.985 - ETA: 2s - loss: 0.0476 - acc: 0.985 - ETA: 2s - loss: 0.0486 - acc: 0.985 - ETA: 2s - loss: 0.0489 - acc: 0.985 - ETA: 2s - loss: 0.0488 - acc: 0.985 - ETA: 2s - loss: 0.0487 - acc: 0.985 - ETA: 2s - loss: 0.0480 - acc: 0.985 - ETA: 2s - loss: 0.0475 - acc: 0.985 - ETA: 2s - loss: 0.0478 - acc: 0.985 - ETA: 2s - loss: 0.0473 - acc: 0.985 - ETA: 2s - loss: 0.0481 - acc: 0.985 - ETA: 2s - loss: 0.0475 - acc: 0.985 - ETA: 1s - loss: 0.0469 - acc: 0.985 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.986 - ETA: 1s - loss: 0.0458 - acc: 0.986 - ETA: 1s - loss: 0.0453 - acc: 0.986 - ETA: 1s - loss: 0.0456 - acc: 0.986 - ETA: 1s - loss: 0.0455 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0455 - acc: 0.986 - ETA: 1s - loss: 0.0454 - acc: 0.986 - ETA: 1s - loss: 0.0456 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0467 - acc: 0.986 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0464 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.985 - ETA: 1s - loss: 0.0459 - acc: 0.986 - ETA: 1s - loss: 0.0454 - acc: 0.986 - ETA: 0s - loss: 0.0458 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - ETA: 0s - loss: 0.0465 - acc: 0.985 - ETA: 0s - loss: 0.0464 - acc: 0.985 - ETA: 0s - loss: 0.0462 - acc: 0.985 - ETA: 0s - loss: 0.0469 - acc: 0.985 - ETA: 0s - loss: 0.0468 - acc: 0.985 - ETA: 0s - loss: 0.0465 - acc: 0.985 - ETA: 0s - loss: 0.0461 - acc: 0.986 - ETA: 0s - loss: 0.0463 - acc: 0.985 - ETA: 0s - loss: 0.0480 - acc: 0.985 - ETA: 0s - loss: 0.0492 - acc: 0.985 - ETA: 0s - loss: 0.0488 - acc: 0.985 - ETA: 0s - loss: 0.0489 - acc: 0.985 - ETA: 0s - loss: 0.0495 - acc: 0.984 - ETA: 0s - loss: 0.0504 - acc: 0.9847Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0504 - acc: 0.9847 - val_loss: 0.8671 - val_acc: 0.8539\n",
      "Epoch 17/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.0200 - acc: 1.000 - ETA: 7s - loss: 0.0132 - acc: 1.000 - ETA: 7s - loss: 0.0109 - acc: 1.000 - ETA: 6s - loss: 0.0081 - acc: 1.000 - ETA: 6s - loss: 0.0081 - acc: 1.000 - ETA: 6s - loss: 0.0070 - acc: 1.000 - ETA: 6s - loss: 0.0088 - acc: 1.000 - ETA: 6s - loss: 0.0111 - acc: 0.997 - ETA: 6s - loss: 0.0103 - acc: 0.997 - ETA: 6s - loss: 0.0165 - acc: 0.996 - ETA: 5s - loss: 0.0154 - acc: 0.996 - ETA: 5s - loss: 0.0199 - acc: 0.995 - ETA: 5s - loss: 0.0216 - acc: 0.995 - ETA: 5s - loss: 0.0215 - acc: 0.994 - ETA: 5s - loss: 0.0206 - acc: 0.995 - ETA: 5s - loss: 0.0203 - acc: 0.995 - ETA: 5s - loss: 0.0225 - acc: 0.994 - ETA: 5s - loss: 0.0212 - acc: 0.995 - ETA: 5s - loss: 0.0237 - acc: 0.994 - ETA: 5s - loss: 0.0236 - acc: 0.994 - ETA: 5s - loss: 0.0228 - acc: 0.995 - ETA: 5s - loss: 0.0222 - acc: 0.995 - ETA: 5s - loss: 0.0213 - acc: 0.995 - ETA: 5s - loss: 0.0208 - acc: 0.995 - ETA: 4s - loss: 0.0213 - acc: 0.995 - ETA: 4s - loss: 0.0242 - acc: 0.994 - ETA: 4s - loss: 0.0238 - acc: 0.994 - ETA: 4s - loss: 0.0239 - acc: 0.994 - ETA: 4s - loss: 0.0231 - acc: 0.994 - ETA: 4s - loss: 0.0229 - acc: 0.994 - ETA: 4s - loss: 0.0237 - acc: 0.994 - ETA: 4s - loss: 0.0256 - acc: 0.994 - ETA: 4s - loss: 0.0263 - acc: 0.993 - ETA: 4s - loss: 0.0289 - acc: 0.992 - ETA: 4s - loss: 0.0312 - acc: 0.992 - ETA: 4s - loss: 0.0305 - acc: 0.992 - ETA: 4s - loss: 0.0342 - acc: 0.991 - ETA: 4s - loss: 0.0348 - acc: 0.991 - ETA: 4s - loss: 0.0354 - acc: 0.990 - ETA: 4s - loss: 0.0349 - acc: 0.991 - ETA: 4s - loss: 0.0342 - acc: 0.991 - ETA: 3s - loss: 0.0340 - acc: 0.991 - ETA: 3s - loss: 0.0336 - acc: 0.991 - ETA: 3s - loss: 0.0330 - acc: 0.991 - ETA: 3s - loss: 0.0327 - acc: 0.991 - ETA: 3s - loss: 0.0326 - acc: 0.991 - ETA: 3s - loss: 0.0325 - acc: 0.991 - ETA: 3s - loss: 0.0325 - acc: 0.991 - ETA: 3s - loss: 0.0324 - acc: 0.991 - ETA: 3s - loss: 0.0318 - acc: 0.991 - ETA: 3s - loss: 0.0334 - acc: 0.991 - ETA: 3s - loss: 0.0330 - acc: 0.991 - ETA: 3s - loss: 0.0335 - acc: 0.991 - ETA: 3s - loss: 0.0350 - acc: 0.990 - ETA: 3s - loss: 0.0359 - acc: 0.990 - ETA: 3s - loss: 0.0356 - acc: 0.990 - ETA: 3s - loss: 0.0351 - acc: 0.991 - ETA: 3s - loss: 0.0351 - acc: 0.990 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0343 - acc: 0.991 - ETA: 2s - loss: 0.0354 - acc: 0.990 - ETA: 2s - loss: 0.0359 - acc: 0.990 - ETA: 2s - loss: 0.0355 - acc: 0.990 - ETA: 2s - loss: 0.0369 - acc: 0.990 - ETA: 2s - loss: 0.0365 - acc: 0.990 - ETA: 2s - loss: 0.0360 - acc: 0.990 - ETA: 2s - loss: 0.0356 - acc: 0.990 - ETA: 2s - loss: 0.0351 - acc: 0.990 - ETA: 2s - loss: 0.0348 - acc: 0.990 - ETA: 2s - loss: 0.0345 - acc: 0.991 - ETA: 2s - loss: 0.0340 - acc: 0.991 - ETA: 2s - loss: 0.0345 - acc: 0.991 - ETA: 2s - loss: 0.0343 - acc: 0.991 - ETA: 2s - loss: 0.0368 - acc: 0.990 - ETA: 2s - loss: 0.0388 - acc: 0.990 - ETA: 2s - loss: 0.0392 - acc: 0.990 - ETA: 1s - loss: 0.0389 - acc: 0.990 - ETA: 1s - loss: 0.0391 - acc: 0.990 - ETA: 1s - loss: 0.0386 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0396 - acc: 0.990 - ETA: 1s - loss: 0.0391 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0392 - acc: 0.990 - ETA: 1s - loss: 0.0398 - acc: 0.990 - ETA: 1s - loss: 0.0397 - acc: 0.989 - ETA: 1s - loss: 0.0395 - acc: 0.989 - ETA: 1s - loss: 0.0405 - acc: 0.989 - ETA: 1s - loss: 0.0412 - acc: 0.989 - ETA: 1s - loss: 0.0410 - acc: 0.989 - ETA: 1s - loss: 0.0413 - acc: 0.989 - ETA: 1s - loss: 0.0414 - acc: 0.989 - ETA: 1s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0419 - acc: 0.989 - ETA: 0s - loss: 0.0415 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0417 - acc: 0.989 - ETA: 0s - loss: 0.0413 - acc: 0.989 - ETA: 0s - loss: 0.0411 - acc: 0.989 - ETA: 0s - loss: 0.0418 - acc: 0.989 - ETA: 0s - loss: 0.0416 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0410 - acc: 0.989 - ETA: 0s - loss: 0.0408 - acc: 0.989 - ETA: 0s - loss: 0.0404 - acc: 0.989 - ETA: 0s - loss: 0.0402 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0424 - acc: 0.988 - ETA: 0s - loss: 0.0422 - acc: 0.988 - ETA: 0s - loss: 0.0420 - acc: 0.9890Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0419 - acc: 0.9891 - val_loss: 0.8921 - val_acc: 0.8551\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0011 - acc: 1.000 - ETA: 5s - loss: 0.0095 - acc: 1.000 - ETA: 5s - loss: 0.0198 - acc: 0.992 - ETA: 5s - loss: 0.0151 - acc: 0.995 - ETA: 5s - loss: 0.0221 - acc: 0.992 - ETA: 5s - loss: 0.0199 - acc: 0.993 - ETA: 5s - loss: 0.0185 - acc: 0.994 - ETA: 5s - loss: 0.0175 - acc: 0.995 - ETA: 5s - loss: 0.0201 - acc: 0.994 - ETA: 5s - loss: 0.0199 - acc: 0.994 - ETA: 5s - loss: 0.0245 - acc: 0.991 - ETA: 5s - loss: 0.0234 - acc: 0.992 - ETA: 5s - loss: 0.0228 - acc: 0.993 - ETA: 5s - loss: 0.0215 - acc: 0.993 - ETA: 5s - loss: 0.0203 - acc: 0.994 - ETA: 5s - loss: 0.0216 - acc: 0.993 - ETA: 5s - loss: 0.0213 - acc: 0.993 - ETA: 5s - loss: 0.0208 - acc: 0.994 - ETA: 5s - loss: 0.0210 - acc: 0.993 - ETA: 5s - loss: 0.0204 - acc: 0.994 - ETA: 5s - loss: 0.0214 - acc: 0.993 - ETA: 4s - loss: 0.0234 - acc: 0.993 - ETA: 4s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.990 - ETA: 4s - loss: 0.0307 - acc: 0.990 - ETA: 4s - loss: 0.0297 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.990 - ETA: 4s - loss: 0.0307 - acc: 0.991 - ETA: 4s - loss: 0.0299 - acc: 0.991 - ETA: 4s - loss: 0.0300 - acc: 0.990 - ETA: 4s - loss: 0.0304 - acc: 0.990 - ETA: 4s - loss: 0.0317 - acc: 0.989 - ETA: 4s - loss: 0.0328 - acc: 0.988 - ETA: 4s - loss: 0.0325 - acc: 0.988 - ETA: 4s - loss: 0.0322 - acc: 0.988 - ETA: 4s - loss: 0.0327 - acc: 0.988 - ETA: 4s - loss: 0.0321 - acc: 0.988 - ETA: 4s - loss: 0.0318 - acc: 0.988 - ETA: 3s - loss: 0.0313 - acc: 0.989 - ETA: 3s - loss: 0.0307 - acc: 0.989 - ETA: 3s - loss: 0.0307 - acc: 0.989 - ETA: 3s - loss: 0.0301 - acc: 0.989 - ETA: 3s - loss: 0.0294 - acc: 0.989 - ETA: 3s - loss: 0.0288 - acc: 0.989 - ETA: 3s - loss: 0.0283 - acc: 0.990 - ETA: 3s - loss: 0.0288 - acc: 0.989 - ETA: 3s - loss: 0.0301 - acc: 0.989 - ETA: 3s - loss: 0.0295 - acc: 0.989 - ETA: 3s - loss: 0.0318 - acc: 0.989 - ETA: 3s - loss: 0.0320 - acc: 0.989 - ETA: 3s - loss: 0.0316 - acc: 0.989 - ETA: 3s - loss: 0.0312 - acc: 0.989 - ETA: 3s - loss: 0.0318 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0319 - acc: 0.989 - ETA: 3s - loss: 0.0315 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0311 - acc: 0.989 - ETA: 2s - loss: 0.0322 - acc: 0.989 - ETA: 2s - loss: 0.0319 - acc: 0.989 - ETA: 2s - loss: 0.0314 - acc: 0.989 - ETA: 2s - loss: 0.0314 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0308 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0309 - acc: 0.989 - ETA: 2s - loss: 0.0306 - acc: 0.990 - ETA: 2s - loss: 0.0303 - acc: 0.990 - ETA: 2s - loss: 0.0300 - acc: 0.990 - ETA: 2s - loss: 0.0309 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.989 - ETA: 2s - loss: 0.0328 - acc: 0.989 - ETA: 2s - loss: 0.0352 - acc: 0.989 - ETA: 2s - loss: 0.0351 - acc: 0.989 - ETA: 1s - loss: 0.0350 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.989 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0389 - acc: 0.988 - ETA: 1s - loss: 0.0385 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.989 - ETA: 1s - loss: 0.0373 - acc: 0.988 - ETA: 1s - loss: 0.0370 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 1s - loss: 0.0376 - acc: 0.988 - ETA: 1s - loss: 0.0377 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 0s - loss: 0.0374 - acc: 0.988 - ETA: 0s - loss: 0.0373 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.987 - ETA: 0s - loss: 0.0382 - acc: 0.987 - ETA: 0s - loss: 0.0378 - acc: 0.987 - ETA: 0s - loss: 0.0377 - acc: 0.987 - ETA: 0s - loss: 0.0374 - acc: 0.987 - ETA: 0s - loss: 0.0372 - acc: 0.987 - ETA: 0s - loss: 0.0370 - acc: 0.987 - ETA: 0s - loss: 0.0370 - acc: 0.987 - ETA: 0s - loss: 0.0368 - acc: 0.988 - ETA: 0s - loss: 0.0368 - acc: 0.988 - ETA: 0s - loss: 0.0367 - acc: 0.988 - ETA: 0s - loss: 0.0365 - acc: 0.988 - ETA: 0s - loss: 0.0363 - acc: 0.988 - ETA: 0s - loss: 0.0365 - acc: 0.988 - ETA: 0s - loss: 0.0368 - acc: 0.9879Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0369 - acc: 0.9879 - val_loss: 0.9159 - val_acc: 0.8479\n",
      "Epoch 19/20\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 0.0123 - acc: 1.000 - ETA: 7s - loss: 0.0471 - acc: 0.975 - ETA: 6s - loss: 0.0320 - acc: 0.985 - ETA: 6s - loss: 0.0232 - acc: 0.990 - ETA: 6s - loss: 0.0192 - acc: 0.992 - ETA: 6s - loss: 0.0318 - acc: 0.990 - ETA: 6s - loss: 0.0280 - acc: 0.992 - ETA: 6s - loss: 0.0264 - acc: 0.993 - ETA: 5s - loss: 0.0440 - acc: 0.990 - ETA: 5s - loss: 0.0397 - acc: 0.991 - ETA: 5s - loss: 0.0364 - acc: 0.991 - ETA: 5s - loss: 0.0395 - acc: 0.989 - ETA: 5s - loss: 0.0367 - acc: 0.990 - ETA: 5s - loss: 0.0356 - acc: 0.990 - ETA: 5s - loss: 0.0336 - acc: 0.990 - ETA: 5s - loss: 0.0317 - acc: 0.991 - ETA: 5s - loss: 0.0315 - acc: 0.990 - ETA: 5s - loss: 0.0298 - acc: 0.991 - ETA: 5s - loss: 0.0315 - acc: 0.990 - ETA: 5s - loss: 0.0300 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.991 - ETA: 5s - loss: 0.0301 - acc: 0.990 - ETA: 5s - loss: 0.0294 - acc: 0.991 - ETA: 4s - loss: 0.0312 - acc: 0.990 - ETA: 4s - loss: 0.0302 - acc: 0.991 - ETA: 4s - loss: 0.0297 - acc: 0.991 - ETA: 4s - loss: 0.0292 - acc: 0.991 - ETA: 4s - loss: 0.0343 - acc: 0.990 - ETA: 4s - loss: 0.0337 - acc: 0.990 - ETA: 4s - loss: 0.0345 - acc: 0.990 - ETA: 4s - loss: 0.0335 - acc: 0.990 - ETA: 4s - loss: 0.0325 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.991 - ETA: 4s - loss: 0.0315 - acc: 0.991 - ETA: 4s - loss: 0.0307 - acc: 0.991 - ETA: 4s - loss: 0.0301 - acc: 0.991 - ETA: 4s - loss: 0.0299 - acc: 0.991 - ETA: 4s - loss: 0.0292 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0281 - acc: 0.991 - ETA: 3s - loss: 0.0283 - acc: 0.991 - ETA: 3s - loss: 0.0280 - acc: 0.991 - ETA: 3s - loss: 0.0274 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0276 - acc: 0.991 - ETA: 3s - loss: 0.0276 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0269 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0279 - acc: 0.991 - ETA: 3s - loss: 0.0293 - acc: 0.991 - ETA: 3s - loss: 0.0290 - acc: 0.991 - ETA: 3s - loss: 0.0298 - acc: 0.991 - ETA: 3s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 2s - loss: 0.0316 - acc: 0.990 - ETA: 2s - loss: 0.0316 - acc: 0.990 - ETA: 2s - loss: 0.0320 - acc: 0.990 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0313 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0312 - acc: 0.990 - ETA: 2s - loss: 0.0314 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0314 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0315 - acc: 0.990 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0322 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0335 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.990 - ETA: 1s - loss: 0.0329 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0332 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0330 - acc: 0.989 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0348 - acc: 0.988 - ETA: 1s - loss: 0.0345 - acc: 0.988 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0343 - acc: 0.989 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0334 - acc: 0.989 - ETA: 1s - loss: 0.0335 - acc: 0.989 - ETA: 0s - loss: 0.0333 - acc: 0.989 - ETA: 0s - loss: 0.0343 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0334 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0329 - acc: 0.989 - ETA: 0s - loss: 0.0328 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0323 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0330 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0329 - acc: 0.9897Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0327 - acc: 0.9898 - val_loss: 0.9622 - val_acc: 0.8431\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0108 - acc: 1.000 - ETA: 7s - loss: 0.0286 - acc: 0.987 - ETA: 6s - loss: 0.0372 - acc: 0.985 - ETA: 7s - loss: 0.0302 - acc: 0.988 - ETA: 6s - loss: 0.0237 - acc: 0.991 - ETA: 6s - loss: 0.0235 - acc: 0.993 - ETA: 6s - loss: 0.0199 - acc: 0.994 - ETA: 6s - loss: 0.0181 - acc: 0.995 - ETA: 6s - loss: 0.0213 - acc: 0.993 - ETA: 6s - loss: 0.0203 - acc: 0.994 - ETA: 6s - loss: 0.0190 - acc: 0.995 - ETA: 5s - loss: 0.0174 - acc: 0.995 - ETA: 5s - loss: 0.0165 - acc: 0.995 - ETA: 5s - loss: 0.0183 - acc: 0.994 - ETA: 5s - loss: 0.0181 - acc: 0.995 - ETA: 5s - loss: 0.0195 - acc: 0.994 - ETA: 5s - loss: 0.0214 - acc: 0.993 - ETA: 5s - loss: 0.0202 - acc: 0.994 - ETA: 5s - loss: 0.0200 - acc: 0.994 - ETA: 5s - loss: 0.0195 - acc: 0.994 - ETA: 5s - loss: 0.0192 - acc: 0.995 - ETA: 5s - loss: 0.0186 - acc: 0.995 - ETA: 5s - loss: 0.0180 - acc: 0.995 - ETA: 5s - loss: 0.0215 - acc: 0.994 - ETA: 4s - loss: 0.0217 - acc: 0.993 - ETA: 4s - loss: 0.0211 - acc: 0.994 - ETA: 4s - loss: 0.0222 - acc: 0.993 - ETA: 4s - loss: 0.0219 - acc: 0.993 - ETA: 4s - loss: 0.0244 - acc: 0.992 - ETA: 4s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0259 - acc: 0.991 - ETA: 4s - loss: 0.0253 - acc: 0.992 - ETA: 4s - loss: 0.0272 - acc: 0.991 - ETA: 4s - loss: 0.0289 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0280 - acc: 0.991 - ETA: 4s - loss: 0.0279 - acc: 0.991 - ETA: 4s - loss: 0.0294 - acc: 0.991 - ETA: 4s - loss: 0.0293 - acc: 0.991 - ETA: 4s - loss: 0.0298 - acc: 0.990 - ETA: 3s - loss: 0.0293 - acc: 0.991 - ETA: 3s - loss: 0.0297 - acc: 0.990 - ETA: 3s - loss: 0.0291 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0280 - acc: 0.991 - ETA: 3s - loss: 0.0277 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 3s - loss: 0.0266 - acc: 0.992 - ETA: 3s - loss: 0.0263 - acc: 0.992 - ETA: 3s - loss: 0.0258 - acc: 0.992 - ETA: 3s - loss: 0.0264 - acc: 0.992 - ETA: 3s - loss: 0.0259 - acc: 0.992 - ETA: 3s - loss: 0.0260 - acc: 0.992 - ETA: 3s - loss: 0.0258 - acc: 0.992 - ETA: 3s - loss: 0.0255 - acc: 0.992 - ETA: 3s - loss: 0.0253 - acc: 0.992 - ETA: 3s - loss: 0.0250 - acc: 0.992 - ETA: 3s - loss: 0.0273 - acc: 0.992 - ETA: 2s - loss: 0.0275 - acc: 0.991 - ETA: 2s - loss: 0.0271 - acc: 0.991 - ETA: 2s - loss: 0.0268 - acc: 0.992 - ETA: 2s - loss: 0.0265 - acc: 0.992 - ETA: 2s - loss: 0.0263 - acc: 0.992 - ETA: 2s - loss: 0.0261 - acc: 0.992 - ETA: 2s - loss: 0.0258 - acc: 0.992 - ETA: 2s - loss: 0.0255 - acc: 0.992 - ETA: 2s - loss: 0.0253 - acc: 0.992 - ETA: 2s - loss: 0.0249 - acc: 0.992 - ETA: 2s - loss: 0.0247 - acc: 0.993 - ETA: 2s - loss: 0.0244 - acc: 0.993 - ETA: 2s - loss: 0.0245 - acc: 0.993 - ETA: 2s - loss: 0.0242 - acc: 0.993 - ETA: 2s - loss: 0.0240 - acc: 0.993 - ETA: 2s - loss: 0.0238 - acc: 0.993 - ETA: 2s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0295 - acc: 0.991 - ETA: 0s - loss: 0.0293 - acc: 0.992 - ETA: 0s - loss: 0.0301 - acc: 0.991 - ETA: 0s - loss: 0.0298 - acc: 0.992 - ETA: 0s - loss: 0.0310 - acc: 0.991 - ETA: 0s - loss: 0.0308 - acc: 0.991 - ETA: 0s - loss: 0.0305 - acc: 0.991 - ETA: 0s - loss: 0.0303 - acc: 0.991 - ETA: 0s - loss: 0.0300 - acc: 0.9919Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0299 - acc: 0.9919 - val_loss: 0.9197 - val_acc: 0.8443\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_6 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 149s - loss: 4.9721 - acc: 0.050 - ETA: 57s - loss: 5.1120 - acc: 0.033 - ETA: 46s - loss: 4.9689 - acc: 0.07 - ETA: 34s - loss: 4.9221 - acc: 0.06 - ETA: 28s - loss: 4.9152 - acc: 0.07 - ETA: 25s - loss: 4.8410 - acc: 0.10 - ETA: 22s - loss: 4.7128 - acc: 0.12 - ETA: 20s - loss: 4.5848 - acc: 0.15 - ETA: 19s - loss: 4.4773 - acc: 0.18 - ETA: 18s - loss: 4.3857 - acc: 0.20 - ETA: 17s - loss: 4.2558 - acc: 0.23 - ETA: 16s - loss: 4.1138 - acc: 0.24 - ETA: 15s - loss: 4.0237 - acc: 0.25 - ETA: 15s - loss: 3.9527 - acc: 0.26 - ETA: 14s - loss: 3.8342 - acc: 0.28 - ETA: 14s - loss: 3.7222 - acc: 0.30 - ETA: 14s - loss: 3.6515 - acc: 0.32 - ETA: 13s - loss: 3.5642 - acc: 0.33 - ETA: 13s - loss: 3.4724 - acc: 0.34 - ETA: 12s - loss: 3.4026 - acc: 0.35 - ETA: 12s - loss: 3.3346 - acc: 0.36 - ETA: 12s - loss: 3.2670 - acc: 0.37 - ETA: 11s - loss: 3.2088 - acc: 0.38 - ETA: 11s - loss: 3.1358 - acc: 0.40 - ETA: 11s - loss: 3.0690 - acc: 0.41 - ETA: 11s - loss: 3.0045 - acc: 0.42 - ETA: 11s - loss: 2.9381 - acc: 0.43 - ETA: 10s - loss: 2.8786 - acc: 0.44 - ETA: 10s - loss: 2.8295 - acc: 0.45 - ETA: 10s - loss: 2.7872 - acc: 0.45 - ETA: 10s - loss: 2.7444 - acc: 0.46 - ETA: 10s - loss: 2.6940 - acc: 0.47 - ETA: 9s - loss: 2.6588 - acc: 0.4789 - ETA: 9s - loss: 2.6127 - acc: 0.486 - ETA: 9s - loss: 2.5756 - acc: 0.492 - ETA: 9s - loss: 2.5336 - acc: 0.500 - ETA: 9s - loss: 2.4910 - acc: 0.506 - ETA: 9s - loss: 2.4525 - acc: 0.512 - ETA: 9s - loss: 2.4175 - acc: 0.519 - ETA: 8s - loss: 2.3712 - acc: 0.528 - ETA: 8s - loss: 2.3385 - acc: 0.532 - ETA: 8s - loss: 2.3057 - acc: 0.537 - ETA: 8s - loss: 2.2739 - acc: 0.543 - ETA: 8s - loss: 2.2452 - acc: 0.547 - ETA: 8s - loss: 2.2178 - acc: 0.550 - ETA: 8s - loss: 2.1884 - acc: 0.555 - ETA: 8s - loss: 2.1645 - acc: 0.557 - ETA: 8s - loss: 2.1347 - acc: 0.562 - ETA: 7s - loss: 2.1158 - acc: 0.564 - ETA: 7s - loss: 2.0859 - acc: 0.570 - ETA: 7s - loss: 2.0575 - acc: 0.575 - ETA: 7s - loss: 2.0376 - acc: 0.576 - ETA: 7s - loss: 2.0161 - acc: 0.579 - ETA: 7s - loss: 1.9928 - acc: 0.584 - ETA: 7s - loss: 1.9791 - acc: 0.584 - ETA: 7s - loss: 1.9549 - acc: 0.589 - ETA: 7s - loss: 1.9347 - acc: 0.593 - ETA: 7s - loss: 1.9184 - acc: 0.595 - ETA: 7s - loss: 1.8974 - acc: 0.599 - ETA: 7s - loss: 1.8826 - acc: 0.600 - ETA: 6s - loss: 1.8685 - acc: 0.602 - ETA: 6s - loss: 1.8499 - acc: 0.604 - ETA: 6s - loss: 1.8348 - acc: 0.606 - ETA: 6s - loss: 1.8156 - acc: 0.610 - ETA: 6s - loss: 1.7963 - acc: 0.614 - ETA: 6s - loss: 1.7818 - acc: 0.615 - ETA: 6s - loss: 1.7644 - acc: 0.618 - ETA: 6s - loss: 1.7448 - acc: 0.623 - ETA: 6s - loss: 1.7292 - acc: 0.626 - ETA: 6s - loss: 1.7171 - acc: 0.627 - ETA: 6s - loss: 1.7027 - acc: 0.630 - ETA: 6s - loss: 1.6876 - acc: 0.633 - ETA: 5s - loss: 1.6722 - acc: 0.636 - ETA: 5s - loss: 1.6558 - acc: 0.639 - ETA: 5s - loss: 1.6428 - acc: 0.642 - ETA: 5s - loss: 1.6304 - acc: 0.644 - ETA: 5s - loss: 1.6160 - acc: 0.646 - ETA: 5s - loss: 1.6003 - acc: 0.649 - ETA: 5s - loss: 1.5942 - acc: 0.648 - ETA: 5s - loss: 1.5812 - acc: 0.650 - ETA: 5s - loss: 1.5683 - acc: 0.653 - ETA: 5s - loss: 1.5566 - acc: 0.656 - ETA: 5s - loss: 1.5465 - acc: 0.657 - ETA: 5s - loss: 1.5362 - acc: 0.658 - ETA: 5s - loss: 1.5241 - acc: 0.661 - ETA: 5s - loss: 1.5139 - acc: 0.663 - ETA: 4s - loss: 1.5026 - acc: 0.664 - ETA: 4s - loss: 1.4952 - acc: 0.665 - ETA: 4s - loss: 1.4863 - acc: 0.665 - ETA: 4s - loss: 1.4769 - acc: 0.666 - ETA: 4s - loss: 1.4676 - acc: 0.668 - ETA: 4s - loss: 1.4579 - acc: 0.670 - ETA: 4s - loss: 1.4475 - acc: 0.671 - ETA: 4s - loss: 1.4412 - acc: 0.672 - ETA: 4s - loss: 1.4306 - acc: 0.674 - ETA: 4s - loss: 1.4190 - acc: 0.677 - ETA: 4s - loss: 1.4082 - acc: 0.679 - ETA: 4s - loss: 1.3997 - acc: 0.681 - ETA: 4s - loss: 1.3906 - acc: 0.682 - ETA: 4s - loss: 1.3846 - acc: 0.682 - ETA: 4s - loss: 1.3747 - acc: 0.684 - ETA: 4s - loss: 1.3666 - acc: 0.685 - ETA: 3s - loss: 1.3588 - acc: 0.687 - ETA: 3s - loss: 1.3516 - acc: 0.688 - ETA: 3s - loss: 1.3454 - acc: 0.690 - ETA: 3s - loss: 1.3390 - acc: 0.691 - ETA: 3s - loss: 1.3323 - acc: 0.692 - ETA: 3s - loss: 1.3262 - acc: 0.693 - ETA: 3s - loss: 1.3201 - acc: 0.694 - ETA: 3s - loss: 1.3116 - acc: 0.696 - ETA: 3s - loss: 1.3040 - acc: 0.697 - ETA: 3s - loss: 1.2982 - acc: 0.697 - ETA: 3s - loss: 1.2942 - acc: 0.698 - ETA: 3s - loss: 1.2889 - acc: 0.699 - ETA: 3s - loss: 1.2808 - acc: 0.700 - ETA: 3s - loss: 1.2746 - acc: 0.701 - ETA: 3s - loss: 1.2676 - acc: 0.702 - ETA: 2s - loss: 1.2647 - acc: 0.703 - ETA: 2s - loss: 1.2578 - acc: 0.704 - ETA: 2s - loss: 1.2526 - acc: 0.705 - ETA: 2s - loss: 1.2475 - acc: 0.706 - ETA: 2s - loss: 1.2435 - acc: 0.706 - ETA: 2s - loss: 1.2371 - acc: 0.707 - ETA: 2s - loss: 1.2300 - acc: 0.709 - ETA: 2s - loss: 1.2272 - acc: 0.710 - ETA: 2s - loss: 1.2224 - acc: 0.710 - ETA: 2s - loss: 1.2179 - acc: 0.711 - ETA: 2s - loss: 1.2110 - acc: 0.712 - ETA: 2s - loss: 1.2064 - acc: 0.713 - ETA: 2s - loss: 1.2008 - acc: 0.714 - ETA: 2s - loss: 1.1962 - acc: 0.715 - ETA: 2s - loss: 1.1914 - acc: 0.716 - ETA: 2s - loss: 1.1864 - acc: 0.717 - ETA: 2s - loss: 1.1804 - acc: 0.718 - ETA: 1s - loss: 1.1779 - acc: 0.718 - ETA: 1s - loss: 1.1753 - acc: 0.719 - ETA: 1s - loss: 1.1704 - acc: 0.719 - ETA: 1s - loss: 1.1663 - acc: 0.720 - ETA: 1s - loss: 1.1604 - acc: 0.721 - ETA: 1s - loss: 1.1555 - acc: 0.721 - ETA: 1s - loss: 1.1499 - acc: 0.722 - ETA: 1s - loss: 1.1449 - acc: 0.723 - ETA: 1s - loss: 1.1411 - acc: 0.724 - ETA: 1s - loss: 1.1351 - acc: 0.725 - ETA: 1s - loss: 1.1302 - acc: 0.726 - ETA: 1s - loss: 1.1244 - acc: 0.727 - ETA: 1s - loss: 1.1210 - acc: 0.728 - ETA: 1s - loss: 1.1160 - acc: 0.729 - ETA: 1s - loss: 1.1137 - acc: 0.730 - ETA: 1s - loss: 1.1079 - acc: 0.731 - ETA: 0s - loss: 1.1043 - acc: 0.732 - ETA: 0s - loss: 1.0993 - acc: 0.733 - ETA: 0s - loss: 1.0951 - acc: 0.734 - ETA: 0s - loss: 1.0913 - acc: 0.735 - ETA: 0s - loss: 1.0884 - acc: 0.735 - ETA: 0s - loss: 1.0861 - acc: 0.736 - ETA: 0s - loss: 1.0815 - acc: 0.737 - ETA: 0s - loss: 1.0792 - acc: 0.737 - ETA: 0s - loss: 1.0762 - acc: 0.738 - ETA: 0s - loss: 1.0723 - acc: 0.738 - ETA: 0s - loss: 1.0707 - acc: 0.738 - ETA: 0s - loss: 1.0701 - acc: 0.738 - ETA: 0s - loss: 1.0680 - acc: 0.738 - ETA: 0s - loss: 1.0643 - acc: 0.739 - ETA: 0s - loss: 1.0610 - acc: 0.739 - ETA: 0s - loss: 1.0600 - acc: 0.739 - ETA: 0s - loss: 1.0567 - acc: 0.7401Epoch 00000: val_loss improved from inf to 0.52450, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 10s - loss: 1.0546 - acc: 0.7400 - val_loss: 0.5245 - val_acc: 0.8287\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.2522 - acc: 0.900 - ETA: 8s - loss: 0.3479 - acc: 0.866 - ETA: 8s - loss: 0.2895 - acc: 0.910 - ETA: 8s - loss: 0.2792 - acc: 0.907 - ETA: 8s - loss: 0.3140 - acc: 0.894 - ETA: 8s - loss: 0.3764 - acc: 0.872 - ETA: 8s - loss: 0.3825 - acc: 0.861 - ETA: 8s - loss: 0.3594 - acc: 0.873 - ETA: 8s - loss: 0.3612 - acc: 0.876 - ETA: 8s - loss: 0.3610 - acc: 0.881 - ETA: 8s - loss: 0.3594 - acc: 0.881 - ETA: 8s - loss: 0.3624 - acc: 0.880 - ETA: 8s - loss: 0.3843 - acc: 0.876 - ETA: 8s - loss: 0.3934 - acc: 0.874 - ETA: 8s - loss: 0.3874 - acc: 0.874 - ETA: 7s - loss: 0.3953 - acc: 0.872 - ETA: 7s - loss: 0.3921 - acc: 0.877 - ETA: 7s - loss: 0.4097 - acc: 0.868 - ETA: 7s - loss: 0.4089 - acc: 0.868 - ETA: 7s - loss: 0.4133 - acc: 0.866 - ETA: 7s - loss: 0.4059 - acc: 0.869 - ETA: 7s - loss: 0.4028 - acc: 0.872 - ETA: 7s - loss: 0.4043 - acc: 0.874 - ETA: 7s - loss: 0.4007 - acc: 0.873 - ETA: 7s - loss: 0.3951 - acc: 0.875 - ETA: 7s - loss: 0.3903 - acc: 0.876 - ETA: 7s - loss: 0.3905 - acc: 0.876 - ETA: 7s - loss: 0.3943 - acc: 0.873 - ETA: 7s - loss: 0.3936 - acc: 0.873 - ETA: 7s - loss: 0.3937 - acc: 0.873 - ETA: 7s - loss: 0.4014 - acc: 0.869 - ETA: 7s - loss: 0.4011 - acc: 0.869 - ETA: 7s - loss: 0.4044 - acc: 0.869 - ETA: 7s - loss: 0.4062 - acc: 0.867 - ETA: 7s - loss: 0.4072 - acc: 0.867 - ETA: 7s - loss: 0.4060 - acc: 0.866 - ETA: 7s - loss: 0.4036 - acc: 0.866 - ETA: 7s - loss: 0.3990 - acc: 0.868 - ETA: 7s - loss: 0.3979 - acc: 0.867 - ETA: 7s - loss: 0.3956 - acc: 0.868 - ETA: 7s - loss: 0.3901 - acc: 0.870 - ETA: 6s - loss: 0.3979 - acc: 0.868 - ETA: 6s - loss: 0.3969 - acc: 0.868 - ETA: 6s - loss: 0.3947 - acc: 0.870 - ETA: 6s - loss: 0.3917 - acc: 0.871 - ETA: 6s - loss: 0.3893 - acc: 0.871 - ETA: 6s - loss: 0.3898 - acc: 0.871 - ETA: 6s - loss: 0.3920 - acc: 0.870 - ETA: 6s - loss: 0.3892 - acc: 0.871 - ETA: 6s - loss: 0.3926 - acc: 0.870 - ETA: 6s - loss: 0.3884 - acc: 0.871 - ETA: 6s - loss: 0.3878 - acc: 0.871 - ETA: 6s - loss: 0.3871 - acc: 0.871 - ETA: 6s - loss: 0.3831 - acc: 0.872 - ETA: 6s - loss: 0.3843 - acc: 0.872 - ETA: 6s - loss: 0.3847 - acc: 0.872 - ETA: 6s - loss: 0.3816 - acc: 0.873 - ETA: 5s - loss: 0.3861 - acc: 0.871 - ETA: 5s - loss: 0.3837 - acc: 0.873 - ETA: 5s - loss: 0.3848 - acc: 0.872 - ETA: 5s - loss: 0.3847 - acc: 0.871 - ETA: 5s - loss: 0.3869 - acc: 0.871 - ETA: 5s - loss: 0.3879 - acc: 0.871 - ETA: 5s - loss: 0.3877 - acc: 0.871 - ETA: 5s - loss: 0.3873 - acc: 0.872 - ETA: 5s - loss: 0.3915 - acc: 0.871 - ETA: 5s - loss: 0.3905 - acc: 0.872 - ETA: 5s - loss: 0.3921 - acc: 0.871 - ETA: 5s - loss: 0.3941 - acc: 0.871 - ETA: 5s - loss: 0.3930 - acc: 0.870 - ETA: 5s - loss: 0.3979 - acc: 0.869 - ETA: 5s - loss: 0.4034 - acc: 0.868 - ETA: 5s - loss: 0.4039 - acc: 0.867 - ETA: 5s - loss: 0.4026 - acc: 0.868 - ETA: 5s - loss: 0.4021 - acc: 0.868 - ETA: 4s - loss: 0.4045 - acc: 0.867 - ETA: 4s - loss: 0.4011 - acc: 0.869 - ETA: 4s - loss: 0.3995 - acc: 0.869 - ETA: 4s - loss: 0.4006 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.868 - ETA: 4s - loss: 0.4018 - acc: 0.869 - ETA: 4s - loss: 0.4028 - acc: 0.869 - ETA: 4s - loss: 0.4035 - acc: 0.869 - ETA: 4s - loss: 0.4002 - acc: 0.870 - ETA: 4s - loss: 0.4037 - acc: 0.869 - ETA: 4s - loss: 0.4042 - acc: 0.869 - ETA: 4s - loss: 0.4032 - acc: 0.869 - ETA: 4s - loss: 0.4053 - acc: 0.868 - ETA: 4s - loss: 0.4037 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.869 - ETA: 4s - loss: 0.4005 - acc: 0.870 - ETA: 4s - loss: 0.3995 - acc: 0.870 - ETA: 3s - loss: 0.4001 - acc: 0.869 - ETA: 3s - loss: 0.4016 - acc: 0.869 - ETA: 3s - loss: 0.3998 - acc: 0.870 - ETA: 3s - loss: 0.4008 - acc: 0.870 - ETA: 3s - loss: 0.3998 - acc: 0.870 - ETA: 3s - loss: 0.3976 - acc: 0.871 - ETA: 3s - loss: 0.3968 - acc: 0.871 - ETA: 3s - loss: 0.3957 - acc: 0.871 - ETA: 3s - loss: 0.3950 - acc: 0.871 - ETA: 3s - loss: 0.3948 - acc: 0.871 - ETA: 3s - loss: 0.3948 - acc: 0.871 - ETA: 3s - loss: 0.3938 - acc: 0.871 - ETA: 3s - loss: 0.3949 - acc: 0.870 - ETA: 3s - loss: 0.3952 - acc: 0.870 - ETA: 3s - loss: 0.3970 - acc: 0.870 - ETA: 3s - loss: 0.3959 - acc: 0.870 - ETA: 3s - loss: 0.3937 - acc: 0.871 - ETA: 3s - loss: 0.3922 - acc: 0.871 - ETA: 2s - loss: 0.3941 - acc: 0.871 - ETA: 2s - loss: 0.3940 - acc: 0.871 - ETA: 2s - loss: 0.3921 - acc: 0.872 - ETA: 2s - loss: 0.3938 - acc: 0.872 - ETA: 2s - loss: 0.3948 - acc: 0.871 - ETA: 2s - loss: 0.3941 - acc: 0.871 - ETA: 2s - loss: 0.3943 - acc: 0.871 - ETA: 2s - loss: 0.3939 - acc: 0.871 - ETA: 2s - loss: 0.3926 - acc: 0.872 - ETA: 2s - loss: 0.3935 - acc: 0.872 - ETA: 2s - loss: 0.3918 - acc: 0.872 - ETA: 2s - loss: 0.3935 - acc: 0.872 - ETA: 2s - loss: 0.3959 - acc: 0.871 - ETA: 2s - loss: 0.3976 - acc: 0.870 - ETA: 2s - loss: 0.3979 - acc: 0.870 - ETA: 2s - loss: 0.3991 - acc: 0.870 - ETA: 2s - loss: 0.4016 - acc: 0.869 - ETA: 2s - loss: 0.4020 - acc: 0.869 - ETA: 2s - loss: 0.4010 - acc: 0.870 - ETA: 1s - loss: 0.3999 - acc: 0.870 - ETA: 1s - loss: 0.4017 - acc: 0.869 - ETA: 1s - loss: 0.4022 - acc: 0.869 - ETA: 1s - loss: 0.4018 - acc: 0.868 - ETA: 1s - loss: 0.4016 - acc: 0.869 - ETA: 1s - loss: 0.4002 - acc: 0.869 - ETA: 1s - loss: 0.4000 - acc: 0.869 - ETA: 1s - loss: 0.3999 - acc: 0.869 - ETA: 1s - loss: 0.3989 - acc: 0.869 - ETA: 1s - loss: 0.3979 - acc: 0.869 - ETA: 1s - loss: 0.3972 - acc: 0.869 - ETA: 1s - loss: 0.3963 - acc: 0.870 - ETA: 1s - loss: 0.3965 - acc: 0.870 - ETA: 1s - loss: 0.3974 - acc: 0.870 - ETA: 1s - loss: 0.3970 - acc: 0.870 - ETA: 1s - loss: 0.3972 - acc: 0.870 - ETA: 1s - loss: 0.3962 - acc: 0.870 - ETA: 1s - loss: 0.3958 - acc: 0.870 - ETA: 0s - loss: 0.3958 - acc: 0.870 - ETA: 0s - loss: 0.3941 - acc: 0.871 - ETA: 0s - loss: 0.3948 - acc: 0.871 - ETA: 0s - loss: 0.3944 - acc: 0.871 - ETA: 0s - loss: 0.3948 - acc: 0.871 - ETA: 0s - loss: 0.3941 - acc: 0.871 - ETA: 0s - loss: 0.3935 - acc: 0.871 - ETA: 0s - loss: 0.3932 - acc: 0.871 - ETA: 0s - loss: 0.3925 - acc: 0.871 - ETA: 0s - loss: 0.3934 - acc: 0.871 - ETA: 0s - loss: 0.3920 - acc: 0.871 - ETA: 0s - loss: 0.3923 - acc: 0.871 - ETA: 0s - loss: 0.3932 - acc: 0.871 - ETA: 0s - loss: 0.3931 - acc: 0.871 - ETA: 0s - loss: 0.3924 - acc: 0.871 - ETA: 0s - loss: 0.3939 - acc: 0.871 - ETA: 0s - loss: 0.3957 - acc: 0.871 - ETA: 0s - loss: 0.3952 - acc: 0.8712Epoch 00001: val_loss improved from 0.52450 to 0.48827, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 0.3951 - acc: 0.8713 - val_loss: 0.4883 - val_acc: 0.8407\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.3868 - acc: 0.800 - ETA: 9s - loss: 0.2796 - acc: 0.900 - ETA: 9s - loss: 0.2060 - acc: 0.930 - ETA: 9s - loss: 0.1949 - acc: 0.935 - ETA: 9s - loss: 0.2653 - acc: 0.916 - ETA: 8s - loss: 0.2927 - acc: 0.913 - ETA: 8s - loss: 0.2928 - acc: 0.911 - ETA: 8s - loss: 0.2932 - acc: 0.913 - ETA: 8s - loss: 0.2808 - acc: 0.911 - ETA: 8s - loss: 0.2847 - acc: 0.913 - ETA: 8s - loss: 0.2669 - acc: 0.919 - ETA: 8s - loss: 0.2844 - acc: 0.913 - ETA: 8s - loss: 0.2814 - acc: 0.914 - ETA: 8s - loss: 0.2828 - acc: 0.913 - ETA: 8s - loss: 0.2745 - acc: 0.913 - ETA: 8s - loss: 0.2693 - acc: 0.914 - ETA: 8s - loss: 0.2714 - acc: 0.913 - ETA: 8s - loss: 0.2698 - acc: 0.914 - ETA: 8s - loss: 0.2744 - acc: 0.913 - ETA: 7s - loss: 0.2802 - acc: 0.912 - ETA: 7s - loss: 0.2823 - acc: 0.911 - ETA: 7s - loss: 0.2823 - acc: 0.912 - ETA: 7s - loss: 0.2744 - acc: 0.915 - ETA: 7s - loss: 0.2820 - acc: 0.913 - ETA: 7s - loss: 0.2831 - acc: 0.912 - ETA: 7s - loss: 0.2775 - acc: 0.914 - ETA: 7s - loss: 0.2743 - acc: 0.916 - ETA: 7s - loss: 0.2733 - acc: 0.915 - ETA: 7s - loss: 0.2706 - acc: 0.915 - ETA: 7s - loss: 0.2744 - acc: 0.914 - ETA: 7s - loss: 0.2714 - acc: 0.915 - ETA: 7s - loss: 0.2698 - acc: 0.915 - ETA: 7s - loss: 0.2680 - acc: 0.916 - ETA: 7s - loss: 0.2640 - acc: 0.918 - ETA: 7s - loss: 0.2616 - acc: 0.920 - ETA: 7s - loss: 0.2674 - acc: 0.917 - ETA: 7s - loss: 0.2640 - acc: 0.918 - ETA: 6s - loss: 0.2671 - acc: 0.917 - ETA: 6s - loss: 0.2715 - acc: 0.916 - ETA: 6s - loss: 0.2709 - acc: 0.917 - ETA: 6s - loss: 0.2769 - acc: 0.916 - ETA: 6s - loss: 0.2752 - acc: 0.916 - ETA: 6s - loss: 0.2871 - acc: 0.915 - ETA: 6s - loss: 0.2864 - acc: 0.915 - ETA: 6s - loss: 0.2880 - acc: 0.914 - ETA: 6s - loss: 0.2872 - acc: 0.914 - ETA: 6s - loss: 0.2888 - acc: 0.914 - ETA: 6s - loss: 0.2878 - acc: 0.914 - ETA: 6s - loss: 0.2955 - acc: 0.913 - ETA: 6s - loss: 0.2973 - acc: 0.913 - ETA: 6s - loss: 0.2966 - acc: 0.912 - ETA: 6s - loss: 0.2983 - acc: 0.911 - ETA: 6s - loss: 0.3012 - acc: 0.910 - ETA: 6s - loss: 0.2994 - acc: 0.910 - ETA: 6s - loss: 0.3017 - acc: 0.909 - ETA: 6s - loss: 0.3000 - acc: 0.910 - ETA: 6s - loss: 0.3010 - acc: 0.910 - ETA: 5s - loss: 0.3030 - acc: 0.909 - ETA: 5s - loss: 0.3068 - acc: 0.909 - ETA: 5s - loss: 0.3067 - acc: 0.908 - ETA: 5s - loss: 0.3063 - acc: 0.907 - ETA: 5s - loss: 0.3057 - acc: 0.907 - ETA: 5s - loss: 0.3062 - acc: 0.906 - ETA: 5s - loss: 0.3081 - acc: 0.906 - ETA: 5s - loss: 0.3093 - acc: 0.906 - ETA: 5s - loss: 0.3091 - acc: 0.905 - ETA: 5s - loss: 0.3083 - acc: 0.904 - ETA: 5s - loss: 0.3063 - acc: 0.905 - ETA: 5s - loss: 0.3081 - acc: 0.903 - ETA: 5s - loss: 0.3077 - acc: 0.903 - ETA: 5s - loss: 0.3104 - acc: 0.902 - ETA: 5s - loss: 0.3173 - acc: 0.901 - ETA: 5s - loss: 0.3178 - acc: 0.901 - ETA: 5s - loss: 0.3187 - acc: 0.900 - ETA: 5s - loss: 0.3199 - acc: 0.899 - ETA: 5s - loss: 0.3209 - acc: 0.899 - ETA: 4s - loss: 0.3213 - acc: 0.898 - ETA: 4s - loss: 0.3199 - acc: 0.899 - ETA: 4s - loss: 0.3250 - acc: 0.897 - ETA: 4s - loss: 0.3237 - acc: 0.898 - ETA: 4s - loss: 0.3216 - acc: 0.898 - ETA: 4s - loss: 0.3219 - acc: 0.897 - ETA: 4s - loss: 0.3231 - acc: 0.896 - ETA: 4s - loss: 0.3236 - acc: 0.896 - ETA: 4s - loss: 0.3235 - acc: 0.896 - ETA: 4s - loss: 0.3241 - acc: 0.896 - ETA: 4s - loss: 0.3216 - acc: 0.897 - ETA: 4s - loss: 0.3229 - acc: 0.897 - ETA: 4s - loss: 0.3223 - acc: 0.896 - ETA: 4s - loss: 0.3223 - acc: 0.896 - ETA: 4s - loss: 0.3220 - acc: 0.896 - ETA: 4s - loss: 0.3226 - acc: 0.897 - ETA: 4s - loss: 0.3220 - acc: 0.897 - ETA: 4s - loss: 0.3216 - acc: 0.897 - ETA: 3s - loss: 0.3216 - acc: 0.897 - ETA: 3s - loss: 0.3192 - acc: 0.898 - ETA: 3s - loss: 0.3198 - acc: 0.898 - ETA: 3s - loss: 0.3218 - acc: 0.897 - ETA: 3s - loss: 0.3256 - acc: 0.897 - ETA: 3s - loss: 0.3246 - acc: 0.897 - ETA: 3s - loss: 0.3280 - acc: 0.897 - ETA: 3s - loss: 0.3294 - acc: 0.896 - ETA: 3s - loss: 0.3288 - acc: 0.896 - ETA: 3s - loss: 0.3274 - acc: 0.896 - ETA: 3s - loss: 0.3279 - acc: 0.896 - ETA: 3s - loss: 0.3301 - acc: 0.895 - ETA: 3s - loss: 0.3295 - acc: 0.896 - ETA: 3s - loss: 0.3292 - acc: 0.896 - ETA: 3s - loss: 0.3302 - acc: 0.895 - ETA: 3s - loss: 0.3303 - acc: 0.895 - ETA: 3s - loss: 0.3281 - acc: 0.896 - ETA: 3s - loss: 0.3261 - acc: 0.897 - ETA: 2s - loss: 0.3244 - acc: 0.897 - ETA: 2s - loss: 0.3233 - acc: 0.898 - ETA: 2s - loss: 0.3231 - acc: 0.898 - ETA: 2s - loss: 0.3218 - acc: 0.898 - ETA: 2s - loss: 0.3209 - acc: 0.899 - ETA: 2s - loss: 0.3216 - acc: 0.899 - ETA: 2s - loss: 0.3207 - acc: 0.898 - ETA: 2s - loss: 0.3195 - acc: 0.899 - ETA: 2s - loss: 0.3196 - acc: 0.899 - ETA: 2s - loss: 0.3197 - acc: 0.899 - ETA: 2s - loss: 0.3208 - acc: 0.899 - ETA: 2s - loss: 0.3207 - acc: 0.899 - ETA: 2s - loss: 0.3187 - acc: 0.899 - ETA: 2s - loss: 0.3178 - acc: 0.899 - ETA: 2s - loss: 0.3163 - acc: 0.900 - ETA: 2s - loss: 0.3167 - acc: 0.899 - ETA: 2s - loss: 0.3174 - acc: 0.899 - ETA: 2s - loss: 0.3161 - acc: 0.899 - ETA: 2s - loss: 0.3157 - acc: 0.899 - ETA: 1s - loss: 0.3169 - acc: 0.899 - ETA: 1s - loss: 0.3167 - acc: 0.899 - ETA: 1s - loss: 0.3172 - acc: 0.899 - ETA: 1s - loss: 0.3189 - acc: 0.898 - ETA: 1s - loss: 0.3193 - acc: 0.898 - ETA: 1s - loss: 0.3187 - acc: 0.898 - ETA: 1s - loss: 0.3192 - acc: 0.897 - ETA: 1s - loss: 0.3194 - acc: 0.897 - ETA: 1s - loss: 0.3186 - acc: 0.897 - ETA: 1s - loss: 0.3203 - acc: 0.897 - ETA: 1s - loss: 0.3224 - acc: 0.897 - ETA: 1s - loss: 0.3225 - acc: 0.897 - ETA: 1s - loss: 0.3225 - acc: 0.896 - ETA: 1s - loss: 0.3220 - acc: 0.897 - ETA: 1s - loss: 0.3222 - acc: 0.897 - ETA: 1s - loss: 0.3218 - acc: 0.897 - ETA: 1s - loss: 0.3212 - acc: 0.897 - ETA: 1s - loss: 0.3211 - acc: 0.897 - ETA: 0s - loss: 0.3221 - acc: 0.897 - ETA: 0s - loss: 0.3233 - acc: 0.896 - ETA: 0s - loss: 0.3229 - acc: 0.896 - ETA: 0s - loss: 0.3228 - acc: 0.896 - ETA: 0s - loss: 0.3224 - acc: 0.896 - ETA: 0s - loss: 0.3232 - acc: 0.895 - ETA: 0s - loss: 0.3235 - acc: 0.896 - ETA: 0s - loss: 0.3222 - acc: 0.896 - ETA: 0s - loss: 0.3214 - acc: 0.896 - ETA: 0s - loss: 0.3244 - acc: 0.896 - ETA: 0s - loss: 0.3234 - acc: 0.896 - ETA: 0s - loss: 0.3229 - acc: 0.896 - ETA: 0s - loss: 0.3226 - acc: 0.896 - ETA: 0s - loss: 0.3221 - acc: 0.896 - ETA: 0s - loss: 0.3212 - acc: 0.896 - ETA: 0s - loss: 0.3217 - acc: 0.896 - ETA: 0s - loss: 0.3210 - acc: 0.897 - ETA: 0s - loss: 0.3217 - acc: 0.8965Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.3212 - acc: 0.8966 - val_loss: 0.5060 - val_acc: 0.8431\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.3634 - acc: 0.900 - ETA: 8s - loss: 0.4092 - acc: 0.850 - ETA: 8s - loss: 0.3306 - acc: 0.900 - ETA: 8s - loss: 0.3132 - acc: 0.900 - ETA: 8s - loss: 0.3101 - acc: 0.905 - ETA: 8s - loss: 0.2988 - acc: 0.904 - ETA: 8s - loss: 0.2897 - acc: 0.903 - ETA: 8s - loss: 0.2771 - acc: 0.910 - ETA: 8s - loss: 0.3025 - acc: 0.905 - ETA: 8s - loss: 0.3059 - acc: 0.897 - ETA: 8s - loss: 0.2902 - acc: 0.904 - ETA: 8s - loss: 0.2793 - acc: 0.910 - ETA: 8s - loss: 0.2757 - acc: 0.912 - ETA: 8s - loss: 0.2654 - acc: 0.913 - ETA: 8s - loss: 0.2668 - acc: 0.913 - ETA: 8s - loss: 0.2666 - acc: 0.917 - ETA: 8s - loss: 0.2526 - acc: 0.922 - ETA: 8s - loss: 0.2424 - acc: 0.925 - ETA: 8s - loss: 0.2381 - acc: 0.925 - ETA: 7s - loss: 0.2507 - acc: 0.924 - ETA: 7s - loss: 0.2481 - acc: 0.923 - ETA: 7s - loss: 0.2526 - acc: 0.923 - ETA: 7s - loss: 0.2487 - acc: 0.923 - ETA: 7s - loss: 0.2543 - acc: 0.920 - ETA: 7s - loss: 0.2493 - acc: 0.922 - ETA: 7s - loss: 0.2512 - acc: 0.922 - ETA: 7s - loss: 0.2501 - acc: 0.921 - ETA: 7s - loss: 0.2441 - acc: 0.923 - ETA: 7s - loss: 0.2437 - acc: 0.922 - ETA: 7s - loss: 0.2427 - acc: 0.922 - ETA: 7s - loss: 0.2384 - acc: 0.923 - ETA: 7s - loss: 0.2391 - acc: 0.922 - ETA: 7s - loss: 0.2432 - acc: 0.920 - ETA: 7s - loss: 0.2467 - acc: 0.917 - ETA: 7s - loss: 0.2436 - acc: 0.918 - ETA: 7s - loss: 0.2425 - acc: 0.919 - ETA: 7s - loss: 0.2439 - acc: 0.920 - ETA: 6s - loss: 0.2425 - acc: 0.922 - ETA: 6s - loss: 0.2419 - acc: 0.922 - ETA: 6s - loss: 0.2405 - acc: 0.922 - ETA: 6s - loss: 0.2436 - acc: 0.922 - ETA: 6s - loss: 0.2425 - acc: 0.922 - ETA: 6s - loss: 0.2385 - acc: 0.923 - ETA: 6s - loss: 0.2467 - acc: 0.922 - ETA: 6s - loss: 0.2480 - acc: 0.921 - ETA: 6s - loss: 0.2473 - acc: 0.920 - ETA: 6s - loss: 0.2481 - acc: 0.921 - ETA: 6s - loss: 0.2459 - acc: 0.921 - ETA: 6s - loss: 0.2479 - acc: 0.921 - ETA: 6s - loss: 0.2469 - acc: 0.922 - ETA: 6s - loss: 0.2453 - acc: 0.922 - ETA: 6s - loss: 0.2427 - acc: 0.923 - ETA: 6s - loss: 0.2425 - acc: 0.923 - ETA: 6s - loss: 0.2393 - acc: 0.924 - ETA: 6s - loss: 0.2380 - acc: 0.924 - ETA: 5s - loss: 0.2391 - acc: 0.923 - ETA: 5s - loss: 0.2404 - acc: 0.923 - ETA: 5s - loss: 0.2393 - acc: 0.923 - ETA: 5s - loss: 0.2411 - acc: 0.923 - ETA: 5s - loss: 0.2432 - acc: 0.922 - ETA: 5s - loss: 0.2445 - acc: 0.921 - ETA: 5s - loss: 0.2491 - acc: 0.919 - ETA: 5s - loss: 0.2525 - acc: 0.918 - ETA: 5s - loss: 0.2543 - acc: 0.917 - ETA: 5s - loss: 0.2527 - acc: 0.918 - ETA: 5s - loss: 0.2544 - acc: 0.918 - ETA: 5s - loss: 0.2534 - acc: 0.918 - ETA: 5s - loss: 0.2551 - acc: 0.918 - ETA: 5s - loss: 0.2522 - acc: 0.919 - ETA: 5s - loss: 0.2530 - acc: 0.917 - ETA: 5s - loss: 0.2550 - acc: 0.917 - ETA: 5s - loss: 0.2544 - acc: 0.917 - ETA: 5s - loss: 0.2567 - acc: 0.917 - ETA: 5s - loss: 0.2557 - acc: 0.917 - ETA: 4s - loss: 0.2555 - acc: 0.918 - ETA: 4s - loss: 0.2587 - acc: 0.917 - ETA: 4s - loss: 0.2573 - acc: 0.918 - ETA: 4s - loss: 0.2616 - acc: 0.916 - ETA: 4s - loss: 0.2594 - acc: 0.917 - ETA: 4s - loss: 0.2587 - acc: 0.917 - ETA: 4s - loss: 0.2580 - acc: 0.917 - ETA: 4s - loss: 0.2607 - acc: 0.917 - ETA: 4s - loss: 0.2624 - acc: 0.917 - ETA: 4s - loss: 0.2633 - acc: 0.916 - ETA: 4s - loss: 0.2636 - acc: 0.916 - ETA: 4s - loss: 0.2639 - acc: 0.916 - ETA: 4s - loss: 0.2625 - acc: 0.917 - ETA: 4s - loss: 0.2602 - acc: 0.918 - ETA: 4s - loss: 0.2599 - acc: 0.917 - ETA: 4s - loss: 0.2600 - acc: 0.917 - ETA: 4s - loss: 0.2593 - acc: 0.917 - ETA: 4s - loss: 0.2605 - acc: 0.916 - ETA: 3s - loss: 0.2588 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2599 - acc: 0.917 - ETA: 3s - loss: 0.2601 - acc: 0.917 - ETA: 3s - loss: 0.2585 - acc: 0.917 - ETA: 3s - loss: 0.2590 - acc: 0.917 - ETA: 3s - loss: 0.2610 - acc: 0.917 - ETA: 3s - loss: 0.2604 - acc: 0.916 - ETA: 3s - loss: 0.2611 - acc: 0.916 - ETA: 3s - loss: 0.2604 - acc: 0.916 - ETA: 3s - loss: 0.2598 - acc: 0.917 - ETA: 3s - loss: 0.2586 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2591 - acc: 0.916 - ETA: 3s - loss: 0.2590 - acc: 0.916 - ETA: 3s - loss: 0.2602 - acc: 0.915 - ETA: 3s - loss: 0.2596 - acc: 0.915 - ETA: 3s - loss: 0.2606 - acc: 0.915 - ETA: 2s - loss: 0.2609 - acc: 0.915 - ETA: 2s - loss: 0.2636 - acc: 0.914 - ETA: 2s - loss: 0.2625 - acc: 0.914 - ETA: 2s - loss: 0.2621 - acc: 0.914 - ETA: 2s - loss: 0.2610 - acc: 0.914 - ETA: 2s - loss: 0.2624 - acc: 0.914 - ETA: 2s - loss: 0.2626 - acc: 0.914 - ETA: 2s - loss: 0.2634 - acc: 0.914 - ETA: 2s - loss: 0.2635 - acc: 0.913 - ETA: 2s - loss: 0.2651 - acc: 0.913 - ETA: 2s - loss: 0.2673 - acc: 0.913 - ETA: 2s - loss: 0.2701 - acc: 0.912 - ETA: 2s - loss: 0.2739 - acc: 0.911 - ETA: 2s - loss: 0.2727 - acc: 0.911 - ETA: 2s - loss: 0.2717 - acc: 0.912 - ETA: 2s - loss: 0.2738 - acc: 0.911 - ETA: 2s - loss: 0.2725 - acc: 0.912 - ETA: 2s - loss: 0.2751 - acc: 0.911 - ETA: 2s - loss: 0.2763 - acc: 0.911 - ETA: 1s - loss: 0.2759 - acc: 0.911 - ETA: 1s - loss: 0.2755 - acc: 0.911 - ETA: 1s - loss: 0.2742 - acc: 0.911 - ETA: 1s - loss: 0.2728 - acc: 0.912 - ETA: 1s - loss: 0.2718 - acc: 0.912 - ETA: 1s - loss: 0.2712 - acc: 0.912 - ETA: 1s - loss: 0.2704 - acc: 0.912 - ETA: 1s - loss: 0.2708 - acc: 0.912 - ETA: 1s - loss: 0.2720 - acc: 0.912 - ETA: 1s - loss: 0.2717 - acc: 0.912 - ETA: 1s - loss: 0.2714 - acc: 0.912 - ETA: 1s - loss: 0.2707 - acc: 0.912 - ETA: 1s - loss: 0.2700 - acc: 0.912 - ETA: 1s - loss: 0.2719 - acc: 0.912 - ETA: 1s - loss: 0.2735 - acc: 0.911 - ETA: 1s - loss: 0.2734 - acc: 0.912 - ETA: 1s - loss: 0.2738 - acc: 0.911 - ETA: 1s - loss: 0.2746 - acc: 0.911 - ETA: 0s - loss: 0.2736 - acc: 0.912 - ETA: 0s - loss: 0.2750 - acc: 0.911 - ETA: 0s - loss: 0.2745 - acc: 0.912 - ETA: 0s - loss: 0.2743 - acc: 0.912 - ETA: 0s - loss: 0.2742 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.911 - ETA: 0s - loss: 0.2743 - acc: 0.912 - ETA: 0s - loss: 0.2740 - acc: 0.912 - ETA: 0s - loss: 0.2744 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.911 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2729 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2755 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.912 - ETA: 0s - loss: 0.2738 - acc: 0.912 - ETA: 0s - loss: 0.2735 - acc: 0.9129Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2737 - acc: 0.9130 - val_loss: 0.4910 - val_acc: 0.8503\n",
      "Epoch 5/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0845 - acc: 0.950 - ETA: 9s - loss: 0.3129 - acc: 0.883 - ETA: 9s - loss: 0.2535 - acc: 0.900 - ETA: 8s - loss: 0.2735 - acc: 0.907 - ETA: 8s - loss: 0.3418 - acc: 0.894 - ETA: 8s - loss: 0.3197 - acc: 0.900 - ETA: 8s - loss: 0.2845 - acc: 0.907 - ETA: 8s - loss: 0.2968 - acc: 0.906 - ETA: 8s - loss: 0.2768 - acc: 0.911 - ETA: 8s - loss: 0.2617 - acc: 0.913 - ETA: 8s - loss: 0.2460 - acc: 0.919 - ETA: 8s - loss: 0.2398 - acc: 0.919 - ETA: 8s - loss: 0.2295 - acc: 0.924 - ETA: 8s - loss: 0.2241 - acc: 0.924 - ETA: 8s - loss: 0.2198 - acc: 0.924 - ETA: 8s - loss: 0.2144 - acc: 0.927 - ETA: 8s - loss: 0.2244 - acc: 0.927 - ETA: 7s - loss: 0.2135 - acc: 0.930 - ETA: 7s - loss: 0.2124 - acc: 0.928 - ETA: 7s - loss: 0.2061 - acc: 0.929 - ETA: 7s - loss: 0.2124 - acc: 0.926 - ETA: 7s - loss: 0.2108 - acc: 0.929 - ETA: 7s - loss: 0.2105 - acc: 0.930 - ETA: 7s - loss: 0.2098 - acc: 0.929 - ETA: 7s - loss: 0.2101 - acc: 0.929 - ETA: 7s - loss: 0.2164 - acc: 0.926 - ETA: 7s - loss: 0.2138 - acc: 0.927 - ETA: 7s - loss: 0.2095 - acc: 0.929 - ETA: 7s - loss: 0.2159 - acc: 0.928 - ETA: 7s - loss: 0.2134 - acc: 0.928 - ETA: 7s - loss: 0.2098 - acc: 0.930 - ETA: 7s - loss: 0.2074 - acc: 0.931 - ETA: 7s - loss: 0.2064 - acc: 0.932 - ETA: 7s - loss: 0.2068 - acc: 0.932 - ETA: 7s - loss: 0.2083 - acc: 0.931 - ETA: 7s - loss: 0.2133 - acc: 0.932 - ETA: 6s - loss: 0.2110 - acc: 0.933 - ETA: 6s - loss: 0.2159 - acc: 0.932 - ETA: 6s - loss: 0.2166 - acc: 0.931 - ETA: 6s - loss: 0.2190 - acc: 0.930 - ETA: 6s - loss: 0.2162 - acc: 0.930 - ETA: 6s - loss: 0.2232 - acc: 0.930 - ETA: 6s - loss: 0.2279 - acc: 0.929 - ETA: 6s - loss: 0.2277 - acc: 0.929 - ETA: 6s - loss: 0.2256 - acc: 0.929 - ETA: 6s - loss: 0.2276 - acc: 0.928 - ETA: 6s - loss: 0.2315 - acc: 0.926 - ETA: 6s - loss: 0.2293 - acc: 0.927 - ETA: 6s - loss: 0.2272 - acc: 0.926 - ETA: 6s - loss: 0.2287 - acc: 0.925 - ETA: 6s - loss: 0.2339 - acc: 0.924 - ETA: 6s - loss: 0.2358 - acc: 0.923 - ETA: 6s - loss: 0.2358 - acc: 0.924 - ETA: 6s - loss: 0.2387 - acc: 0.923 - ETA: 5s - loss: 0.2381 - acc: 0.923 - ETA: 5s - loss: 0.2372 - acc: 0.923 - ETA: 5s - loss: 0.2370 - acc: 0.923 - ETA: 5s - loss: 0.2341 - acc: 0.924 - ETA: 5s - loss: 0.2339 - acc: 0.924 - ETA: 5s - loss: 0.2344 - acc: 0.924 - ETA: 5s - loss: 0.2339 - acc: 0.924 - ETA: 5s - loss: 0.2346 - acc: 0.923 - ETA: 5s - loss: 0.2345 - acc: 0.923 - ETA: 5s - loss: 0.2331 - acc: 0.924 - ETA: 5s - loss: 0.2323 - acc: 0.924 - ETA: 5s - loss: 0.2320 - acc: 0.923 - ETA: 5s - loss: 0.2300 - acc: 0.924 - ETA: 5s - loss: 0.2313 - acc: 0.923 - ETA: 5s - loss: 0.2325 - acc: 0.922 - ETA: 5s - loss: 0.2341 - acc: 0.922 - ETA: 5s - loss: 0.2347 - acc: 0.923 - ETA: 5s - loss: 0.2351 - acc: 0.923 - ETA: 5s - loss: 0.2358 - acc: 0.923 - ETA: 4s - loss: 0.2347 - acc: 0.923 - ETA: 4s - loss: 0.2336 - acc: 0.924 - ETA: 4s - loss: 0.2329 - acc: 0.924 - ETA: 4s - loss: 0.2344 - acc: 0.924 - ETA: 4s - loss: 0.2347 - acc: 0.923 - ETA: 4s - loss: 0.2339 - acc: 0.923 - ETA: 4s - loss: 0.2357 - acc: 0.923 - ETA: 4s - loss: 0.2339 - acc: 0.924 - ETA: 4s - loss: 0.2339 - acc: 0.924 - ETA: 4s - loss: 0.2324 - acc: 0.925 - ETA: 4s - loss: 0.2325 - acc: 0.925 - ETA: 4s - loss: 0.2335 - acc: 0.925 - ETA: 4s - loss: 0.2323 - acc: 0.925 - ETA: 4s - loss: 0.2325 - acc: 0.925 - ETA: 4s - loss: 0.2354 - acc: 0.925 - ETA: 4s - loss: 0.2349 - acc: 0.925 - ETA: 4s - loss: 0.2356 - acc: 0.925 - ETA: 4s - loss: 0.2347 - acc: 0.925 - ETA: 4s - loss: 0.2341 - acc: 0.925 - ETA: 3s - loss: 0.2328 - acc: 0.925 - ETA: 3s - loss: 0.2324 - acc: 0.925 - ETA: 3s - loss: 0.2358 - acc: 0.925 - ETA: 3s - loss: 0.2338 - acc: 0.925 - ETA: 3s - loss: 0.2340 - acc: 0.925 - ETA: 3s - loss: 0.2333 - acc: 0.925 - ETA: 3s - loss: 0.2340 - acc: 0.924 - ETA: 3s - loss: 0.2353 - acc: 0.924 - ETA: 3s - loss: 0.2358 - acc: 0.925 - ETA: 3s - loss: 0.2375 - acc: 0.925 - ETA: 3s - loss: 0.2370 - acc: 0.925 - ETA: 3s - loss: 0.2389 - acc: 0.925 - ETA: 3s - loss: 0.2398 - acc: 0.925 - ETA: 3s - loss: 0.2404 - acc: 0.925 - ETA: 3s - loss: 0.2391 - acc: 0.926 - ETA: 3s - loss: 0.2389 - acc: 0.926 - ETA: 3s - loss: 0.2371 - acc: 0.926 - ETA: 3s - loss: 0.2380 - acc: 0.926 - ETA: 2s - loss: 0.2387 - acc: 0.926 - ETA: 2s - loss: 0.2398 - acc: 0.926 - ETA: 2s - loss: 0.2392 - acc: 0.926 - ETA: 2s - loss: 0.2393 - acc: 0.926 - ETA: 2s - loss: 0.2398 - acc: 0.925 - ETA: 2s - loss: 0.2393 - acc: 0.926 - ETA: 2s - loss: 0.2385 - acc: 0.926 - ETA: 2s - loss: 0.2392 - acc: 0.926 - ETA: 2s - loss: 0.2385 - acc: 0.925 - ETA: 2s - loss: 0.2397 - acc: 0.925 - ETA: 2s - loss: 0.2391 - acc: 0.925 - ETA: 2s - loss: 0.2392 - acc: 0.925 - ETA: 2s - loss: 0.2409 - acc: 0.925 - ETA: 2s - loss: 0.2417 - acc: 0.925 - ETA: 2s - loss: 0.2420 - acc: 0.925 - ETA: 2s - loss: 0.2436 - acc: 0.924 - ETA: 2s - loss: 0.2456 - acc: 0.924 - ETA: 2s - loss: 0.2468 - acc: 0.923 - ETA: 1s - loss: 0.2461 - acc: 0.924 - ETA: 1s - loss: 0.2468 - acc: 0.924 - ETA: 1s - loss: 0.2454 - acc: 0.924 - ETA: 1s - loss: 0.2449 - acc: 0.924 - ETA: 1s - loss: 0.2446 - acc: 0.924 - ETA: 1s - loss: 0.2456 - acc: 0.924 - ETA: 1s - loss: 0.2453 - acc: 0.924 - ETA: 1s - loss: 0.2445 - acc: 0.924 - ETA: 1s - loss: 0.2469 - acc: 0.924 - ETA: 1s - loss: 0.2465 - acc: 0.924 - ETA: 1s - loss: 0.2475 - acc: 0.923 - ETA: 1s - loss: 0.2474 - acc: 0.923 - ETA: 1s - loss: 0.2476 - acc: 0.923 - ETA: 1s - loss: 0.2464 - acc: 0.924 - ETA: 1s - loss: 0.2460 - acc: 0.924 - ETA: 1s - loss: 0.2476 - acc: 0.923 - ETA: 1s - loss: 0.2474 - acc: 0.923 - ETA: 1s - loss: 0.2465 - acc: 0.924 - ETA: 1s - loss: 0.2478 - acc: 0.923 - ETA: 0s - loss: 0.2480 - acc: 0.923 - ETA: 0s - loss: 0.2501 - acc: 0.923 - ETA: 0s - loss: 0.2509 - acc: 0.923 - ETA: 0s - loss: 0.2503 - acc: 0.923 - ETA: 0s - loss: 0.2492 - acc: 0.923 - ETA: 0s - loss: 0.2484 - acc: 0.924 - ETA: 0s - loss: 0.2474 - acc: 0.924 - ETA: 0s - loss: 0.2464 - acc: 0.924 - ETA: 0s - loss: 0.2454 - acc: 0.925 - ETA: 0s - loss: 0.2459 - acc: 0.924 - ETA: 0s - loss: 0.2452 - acc: 0.924 - ETA: 0s - loss: 0.2444 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2446 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2437 - acc: 0.925 - ETA: 0s - loss: 0.2427 - acc: 0.925 - ETA: 0s - loss: 0.2423 - acc: 0.9251Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2423 - acc: 0.9250 - val_loss: 0.5193 - val_acc: 0.8551\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.1784 - acc: 0.950 - ETA: 9s - loss: 0.1546 - acc: 0.950 - ETA: 9s - loss: 0.1164 - acc: 0.970 - ETA: 9s - loss: 0.1314 - acc: 0.964 - ETA: 9s - loss: 0.1246 - acc: 0.966 - ETA: 8s - loss: 0.1258 - acc: 0.963 - ETA: 8s - loss: 0.1210 - acc: 0.965 - ETA: 8s - loss: 0.1135 - acc: 0.966 - ETA: 8s - loss: 0.1535 - acc: 0.961 - ETA: 8s - loss: 0.1592 - acc: 0.957 - ETA: 8s - loss: 0.1829 - acc: 0.950 - ETA: 8s - loss: 0.2078 - acc: 0.947 - ETA: 8s - loss: 0.2005 - acc: 0.948 - ETA: 8s - loss: 0.1952 - acc: 0.948 - ETA: 8s - loss: 0.1895 - acc: 0.950 - ETA: 8s - loss: 0.1895 - acc: 0.950 - ETA: 8s - loss: 0.1881 - acc: 0.947 - ETA: 8s - loss: 0.1847 - acc: 0.948 - ETA: 8s - loss: 0.1819 - acc: 0.947 - ETA: 7s - loss: 0.1775 - acc: 0.947 - ETA: 7s - loss: 0.1729 - acc: 0.948 - ETA: 7s - loss: 0.1823 - acc: 0.946 - ETA: 7s - loss: 0.1900 - acc: 0.944 - ETA: 7s - loss: 0.1887 - acc: 0.943 - ETA: 7s - loss: 0.1918 - acc: 0.940 - ETA: 7s - loss: 0.1894 - acc: 0.940 - ETA: 7s - loss: 0.1958 - acc: 0.937 - ETA: 7s - loss: 0.1932 - acc: 0.937 - ETA: 7s - loss: 0.1890 - acc: 0.938 - ETA: 7s - loss: 0.1886 - acc: 0.938 - ETA: 7s - loss: 0.1863 - acc: 0.939 - ETA: 7s - loss: 0.1892 - acc: 0.937 - ETA: 7s - loss: 0.1873 - acc: 0.936 - ETA: 7s - loss: 0.1840 - acc: 0.938 - ETA: 7s - loss: 0.1902 - acc: 0.936 - ETA: 7s - loss: 0.1877 - acc: 0.937 - ETA: 7s - loss: 0.1918 - acc: 0.935 - ETA: 6s - loss: 0.1886 - acc: 0.936 - ETA: 6s - loss: 0.1904 - acc: 0.937 - ETA: 6s - loss: 0.1918 - acc: 0.936 - ETA: 6s - loss: 0.1925 - acc: 0.935 - ETA: 6s - loss: 0.1890 - acc: 0.936 - ETA: 6s - loss: 0.1874 - acc: 0.937 - ETA: 6s - loss: 0.1879 - acc: 0.936 - ETA: 6s - loss: 0.1854 - acc: 0.937 - ETA: 6s - loss: 0.1847 - acc: 0.937 - ETA: 6s - loss: 0.1855 - acc: 0.937 - ETA: 6s - loss: 0.1835 - acc: 0.937 - ETA: 6s - loss: 0.1827 - acc: 0.937 - ETA: 6s - loss: 0.1826 - acc: 0.938 - ETA: 6s - loss: 0.1831 - acc: 0.938 - ETA: 6s - loss: 0.1822 - acc: 0.938 - ETA: 6s - loss: 0.1824 - acc: 0.939 - ETA: 6s - loss: 0.1841 - acc: 0.938 - ETA: 6s - loss: 0.1827 - acc: 0.939 - ETA: 6s - loss: 0.1807 - acc: 0.939 - ETA: 5s - loss: 0.1846 - acc: 0.937 - ETA: 5s - loss: 0.1836 - acc: 0.937 - ETA: 5s - loss: 0.1876 - acc: 0.936 - ETA: 5s - loss: 0.1879 - acc: 0.936 - ETA: 5s - loss: 0.1889 - acc: 0.935 - ETA: 5s - loss: 0.1879 - acc: 0.935 - ETA: 5s - loss: 0.1856 - acc: 0.936 - ETA: 5s - loss: 0.1848 - acc: 0.936 - ETA: 5s - loss: 0.1831 - acc: 0.936 - ETA: 5s - loss: 0.1828 - acc: 0.937 - ETA: 5s - loss: 0.1828 - acc: 0.936 - ETA: 5s - loss: 0.1829 - acc: 0.936 - ETA: 5s - loss: 0.1816 - acc: 0.937 - ETA: 5s - loss: 0.1827 - acc: 0.936 - ETA: 5s - loss: 0.1851 - acc: 0.935 - ETA: 5s - loss: 0.1880 - acc: 0.933 - ETA: 5s - loss: 0.1878 - acc: 0.934 - ETA: 5s - loss: 0.1884 - acc: 0.933 - ETA: 4s - loss: 0.1887 - acc: 0.932 - ETA: 4s - loss: 0.1912 - acc: 0.932 - ETA: 4s - loss: 0.1917 - acc: 0.932 - ETA: 4s - loss: 0.1910 - acc: 0.932 - ETA: 4s - loss: 0.1928 - acc: 0.932 - ETA: 4s - loss: 0.1909 - acc: 0.933 - ETA: 4s - loss: 0.1909 - acc: 0.933 - ETA: 4s - loss: 0.1891 - acc: 0.934 - ETA: 4s - loss: 0.1916 - acc: 0.933 - ETA: 4s - loss: 0.1939 - acc: 0.933 - ETA: 4s - loss: 0.1951 - acc: 0.933 - ETA: 4s - loss: 0.1941 - acc: 0.933 - ETA: 4s - loss: 0.1937 - acc: 0.933 - ETA: 4s - loss: 0.1950 - acc: 0.933 - ETA: 4s - loss: 0.1959 - acc: 0.932 - ETA: 4s - loss: 0.1983 - acc: 0.931 - ETA: 4s - loss: 0.1997 - acc: 0.932 - ETA: 4s - loss: 0.1992 - acc: 0.932 - ETA: 4s - loss: 0.1998 - acc: 0.931 - ETA: 3s - loss: 0.1981 - acc: 0.932 - ETA: 3s - loss: 0.1972 - acc: 0.933 - ETA: 3s - loss: 0.2006 - acc: 0.932 - ETA: 3s - loss: 0.1991 - acc: 0.933 - ETA: 3s - loss: 0.2013 - acc: 0.932 - ETA: 3s - loss: 0.2008 - acc: 0.932 - ETA: 3s - loss: 0.2047 - acc: 0.931 - ETA: 3s - loss: 0.2062 - acc: 0.931 - ETA: 3s - loss: 0.2048 - acc: 0.932 - ETA: 3s - loss: 0.2042 - acc: 0.932 - ETA: 3s - loss: 0.2060 - acc: 0.931 - ETA: 3s - loss: 0.2059 - acc: 0.931 - ETA: 3s - loss: 0.2049 - acc: 0.931 - ETA: 3s - loss: 0.2068 - acc: 0.931 - ETA: 3s - loss: 0.2061 - acc: 0.931 - ETA: 3s - loss: 0.2076 - acc: 0.931 - ETA: 3s - loss: 0.2070 - acc: 0.931 - ETA: 3s - loss: 0.2065 - acc: 0.931 - ETA: 2s - loss: 0.2079 - acc: 0.930 - ETA: 2s - loss: 0.2072 - acc: 0.931 - ETA: 2s - loss: 0.2062 - acc: 0.931 - ETA: 2s - loss: 0.2084 - acc: 0.931 - ETA: 2s - loss: 0.2086 - acc: 0.931 - ETA: 2s - loss: 0.2089 - acc: 0.931 - ETA: 2s - loss: 0.2110 - acc: 0.930 - ETA: 2s - loss: 0.2103 - acc: 0.930 - ETA: 2s - loss: 0.2123 - acc: 0.931 - ETA: 2s - loss: 0.2120 - acc: 0.931 - ETA: 2s - loss: 0.2111 - acc: 0.931 - ETA: 2s - loss: 0.2106 - acc: 0.931 - ETA: 2s - loss: 0.2109 - acc: 0.931 - ETA: 2s - loss: 0.2115 - acc: 0.931 - ETA: 2s - loss: 0.2111 - acc: 0.932 - ETA: 2s - loss: 0.2112 - acc: 0.931 - ETA: 2s - loss: 0.2115 - acc: 0.931 - ETA: 2s - loss: 0.2105 - acc: 0.932 - ETA: 1s - loss: 0.2106 - acc: 0.931 - ETA: 1s - loss: 0.2112 - acc: 0.931 - ETA: 1s - loss: 0.2105 - acc: 0.931 - ETA: 1s - loss: 0.2110 - acc: 0.931 - ETA: 1s - loss: 0.2115 - acc: 0.931 - ETA: 1s - loss: 0.2108 - acc: 0.931 - ETA: 1s - loss: 0.2108 - acc: 0.931 - ETA: 1s - loss: 0.2098 - acc: 0.931 - ETA: 1s - loss: 0.2086 - acc: 0.932 - ETA: 1s - loss: 0.2077 - acc: 0.932 - ETA: 1s - loss: 0.2067 - acc: 0.932 - ETA: 1s - loss: 0.2056 - acc: 0.933 - ETA: 1s - loss: 0.2067 - acc: 0.932 - ETA: 1s - loss: 0.2078 - acc: 0.932 - ETA: 1s - loss: 0.2087 - acc: 0.932 - ETA: 1s - loss: 0.2101 - acc: 0.931 - ETA: 1s - loss: 0.2114 - acc: 0.931 - ETA: 1s - loss: 0.2123 - acc: 0.931 - ETA: 1s - loss: 0.2127 - acc: 0.931 - ETA: 0s - loss: 0.2122 - acc: 0.931 - ETA: 0s - loss: 0.2126 - acc: 0.931 - ETA: 0s - loss: 0.2125 - acc: 0.931 - ETA: 0s - loss: 0.2128 - acc: 0.931 - ETA: 0s - loss: 0.2121 - acc: 0.931 - ETA: 0s - loss: 0.2113 - acc: 0.932 - ETA: 0s - loss: 0.2110 - acc: 0.932 - ETA: 0s - loss: 0.2125 - acc: 0.932 - ETA: 0s - loss: 0.2121 - acc: 0.932 - ETA: 0s - loss: 0.2123 - acc: 0.931 - ETA: 0s - loss: 0.2134 - acc: 0.930 - ETA: 0s - loss: 0.2129 - acc: 0.930 - ETA: 0s - loss: 0.2141 - acc: 0.930 - ETA: 0s - loss: 0.2131 - acc: 0.930 - ETA: 0s - loss: 0.2144 - acc: 0.930 - ETA: 0s - loss: 0.2152 - acc: 0.930 - ETA: 0s - loss: 0.2157 - acc: 0.930 - ETA: 0s - loss: 0.2147 - acc: 0.9304Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2142 - acc: 0.9305 - val_loss: 0.5187 - val_acc: 0.8563\n",
      "Epoch 7/20\n",
      "6640/6680 [============================>.] - ETA: 9s - loss: 0.4738 - acc: 0.850 - ETA: 10s - loss: 0.1968 - acc: 0.93 - ETA: 10s - loss: 0.1823 - acc: 0.93 - ETA: 9s - loss: 0.2099 - acc: 0.9214 - ETA: 9s - loss: 0.1875 - acc: 0.933 - ETA: 9s - loss: 0.1944 - acc: 0.940 - ETA: 9s - loss: 0.1757 - acc: 0.946 - ETA: 9s - loss: 0.2056 - acc: 0.933 - ETA: 8s - loss: 0.2013 - acc: 0.935 - ETA: 8s - loss: 0.1948 - acc: 0.936 - ETA: 8s - loss: 0.1953 - acc: 0.935 - ETA: 8s - loss: 0.1936 - acc: 0.934 - ETA: 8s - loss: 0.2034 - acc: 0.934 - ETA: 8s - loss: 0.1939 - acc: 0.937 - ETA: 8s - loss: 0.1848 - acc: 0.941 - ETA: 8s - loss: 0.1808 - acc: 0.940 - ETA: 8s - loss: 0.1933 - acc: 0.936 - ETA: 8s - loss: 0.1857 - acc: 0.940 - ETA: 8s - loss: 0.1905 - acc: 0.939 - ETA: 8s - loss: 0.1944 - acc: 0.938 - ETA: 7s - loss: 0.1878 - acc: 0.941 - ETA: 7s - loss: 0.1868 - acc: 0.941 - ETA: 7s - loss: 0.1826 - acc: 0.943 - ETA: 7s - loss: 0.1935 - acc: 0.940 - ETA: 7s - loss: 0.1960 - acc: 0.941 - ETA: 7s - loss: 0.1910 - acc: 0.943 - ETA: 7s - loss: 0.1886 - acc: 0.943 - ETA: 7s - loss: 0.1841 - acc: 0.944 - ETA: 7s - loss: 0.1813 - acc: 0.945 - ETA: 7s - loss: 0.1810 - acc: 0.944 - ETA: 7s - loss: 0.1818 - acc: 0.943 - ETA: 7s - loss: 0.1848 - acc: 0.943 - ETA: 7s - loss: 0.1826 - acc: 0.943 - ETA: 7s - loss: 0.1800 - acc: 0.943 - ETA: 7s - loss: 0.1793 - acc: 0.942 - ETA: 7s - loss: 0.1755 - acc: 0.943 - ETA: 7s - loss: 0.1742 - acc: 0.944 - ETA: 6s - loss: 0.1770 - acc: 0.944 - ETA: 6s - loss: 0.1874 - acc: 0.942 - ETA: 6s - loss: 0.1835 - acc: 0.943 - ETA: 6s - loss: 0.1852 - acc: 0.944 - ETA: 6s - loss: 0.1827 - acc: 0.945 - ETA: 6s - loss: 0.1812 - acc: 0.944 - ETA: 6s - loss: 0.1782 - acc: 0.945 - ETA: 6s - loss: 0.1760 - acc: 0.946 - ETA: 6s - loss: 0.1806 - acc: 0.945 - ETA: 6s - loss: 0.1831 - acc: 0.944 - ETA: 6s - loss: 0.1912 - acc: 0.942 - ETA: 6s - loss: 0.1939 - acc: 0.941 - ETA: 6s - loss: 0.1919 - acc: 0.942 - ETA: 6s - loss: 0.1903 - acc: 0.943 - ETA: 6s - loss: 0.1899 - acc: 0.942 - ETA: 6s - loss: 0.1871 - acc: 0.943 - ETA: 6s - loss: 0.1869 - acc: 0.943 - ETA: 6s - loss: 0.1894 - acc: 0.942 - ETA: 6s - loss: 0.1887 - acc: 0.941 - ETA: 5s - loss: 0.1890 - acc: 0.941 - ETA: 5s - loss: 0.1864 - acc: 0.942 - ETA: 5s - loss: 0.1852 - acc: 0.942 - ETA: 5s - loss: 0.1872 - acc: 0.942 - ETA: 5s - loss: 0.1857 - acc: 0.943 - ETA: 5s - loss: 0.1838 - acc: 0.943 - ETA: 5s - loss: 0.1833 - acc: 0.944 - ETA: 5s - loss: 0.1836 - acc: 0.943 - ETA: 5s - loss: 0.1817 - acc: 0.944 - ETA: 5s - loss: 0.1857 - acc: 0.944 - ETA: 5s - loss: 0.1884 - acc: 0.943 - ETA: 5s - loss: 0.1862 - acc: 0.943 - ETA: 5s - loss: 0.1881 - acc: 0.943 - ETA: 5s - loss: 0.1892 - acc: 0.943 - ETA: 5s - loss: 0.1892 - acc: 0.942 - ETA: 5s - loss: 0.1891 - acc: 0.942 - ETA: 5s - loss: 0.1909 - acc: 0.942 - ETA: 5s - loss: 0.1916 - acc: 0.941 - ETA: 4s - loss: 0.1900 - acc: 0.941 - ETA: 4s - loss: 0.1898 - acc: 0.941 - ETA: 4s - loss: 0.1901 - acc: 0.941 - ETA: 4s - loss: 0.1952 - acc: 0.940 - ETA: 4s - loss: 0.1956 - acc: 0.940 - ETA: 4s - loss: 0.1941 - acc: 0.940 - ETA: 4s - loss: 0.1978 - acc: 0.940 - ETA: 4s - loss: 0.1987 - acc: 0.939 - ETA: 4s - loss: 0.2005 - acc: 0.939 - ETA: 4s - loss: 0.2016 - acc: 0.939 - ETA: 4s - loss: 0.2003 - acc: 0.940 - ETA: 4s - loss: 0.2016 - acc: 0.939 - ETA: 4s - loss: 0.2007 - acc: 0.939 - ETA: 4s - loss: 0.1998 - acc: 0.940 - ETA: 4s - loss: 0.1993 - acc: 0.939 - ETA: 4s - loss: 0.1990 - acc: 0.940 - ETA: 4s - loss: 0.1983 - acc: 0.940 - ETA: 4s - loss: 0.1999 - acc: 0.939 - ETA: 3s - loss: 0.2004 - acc: 0.939 - ETA: 3s - loss: 0.2015 - acc: 0.938 - ETA: 3s - loss: 0.2001 - acc: 0.939 - ETA: 3s - loss: 0.2000 - acc: 0.939 - ETA: 3s - loss: 0.2031 - acc: 0.938 - ETA: 3s - loss: 0.2041 - acc: 0.938 - ETA: 3s - loss: 0.2030 - acc: 0.938 - ETA: 3s - loss: 0.2028 - acc: 0.938 - ETA: 3s - loss: 0.2016 - acc: 0.938 - ETA: 3s - loss: 0.2012 - acc: 0.939 - ETA: 3s - loss: 0.2027 - acc: 0.939 - ETA: 3s - loss: 0.2046 - acc: 0.938 - ETA: 3s - loss: 0.2036 - acc: 0.938 - ETA: 3s - loss: 0.2022 - acc: 0.939 - ETA: 3s - loss: 0.2020 - acc: 0.939 - ETA: 3s - loss: 0.2017 - acc: 0.939 - ETA: 3s - loss: 0.2018 - acc: 0.939 - ETA: 3s - loss: 0.2045 - acc: 0.938 - ETA: 3s - loss: 0.2034 - acc: 0.938 - ETA: 2s - loss: 0.2059 - acc: 0.937 - ETA: 2s - loss: 0.2050 - acc: 0.938 - ETA: 2s - loss: 0.2053 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.937 - ETA: 2s - loss: 0.2042 - acc: 0.938 - ETA: 2s - loss: 0.2049 - acc: 0.938 - ETA: 2s - loss: 0.2041 - acc: 0.938 - ETA: 2s - loss: 0.2032 - acc: 0.938 - ETA: 2s - loss: 0.2032 - acc: 0.938 - ETA: 2s - loss: 0.2042 - acc: 0.938 - ETA: 2s - loss: 0.2044 - acc: 0.938 - ETA: 2s - loss: 0.2039 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.938 - ETA: 2s - loss: 0.2033 - acc: 0.938 - ETA: 2s - loss: 0.2020 - acc: 0.939 - ETA: 2s - loss: 0.2013 - acc: 0.939 - ETA: 2s - loss: 0.2026 - acc: 0.939 - ETA: 1s - loss: 0.2032 - acc: 0.938 - ETA: 1s - loss: 0.2024 - acc: 0.938 - ETA: 1s - loss: 0.2010 - acc: 0.939 - ETA: 1s - loss: 0.2014 - acc: 0.939 - ETA: 1s - loss: 0.2006 - acc: 0.939 - ETA: 1s - loss: 0.1999 - acc: 0.939 - ETA: 1s - loss: 0.1986 - acc: 0.940 - ETA: 1s - loss: 0.1987 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.940 - ETA: 1s - loss: 0.1972 - acc: 0.940 - ETA: 1s - loss: 0.1976 - acc: 0.939 - ETA: 1s - loss: 0.1969 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.940 - ETA: 1s - loss: 0.1976 - acc: 0.940 - ETA: 1s - loss: 0.1989 - acc: 0.939 - ETA: 1s - loss: 0.2009 - acc: 0.938 - ETA: 1s - loss: 0.2000 - acc: 0.938 - ETA: 1s - loss: 0.2013 - acc: 0.938 - ETA: 1s - loss: 0.2011 - acc: 0.938 - ETA: 0s - loss: 0.2002 - acc: 0.939 - ETA: 0s - loss: 0.1992 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1990 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1977 - acc: 0.939 - ETA: 0s - loss: 0.1981 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1988 - acc: 0.939 - ETA: 0s - loss: 0.2000 - acc: 0.939 - ETA: 0s - loss: 0.1996 - acc: 0.939 - ETA: 0s - loss: 0.1999 - acc: 0.939 - ETA: 0s - loss: 0.1994 - acc: 0.939 - ETA: 0s - loss: 0.1988 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1991 - acc: 0.939 - ETA: 0s - loss: 0.1989 - acc: 0.939 - ETA: 0s - loss: 0.1979 - acc: 0.9399Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1978 - acc: 0.9400 - val_loss: 0.5460 - val_acc: 0.8467\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.2594 - acc: 0.950 - ETA: 9s - loss: 0.3138 - acc: 0.950 - ETA: 9s - loss: 0.2051 - acc: 0.970 - ETA: 9s - loss: 0.1795 - acc: 0.971 - ETA: 9s - loss: 0.1771 - acc: 0.972 - ETA: 9s - loss: 0.1664 - acc: 0.968 - ETA: 9s - loss: 0.1544 - acc: 0.969 - ETA: 8s - loss: 0.1602 - acc: 0.963 - ETA: 8s - loss: 0.1690 - acc: 0.955 - ETA: 8s - loss: 0.1625 - acc: 0.957 - ETA: 8s - loss: 0.1718 - acc: 0.957 - ETA: 8s - loss: 0.1640 - acc: 0.958 - ETA: 8s - loss: 0.1583 - acc: 0.958 - ETA: 8s - loss: 0.1594 - acc: 0.957 - ETA: 8s - loss: 0.1687 - acc: 0.953 - ETA: 8s - loss: 0.1662 - acc: 0.953 - ETA: 8s - loss: 0.1639 - acc: 0.954 - ETA: 8s - loss: 0.1588 - acc: 0.955 - ETA: 8s - loss: 0.1613 - acc: 0.952 - ETA: 8s - loss: 0.1726 - acc: 0.952 - ETA: 7s - loss: 0.1762 - acc: 0.948 - ETA: 7s - loss: 0.1804 - acc: 0.948 - ETA: 7s - loss: 0.1761 - acc: 0.948 - ETA: 7s - loss: 0.1756 - acc: 0.948 - ETA: 7s - loss: 0.1752 - acc: 0.946 - ETA: 7s - loss: 0.1727 - acc: 0.947 - ETA: 7s - loss: 0.1748 - acc: 0.947 - ETA: 7s - loss: 0.1729 - acc: 0.947 - ETA: 7s - loss: 0.1682 - acc: 0.949 - ETA: 7s - loss: 0.1665 - acc: 0.949 - ETA: 7s - loss: 0.1726 - acc: 0.946 - ETA: 7s - loss: 0.1734 - acc: 0.946 - ETA: 7s - loss: 0.1749 - acc: 0.944 - ETA: 7s - loss: 0.1740 - acc: 0.943 - ETA: 7s - loss: 0.1754 - acc: 0.941 - ETA: 7s - loss: 0.1718 - acc: 0.942 - ETA: 7s - loss: 0.1706 - acc: 0.941 - ETA: 7s - loss: 0.1718 - acc: 0.942 - ETA: 6s - loss: 0.1755 - acc: 0.942 - ETA: 6s - loss: 0.1718 - acc: 0.944 - ETA: 6s - loss: 0.1689 - acc: 0.945 - ETA: 6s - loss: 0.1692 - acc: 0.944 - ETA: 6s - loss: 0.1703 - acc: 0.943 - ETA: 6s - loss: 0.1758 - acc: 0.942 - ETA: 6s - loss: 0.1759 - acc: 0.942 - ETA: 6s - loss: 0.1742 - acc: 0.942 - ETA: 6s - loss: 0.1764 - acc: 0.941 - ETA: 6s - loss: 0.1829 - acc: 0.940 - ETA: 6s - loss: 0.1802 - acc: 0.941 - ETA: 6s - loss: 0.1776 - acc: 0.942 - ETA: 6s - loss: 0.1790 - acc: 0.942 - ETA: 6s - loss: 0.1765 - acc: 0.943 - ETA: 6s - loss: 0.1736 - acc: 0.944 - ETA: 6s - loss: 0.1727 - acc: 0.943 - ETA: 6s - loss: 0.1722 - acc: 0.943 - ETA: 5s - loss: 0.1703 - acc: 0.944 - ETA: 5s - loss: 0.1694 - acc: 0.944 - ETA: 5s - loss: 0.1690 - acc: 0.944 - ETA: 5s - loss: 0.1687 - acc: 0.944 - ETA: 5s - loss: 0.1723 - acc: 0.943 - ETA: 5s - loss: 0.1711 - acc: 0.943 - ETA: 5s - loss: 0.1742 - acc: 0.942 - ETA: 5s - loss: 0.1732 - acc: 0.942 - ETA: 5s - loss: 0.1713 - acc: 0.942 - ETA: 5s - loss: 0.1732 - acc: 0.942 - ETA: 5s - loss: 0.1740 - acc: 0.941 - ETA: 5s - loss: 0.1742 - acc: 0.942 - ETA: 5s - loss: 0.1754 - acc: 0.941 - ETA: 5s - loss: 0.1803 - acc: 0.941 - ETA: 5s - loss: 0.1797 - acc: 0.941 - ETA: 5s - loss: 0.1806 - acc: 0.941 - ETA: 5s - loss: 0.1809 - acc: 0.941 - ETA: 5s - loss: 0.1796 - acc: 0.942 - ETA: 5s - loss: 0.1782 - acc: 0.942 - ETA: 4s - loss: 0.1781 - acc: 0.942 - ETA: 4s - loss: 0.1795 - acc: 0.941 - ETA: 4s - loss: 0.1810 - acc: 0.941 - ETA: 4s - loss: 0.1818 - acc: 0.940 - ETA: 4s - loss: 0.1819 - acc: 0.940 - ETA: 4s - loss: 0.1812 - acc: 0.940 - ETA: 4s - loss: 0.1797 - acc: 0.941 - ETA: 4s - loss: 0.1793 - acc: 0.941 - ETA: 4s - loss: 0.1788 - acc: 0.940 - ETA: 4s - loss: 0.1782 - acc: 0.941 - ETA: 4s - loss: 0.1777 - acc: 0.941 - ETA: 4s - loss: 0.1772 - acc: 0.941 - ETA: 4s - loss: 0.1763 - acc: 0.941 - ETA: 4s - loss: 0.1745 - acc: 0.942 - ETA: 4s - loss: 0.1735 - acc: 0.942 - ETA: 4s - loss: 0.1736 - acc: 0.942 - ETA: 4s - loss: 0.1731 - acc: 0.942 - ETA: 4s - loss: 0.1719 - acc: 0.943 - ETA: 4s - loss: 0.1719 - acc: 0.943 - ETA: 3s - loss: 0.1712 - acc: 0.942 - ETA: 3s - loss: 0.1719 - acc: 0.942 - ETA: 3s - loss: 0.1713 - acc: 0.942 - ETA: 3s - loss: 0.1703 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.943 - ETA: 3s - loss: 0.1681 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.943 - ETA: 3s - loss: 0.1719 - acc: 0.943 - ETA: 3s - loss: 0.1720 - acc: 0.943 - ETA: 3s - loss: 0.1708 - acc: 0.943 - ETA: 3s - loss: 0.1708 - acc: 0.943 - ETA: 3s - loss: 0.1702 - acc: 0.943 - ETA: 3s - loss: 0.1705 - acc: 0.943 - ETA: 3s - loss: 0.1701 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.944 - ETA: 3s - loss: 0.1685 - acc: 0.944 - ETA: 3s - loss: 0.1682 - acc: 0.943 - ETA: 3s - loss: 0.1671 - acc: 0.944 - ETA: 2s - loss: 0.1663 - acc: 0.944 - ETA: 2s - loss: 0.1651 - acc: 0.944 - ETA: 2s - loss: 0.1643 - acc: 0.944 - ETA: 2s - loss: 0.1647 - acc: 0.945 - ETA: 2s - loss: 0.1678 - acc: 0.944 - ETA: 2s - loss: 0.1671 - acc: 0.944 - ETA: 2s - loss: 0.1670 - acc: 0.945 - ETA: 2s - loss: 0.1663 - acc: 0.945 - ETA: 2s - loss: 0.1689 - acc: 0.944 - ETA: 2s - loss: 0.1706 - acc: 0.944 - ETA: 2s - loss: 0.1709 - acc: 0.943 - ETA: 2s - loss: 0.1697 - acc: 0.944 - ETA: 2s - loss: 0.1695 - acc: 0.943 - ETA: 2s - loss: 0.1706 - acc: 0.943 - ETA: 2s - loss: 0.1700 - acc: 0.943 - ETA: 2s - loss: 0.1695 - acc: 0.944 - ETA: 2s - loss: 0.1702 - acc: 0.943 - ETA: 2s - loss: 0.1696 - acc: 0.943 - ETA: 2s - loss: 0.1693 - acc: 0.944 - ETA: 1s - loss: 0.1697 - acc: 0.943 - ETA: 1s - loss: 0.1698 - acc: 0.943 - ETA: 1s - loss: 0.1691 - acc: 0.944 - ETA: 1s - loss: 0.1695 - acc: 0.944 - ETA: 1s - loss: 0.1695 - acc: 0.944 - ETA: 1s - loss: 0.1693 - acc: 0.944 - ETA: 1s - loss: 0.1683 - acc: 0.944 - ETA: 1s - loss: 0.1681 - acc: 0.944 - ETA: 1s - loss: 0.1683 - acc: 0.944 - ETA: 1s - loss: 0.1685 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.944 - ETA: 1s - loss: 0.1692 - acc: 0.944 - ETA: 1s - loss: 0.1704 - acc: 0.944 - ETA: 1s - loss: 0.1704 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.944 - ETA: 1s - loss: 0.1701 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.945 - ETA: 1s - loss: 0.1694 - acc: 0.945 - ETA: 0s - loss: 0.1686 - acc: 0.945 - ETA: 0s - loss: 0.1703 - acc: 0.945 - ETA: 0s - loss: 0.1715 - acc: 0.945 - ETA: 0s - loss: 0.1711 - acc: 0.945 - ETA: 0s - loss: 0.1708 - acc: 0.945 - ETA: 0s - loss: 0.1704 - acc: 0.945 - ETA: 0s - loss: 0.1708 - acc: 0.945 - ETA: 0s - loss: 0.1713 - acc: 0.945 - ETA: 0s - loss: 0.1723 - acc: 0.945 - ETA: 0s - loss: 0.1721 - acc: 0.945 - ETA: 0s - loss: 0.1737 - acc: 0.945 - ETA: 0s - loss: 0.1733 - acc: 0.945 - ETA: 0s - loss: 0.1730 - acc: 0.945 - ETA: 0s - loss: 0.1744 - acc: 0.944 - ETA: 0s - loss: 0.1746 - acc: 0.944 - ETA: 0s - loss: 0.1746 - acc: 0.945 - ETA: 0s - loss: 0.1739 - acc: 0.945 - ETA: 0s - loss: 0.1756 - acc: 0.944 - ETA: 0s - loss: 0.1756 - acc: 0.9444Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1753 - acc: 0.9445 - val_loss: 0.5412 - val_acc: 0.8551\n",
      "Epoch 9/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0574 - acc: 0.950 - ETA: 9s - loss: 0.0555 - acc: 0.983 - ETA: 9s - loss: 0.0542 - acc: 0.990 - ETA: 9s - loss: 0.0561 - acc: 0.985 - ETA: 9s - loss: 0.0568 - acc: 0.983 - ETA: 9s - loss: 0.0938 - acc: 0.968 - ETA: 9s - loss: 0.1167 - acc: 0.961 - ETA: 9s - loss: 0.1215 - acc: 0.956 - ETA: 8s - loss: 0.1126 - acc: 0.958 - ETA: 8s - loss: 0.1044 - acc: 0.963 - ETA: 8s - loss: 0.0983 - acc: 0.966 - ETA: 8s - loss: 0.1017 - acc: 0.963 - ETA: 8s - loss: 0.0977 - acc: 0.966 - ETA: 8s - loss: 0.0968 - acc: 0.966 - ETA: 8s - loss: 0.1113 - acc: 0.958 - ETA: 8s - loss: 0.1068 - acc: 0.959 - ETA: 8s - loss: 0.1104 - acc: 0.959 - ETA: 8s - loss: 0.1079 - acc: 0.960 - ETA: 8s - loss: 0.1043 - acc: 0.960 - ETA: 8s - loss: 0.1122 - acc: 0.960 - ETA: 8s - loss: 0.1091 - acc: 0.962 - ETA: 7s - loss: 0.1048 - acc: 0.964 - ETA: 7s - loss: 0.1107 - acc: 0.961 - ETA: 7s - loss: 0.1093 - acc: 0.961 - ETA: 7s - loss: 0.1183 - acc: 0.960 - ETA: 7s - loss: 0.1185 - acc: 0.958 - ETA: 7s - loss: 0.1217 - acc: 0.956 - ETA: 7s - loss: 0.1208 - acc: 0.957 - ETA: 7s - loss: 0.1241 - acc: 0.957 - ETA: 7s - loss: 0.1235 - acc: 0.956 - ETA: 7s - loss: 0.1228 - acc: 0.956 - ETA: 7s - loss: 0.1216 - acc: 0.957 - ETA: 7s - loss: 0.1188 - acc: 0.958 - ETA: 7s - loss: 0.1212 - acc: 0.958 - ETA: 7s - loss: 0.1247 - acc: 0.958 - ETA: 7s - loss: 0.1265 - acc: 0.957 - ETA: 7s - loss: 0.1275 - acc: 0.957 - ETA: 7s - loss: 0.1295 - acc: 0.957 - ETA: 6s - loss: 0.1268 - acc: 0.958 - ETA: 6s - loss: 0.1301 - acc: 0.956 - ETA: 6s - loss: 0.1298 - acc: 0.956 - ETA: 6s - loss: 0.1277 - acc: 0.957 - ETA: 6s - loss: 0.1261 - acc: 0.957 - ETA: 6s - loss: 0.1272 - acc: 0.956 - ETA: 6s - loss: 0.1253 - acc: 0.956 - ETA: 6s - loss: 0.1235 - acc: 0.957 - ETA: 6s - loss: 0.1267 - acc: 0.956 - ETA: 6s - loss: 0.1272 - acc: 0.955 - ETA: 6s - loss: 0.1266 - acc: 0.955 - ETA: 6s - loss: 0.1290 - acc: 0.955 - ETA: 6s - loss: 0.1272 - acc: 0.955 - ETA: 6s - loss: 0.1319 - acc: 0.953 - ETA: 6s - loss: 0.1343 - acc: 0.951 - ETA: 6s - loss: 0.1354 - acc: 0.951 - ETA: 6s - loss: 0.1436 - acc: 0.950 - ETA: 6s - loss: 0.1467 - acc: 0.950 - ETA: 5s - loss: 0.1447 - acc: 0.950 - ETA: 5s - loss: 0.1453 - acc: 0.950 - ETA: 5s - loss: 0.1434 - acc: 0.951 - ETA: 5s - loss: 0.1443 - acc: 0.950 - ETA: 5s - loss: 0.1422 - acc: 0.951 - ETA: 5s - loss: 0.1460 - acc: 0.950 - ETA: 5s - loss: 0.1450 - acc: 0.951 - ETA: 5s - loss: 0.1444 - acc: 0.951 - ETA: 5s - loss: 0.1424 - acc: 0.952 - ETA: 5s - loss: 0.1406 - acc: 0.953 - ETA: 5s - loss: 0.1425 - acc: 0.952 - ETA: 5s - loss: 0.1407 - acc: 0.953 - ETA: 5s - loss: 0.1390 - acc: 0.954 - ETA: 5s - loss: 0.1425 - acc: 0.952 - ETA: 5s - loss: 0.1421 - acc: 0.952 - ETA: 5s - loss: 0.1408 - acc: 0.953 - ETA: 5s - loss: 0.1402 - acc: 0.953 - ETA: 4s - loss: 0.1394 - acc: 0.954 - ETA: 4s - loss: 0.1379 - acc: 0.954 - ETA: 4s - loss: 0.1378 - acc: 0.954 - ETA: 4s - loss: 0.1369 - acc: 0.954 - ETA: 4s - loss: 0.1373 - acc: 0.954 - ETA: 4s - loss: 0.1388 - acc: 0.953 - ETA: 4s - loss: 0.1416 - acc: 0.953 - ETA: 4s - loss: 0.1418 - acc: 0.952 - ETA: 4s - loss: 0.1431 - acc: 0.952 - ETA: 4s - loss: 0.1451 - acc: 0.952 - ETA: 4s - loss: 0.1458 - acc: 0.952 - ETA: 4s - loss: 0.1465 - acc: 0.952 - ETA: 4s - loss: 0.1481 - acc: 0.951 - ETA: 4s - loss: 0.1495 - acc: 0.951 - ETA: 4s - loss: 0.1506 - acc: 0.951 - ETA: 4s - loss: 0.1502 - acc: 0.951 - ETA: 4s - loss: 0.1497 - acc: 0.951 - ETA: 4s - loss: 0.1497 - acc: 0.951 - ETA: 3s - loss: 0.1490 - acc: 0.951 - ETA: 3s - loss: 0.1510 - acc: 0.952 - ETA: 3s - loss: 0.1516 - acc: 0.951 - ETA: 3s - loss: 0.1511 - acc: 0.952 - ETA: 3s - loss: 0.1518 - acc: 0.951 - ETA: 3s - loss: 0.1510 - acc: 0.952 - ETA: 3s - loss: 0.1506 - acc: 0.952 - ETA: 3s - loss: 0.1494 - acc: 0.952 - ETA: 3s - loss: 0.1497 - acc: 0.952 - ETA: 3s - loss: 0.1487 - acc: 0.952 - ETA: 3s - loss: 0.1503 - acc: 0.952 - ETA: 3s - loss: 0.1509 - acc: 0.952 - ETA: 3s - loss: 0.1545 - acc: 0.951 - ETA: 3s - loss: 0.1535 - acc: 0.951 - ETA: 3s - loss: 0.1538 - acc: 0.951 - ETA: 3s - loss: 0.1554 - acc: 0.951 - ETA: 3s - loss: 0.1574 - acc: 0.950 - ETA: 3s - loss: 0.1564 - acc: 0.951 - ETA: 3s - loss: 0.1559 - acc: 0.951 - ETA: 2s - loss: 0.1560 - acc: 0.951 - ETA: 2s - loss: 0.1562 - acc: 0.951 - ETA: 2s - loss: 0.1583 - acc: 0.950 - ETA: 2s - loss: 0.1581 - acc: 0.949 - ETA: 2s - loss: 0.1587 - acc: 0.949 - ETA: 2s - loss: 0.1598 - acc: 0.948 - ETA: 2s - loss: 0.1599 - acc: 0.947 - ETA: 2s - loss: 0.1589 - acc: 0.948 - ETA: 2s - loss: 0.1581 - acc: 0.948 - ETA: 2s - loss: 0.1574 - acc: 0.948 - ETA: 2s - loss: 0.1570 - acc: 0.949 - ETA: 2s - loss: 0.1559 - acc: 0.949 - ETA: 2s - loss: 0.1553 - acc: 0.949 - ETA: 2s - loss: 0.1545 - acc: 0.949 - ETA: 2s - loss: 0.1547 - acc: 0.949 - ETA: 2s - loss: 0.1556 - acc: 0.949 - ETA: 2s - loss: 0.1554 - acc: 0.949 - ETA: 2s - loss: 0.1547 - acc: 0.949 - ETA: 2s - loss: 0.1540 - acc: 0.949 - ETA: 1s - loss: 0.1536 - acc: 0.949 - ETA: 1s - loss: 0.1540 - acc: 0.949 - ETA: 1s - loss: 0.1539 - acc: 0.949 - ETA: 1s - loss: 0.1547 - acc: 0.948 - ETA: 1s - loss: 0.1538 - acc: 0.949 - ETA: 1s - loss: 0.1535 - acc: 0.949 - ETA: 1s - loss: 0.1552 - acc: 0.949 - ETA: 1s - loss: 0.1545 - acc: 0.949 - ETA: 1s - loss: 0.1539 - acc: 0.949 - ETA: 1s - loss: 0.1537 - acc: 0.949 - ETA: 1s - loss: 0.1535 - acc: 0.949 - ETA: 1s - loss: 0.1557 - acc: 0.949 - ETA: 1s - loss: 0.1554 - acc: 0.949 - ETA: 1s - loss: 0.1556 - acc: 0.949 - ETA: 1s - loss: 0.1555 - acc: 0.949 - ETA: 1s - loss: 0.1572 - acc: 0.949 - ETA: 1s - loss: 0.1564 - acc: 0.949 - ETA: 1s - loss: 0.1554 - acc: 0.949 - ETA: 0s - loss: 0.1548 - acc: 0.949 - ETA: 0s - loss: 0.1542 - acc: 0.950 - ETA: 0s - loss: 0.1538 - acc: 0.950 - ETA: 0s - loss: 0.1545 - acc: 0.950 - ETA: 0s - loss: 0.1555 - acc: 0.950 - ETA: 0s - loss: 0.1558 - acc: 0.949 - ETA: 0s - loss: 0.1573 - acc: 0.949 - ETA: 0s - loss: 0.1572 - acc: 0.949 - ETA: 0s - loss: 0.1598 - acc: 0.949 - ETA: 0s - loss: 0.1595 - acc: 0.949 - ETA: 0s - loss: 0.1618 - acc: 0.948 - ETA: 0s - loss: 0.1619 - acc: 0.948 - ETA: 0s - loss: 0.1622 - acc: 0.948 - ETA: 0s - loss: 0.1622 - acc: 0.948 - ETA: 0s - loss: 0.1620 - acc: 0.948 - ETA: 0s - loss: 0.1630 - acc: 0.948 - ETA: 0s - loss: 0.1629 - acc: 0.948 - ETA: 0s - loss: 0.1648 - acc: 0.947 - ETA: 0s - loss: 0.1643 - acc: 0.9480Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1641 - acc: 0.9481 - val_loss: 0.5540 - val_acc: 0.8587\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1739 - acc: 0.900 - ETA: 9s - loss: 0.2295 - acc: 0.933 - ETA: 9s - loss: 0.2222 - acc: 0.940 - ETA: 9s - loss: 0.1620 - acc: 0.957 - ETA: 9s - loss: 0.1885 - acc: 0.950 - ETA: 9s - loss: 0.1715 - acc: 0.954 - ETA: 9s - loss: 0.1852 - acc: 0.957 - ETA: 8s - loss: 0.1658 - acc: 0.963 - ETA: 8s - loss: 0.1644 - acc: 0.958 - ETA: 8s - loss: 0.1551 - acc: 0.960 - ETA: 8s - loss: 0.1557 - acc: 0.959 - ETA: 8s - loss: 0.1472 - acc: 0.960 - ETA: 8s - loss: 0.1384 - acc: 0.964 - ETA: 8s - loss: 0.1297 - acc: 0.966 - ETA: 8s - loss: 0.1246 - acc: 0.967 - ETA: 8s - loss: 0.1278 - acc: 0.964 - ETA: 8s - loss: 0.1258 - acc: 0.963 - ETA: 8s - loss: 0.1196 - acc: 0.965 - ETA: 8s - loss: 0.1271 - acc: 0.963 - ETA: 8s - loss: 0.1219 - acc: 0.965 - ETA: 8s - loss: 0.1187 - acc: 0.965 - ETA: 8s - loss: 0.1157 - acc: 0.966 - ETA: 8s - loss: 0.1143 - acc: 0.965 - ETA: 7s - loss: 0.1112 - acc: 0.967 - ETA: 7s - loss: 0.1077 - acc: 0.968 - ETA: 7s - loss: 0.1170 - acc: 0.967 - ETA: 7s - loss: 0.1132 - acc: 0.968 - ETA: 7s - loss: 0.1110 - acc: 0.970 - ETA: 7s - loss: 0.1152 - acc: 0.969 - ETA: 7s - loss: 0.1119 - acc: 0.970 - ETA: 7s - loss: 0.1159 - acc: 0.969 - ETA: 7s - loss: 0.1189 - acc: 0.969 - ETA: 7s - loss: 0.1175 - acc: 0.969 - ETA: 7s - loss: 0.1206 - acc: 0.968 - ETA: 7s - loss: 0.1218 - acc: 0.968 - ETA: 7s - loss: 0.1237 - acc: 0.968 - ETA: 7s - loss: 0.1267 - acc: 0.967 - ETA: 7s - loss: 0.1254 - acc: 0.968 - ETA: 7s - loss: 0.1269 - acc: 0.966 - ETA: 6s - loss: 0.1255 - acc: 0.966 - ETA: 6s - loss: 0.1270 - acc: 0.966 - ETA: 6s - loss: 0.1295 - acc: 0.965 - ETA: 6s - loss: 0.1292 - acc: 0.964 - ETA: 6s - loss: 0.1314 - acc: 0.962 - ETA: 6s - loss: 0.1358 - acc: 0.961 - ETA: 6s - loss: 0.1394 - acc: 0.960 - ETA: 6s - loss: 0.1383 - acc: 0.960 - ETA: 6s - loss: 0.1362 - acc: 0.961 - ETA: 6s - loss: 0.1354 - acc: 0.961 - ETA: 6s - loss: 0.1338 - acc: 0.962 - ETA: 6s - loss: 0.1316 - acc: 0.962 - ETA: 6s - loss: 0.1325 - acc: 0.962 - ETA: 6s - loss: 0.1318 - acc: 0.962 - ETA: 6s - loss: 0.1298 - acc: 0.963 - ETA: 6s - loss: 0.1299 - acc: 0.962 - ETA: 6s - loss: 0.1287 - acc: 0.963 - ETA: 6s - loss: 0.1282 - acc: 0.963 - ETA: 5s - loss: 0.1289 - acc: 0.963 - ETA: 5s - loss: 0.1293 - acc: 0.962 - ETA: 5s - loss: 0.1307 - acc: 0.961 - ETA: 5s - loss: 0.1296 - acc: 0.962 - ETA: 5s - loss: 0.1350 - acc: 0.962 - ETA: 5s - loss: 0.1333 - acc: 0.962 - ETA: 5s - loss: 0.1331 - acc: 0.962 - ETA: 5s - loss: 0.1314 - acc: 0.963 - ETA: 5s - loss: 0.1300 - acc: 0.963 - ETA: 5s - loss: 0.1307 - acc: 0.962 - ETA: 5s - loss: 0.1351 - acc: 0.961 - ETA: 5s - loss: 0.1355 - acc: 0.960 - ETA: 5s - loss: 0.1373 - acc: 0.960 - ETA: 5s - loss: 0.1357 - acc: 0.961 - ETA: 5s - loss: 0.1396 - acc: 0.960 - ETA: 5s - loss: 0.1394 - acc: 0.959 - ETA: 5s - loss: 0.1394 - acc: 0.958 - ETA: 5s - loss: 0.1469 - acc: 0.957 - ETA: 4s - loss: 0.1456 - acc: 0.957 - ETA: 4s - loss: 0.1476 - acc: 0.956 - ETA: 4s - loss: 0.1490 - acc: 0.956 - ETA: 4s - loss: 0.1474 - acc: 0.956 - ETA: 4s - loss: 0.1471 - acc: 0.956 - ETA: 4s - loss: 0.1461 - acc: 0.956 - ETA: 4s - loss: 0.1470 - acc: 0.955 - ETA: 4s - loss: 0.1468 - acc: 0.955 - ETA: 4s - loss: 0.1456 - acc: 0.956 - ETA: 4s - loss: 0.1465 - acc: 0.955 - ETA: 4s - loss: 0.1459 - acc: 0.955 - ETA: 4s - loss: 0.1457 - acc: 0.955 - ETA: 4s - loss: 0.1471 - acc: 0.955 - ETA: 4s - loss: 0.1493 - acc: 0.954 - ETA: 4s - loss: 0.1483 - acc: 0.955 - ETA: 4s - loss: 0.1469 - acc: 0.955 - ETA: 4s - loss: 0.1513 - acc: 0.955 - ETA: 4s - loss: 0.1503 - acc: 0.955 - ETA: 4s - loss: 0.1539 - acc: 0.955 - ETA: 3s - loss: 0.1530 - acc: 0.956 - ETA: 3s - loss: 0.1533 - acc: 0.956 - ETA: 3s - loss: 0.1528 - acc: 0.956 - ETA: 3s - loss: 0.1527 - acc: 0.956 - ETA: 3s - loss: 0.1513 - acc: 0.956 - ETA: 3s - loss: 0.1520 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1512 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1504 - acc: 0.957 - ETA: 3s - loss: 0.1502 - acc: 0.956 - ETA: 3s - loss: 0.1507 - acc: 0.956 - ETA: 3s - loss: 0.1506 - acc: 0.956 - ETA: 3s - loss: 0.1495 - acc: 0.957 - ETA: 3s - loss: 0.1499 - acc: 0.956 - ETA: 3s - loss: 0.1514 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1511 - acc: 0.956 - ETA: 2s - loss: 0.1505 - acc: 0.956 - ETA: 2s - loss: 0.1507 - acc: 0.956 - ETA: 2s - loss: 0.1503 - acc: 0.956 - ETA: 2s - loss: 0.1509 - acc: 0.956 - ETA: 2s - loss: 0.1508 - acc: 0.956 - ETA: 2s - loss: 0.1511 - acc: 0.956 - ETA: 2s - loss: 0.1517 - acc: 0.955 - ETA: 2s - loss: 0.1509 - acc: 0.956 - ETA: 2s - loss: 0.1504 - acc: 0.956 - ETA: 2s - loss: 0.1501 - acc: 0.956 - ETA: 2s - loss: 0.1494 - acc: 0.956 - ETA: 2s - loss: 0.1485 - acc: 0.956 - ETA: 2s - loss: 0.1488 - acc: 0.956 - ETA: 2s - loss: 0.1496 - acc: 0.956 - ETA: 2s - loss: 0.1491 - acc: 0.956 - ETA: 2s - loss: 0.1487 - acc: 0.956 - ETA: 2s - loss: 0.1483 - acc: 0.956 - ETA: 2s - loss: 0.1480 - acc: 0.956 - ETA: 1s - loss: 0.1470 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1453 - acc: 0.957 - ETA: 1s - loss: 0.1452 - acc: 0.957 - ETA: 1s - loss: 0.1450 - acc: 0.957 - ETA: 1s - loss: 0.1453 - acc: 0.957 - ETA: 1s - loss: 0.1452 - acc: 0.957 - ETA: 1s - loss: 0.1446 - acc: 0.957 - ETA: 1s - loss: 0.1444 - acc: 0.957 - ETA: 1s - loss: 0.1438 - acc: 0.957 - ETA: 1s - loss: 0.1461 - acc: 0.956 - ETA: 1s - loss: 0.1461 - acc: 0.956 - ETA: 1s - loss: 0.1452 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1459 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1474 - acc: 0.955 - ETA: 1s - loss: 0.1467 - acc: 0.955 - ETA: 1s - loss: 0.1470 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1473 - acc: 0.955 - ETA: 0s - loss: 0.1478 - acc: 0.955 - ETA: 0s - loss: 0.1486 - acc: 0.954 - ETA: 0s - loss: 0.1484 - acc: 0.954 - ETA: 0s - loss: 0.1484 - acc: 0.954 - ETA: 0s - loss: 0.1479 - acc: 0.954 - ETA: 0s - loss: 0.1474 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1462 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1471 - acc: 0.955 - ETA: 0s - loss: 0.1470 - acc: 0.9554Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1467 - acc: 0.9555 - val_loss: 0.5868 - val_acc: 0.8563\n",
      "Epoch 11/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1681 - acc: 0.900 - ETA: 8s - loss: 0.1590 - acc: 0.950 - ETA: 8s - loss: 0.0993 - acc: 0.970 - ETA: 8s - loss: 0.0927 - acc: 0.964 - ETA: 8s - loss: 0.1060 - acc: 0.955 - ETA: 8s - loss: 0.1220 - acc: 0.954 - ETA: 8s - loss: 0.1073 - acc: 0.961 - ETA: 8s - loss: 0.0991 - acc: 0.966 - ETA: 8s - loss: 0.1053 - acc: 0.958 - ETA: 8s - loss: 0.1268 - acc: 0.955 - ETA: 8s - loss: 0.1202 - acc: 0.957 - ETA: 8s - loss: 0.1146 - acc: 0.960 - ETA: 8s - loss: 0.1118 - acc: 0.962 - ETA: 8s - loss: 0.1053 - acc: 0.964 - ETA: 8s - loss: 0.1033 - acc: 0.965 - ETA: 8s - loss: 0.1121 - acc: 0.962 - ETA: 8s - loss: 0.1149 - acc: 0.962 - ETA: 7s - loss: 0.1261 - acc: 0.958 - ETA: 7s - loss: 0.1317 - acc: 0.959 - ETA: 7s - loss: 0.1268 - acc: 0.961 - ETA: 7s - loss: 0.1253 - acc: 0.961 - ETA: 7s - loss: 0.1298 - acc: 0.958 - ETA: 7s - loss: 0.1294 - acc: 0.957 - ETA: 7s - loss: 0.1251 - acc: 0.959 - ETA: 7s - loss: 0.1235 - acc: 0.959 - ETA: 7s - loss: 0.1275 - acc: 0.958 - ETA: 7s - loss: 0.1237 - acc: 0.960 - ETA: 7s - loss: 0.1334 - acc: 0.959 - ETA: 7s - loss: 0.1327 - acc: 0.958 - ETA: 7s - loss: 0.1321 - acc: 0.959 - ETA: 7s - loss: 0.1315 - acc: 0.959 - ETA: 7s - loss: 0.1443 - acc: 0.958 - ETA: 7s - loss: 0.1421 - acc: 0.959 - ETA: 7s - loss: 0.1403 - acc: 0.959 - ETA: 7s - loss: 0.1378 - acc: 0.959 - ETA: 7s - loss: 0.1356 - acc: 0.959 - ETA: 6s - loss: 0.1361 - acc: 0.958 - ETA: 6s - loss: 0.1368 - acc: 0.958 - ETA: 6s - loss: 0.1347 - acc: 0.959 - ETA: 6s - loss: 0.1321 - acc: 0.960 - ETA: 6s - loss: 0.1291 - acc: 0.961 - ETA: 6s - loss: 0.1319 - acc: 0.959 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1282 - acc: 0.960 - ETA: 6s - loss: 0.1290 - acc: 0.959 - ETA: 6s - loss: 0.1297 - acc: 0.959 - ETA: 6s - loss: 0.1313 - acc: 0.958 - ETA: 6s - loss: 0.1315 - acc: 0.958 - ETA: 6s - loss: 0.1323 - acc: 0.958 - ETA: 6s - loss: 0.1298 - acc: 0.958 - ETA: 6s - loss: 0.1315 - acc: 0.959 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1283 - acc: 0.960 - ETA: 6s - loss: 0.1263 - acc: 0.961 - ETA: 6s - loss: 0.1271 - acc: 0.961 - ETA: 5s - loss: 0.1274 - acc: 0.961 - ETA: 5s - loss: 0.1256 - acc: 0.961 - ETA: 5s - loss: 0.1261 - acc: 0.961 - ETA: 5s - loss: 0.1261 - acc: 0.961 - ETA: 5s - loss: 0.1291 - acc: 0.960 - ETA: 5s - loss: 0.1292 - acc: 0.960 - ETA: 5s - loss: 0.1279 - acc: 0.960 - ETA: 5s - loss: 0.1291 - acc: 0.960 - ETA: 5s - loss: 0.1287 - acc: 0.960 - ETA: 5s - loss: 0.1275 - acc: 0.961 - ETA: 5s - loss: 0.1277 - acc: 0.960 - ETA: 5s - loss: 0.1313 - acc: 0.960 - ETA: 5s - loss: 0.1304 - acc: 0.960 - ETA: 5s - loss: 0.1307 - acc: 0.960 - ETA: 5s - loss: 0.1300 - acc: 0.960 - ETA: 5s - loss: 0.1309 - acc: 0.960 - ETA: 5s - loss: 0.1295 - acc: 0.960 - ETA: 5s - loss: 0.1312 - acc: 0.960 - ETA: 4s - loss: 0.1299 - acc: 0.960 - ETA: 4s - loss: 0.1294 - acc: 0.960 - ETA: 4s - loss: 0.1291 - acc: 0.960 - ETA: 4s - loss: 0.1309 - acc: 0.960 - ETA: 4s - loss: 0.1297 - acc: 0.960 - ETA: 4s - loss: 0.1305 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1328 - acc: 0.960 - ETA: 4s - loss: 0.1315 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1316 - acc: 0.960 - ETA: 4s - loss: 0.1317 - acc: 0.960 - ETA: 4s - loss: 0.1340 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1328 - acc: 0.960 - ETA: 4s - loss: 0.1339 - acc: 0.960 - ETA: 4s - loss: 0.1330 - acc: 0.960 - ETA: 4s - loss: 0.1334 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.960 - ETA: 3s - loss: 0.1321 - acc: 0.960 - ETA: 3s - loss: 0.1313 - acc: 0.960 - ETA: 3s - loss: 0.1302 - acc: 0.961 - ETA: 3s - loss: 0.1298 - acc: 0.960 - ETA: 3s - loss: 0.1299 - acc: 0.960 - ETA: 3s - loss: 0.1311 - acc: 0.960 - ETA: 3s - loss: 0.1306 - acc: 0.960 - ETA: 3s - loss: 0.1313 - acc: 0.960 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1326 - acc: 0.959 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1323 - acc: 0.959 - ETA: 3s - loss: 0.1315 - acc: 0.959 - ETA: 3s - loss: 0.1323 - acc: 0.959 - ETA: 3s - loss: 0.1316 - acc: 0.959 - ETA: 3s - loss: 0.1316 - acc: 0.959 - ETA: 3s - loss: 0.1329 - acc: 0.959 - ETA: 3s - loss: 0.1329 - acc: 0.959 - ETA: 2s - loss: 0.1330 - acc: 0.959 - ETA: 2s - loss: 0.1345 - acc: 0.959 - ETA: 2s - loss: 0.1341 - acc: 0.959 - ETA: 2s - loss: 0.1340 - acc: 0.959 - ETA: 2s - loss: 0.1350 - acc: 0.959 - ETA: 2s - loss: 0.1366 - acc: 0.958 - ETA: 2s - loss: 0.1362 - acc: 0.958 - ETA: 2s - loss: 0.1369 - acc: 0.958 - ETA: 2s - loss: 0.1367 - acc: 0.957 - ETA: 2s - loss: 0.1362 - acc: 0.958 - ETA: 2s - loss: 0.1371 - acc: 0.957 - ETA: 2s - loss: 0.1377 - acc: 0.957 - ETA: 2s - loss: 0.1377 - acc: 0.957 - ETA: 2s - loss: 0.1379 - acc: 0.957 - ETA: 2s - loss: 0.1372 - acc: 0.957 - ETA: 2s - loss: 0.1364 - acc: 0.957 - ETA: 2s - loss: 0.1375 - acc: 0.957 - ETA: 2s - loss: 0.1372 - acc: 0.958 - ETA: 2s - loss: 0.1366 - acc: 0.958 - ETA: 1s - loss: 0.1358 - acc: 0.958 - ETA: 1s - loss: 0.1350 - acc: 0.958 - ETA: 1s - loss: 0.1342 - acc: 0.958 - ETA: 1s - loss: 0.1335 - acc: 0.959 - ETA: 1s - loss: 0.1327 - acc: 0.959 - ETA: 1s - loss: 0.1321 - acc: 0.959 - ETA: 1s - loss: 0.1315 - acc: 0.959 - ETA: 1s - loss: 0.1318 - acc: 0.959 - ETA: 1s - loss: 0.1321 - acc: 0.959 - ETA: 1s - loss: 0.1336 - acc: 0.959 - ETA: 1s - loss: 0.1343 - acc: 0.958 - ETA: 1s - loss: 0.1343 - acc: 0.958 - ETA: 1s - loss: 0.1338 - acc: 0.958 - ETA: 1s - loss: 0.1333 - acc: 0.959 - ETA: 1s - loss: 0.1328 - acc: 0.959 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1329 - acc: 0.958 - ETA: 1s - loss: 0.1325 - acc: 0.958 - ETA: 1s - loss: 0.1318 - acc: 0.959 - ETA: 0s - loss: 0.1310 - acc: 0.959 - ETA: 0s - loss: 0.1323 - acc: 0.959 - ETA: 0s - loss: 0.1342 - acc: 0.959 - ETA: 0s - loss: 0.1342 - acc: 0.959 - ETA: 0s - loss: 0.1341 - acc: 0.959 - ETA: 0s - loss: 0.1346 - acc: 0.959 - ETA: 0s - loss: 0.1341 - acc: 0.959 - ETA: 0s - loss: 0.1346 - acc: 0.959 - ETA: 0s - loss: 0.1339 - acc: 0.959 - ETA: 0s - loss: 0.1350 - acc: 0.959 - ETA: 0s - loss: 0.1345 - acc: 0.959 - ETA: 0s - loss: 0.1362 - acc: 0.958 - ETA: 0s - loss: 0.1366 - acc: 0.958 - ETA: 0s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1374 - acc: 0.958 - ETA: 0s - loss: 0.1373 - acc: 0.958 - ETA: 0s - loss: 0.1368 - acc: 0.958 - ETA: 0s - loss: 0.1390 - acc: 0.9587Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1391 - acc: 0.9585 - val_loss: 0.5643 - val_acc: 0.8599\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.3356 - acc: 0.950 - ETA: 9s - loss: 0.1709 - acc: 0.966 - ETA: 9s - loss: 0.1071 - acc: 0.980 - ETA: 9s - loss: 0.0961 - acc: 0.978 - ETA: 8s - loss: 0.0794 - acc: 0.983 - ETA: 8s - loss: 0.0931 - acc: 0.981 - ETA: 8s - loss: 0.0909 - acc: 0.980 - ETA: 8s - loss: 0.0969 - acc: 0.976 - ETA: 8s - loss: 0.0993 - acc: 0.973 - ETA: 8s - loss: 0.1060 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.973 - ETA: 8s - loss: 0.1072 - acc: 0.973 - ETA: 8s - loss: 0.1095 - acc: 0.974 - ETA: 8s - loss: 0.1156 - acc: 0.972 - ETA: 8s - loss: 0.1101 - acc: 0.972 - ETA: 8s - loss: 0.1095 - acc: 0.971 - ETA: 8s - loss: 0.1042 - acc: 0.973 - ETA: 7s - loss: 0.1011 - acc: 0.973 - ETA: 7s - loss: 0.0984 - acc: 0.973 - ETA: 7s - loss: 0.0960 - acc: 0.973 - ETA: 7s - loss: 0.1033 - acc: 0.972 - ETA: 7s - loss: 0.0989 - acc: 0.973 - ETA: 7s - loss: 0.0980 - acc: 0.973 - ETA: 7s - loss: 0.0975 - acc: 0.974 - ETA: 7s - loss: 0.0954 - acc: 0.974 - ETA: 7s - loss: 0.0990 - acc: 0.971 - ETA: 7s - loss: 0.0976 - acc: 0.971 - ETA: 7s - loss: 0.0958 - acc: 0.971 - ETA: 7s - loss: 0.0967 - acc: 0.969 - ETA: 7s - loss: 0.0972 - acc: 0.969 - ETA: 7s - loss: 0.0997 - acc: 0.968 - ETA: 7s - loss: 0.1007 - acc: 0.968 - ETA: 7s - loss: 0.1021 - acc: 0.968 - ETA: 7s - loss: 0.1009 - acc: 0.968 - ETA: 7s - loss: 0.1120 - acc: 0.966 - ETA: 7s - loss: 0.1097 - acc: 0.967 - ETA: 7s - loss: 0.1088 - acc: 0.966 - ETA: 6s - loss: 0.1137 - acc: 0.965 - ETA: 6s - loss: 0.1137 - acc: 0.965 - ETA: 6s - loss: 0.1160 - acc: 0.965 - ETA: 6s - loss: 0.1181 - acc: 0.964 - ETA: 6s - loss: 0.1199 - acc: 0.964 - ETA: 6s - loss: 0.1190 - acc: 0.964 - ETA: 6s - loss: 0.1166 - acc: 0.965 - ETA: 6s - loss: 0.1147 - acc: 0.966 - ETA: 6s - loss: 0.1143 - acc: 0.965 - ETA: 6s - loss: 0.1170 - acc: 0.964 - ETA: 6s - loss: 0.1162 - acc: 0.964 - ETA: 6s - loss: 0.1142 - acc: 0.964 - ETA: 6s - loss: 0.1151 - acc: 0.964 - ETA: 6s - loss: 0.1178 - acc: 0.963 - ETA: 6s - loss: 0.1164 - acc: 0.963 - ETA: 6s - loss: 0.1184 - acc: 0.962 - ETA: 6s - loss: 0.1182 - acc: 0.963 - ETA: 6s - loss: 0.1185 - acc: 0.962 - ETA: 6s - loss: 0.1176 - acc: 0.962 - ETA: 5s - loss: 0.1173 - acc: 0.962 - ETA: 5s - loss: 0.1175 - acc: 0.962 - ETA: 5s - loss: 0.1180 - acc: 0.962 - ETA: 5s - loss: 0.1166 - acc: 0.963 - ETA: 5s - loss: 0.1195 - acc: 0.962 - ETA: 5s - loss: 0.1198 - acc: 0.962 - ETA: 5s - loss: 0.1252 - acc: 0.962 - ETA: 5s - loss: 0.1238 - acc: 0.962 - ETA: 5s - loss: 0.1224 - acc: 0.963 - ETA: 5s - loss: 0.1208 - acc: 0.963 - ETA: 5s - loss: 0.1208 - acc: 0.963 - ETA: 5s - loss: 0.1194 - acc: 0.964 - ETA: 5s - loss: 0.1188 - acc: 0.963 - ETA: 5s - loss: 0.1184 - acc: 0.963 - ETA: 5s - loss: 0.1172 - acc: 0.964 - ETA: 5s - loss: 0.1189 - acc: 0.964 - ETA: 5s - loss: 0.1192 - acc: 0.964 - ETA: 5s - loss: 0.1192 - acc: 0.964 - ETA: 4s - loss: 0.1184 - acc: 0.964 - ETA: 4s - loss: 0.1173 - acc: 0.964 - ETA: 4s - loss: 0.1192 - acc: 0.964 - ETA: 4s - loss: 0.1181 - acc: 0.965 - ETA: 4s - loss: 0.1170 - acc: 0.965 - ETA: 4s - loss: 0.1173 - acc: 0.965 - ETA: 4s - loss: 0.1164 - acc: 0.965 - ETA: 4s - loss: 0.1152 - acc: 0.965 - ETA: 4s - loss: 0.1161 - acc: 0.965 - ETA: 4s - loss: 0.1205 - acc: 0.965 - ETA: 4s - loss: 0.1194 - acc: 0.965 - ETA: 4s - loss: 0.1182 - acc: 0.966 - ETA: 4s - loss: 0.1180 - acc: 0.966 - ETA: 4s - loss: 0.1174 - acc: 0.966 - ETA: 4s - loss: 0.1183 - acc: 0.966 - ETA: 4s - loss: 0.1185 - acc: 0.966 - ETA: 4s - loss: 0.1176 - acc: 0.966 - ETA: 4s - loss: 0.1182 - acc: 0.966 - ETA: 4s - loss: 0.1187 - acc: 0.965 - ETA: 3s - loss: 0.1178 - acc: 0.965 - ETA: 3s - loss: 0.1177 - acc: 0.965 - ETA: 3s - loss: 0.1191 - acc: 0.965 - ETA: 3s - loss: 0.1189 - acc: 0.965 - ETA: 3s - loss: 0.1184 - acc: 0.965 - ETA: 3s - loss: 0.1197 - acc: 0.965 - ETA: 3s - loss: 0.1191 - acc: 0.965 - ETA: 3s - loss: 0.1190 - acc: 0.965 - ETA: 3s - loss: 0.1207 - acc: 0.965 - ETA: 3s - loss: 0.1197 - acc: 0.966 - ETA: 3s - loss: 0.1188 - acc: 0.966 - ETA: 3s - loss: 0.1183 - acc: 0.966 - ETA: 3s - loss: 0.1190 - acc: 0.966 - ETA: 3s - loss: 0.1188 - acc: 0.966 - ETA: 3s - loss: 0.1186 - acc: 0.966 - ETA: 3s - loss: 0.1179 - acc: 0.966 - ETA: 3s - loss: 0.1178 - acc: 0.966 - ETA: 3s - loss: 0.1182 - acc: 0.966 - ETA: 2s - loss: 0.1178 - acc: 0.965 - ETA: 2s - loss: 0.1173 - acc: 0.966 - ETA: 2s - loss: 0.1169 - acc: 0.966 - ETA: 2s - loss: 0.1177 - acc: 0.965 - ETA: 2s - loss: 0.1168 - acc: 0.966 - ETA: 2s - loss: 0.1177 - acc: 0.966 - ETA: 2s - loss: 0.1183 - acc: 0.965 - ETA: 2s - loss: 0.1186 - acc: 0.965 - ETA: 2s - loss: 0.1207 - acc: 0.965 - ETA: 2s - loss: 0.1217 - acc: 0.965 - ETA: 2s - loss: 0.1214 - acc: 0.965 - ETA: 2s - loss: 0.1218 - acc: 0.965 - ETA: 2s - loss: 0.1226 - acc: 0.964 - ETA: 2s - loss: 0.1254 - acc: 0.964 - ETA: 2s - loss: 0.1257 - acc: 0.964 - ETA: 2s - loss: 0.1248 - acc: 0.965 - ETA: 2s - loss: 0.1245 - acc: 0.964 - ETA: 2s - loss: 0.1236 - acc: 0.965 - ETA: 2s - loss: 0.1264 - acc: 0.964 - ETA: 1s - loss: 0.1255 - acc: 0.965 - ETA: 1s - loss: 0.1248 - acc: 0.965 - ETA: 1s - loss: 0.1244 - acc: 0.965 - ETA: 1s - loss: 0.1262 - acc: 0.964 - ETA: 1s - loss: 0.1256 - acc: 0.964 - ETA: 1s - loss: 0.1249 - acc: 0.964 - ETA: 1s - loss: 0.1252 - acc: 0.965 - ETA: 1s - loss: 0.1249 - acc: 0.965 - ETA: 1s - loss: 0.1252 - acc: 0.965 - ETA: 1s - loss: 0.1263 - acc: 0.965 - ETA: 1s - loss: 0.1266 - acc: 0.964 - ETA: 1s - loss: 0.1270 - acc: 0.964 - ETA: 1s - loss: 0.1276 - acc: 0.964 - ETA: 1s - loss: 0.1270 - acc: 0.964 - ETA: 1s - loss: 0.1265 - acc: 0.964 - ETA: 1s - loss: 0.1263 - acc: 0.964 - ETA: 1s - loss: 0.1256 - acc: 0.964 - ETA: 1s - loss: 0.1264 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1280 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1271 - acc: 0.964 - ETA: 0s - loss: 0.1273 - acc: 0.964 - ETA: 0s - loss: 0.1275 - acc: 0.964 - ETA: 0s - loss: 0.1273 - acc: 0.964 - ETA: 0s - loss: 0.1269 - acc: 0.964 - ETA: 0s - loss: 0.1268 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1278 - acc: 0.963 - ETA: 0s - loss: 0.1273 - acc: 0.963 - ETA: 0s - loss: 0.1278 - acc: 0.963 - ETA: 0s - loss: 0.1277 - acc: 0.963 - ETA: 0s - loss: 0.1277 - acc: 0.963 - ETA: 0s - loss: 0.1275 - acc: 0.963 - ETA: 0s - loss: 0.1283 - acc: 0.963 - ETA: 0s - loss: 0.1282 - acc: 0.9637Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1280 - acc: 0.9638 - val_loss: 0.6099 - val_acc: 0.8539\n",
      "Epoch 13/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0916 - acc: 0.950 - ETA: 8s - loss: 0.1493 - acc: 0.950 - ETA: 8s - loss: 0.1048 - acc: 0.970 - ETA: 8s - loss: 0.0780 - acc: 0.978 - ETA: 8s - loss: 0.1037 - acc: 0.977 - ETA: 8s - loss: 0.1021 - acc: 0.977 - ETA: 8s - loss: 0.1030 - acc: 0.969 - ETA: 8s - loss: 0.1076 - acc: 0.963 - ETA: 8s - loss: 0.1017 - acc: 0.964 - ETA: 8s - loss: 0.1327 - acc: 0.963 - ETA: 8s - loss: 0.1258 - acc: 0.964 - ETA: 8s - loss: 0.1312 - acc: 0.965 - ETA: 8s - loss: 0.1353 - acc: 0.962 - ETA: 8s - loss: 0.1284 - acc: 0.964 - ETA: 8s - loss: 0.1212 - acc: 0.967 - ETA: 8s - loss: 0.1147 - acc: 0.969 - ETA: 8s - loss: 0.1433 - acc: 0.965 - ETA: 8s - loss: 0.1441 - acc: 0.964 - ETA: 7s - loss: 0.1371 - acc: 0.966 - ETA: 7s - loss: 0.1360 - acc: 0.965 - ETA: 7s - loss: 0.1326 - acc: 0.964 - ETA: 7s - loss: 0.1325 - acc: 0.964 - ETA: 7s - loss: 0.1301 - acc: 0.964 - ETA: 7s - loss: 0.1280 - acc: 0.964 - ETA: 7s - loss: 0.1240 - acc: 0.965 - ETA: 7s - loss: 0.1217 - acc: 0.965 - ETA: 7s - loss: 0.1254 - acc: 0.964 - ETA: 7s - loss: 0.1221 - acc: 0.965 - ETA: 7s - loss: 0.1357 - acc: 0.964 - ETA: 7s - loss: 0.1349 - acc: 0.963 - ETA: 7s - loss: 0.1330 - acc: 0.963 - ETA: 7s - loss: 0.1303 - acc: 0.965 - ETA: 7s - loss: 0.1286 - acc: 0.965 - ETA: 7s - loss: 0.1273 - acc: 0.965 - ETA: 7s - loss: 0.1270 - acc: 0.964 - ETA: 7s - loss: 0.1241 - acc: 0.965 - ETA: 7s - loss: 0.1231 - acc: 0.965 - ETA: 6s - loss: 0.1223 - acc: 0.964 - ETA: 6s - loss: 0.1218 - acc: 0.964 - ETA: 6s - loss: 0.1202 - acc: 0.965 - ETA: 6s - loss: 0.1179 - acc: 0.966 - ETA: 6s - loss: 0.1231 - acc: 0.965 - ETA: 6s - loss: 0.1211 - acc: 0.966 - ETA: 6s - loss: 0.1204 - acc: 0.966 - ETA: 6s - loss: 0.1190 - acc: 0.966 - ETA: 6s - loss: 0.1235 - acc: 0.965 - ETA: 6s - loss: 0.1216 - acc: 0.965 - ETA: 6s - loss: 0.1197 - acc: 0.965 - ETA: 6s - loss: 0.1177 - acc: 0.966 - ETA: 6s - loss: 0.1182 - acc: 0.965 - ETA: 6s - loss: 0.1179 - acc: 0.964 - ETA: 6s - loss: 0.1170 - acc: 0.964 - ETA: 6s - loss: 0.1190 - acc: 0.963 - ETA: 6s - loss: 0.1185 - acc: 0.963 - ETA: 6s - loss: 0.1234 - acc: 0.962 - ETA: 6s - loss: 0.1227 - acc: 0.963 - ETA: 5s - loss: 0.1261 - acc: 0.962 - ETA: 5s - loss: 0.1255 - acc: 0.962 - ETA: 5s - loss: 0.1267 - acc: 0.962 - ETA: 5s - loss: 0.1339 - acc: 0.960 - ETA: 5s - loss: 0.1325 - acc: 0.961 - ETA: 5s - loss: 0.1344 - acc: 0.961 - ETA: 5s - loss: 0.1346 - acc: 0.960 - ETA: 5s - loss: 0.1341 - acc: 0.961 - ETA: 5s - loss: 0.1334 - acc: 0.960 - ETA: 5s - loss: 0.1316 - acc: 0.961 - ETA: 5s - loss: 0.1323 - acc: 0.961 - ETA: 5s - loss: 0.1311 - acc: 0.961 - ETA: 5s - loss: 0.1296 - acc: 0.962 - ETA: 5s - loss: 0.1297 - acc: 0.961 - ETA: 5s - loss: 0.1289 - acc: 0.961 - ETA: 5s - loss: 0.1322 - acc: 0.961 - ETA: 5s - loss: 0.1345 - acc: 0.960 - ETA: 5s - loss: 0.1331 - acc: 0.960 - ETA: 4s - loss: 0.1334 - acc: 0.961 - ETA: 4s - loss: 0.1332 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.960 - ETA: 4s - loss: 0.1309 - acc: 0.960 - ETA: 4s - loss: 0.1314 - acc: 0.960 - ETA: 4s - loss: 0.1304 - acc: 0.960 - ETA: 4s - loss: 0.1297 - acc: 0.960 - ETA: 4s - loss: 0.1305 - acc: 0.960 - ETA: 4s - loss: 0.1317 - acc: 0.960 - ETA: 4s - loss: 0.1307 - acc: 0.960 - ETA: 4s - loss: 0.1302 - acc: 0.960 - ETA: 4s - loss: 0.1291 - acc: 0.960 - ETA: 4s - loss: 0.1282 - acc: 0.961 - ETA: 4s - loss: 0.1283 - acc: 0.960 - ETA: 4s - loss: 0.1292 - acc: 0.960 - ETA: 4s - loss: 0.1280 - acc: 0.961 - ETA: 4s - loss: 0.1278 - acc: 0.961 - ETA: 4s - loss: 0.1293 - acc: 0.960 - ETA: 4s - loss: 0.1290 - acc: 0.960 - ETA: 3s - loss: 0.1298 - acc: 0.960 - ETA: 3s - loss: 0.1285 - acc: 0.960 - ETA: 3s - loss: 0.1280 - acc: 0.961 - ETA: 3s - loss: 0.1271 - acc: 0.961 - ETA: 3s - loss: 0.1266 - acc: 0.961 - ETA: 3s - loss: 0.1257 - acc: 0.961 - ETA: 3s - loss: 0.1254 - acc: 0.961 - ETA: 3s - loss: 0.1249 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1235 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1236 - acc: 0.960 - ETA: 3s - loss: 0.1240 - acc: 0.960 - ETA: 3s - loss: 0.1234 - acc: 0.960 - ETA: 3s - loss: 0.1223 - acc: 0.961 - ETA: 3s - loss: 0.1216 - acc: 0.961 - ETA: 3s - loss: 0.1220 - acc: 0.961 - ETA: 2s - loss: 0.1235 - acc: 0.961 - ETA: 2s - loss: 0.1227 - acc: 0.962 - ETA: 2s - loss: 0.1227 - acc: 0.961 - ETA: 2s - loss: 0.1233 - acc: 0.961 - ETA: 2s - loss: 0.1230 - acc: 0.961 - ETA: 2s - loss: 0.1221 - acc: 0.962 - ETA: 2s - loss: 0.1214 - acc: 0.962 - ETA: 2s - loss: 0.1205 - acc: 0.962 - ETA: 2s - loss: 0.1203 - acc: 0.962 - ETA: 2s - loss: 0.1207 - acc: 0.962 - ETA: 2s - loss: 0.1208 - acc: 0.962 - ETA: 2s - loss: 0.1201 - acc: 0.962 - ETA: 2s - loss: 0.1194 - acc: 0.962 - ETA: 2s - loss: 0.1204 - acc: 0.962 - ETA: 2s - loss: 0.1198 - acc: 0.962 - ETA: 2s - loss: 0.1198 - acc: 0.962 - ETA: 2s - loss: 0.1213 - acc: 0.962 - ETA: 2s - loss: 0.1219 - acc: 0.962 - ETA: 2s - loss: 0.1227 - acc: 0.962 - ETA: 1s - loss: 0.1225 - acc: 0.962 - ETA: 1s - loss: 0.1235 - acc: 0.962 - ETA: 1s - loss: 0.1255 - acc: 0.961 - ETA: 1s - loss: 0.1267 - acc: 0.961 - ETA: 1s - loss: 0.1264 - acc: 0.961 - ETA: 1s - loss: 0.1274 - acc: 0.960 - ETA: 1s - loss: 0.1266 - acc: 0.961 - ETA: 1s - loss: 0.1258 - acc: 0.961 - ETA: 1s - loss: 0.1257 - acc: 0.961 - ETA: 1s - loss: 0.1249 - acc: 0.961 - ETA: 1s - loss: 0.1245 - acc: 0.961 - ETA: 1s - loss: 0.1244 - acc: 0.961 - ETA: 1s - loss: 0.1247 - acc: 0.961 - ETA: 1s - loss: 0.1242 - acc: 0.961 - ETA: 1s - loss: 0.1236 - acc: 0.961 - ETA: 1s - loss: 0.1231 - acc: 0.961 - ETA: 1s - loss: 0.1228 - acc: 0.961 - ETA: 1s - loss: 0.1221 - acc: 0.961 - ETA: 0s - loss: 0.1220 - acc: 0.962 - ETA: 0s - loss: 0.1217 - acc: 0.962 - ETA: 0s - loss: 0.1213 - acc: 0.962 - ETA: 0s - loss: 0.1220 - acc: 0.962 - ETA: 0s - loss: 0.1218 - acc: 0.962 - ETA: 0s - loss: 0.1212 - acc: 0.962 - ETA: 0s - loss: 0.1207 - acc: 0.962 - ETA: 0s - loss: 0.1203 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1194 - acc: 0.962 - ETA: 0s - loss: 0.1198 - acc: 0.962 - ETA: 0s - loss: 0.1198 - acc: 0.962 - ETA: 0s - loss: 0.1193 - acc: 0.962 - ETA: 0s - loss: 0.1195 - acc: 0.962 - ETA: 0s - loss: 0.1189 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1192 - acc: 0.962 - ETA: 0s - loss: 0.1187 - acc: 0.9628Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1185 - acc: 0.9629 - val_loss: 0.6032 - val_acc: 0.8467\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0441 - acc: 1.000 - ETA: 9s - loss: 0.0349 - acc: 1.000 - ETA: 9s - loss: 0.0720 - acc: 0.970 - ETA: 9s - loss: 0.0633 - acc: 0.971 - ETA: 9s - loss: 0.1019 - acc: 0.961 - ETA: 9s - loss: 0.1228 - acc: 0.950 - ETA: 9s - loss: 0.1226 - acc: 0.953 - ETA: 8s - loss: 0.1694 - acc: 0.950 - ETA: 8s - loss: 0.1504 - acc: 0.955 - ETA: 8s - loss: 0.1389 - acc: 0.960 - ETA: 8s - loss: 0.1395 - acc: 0.959 - ETA: 8s - loss: 0.1490 - acc: 0.960 - ETA: 8s - loss: 0.1471 - acc: 0.960 - ETA: 8s - loss: 0.1384 - acc: 0.963 - ETA: 8s - loss: 0.1406 - acc: 0.962 - ETA: 8s - loss: 0.1324 - acc: 0.964 - ETA: 8s - loss: 0.1267 - acc: 0.965 - ETA: 8s - loss: 0.1289 - acc: 0.964 - ETA: 8s - loss: 0.1257 - acc: 0.964 - ETA: 8s - loss: 0.1306 - acc: 0.965 - ETA: 8s - loss: 0.1253 - acc: 0.967 - ETA: 8s - loss: 0.1215 - acc: 0.967 - ETA: 7s - loss: 0.1175 - acc: 0.968 - ETA: 7s - loss: 0.1151 - acc: 0.969 - ETA: 7s - loss: 0.1128 - acc: 0.969 - ETA: 7s - loss: 0.1093 - acc: 0.970 - ETA: 7s - loss: 0.1106 - acc: 0.969 - ETA: 7s - loss: 0.1072 - acc: 0.970 - ETA: 7s - loss: 0.1091 - acc: 0.971 - ETA: 7s - loss: 0.1061 - acc: 0.972 - ETA: 7s - loss: 0.1047 - acc: 0.972 - ETA: 7s - loss: 0.1030 - acc: 0.972 - ETA: 7s - loss: 0.1012 - acc: 0.973 - ETA: 7s - loss: 0.1040 - acc: 0.972 - ETA: 7s - loss: 0.1011 - acc: 0.973 - ETA: 7s - loss: 0.0993 - acc: 0.973 - ETA: 7s - loss: 0.0985 - acc: 0.973 - ETA: 7s - loss: 0.0973 - acc: 0.973 - ETA: 7s - loss: 0.0952 - acc: 0.974 - ETA: 6s - loss: 0.0957 - acc: 0.972 - ETA: 6s - loss: 0.0943 - acc: 0.973 - ETA: 6s - loss: 0.0929 - acc: 0.974 - ETA: 6s - loss: 0.0922 - acc: 0.974 - ETA: 6s - loss: 0.0902 - acc: 0.974 - ETA: 6s - loss: 0.0901 - acc: 0.973 - ETA: 6s - loss: 0.0893 - acc: 0.973 - ETA: 6s - loss: 0.0902 - acc: 0.973 - ETA: 6s - loss: 0.0887 - acc: 0.973 - ETA: 6s - loss: 0.0878 - acc: 0.973 - ETA: 6s - loss: 0.0885 - acc: 0.973 - ETA: 6s - loss: 0.0873 - acc: 0.973 - ETA: 6s - loss: 0.0860 - acc: 0.973 - ETA: 6s - loss: 0.0851 - acc: 0.974 - ETA: 6s - loss: 0.0907 - acc: 0.973 - ETA: 6s - loss: 0.0904 - acc: 0.972 - ETA: 6s - loss: 0.0966 - acc: 0.971 - ETA: 5s - loss: 0.0961 - acc: 0.971 - ETA: 5s - loss: 0.0996 - acc: 0.971 - ETA: 5s - loss: 0.0987 - acc: 0.971 - ETA: 5s - loss: 0.0989 - acc: 0.971 - ETA: 5s - loss: 0.0990 - acc: 0.970 - ETA: 5s - loss: 0.0986 - acc: 0.970 - ETA: 5s - loss: 0.0975 - acc: 0.970 - ETA: 5s - loss: 0.0980 - acc: 0.970 - ETA: 5s - loss: 0.0968 - acc: 0.971 - ETA: 5s - loss: 0.0971 - acc: 0.970 - ETA: 5s - loss: 0.0961 - acc: 0.970 - ETA: 5s - loss: 0.0965 - acc: 0.970 - ETA: 5s - loss: 0.0954 - acc: 0.971 - ETA: 5s - loss: 0.0941 - acc: 0.971 - ETA: 5s - loss: 0.0953 - acc: 0.971 - ETA: 5s - loss: 0.0958 - acc: 0.971 - ETA: 5s - loss: 0.0952 - acc: 0.971 - ETA: 5s - loss: 0.0947 - acc: 0.971 - ETA: 5s - loss: 0.0949 - acc: 0.970 - ETA: 4s - loss: 0.0943 - acc: 0.970 - ETA: 4s - loss: 0.0937 - acc: 0.970 - ETA: 4s - loss: 0.0954 - acc: 0.970 - ETA: 4s - loss: 0.0943 - acc: 0.970 - ETA: 4s - loss: 0.0956 - acc: 0.970 - ETA: 4s - loss: 0.0975 - acc: 0.970 - ETA: 4s - loss: 0.0970 - acc: 0.970 - ETA: 4s - loss: 0.0964 - acc: 0.970 - ETA: 4s - loss: 0.0953 - acc: 0.970 - ETA: 4s - loss: 0.0949 - acc: 0.970 - ETA: 4s - loss: 0.0956 - acc: 0.970 - ETA: 4s - loss: 0.0946 - acc: 0.970 - ETA: 4s - loss: 0.0938 - acc: 0.970 - ETA: 4s - loss: 0.0938 - acc: 0.970 - ETA: 4s - loss: 0.0932 - acc: 0.970 - ETA: 4s - loss: 0.0933 - acc: 0.970 - ETA: 4s - loss: 0.0948 - acc: 0.970 - ETA: 4s - loss: 0.0967 - acc: 0.970 - ETA: 3s - loss: 0.0960 - acc: 0.970 - ETA: 3s - loss: 0.0953 - acc: 0.970 - ETA: 3s - loss: 0.0972 - acc: 0.970 - ETA: 3s - loss: 0.0969 - acc: 0.970 - ETA: 3s - loss: 0.0963 - acc: 0.970 - ETA: 3s - loss: 0.0959 - acc: 0.970 - ETA: 3s - loss: 0.0978 - acc: 0.969 - ETA: 3s - loss: 0.0982 - acc: 0.969 - ETA: 3s - loss: 0.0980 - acc: 0.969 - ETA: 3s - loss: 0.1007 - acc: 0.969 - ETA: 3s - loss: 0.1011 - acc: 0.969 - ETA: 3s - loss: 0.1011 - acc: 0.969 - ETA: 3s - loss: 0.1031 - acc: 0.969 - ETA: 3s - loss: 0.1023 - acc: 0.969 - ETA: 3s - loss: 0.1030 - acc: 0.969 - ETA: 3s - loss: 0.1034 - acc: 0.969 - ETA: 3s - loss: 0.1025 - acc: 0.969 - ETA: 3s - loss: 0.1036 - acc: 0.969 - ETA: 2s - loss: 0.1032 - acc: 0.969 - ETA: 2s - loss: 0.1037 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.968 - ETA: 2s - loss: 0.1038 - acc: 0.968 - ETA: 2s - loss: 0.1047 - acc: 0.968 - ETA: 2s - loss: 0.1046 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.967 - ETA: 2s - loss: 0.1047 - acc: 0.967 - ETA: 2s - loss: 0.1044 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1040 - acc: 0.968 - ETA: 2s - loss: 0.1063 - acc: 0.967 - ETA: 2s - loss: 0.1080 - acc: 0.967 - ETA: 2s - loss: 0.1085 - acc: 0.966 - ETA: 2s - loss: 0.1094 - acc: 0.966 - ETA: 2s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1105 - acc: 0.966 - ETA: 1s - loss: 0.1101 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1098 - acc: 0.966 - ETA: 1s - loss: 0.1095 - acc: 0.966 - ETA: 1s - loss: 0.1096 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1106 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1091 - acc: 0.967 - ETA: 1s - loss: 0.1085 - acc: 0.967 - ETA: 1s - loss: 0.1079 - acc: 0.967 - ETA: 0s - loss: 0.1078 - acc: 0.967 - ETA: 0s - loss: 0.1085 - acc: 0.967 - ETA: 0s - loss: 0.1082 - acc: 0.967 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1079 - acc: 0.966 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1081 - acc: 0.966 - ETA: 0s - loss: 0.1082 - acc: 0.966 - ETA: 0s - loss: 0.1077 - acc: 0.966 - ETA: 0s - loss: 0.1071 - acc: 0.967 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1080 - acc: 0.966 - ETA: 0s - loss: 0.1076 - acc: 0.966 - ETA: 0s - loss: 0.1070 - acc: 0.967 - ETA: 0s - loss: 0.1066 - acc: 0.967 - ETA: 0s - loss: 0.1061 - acc: 0.967 - ETA: 0s - loss: 0.1085 - acc: 0.967 - ETA: 0s - loss: 0.1079 - acc: 0.967 - ETA: 0s - loss: 0.1076 - acc: 0.9674Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1088 - acc: 0.9674 - val_loss: 0.6280 - val_acc: 0.8611\n",
      "Epoch 15/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.2747 - acc: 0.950 - ETA: 9s - loss: 0.2058 - acc: 0.966 - ETA: 9s - loss: 0.1702 - acc: 0.970 - ETA: 9s - loss: 0.1245 - acc: 0.978 - ETA: 9s - loss: 0.1100 - acc: 0.977 - ETA: 9s - loss: 0.0962 - acc: 0.977 - ETA: 9s - loss: 0.0863 - acc: 0.976 - ETA: 9s - loss: 0.0816 - acc: 0.976 - ETA: 8s - loss: 0.0739 - acc: 0.979 - ETA: 8s - loss: 0.0823 - acc: 0.976 - ETA: 8s - loss: 0.0939 - acc: 0.976 - ETA: 8s - loss: 0.0882 - acc: 0.978 - ETA: 8s - loss: 0.0819 - acc: 0.980 - ETA: 8s - loss: 0.0767 - acc: 0.981 - ETA: 8s - loss: 0.0740 - acc: 0.981 - ETA: 8s - loss: 0.0741 - acc: 0.980 - ETA: 8s - loss: 0.0730 - acc: 0.980 - ETA: 8s - loss: 0.0718 - acc: 0.980 - ETA: 8s - loss: 0.0715 - acc: 0.979 - ETA: 8s - loss: 0.0721 - acc: 0.978 - ETA: 7s - loss: 0.0743 - acc: 0.976 - ETA: 7s - loss: 0.0848 - acc: 0.974 - ETA: 7s - loss: 0.0817 - acc: 0.975 - ETA: 7s - loss: 0.0841 - acc: 0.975 - ETA: 7s - loss: 0.0918 - acc: 0.973 - ETA: 7s - loss: 0.0897 - acc: 0.973 - ETA: 7s - loss: 0.0915 - acc: 0.973 - ETA: 7s - loss: 0.0923 - acc: 0.972 - ETA: 7s - loss: 0.0911 - acc: 0.972 - ETA: 7s - loss: 0.0901 - acc: 0.972 - ETA: 7s - loss: 0.0919 - acc: 0.971 - ETA: 7s - loss: 0.1017 - acc: 0.969 - ETA: 7s - loss: 0.0998 - acc: 0.969 - ETA: 7s - loss: 0.0989 - acc: 0.969 - ETA: 7s - loss: 0.0988 - acc: 0.968 - ETA: 7s - loss: 0.0985 - acc: 0.968 - ETA: 7s - loss: 0.0979 - acc: 0.968 - ETA: 6s - loss: 0.0964 - acc: 0.968 - ETA: 6s - loss: 0.0999 - acc: 0.968 - ETA: 6s - loss: 0.0980 - acc: 0.969 - ETA: 6s - loss: 0.0965 - acc: 0.969 - ETA: 6s - loss: 0.1002 - acc: 0.968 - ETA: 6s - loss: 0.0980 - acc: 0.968 - ETA: 6s - loss: 0.0959 - acc: 0.969 - ETA: 6s - loss: 0.0959 - acc: 0.969 - ETA: 6s - loss: 0.1049 - acc: 0.968 - ETA: 6s - loss: 0.1046 - acc: 0.968 - ETA: 6s - loss: 0.1048 - acc: 0.967 - ETA: 6s - loss: 0.1049 - acc: 0.968 - ETA: 6s - loss: 0.1064 - acc: 0.967 - ETA: 6s - loss: 0.1045 - acc: 0.968 - ETA: 6s - loss: 0.1047 - acc: 0.968 - ETA: 6s - loss: 0.1030 - acc: 0.968 - ETA: 6s - loss: 0.1043 - acc: 0.968 - ETA: 6s - loss: 0.1041 - acc: 0.967 - ETA: 6s - loss: 0.1029 - acc: 0.968 - ETA: 5s - loss: 0.1017 - acc: 0.968 - ETA: 5s - loss: 0.1002 - acc: 0.968 - ETA: 5s - loss: 0.1010 - acc: 0.968 - ETA: 5s - loss: 0.1000 - acc: 0.968 - ETA: 5s - loss: 0.0986 - acc: 0.969 - ETA: 5s - loss: 0.0972 - acc: 0.969 - ETA: 5s - loss: 0.0995 - acc: 0.969 - ETA: 5s - loss: 0.0995 - acc: 0.968 - ETA: 5s - loss: 0.0986 - acc: 0.969 - ETA: 5s - loss: 0.0982 - acc: 0.969 - ETA: 5s - loss: 0.0971 - acc: 0.969 - ETA: 5s - loss: 0.0958 - acc: 0.970 - ETA: 5s - loss: 0.0982 - acc: 0.969 - ETA: 5s - loss: 0.0980 - acc: 0.969 - ETA: 5s - loss: 0.0997 - acc: 0.968 - ETA: 5s - loss: 0.0994 - acc: 0.968 - ETA: 5s - loss: 0.1005 - acc: 0.967 - ETA: 5s - loss: 0.1000 - acc: 0.967 - ETA: 4s - loss: 0.0991 - acc: 0.967 - ETA: 4s - loss: 0.0987 - acc: 0.967 - ETA: 4s - loss: 0.1005 - acc: 0.966 - ETA: 4s - loss: 0.0993 - acc: 0.967 - ETA: 4s - loss: 0.0987 - acc: 0.967 - ETA: 4s - loss: 0.0999 - acc: 0.967 - ETA: 4s - loss: 0.0994 - acc: 0.967 - ETA: 4s - loss: 0.1008 - acc: 0.966 - ETA: 4s - loss: 0.0997 - acc: 0.966 - ETA: 4s - loss: 0.0990 - acc: 0.966 - ETA: 4s - loss: 0.0980 - acc: 0.967 - ETA: 4s - loss: 0.0980 - acc: 0.967 - ETA: 4s - loss: 0.0971 - acc: 0.967 - ETA: 4s - loss: 0.0964 - acc: 0.967 - ETA: 4s - loss: 0.0956 - acc: 0.967 - ETA: 4s - loss: 0.0953 - acc: 0.967 - ETA: 4s - loss: 0.0944 - acc: 0.968 - ETA: 4s - loss: 0.0968 - acc: 0.968 - ETA: 4s - loss: 0.0979 - acc: 0.967 - ETA: 3s - loss: 0.0976 - acc: 0.967 - ETA: 3s - loss: 0.0968 - acc: 0.968 - ETA: 3s - loss: 0.0959 - acc: 0.968 - ETA: 3s - loss: 0.0952 - acc: 0.968 - ETA: 3s - loss: 0.0944 - acc: 0.969 - ETA: 3s - loss: 0.0949 - acc: 0.968 - ETA: 3s - loss: 0.0941 - acc: 0.969 - ETA: 3s - loss: 0.0941 - acc: 0.968 - ETA: 3s - loss: 0.0934 - acc: 0.969 - ETA: 3s - loss: 0.0928 - acc: 0.969 - ETA: 3s - loss: 0.0928 - acc: 0.969 - ETA: 3s - loss: 0.0924 - acc: 0.969 - ETA: 3s - loss: 0.0921 - acc: 0.969 - ETA: 3s - loss: 0.0922 - acc: 0.969 - ETA: 3s - loss: 0.0933 - acc: 0.969 - ETA: 3s - loss: 0.0929 - acc: 0.969 - ETA: 3s - loss: 0.0948 - acc: 0.968 - ETA: 3s - loss: 0.0946 - acc: 0.968 - ETA: 2s - loss: 0.0938 - acc: 0.968 - ETA: 2s - loss: 0.0937 - acc: 0.968 - ETA: 2s - loss: 0.0935 - acc: 0.968 - ETA: 2s - loss: 0.0962 - acc: 0.968 - ETA: 2s - loss: 0.0964 - acc: 0.968 - ETA: 2s - loss: 0.0969 - acc: 0.968 - ETA: 2s - loss: 0.0984 - acc: 0.967 - ETA: 2s - loss: 0.0982 - acc: 0.967 - ETA: 2s - loss: 0.0978 - acc: 0.967 - ETA: 2s - loss: 0.0971 - acc: 0.968 - ETA: 2s - loss: 0.0973 - acc: 0.968 - ETA: 2s - loss: 0.0981 - acc: 0.967 - ETA: 2s - loss: 0.0975 - acc: 0.968 - ETA: 2s - loss: 0.0977 - acc: 0.967 - ETA: 2s - loss: 0.0970 - acc: 0.967 - ETA: 2s - loss: 0.0973 - acc: 0.968 - ETA: 2s - loss: 0.0967 - acc: 0.968 - ETA: 2s - loss: 0.0980 - acc: 0.968 - ETA: 2s - loss: 0.0978 - acc: 0.968 - ETA: 1s - loss: 0.0976 - acc: 0.968 - ETA: 1s - loss: 0.0975 - acc: 0.968 - ETA: 1s - loss: 0.0968 - acc: 0.968 - ETA: 1s - loss: 0.0963 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0961 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0970 - acc: 0.968 - ETA: 1s - loss: 0.0977 - acc: 0.968 - ETA: 1s - loss: 0.0973 - acc: 0.968 - ETA: 1s - loss: 0.0969 - acc: 0.968 - ETA: 1s - loss: 0.0965 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 1s - loss: 0.0956 - acc: 0.968 - ETA: 1s - loss: 0.0966 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 0s - loss: 0.0971 - acc: 0.968 - ETA: 0s - loss: 0.0982 - acc: 0.968 - ETA: 0s - loss: 0.0991 - acc: 0.968 - ETA: 0s - loss: 0.0995 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1011 - acc: 0.967 - ETA: 0s - loss: 0.1009 - acc: 0.967 - ETA: 0s - loss: 0.1004 - acc: 0.967 - ETA: 0s - loss: 0.1021 - acc: 0.967 - ETA: 0s - loss: 0.1016 - acc: 0.967 - ETA: 0s - loss: 0.1012 - acc: 0.967 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1006 - acc: 0.968 - ETA: 0s - loss: 0.1004 - acc: 0.968 - ETA: 0s - loss: 0.1000 - acc: 0.968 - ETA: 0s - loss: 0.1006 - acc: 0.968 - ETA: 0s - loss: 0.1002 - acc: 0.9685Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1001 - acc: 0.9684 - val_loss: 0.6567 - val_acc: 0.8467\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0088 - acc: 1.000 - ETA: 10s - loss: 0.2076 - acc: 0.95 - ETA: 10s - loss: 0.2090 - acc: 0.96 - ETA: 10s - loss: 0.1576 - acc: 0.96 - ETA: 10s - loss: 0.1258 - acc: 0.97 - ETA: 9s - loss: 0.1247 - acc: 0.9727 - ETA: 9s - loss: 0.1289 - acc: 0.969 - ETA: 9s - loss: 0.1225 - acc: 0.966 - ETA: 9s - loss: 0.1084 - acc: 0.970 - ETA: 9s - loss: 0.0996 - acc: 0.973 - ETA: 9s - loss: 0.0915 - acc: 0.976 - ETA: 9s - loss: 0.0881 - acc: 0.978 - ETA: 8s - loss: 0.0825 - acc: 0.980 - ETA: 8s - loss: 0.0785 - acc: 0.981 - ETA: 8s - loss: 0.0834 - acc: 0.981 - ETA: 8s - loss: 0.0881 - acc: 0.980 - ETA: 8s - loss: 0.0833 - acc: 0.981 - ETA: 8s - loss: 0.0921 - acc: 0.981 - ETA: 8s - loss: 0.0885 - acc: 0.982 - ETA: 8s - loss: 0.0866 - acc: 0.982 - ETA: 8s - loss: 0.0931 - acc: 0.980 - ETA: 8s - loss: 0.0897 - acc: 0.981 - ETA: 8s - loss: 0.0946 - acc: 0.980 - ETA: 8s - loss: 0.0909 - acc: 0.980 - ETA: 8s - loss: 0.0892 - acc: 0.980 - ETA: 8s - loss: 0.0890 - acc: 0.980 - ETA: 8s - loss: 0.0869 - acc: 0.981 - ETA: 8s - loss: 0.0996 - acc: 0.980 - ETA: 7s - loss: 0.0966 - acc: 0.980 - ETA: 7s - loss: 0.0969 - acc: 0.980 - ETA: 7s - loss: 0.1020 - acc: 0.978 - ETA: 7s - loss: 0.1021 - acc: 0.977 - ETA: 7s - loss: 0.1062 - acc: 0.976 - ETA: 7s - loss: 0.1035 - acc: 0.976 - ETA: 7s - loss: 0.1066 - acc: 0.976 - ETA: 7s - loss: 0.1044 - acc: 0.976 - ETA: 7s - loss: 0.1036 - acc: 0.976 - ETA: 7s - loss: 0.1041 - acc: 0.976 - ETA: 7s - loss: 0.1015 - acc: 0.977 - ETA: 7s - loss: 0.1000 - acc: 0.977 - ETA: 7s - loss: 0.0979 - acc: 0.977 - ETA: 7s - loss: 0.0959 - acc: 0.978 - ETA: 7s - loss: 0.0961 - acc: 0.978 - ETA: 7s - loss: 0.0953 - acc: 0.977 - ETA: 6s - loss: 0.0955 - acc: 0.977 - ETA: 6s - loss: 0.0961 - acc: 0.976 - ETA: 6s - loss: 0.0943 - acc: 0.976 - ETA: 6s - loss: 0.0949 - acc: 0.976 - ETA: 6s - loss: 0.0958 - acc: 0.975 - ETA: 6s - loss: 0.0945 - acc: 0.976 - ETA: 6s - loss: 0.0930 - acc: 0.976 - ETA: 6s - loss: 0.0921 - acc: 0.977 - ETA: 6s - loss: 0.0928 - acc: 0.977 - ETA: 6s - loss: 0.0959 - acc: 0.976 - ETA: 6s - loss: 0.0943 - acc: 0.976 - ETA: 6s - loss: 0.0945 - acc: 0.976 - ETA: 6s - loss: 0.0955 - acc: 0.975 - ETA: 6s - loss: 0.0942 - acc: 0.975 - ETA: 6s - loss: 0.0936 - acc: 0.975 - ETA: 6s - loss: 0.0936 - acc: 0.975 - ETA: 5s - loss: 0.0924 - acc: 0.975 - ETA: 5s - loss: 0.0912 - acc: 0.976 - ETA: 5s - loss: 0.0906 - acc: 0.975 - ETA: 5s - loss: 0.0897 - acc: 0.975 - ETA: 5s - loss: 0.0910 - acc: 0.975 - ETA: 5s - loss: 0.0902 - acc: 0.975 - ETA: 5s - loss: 0.0903 - acc: 0.974 - ETA: 5s - loss: 0.0911 - acc: 0.974 - ETA: 5s - loss: 0.0915 - acc: 0.974 - ETA: 5s - loss: 0.0910 - acc: 0.974 - ETA: 5s - loss: 0.0898 - acc: 0.974 - ETA: 5s - loss: 0.0891 - acc: 0.974 - ETA: 5s - loss: 0.0887 - acc: 0.974 - ETA: 5s - loss: 0.0875 - acc: 0.975 - ETA: 5s - loss: 0.0865 - acc: 0.975 - ETA: 5s - loss: 0.0866 - acc: 0.975 - ETA: 5s - loss: 0.0860 - acc: 0.975 - ETA: 4s - loss: 0.0859 - acc: 0.975 - ETA: 4s - loss: 0.0854 - acc: 0.975 - ETA: 4s - loss: 0.0855 - acc: 0.975 - ETA: 4s - loss: 0.0864 - acc: 0.975 - ETA: 4s - loss: 0.0857 - acc: 0.975 - ETA: 4s - loss: 0.0871 - acc: 0.975 - ETA: 4s - loss: 0.0865 - acc: 0.975 - ETA: 4s - loss: 0.0861 - acc: 0.975 - ETA: 4s - loss: 0.0856 - acc: 0.975 - ETA: 4s - loss: 0.0850 - acc: 0.975 - ETA: 4s - loss: 0.0843 - acc: 0.975 - ETA: 4s - loss: 0.0834 - acc: 0.975 - ETA: 4s - loss: 0.0827 - acc: 0.976 - ETA: 4s - loss: 0.0853 - acc: 0.975 - ETA: 4s - loss: 0.0845 - acc: 0.975 - ETA: 4s - loss: 0.0837 - acc: 0.975 - ETA: 4s - loss: 0.0852 - acc: 0.975 - ETA: 4s - loss: 0.0864 - acc: 0.974 - ETA: 3s - loss: 0.0870 - acc: 0.974 - ETA: 3s - loss: 0.0874 - acc: 0.974 - ETA: 3s - loss: 0.0887 - acc: 0.974 - ETA: 3s - loss: 0.0887 - acc: 0.974 - ETA: 3s - loss: 0.0883 - acc: 0.974 - ETA: 3s - loss: 0.0898 - acc: 0.974 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0910 - acc: 0.973 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0898 - acc: 0.973 - ETA: 3s - loss: 0.0892 - acc: 0.974 - ETA: 3s - loss: 0.0890 - acc: 0.974 - ETA: 3s - loss: 0.0885 - acc: 0.974 - ETA: 3s - loss: 0.0884 - acc: 0.973 - ETA: 3s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0916 - acc: 0.973 - ETA: 2s - loss: 0.0910 - acc: 0.973 - ETA: 2s - loss: 0.0902 - acc: 0.973 - ETA: 2s - loss: 0.0920 - acc: 0.973 - ETA: 2s - loss: 0.0915 - acc: 0.973 - ETA: 2s - loss: 0.0916 - acc: 0.973 - ETA: 2s - loss: 0.0912 - acc: 0.973 - ETA: 2s - loss: 0.0914 - acc: 0.972 - ETA: 2s - loss: 0.0915 - acc: 0.972 - ETA: 2s - loss: 0.0919 - acc: 0.972 - ETA: 2s - loss: 0.0912 - acc: 0.972 - ETA: 2s - loss: 0.0909 - acc: 0.972 - ETA: 2s - loss: 0.0909 - acc: 0.972 - ETA: 2s - loss: 0.0908 - acc: 0.972 - ETA: 2s - loss: 0.0919 - acc: 0.972 - ETA: 2s - loss: 0.0913 - acc: 0.972 - ETA: 2s - loss: 0.0906 - acc: 0.973 - ETA: 2s - loss: 0.0912 - acc: 0.972 - ETA: 1s - loss: 0.0911 - acc: 0.972 - ETA: 1s - loss: 0.0908 - acc: 0.972 - ETA: 1s - loss: 0.0905 - acc: 0.972 - ETA: 1s - loss: 0.0909 - acc: 0.972 - ETA: 1s - loss: 0.0923 - acc: 0.972 - ETA: 1s - loss: 0.0917 - acc: 0.972 - ETA: 1s - loss: 0.0913 - acc: 0.972 - ETA: 1s - loss: 0.0909 - acc: 0.972 - ETA: 1s - loss: 0.0938 - acc: 0.972 - ETA: 1s - loss: 0.0946 - acc: 0.972 - ETA: 1s - loss: 0.0940 - acc: 0.972 - ETA: 1s - loss: 0.0941 - acc: 0.972 - ETA: 1s - loss: 0.0943 - acc: 0.972 - ETA: 1s - loss: 0.0937 - acc: 0.972 - ETA: 1s - loss: 0.0932 - acc: 0.972 - ETA: 1s - loss: 0.0930 - acc: 0.972 - ETA: 1s - loss: 0.0934 - acc: 0.972 - ETA: 1s - loss: 0.0944 - acc: 0.971 - ETA: 0s - loss: 0.0952 - acc: 0.971 - ETA: 0s - loss: 0.0947 - acc: 0.971 - ETA: 0s - loss: 0.0945 - acc: 0.971 - ETA: 0s - loss: 0.0944 - acc: 0.971 - ETA: 0s - loss: 0.0943 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.971 - ETA: 0s - loss: 0.0940 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.971 - ETA: 0s - loss: 0.0937 - acc: 0.971 - ETA: 0s - loss: 0.0938 - acc: 0.971 - ETA: 0s - loss: 0.0934 - acc: 0.971 - ETA: 0s - loss: 0.0930 - acc: 0.972 - ETA: 0s - loss: 0.0934 - acc: 0.972 - ETA: 0s - loss: 0.0940 - acc: 0.971 - ETA: 0s - loss: 0.0938 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.9719Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0944 - acc: 0.9720 - val_loss: 0.6636 - val_acc: 0.8479\n",
      "Epoch 17/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1768 - acc: 0.900 - ETA: 11s - loss: 0.1312 - acc: 0.93 - ETA: 10s - loss: 0.0970 - acc: 0.95 - ETA: 10s - loss: 0.0804 - acc: 0.95 - ETA: 9s - loss: 0.1457 - acc: 0.9556 - ETA: 9s - loss: 0.2187 - acc: 0.945 - ETA: 9s - loss: 0.1951 - acc: 0.950 - ETA: 9s - loss: 0.1724 - acc: 0.956 - ETA: 9s - loss: 0.1878 - acc: 0.952 - ETA: 9s - loss: 0.1693 - acc: 0.957 - ETA: 8s - loss: 0.1562 - acc: 0.961 - ETA: 8s - loss: 0.1444 - acc: 0.965 - ETA: 8s - loss: 0.1337 - acc: 0.968 - ETA: 8s - loss: 0.1288 - acc: 0.968 - ETA: 8s - loss: 0.1322 - acc: 0.969 - ETA: 8s - loss: 0.1249 - acc: 0.971 - ETA: 8s - loss: 0.1212 - acc: 0.971 - ETA: 8s - loss: 0.1148 - acc: 0.972 - ETA: 8s - loss: 0.1131 - acc: 0.973 - ETA: 8s - loss: 0.1095 - acc: 0.973 - ETA: 8s - loss: 0.1076 - acc: 0.972 - ETA: 8s - loss: 0.1105 - acc: 0.970 - ETA: 7s - loss: 0.1232 - acc: 0.970 - ETA: 7s - loss: 0.1182 - acc: 0.971 - ETA: 7s - loss: 0.1183 - acc: 0.970 - ETA: 7s - loss: 0.1203 - acc: 0.969 - ETA: 7s - loss: 0.1237 - acc: 0.969 - ETA: 7s - loss: 0.1269 - acc: 0.969 - ETA: 7s - loss: 0.1226 - acc: 0.970 - ETA: 7s - loss: 0.1234 - acc: 0.969 - ETA: 7s - loss: 0.1239 - acc: 0.968 - ETA: 7s - loss: 0.1204 - acc: 0.969 - ETA: 7s - loss: 0.1193 - acc: 0.969 - ETA: 7s - loss: 0.1185 - acc: 0.969 - ETA: 7s - loss: 0.1167 - acc: 0.969 - ETA: 7s - loss: 0.1146 - acc: 0.969 - ETA: 7s - loss: 0.1120 - acc: 0.970 - ETA: 7s - loss: 0.1097 - acc: 0.971 - ETA: 7s - loss: 0.1085 - acc: 0.971 - ETA: 7s - loss: 0.1082 - acc: 0.971 - ETA: 6s - loss: 0.1109 - acc: 0.971 - ETA: 6s - loss: 0.1106 - acc: 0.971 - ETA: 6s - loss: 0.1123 - acc: 0.971 - ETA: 6s - loss: 0.1099 - acc: 0.971 - ETA: 6s - loss: 0.1079 - acc: 0.972 - ETA: 6s - loss: 0.1058 - acc: 0.973 - ETA: 6s - loss: 0.1050 - acc: 0.973 - ETA: 6s - loss: 0.1058 - acc: 0.972 - ETA: 6s - loss: 0.1043 - acc: 0.972 - ETA: 6s - loss: 0.1049 - acc: 0.972 - ETA: 6s - loss: 0.1032 - acc: 0.973 - ETA: 6s - loss: 0.1019 - acc: 0.973 - ETA: 6s - loss: 0.1006 - acc: 0.973 - ETA: 6s - loss: 0.0993 - acc: 0.973 - ETA: 6s - loss: 0.0985 - acc: 0.973 - ETA: 6s - loss: 0.1017 - acc: 0.973 - ETA: 6s - loss: 0.1000 - acc: 0.973 - ETA: 5s - loss: 0.0984 - acc: 0.974 - ETA: 5s - loss: 0.0988 - acc: 0.974 - ETA: 5s - loss: 0.0986 - acc: 0.974 - ETA: 5s - loss: 0.1060 - acc: 0.973 - ETA: 5s - loss: 0.1046 - acc: 0.973 - ETA: 5s - loss: 0.1037 - acc: 0.973 - ETA: 5s - loss: 0.1047 - acc: 0.973 - ETA: 5s - loss: 0.1043 - acc: 0.973 - ETA: 5s - loss: 0.1048 - acc: 0.973 - ETA: 5s - loss: 0.1038 - acc: 0.974 - ETA: 5s - loss: 0.1036 - acc: 0.974 - ETA: 5s - loss: 0.1025 - acc: 0.974 - ETA: 5s - loss: 0.1025 - acc: 0.974 - ETA: 5s - loss: 0.1034 - acc: 0.974 - ETA: 5s - loss: 0.1055 - acc: 0.973 - ETA: 5s - loss: 0.1043 - acc: 0.974 - ETA: 5s - loss: 0.1033 - acc: 0.974 - ETA: 5s - loss: 0.1027 - acc: 0.974 - ETA: 4s - loss: 0.1019 - acc: 0.974 - ETA: 4s - loss: 0.1008 - acc: 0.974 - ETA: 4s - loss: 0.1011 - acc: 0.974 - ETA: 4s - loss: 0.1050 - acc: 0.973 - ETA: 4s - loss: 0.1054 - acc: 0.973 - ETA: 4s - loss: 0.1046 - acc: 0.973 - ETA: 4s - loss: 0.1054 - acc: 0.973 - ETA: 4s - loss: 0.1051 - acc: 0.973 - ETA: 4s - loss: 0.1058 - acc: 0.973 - ETA: 4s - loss: 0.1065 - acc: 0.973 - ETA: 4s - loss: 0.1059 - acc: 0.973 - ETA: 4s - loss: 0.1050 - acc: 0.973 - ETA: 4s - loss: 0.1039 - acc: 0.973 - ETA: 4s - loss: 0.1030 - acc: 0.974 - ETA: 4s - loss: 0.1023 - acc: 0.974 - ETA: 4s - loss: 0.1014 - acc: 0.974 - ETA: 4s - loss: 0.1003 - acc: 0.974 - ETA: 4s - loss: 0.1006 - acc: 0.974 - ETA: 3s - loss: 0.1008 - acc: 0.973 - ETA: 3s - loss: 0.0999 - acc: 0.974 - ETA: 3s - loss: 0.0990 - acc: 0.974 - ETA: 3s - loss: 0.0982 - acc: 0.974 - ETA: 3s - loss: 0.0973 - acc: 0.974 - ETA: 3s - loss: 0.0964 - acc: 0.975 - ETA: 3s - loss: 0.0959 - acc: 0.975 - ETA: 3s - loss: 0.0953 - acc: 0.975 - ETA: 3s - loss: 0.0950 - acc: 0.975 - ETA: 3s - loss: 0.0944 - acc: 0.975 - ETA: 3s - loss: 0.0946 - acc: 0.975 - ETA: 3s - loss: 0.0944 - acc: 0.975 - ETA: 3s - loss: 0.0953 - acc: 0.974 - ETA: 3s - loss: 0.0946 - acc: 0.974 - ETA: 3s - loss: 0.0938 - acc: 0.974 - ETA: 3s - loss: 0.0941 - acc: 0.974 - ETA: 3s - loss: 0.0947 - acc: 0.974 - ETA: 3s - loss: 0.0941 - acc: 0.974 - ETA: 3s - loss: 0.0934 - acc: 0.974 - ETA: 2s - loss: 0.0930 - acc: 0.974 - ETA: 2s - loss: 0.0929 - acc: 0.974 - ETA: 2s - loss: 0.0927 - acc: 0.974 - ETA: 2s - loss: 0.0923 - acc: 0.975 - ETA: 2s - loss: 0.0921 - acc: 0.975 - ETA: 2s - loss: 0.0918 - acc: 0.974 - ETA: 2s - loss: 0.0922 - acc: 0.974 - ETA: 2s - loss: 0.0928 - acc: 0.974 - ETA: 2s - loss: 0.0932 - acc: 0.974 - ETA: 2s - loss: 0.0925 - acc: 0.974 - ETA: 2s - loss: 0.0922 - acc: 0.974 - ETA: 2s - loss: 0.0926 - acc: 0.973 - ETA: 2s - loss: 0.0922 - acc: 0.973 - ETA: 2s - loss: 0.0928 - acc: 0.973 - ETA: 2s - loss: 0.0923 - acc: 0.973 - ETA: 2s - loss: 0.0917 - acc: 0.973 - ETA: 2s - loss: 0.0918 - acc: 0.973 - ETA: 2s - loss: 0.0917 - acc: 0.973 - ETA: 1s - loss: 0.0915 - acc: 0.974 - ETA: 1s - loss: 0.0909 - acc: 0.974 - ETA: 1s - loss: 0.0910 - acc: 0.974 - ETA: 1s - loss: 0.0906 - acc: 0.974 - ETA: 1s - loss: 0.0900 - acc: 0.974 - ETA: 1s - loss: 0.0902 - acc: 0.974 - ETA: 1s - loss: 0.0897 - acc: 0.974 - ETA: 1s - loss: 0.0900 - acc: 0.974 - ETA: 1s - loss: 0.0892 - acc: 0.974 - ETA: 1s - loss: 0.0914 - acc: 0.974 - ETA: 1s - loss: 0.0910 - acc: 0.974 - ETA: 1s - loss: 0.0914 - acc: 0.974 - ETA: 1s - loss: 0.0913 - acc: 0.974 - ETA: 1s - loss: 0.0907 - acc: 0.974 - ETA: 1s - loss: 0.0901 - acc: 0.974 - ETA: 1s - loss: 0.0902 - acc: 0.974 - ETA: 1s - loss: 0.0901 - acc: 0.974 - ETA: 0s - loss: 0.0896 - acc: 0.974 - ETA: 0s - loss: 0.0890 - acc: 0.974 - ETA: 0s - loss: 0.0888 - acc: 0.974 - ETA: 0s - loss: 0.0906 - acc: 0.974 - ETA: 0s - loss: 0.0909 - acc: 0.974 - ETA: 0s - loss: 0.0911 - acc: 0.974 - ETA: 0s - loss: 0.0907 - acc: 0.974 - ETA: 0s - loss: 0.0905 - acc: 0.974 - ETA: 0s - loss: 0.0900 - acc: 0.974 - ETA: 0s - loss: 0.0900 - acc: 0.974 - ETA: 0s - loss: 0.0895 - acc: 0.974 - ETA: 0s - loss: 0.0891 - acc: 0.974 - ETA: 0s - loss: 0.0889 - acc: 0.974 - ETA: 0s - loss: 0.0884 - acc: 0.975 - ETA: 0s - loss: 0.0894 - acc: 0.974 - ETA: 0s - loss: 0.0894 - acc: 0.974 - ETA: 0s - loss: 0.0889 - acc: 0.974 - ETA: 0s - loss: 0.0887 - acc: 0.9749Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0889 - acc: 0.9749 - val_loss: 0.6564 - val_acc: 0.8479\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.0029 - acc: 1.000 - ETA: 8s - loss: 0.0327 - acc: 0.983 - ETA: 8s - loss: 0.1211 - acc: 0.950 - ETA: 8s - loss: 0.1113 - acc: 0.957 - ETA: 8s - loss: 0.0878 - acc: 0.966 - ETA: 8s - loss: 0.0781 - acc: 0.968 - ETA: 8s - loss: 0.0673 - acc: 0.973 - ETA: 8s - loss: 0.0615 - acc: 0.976 - ETA: 8s - loss: 0.0613 - acc: 0.976 - ETA: 8s - loss: 0.0579 - acc: 0.978 - ETA: 8s - loss: 0.0544 - acc: 0.981 - ETA: 8s - loss: 0.0660 - acc: 0.980 - ETA: 8s - loss: 0.0633 - acc: 0.982 - ETA: 8s - loss: 0.0750 - acc: 0.977 - ETA: 8s - loss: 0.0774 - acc: 0.977 - ETA: 8s - loss: 0.0743 - acc: 0.977 - ETA: 8s - loss: 0.0731 - acc: 0.977 - ETA: 7s - loss: 0.0696 - acc: 0.978 - ETA: 7s - loss: 0.0708 - acc: 0.977 - ETA: 7s - loss: 0.0676 - acc: 0.978 - ETA: 7s - loss: 0.0698 - acc: 0.978 - ETA: 7s - loss: 0.0670 - acc: 0.979 - ETA: 7s - loss: 0.0713 - acc: 0.978 - ETA: 7s - loss: 0.0726 - acc: 0.978 - ETA: 7s - loss: 0.0756 - acc: 0.978 - ETA: 7s - loss: 0.0749 - acc: 0.978 - ETA: 7s - loss: 0.0757 - acc: 0.977 - ETA: 7s - loss: 0.0732 - acc: 0.978 - ETA: 7s - loss: 0.0752 - acc: 0.977 - ETA: 7s - loss: 0.0740 - acc: 0.977 - ETA: 7s - loss: 0.0719 - acc: 0.977 - ETA: 7s - loss: 0.0703 - acc: 0.978 - ETA: 7s - loss: 0.0689 - acc: 0.979 - ETA: 7s - loss: 0.0673 - acc: 0.979 - ETA: 7s - loss: 0.0662 - acc: 0.980 - ETA: 7s - loss: 0.0654 - acc: 0.980 - ETA: 6s - loss: 0.0657 - acc: 0.980 - ETA: 6s - loss: 0.0674 - acc: 0.980 - ETA: 6s - loss: 0.0758 - acc: 0.979 - ETA: 6s - loss: 0.0746 - acc: 0.979 - ETA: 6s - loss: 0.0748 - acc: 0.979 - ETA: 6s - loss: 0.0734 - acc: 0.980 - ETA: 6s - loss: 0.0718 - acc: 0.980 - ETA: 6s - loss: 0.0706 - acc: 0.981 - ETA: 6s - loss: 0.0721 - acc: 0.980 - ETA: 6s - loss: 0.0708 - acc: 0.980 - ETA: 6s - loss: 0.0697 - acc: 0.981 - ETA: 6s - loss: 0.0690 - acc: 0.981 - ETA: 6s - loss: 0.0691 - acc: 0.981 - ETA: 6s - loss: 0.0698 - acc: 0.980 - ETA: 6s - loss: 0.0725 - acc: 0.980 - ETA: 6s - loss: 0.0714 - acc: 0.980 - ETA: 6s - loss: 0.0708 - acc: 0.980 - ETA: 6s - loss: 0.0713 - acc: 0.980 - ETA: 6s - loss: 0.0703 - acc: 0.980 - ETA: 5s - loss: 0.0709 - acc: 0.980 - ETA: 5s - loss: 0.0711 - acc: 0.979 - ETA: 5s - loss: 0.0702 - acc: 0.980 - ETA: 5s - loss: 0.0763 - acc: 0.979 - ETA: 5s - loss: 0.0753 - acc: 0.980 - ETA: 5s - loss: 0.0745 - acc: 0.980 - ETA: 5s - loss: 0.0745 - acc: 0.980 - ETA: 5s - loss: 0.0743 - acc: 0.980 - ETA: 5s - loss: 0.0734 - acc: 0.980 - ETA: 5s - loss: 0.0731 - acc: 0.981 - ETA: 5s - loss: 0.0761 - acc: 0.980 - ETA: 5s - loss: 0.0756 - acc: 0.980 - ETA: 5s - loss: 0.0747 - acc: 0.980 - ETA: 5s - loss: 0.0748 - acc: 0.980 - ETA: 5s - loss: 0.0743 - acc: 0.980 - ETA: 5s - loss: 0.0734 - acc: 0.980 - ETA: 5s - loss: 0.0727 - acc: 0.981 - ETA: 5s - loss: 0.0717 - acc: 0.981 - ETA: 5s - loss: 0.0714 - acc: 0.981 - ETA: 4s - loss: 0.0714 - acc: 0.981 - ETA: 4s - loss: 0.0710 - acc: 0.981 - ETA: 4s - loss: 0.0716 - acc: 0.980 - ETA: 4s - loss: 0.0746 - acc: 0.980 - ETA: 4s - loss: 0.0739 - acc: 0.980 - ETA: 4s - loss: 0.0738 - acc: 0.980 - ETA: 4s - loss: 0.0762 - acc: 0.979 - ETA: 4s - loss: 0.0762 - acc: 0.979 - ETA: 4s - loss: 0.0754 - acc: 0.980 - ETA: 4s - loss: 0.0761 - acc: 0.979 - ETA: 4s - loss: 0.0772 - acc: 0.979 - ETA: 4s - loss: 0.0764 - acc: 0.979 - ETA: 4s - loss: 0.0785 - acc: 0.978 - ETA: 4s - loss: 0.0786 - acc: 0.978 - ETA: 4s - loss: 0.0778 - acc: 0.978 - ETA: 4s - loss: 0.0774 - acc: 0.978 - ETA: 4s - loss: 0.0784 - acc: 0.978 - ETA: 4s - loss: 0.0781 - acc: 0.978 - ETA: 4s - loss: 0.0774 - acc: 0.978 - ETA: 3s - loss: 0.0782 - acc: 0.978 - ETA: 3s - loss: 0.0775 - acc: 0.978 - ETA: 3s - loss: 0.0768 - acc: 0.979 - ETA: 3s - loss: 0.0762 - acc: 0.979 - ETA: 3s - loss: 0.0760 - acc: 0.979 - ETA: 3s - loss: 0.0769 - acc: 0.979 - ETA: 3s - loss: 0.0762 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0751 - acc: 0.979 - ETA: 3s - loss: 0.0760 - acc: 0.978 - ETA: 3s - loss: 0.0765 - acc: 0.978 - ETA: 3s - loss: 0.0761 - acc: 0.978 - ETA: 3s - loss: 0.0767 - acc: 0.978 - ETA: 3s - loss: 0.0762 - acc: 0.978 - ETA: 3s - loss: 0.0765 - acc: 0.978 - ETA: 3s - loss: 0.0782 - acc: 0.977 - ETA: 3s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0777 - acc: 0.977 - ETA: 2s - loss: 0.0773 - acc: 0.977 - ETA: 2s - loss: 0.0772 - acc: 0.977 - ETA: 2s - loss: 0.0779 - acc: 0.977 - ETA: 2s - loss: 0.0775 - acc: 0.977 - ETA: 2s - loss: 0.0774 - acc: 0.977 - ETA: 2s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0779 - acc: 0.977 - ETA: 2s - loss: 0.0787 - acc: 0.976 - ETA: 2s - loss: 0.0781 - acc: 0.977 - ETA: 2s - loss: 0.0781 - acc: 0.976 - ETA: 2s - loss: 0.0806 - acc: 0.976 - ETA: 2s - loss: 0.0801 - acc: 0.976 - ETA: 2s - loss: 0.0805 - acc: 0.976 - ETA: 2s - loss: 0.0800 - acc: 0.977 - ETA: 2s - loss: 0.0808 - acc: 0.976 - ETA: 2s - loss: 0.0803 - acc: 0.977 - ETA: 2s - loss: 0.0811 - acc: 0.977 - ETA: 2s - loss: 0.0807 - acc: 0.977 - ETA: 1s - loss: 0.0805 - acc: 0.977 - ETA: 1s - loss: 0.0824 - acc: 0.976 - ETA: 1s - loss: 0.0830 - acc: 0.976 - ETA: 1s - loss: 0.0828 - acc: 0.976 - ETA: 1s - loss: 0.0825 - acc: 0.976 - ETA: 1s - loss: 0.0841 - acc: 0.976 - ETA: 1s - loss: 0.0835 - acc: 0.976 - ETA: 1s - loss: 0.0854 - acc: 0.976 - ETA: 1s - loss: 0.0850 - acc: 0.976 - ETA: 1s - loss: 0.0852 - acc: 0.976 - ETA: 1s - loss: 0.0848 - acc: 0.976 - ETA: 1s - loss: 0.0843 - acc: 0.976 - ETA: 1s - loss: 0.0838 - acc: 0.976 - ETA: 1s - loss: 0.0834 - acc: 0.976 - ETA: 1s - loss: 0.0830 - acc: 0.976 - ETA: 1s - loss: 0.0825 - acc: 0.976 - ETA: 1s - loss: 0.0822 - acc: 0.976 - ETA: 1s - loss: 0.0820 - acc: 0.976 - ETA: 0s - loss: 0.0819 - acc: 0.976 - ETA: 0s - loss: 0.0830 - acc: 0.976 - ETA: 0s - loss: 0.0826 - acc: 0.976 - ETA: 0s - loss: 0.0832 - acc: 0.976 - ETA: 0s - loss: 0.0828 - acc: 0.976 - ETA: 0s - loss: 0.0828 - acc: 0.976 - ETA: 0s - loss: 0.0844 - acc: 0.976 - ETA: 0s - loss: 0.0841 - acc: 0.976 - ETA: 0s - loss: 0.0836 - acc: 0.976 - ETA: 0s - loss: 0.0843 - acc: 0.976 - ETA: 0s - loss: 0.0839 - acc: 0.976 - ETA: 0s - loss: 0.0834 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.976 - ETA: 0s - loss: 0.0853 - acc: 0.976 - ETA: 0s - loss: 0.0849 - acc: 0.976 - ETA: 0s - loss: 0.0845 - acc: 0.976 - ETA: 0s - loss: 0.0846 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.9770Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0841 - acc: 0.9769 - val_loss: 0.7012 - val_acc: 0.8575\n",
      "Epoch 19/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1307 - acc: 0.950 - ETA: 10s - loss: 0.0482 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 10s - loss: 0.0354 - acc: 0.98 - ETA: 10s - loss: 0.0293 - acc: 0.98 - ETA: 9s - loss: 0.0472 - acc: 0.9818 - ETA: 9s - loss: 0.0924 - acc: 0.980 - ETA: 9s - loss: 0.0901 - acc: 0.980 - ETA: 9s - loss: 0.1124 - acc: 0.976 - ETA: 9s - loss: 0.1164 - acc: 0.976 - ETA: 9s - loss: 0.1165 - acc: 0.976 - ETA: 8s - loss: 0.1135 - acc: 0.976 - ETA: 8s - loss: 0.1386 - acc: 0.976 - ETA: 8s - loss: 0.1287 - acc: 0.977 - ETA: 8s - loss: 0.1214 - acc: 0.979 - ETA: 8s - loss: 0.1268 - acc: 0.979 - ETA: 8s - loss: 0.1194 - acc: 0.980 - ETA: 8s - loss: 0.1228 - acc: 0.980 - ETA: 8s - loss: 0.1164 - acc: 0.981 - ETA: 8s - loss: 0.1163 - acc: 0.979 - ETA: 8s - loss: 0.1152 - acc: 0.979 - ETA: 8s - loss: 0.1113 - acc: 0.980 - ETA: 8s - loss: 0.1076 - acc: 0.981 - ETA: 7s - loss: 0.1064 - acc: 0.979 - ETA: 7s - loss: 0.1021 - acc: 0.980 - ETA: 7s - loss: 0.0994 - acc: 0.980 - ETA: 7s - loss: 0.1042 - acc: 0.979 - ETA: 7s - loss: 0.1165 - acc: 0.979 - ETA: 7s - loss: 0.1160 - acc: 0.978 - ETA: 7s - loss: 0.1163 - acc: 0.977 - ETA: 7s - loss: 0.1144 - acc: 0.977 - ETA: 7s - loss: 0.1109 - acc: 0.977 - ETA: 7s - loss: 0.1089 - acc: 0.978 - ETA: 7s - loss: 0.1059 - acc: 0.979 - ETA: 7s - loss: 0.1036 - acc: 0.979 - ETA: 7s - loss: 0.1024 - acc: 0.978 - ETA: 7s - loss: 0.1016 - acc: 0.978 - ETA: 7s - loss: 0.0991 - acc: 0.979 - ETA: 7s - loss: 0.0997 - acc: 0.977 - ETA: 6s - loss: 0.0999 - acc: 0.977 - ETA: 6s - loss: 0.0998 - acc: 0.977 - ETA: 6s - loss: 0.0976 - acc: 0.977 - ETA: 6s - loss: 0.0955 - acc: 0.978 - ETA: 6s - loss: 0.0935 - acc: 0.978 - ETA: 6s - loss: 0.0933 - acc: 0.978 - ETA: 6s - loss: 0.0922 - acc: 0.978 - ETA: 6s - loss: 0.0922 - acc: 0.978 - ETA: 6s - loss: 0.0904 - acc: 0.978 - ETA: 6s - loss: 0.0898 - acc: 0.977 - ETA: 6s - loss: 0.0889 - acc: 0.977 - ETA: 6s - loss: 0.0889 - acc: 0.977 - ETA: 6s - loss: 0.0887 - acc: 0.977 - ETA: 6s - loss: 0.0873 - acc: 0.977 - ETA: 6s - loss: 0.0872 - acc: 0.977 - ETA: 6s - loss: 0.0859 - acc: 0.978 - ETA: 6s - loss: 0.0846 - acc: 0.978 - ETA: 5s - loss: 0.0838 - acc: 0.978 - ETA: 5s - loss: 0.0856 - acc: 0.977 - ETA: 5s - loss: 0.0847 - acc: 0.977 - ETA: 5s - loss: 0.0851 - acc: 0.977 - ETA: 5s - loss: 0.0839 - acc: 0.978 - ETA: 5s - loss: 0.0836 - acc: 0.978 - ETA: 5s - loss: 0.0830 - acc: 0.977 - ETA: 5s - loss: 0.0830 - acc: 0.977 - ETA: 5s - loss: 0.0866 - acc: 0.976 - ETA: 5s - loss: 0.0857 - acc: 0.977 - ETA: 5s - loss: 0.0876 - acc: 0.976 - ETA: 5s - loss: 0.0880 - acc: 0.976 - ETA: 5s - loss: 0.0868 - acc: 0.976 - ETA: 5s - loss: 0.0875 - acc: 0.975 - ETA: 5s - loss: 0.0867 - acc: 0.976 - ETA: 5s - loss: 0.0858 - acc: 0.976 - ETA: 5s - loss: 0.0857 - acc: 0.976 - ETA: 5s - loss: 0.0845 - acc: 0.976 - ETA: 4s - loss: 0.0837 - acc: 0.976 - ETA: 4s - loss: 0.0882 - acc: 0.976 - ETA: 4s - loss: 0.0873 - acc: 0.976 - ETA: 4s - loss: 0.0864 - acc: 0.977 - ETA: 4s - loss: 0.0867 - acc: 0.976 - ETA: 4s - loss: 0.0857 - acc: 0.977 - ETA: 4s - loss: 0.0849 - acc: 0.977 - ETA: 4s - loss: 0.0841 - acc: 0.977 - ETA: 4s - loss: 0.0834 - acc: 0.977 - ETA: 4s - loss: 0.0827 - acc: 0.977 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0819 - acc: 0.978 - ETA: 4s - loss: 0.0814 - acc: 0.978 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0838 - acc: 0.977 - ETA: 4s - loss: 0.0846 - acc: 0.977 - ETA: 4s - loss: 0.0858 - acc: 0.977 - ETA: 4s - loss: 0.0852 - acc: 0.977 - ETA: 3s - loss: 0.0848 - acc: 0.977 - ETA: 3s - loss: 0.0847 - acc: 0.977 - ETA: 3s - loss: 0.0861 - acc: 0.977 - ETA: 3s - loss: 0.0857 - acc: 0.977 - ETA: 3s - loss: 0.0859 - acc: 0.977 - ETA: 3s - loss: 0.0852 - acc: 0.977 - ETA: 3s - loss: 0.0845 - acc: 0.977 - ETA: 3s - loss: 0.0843 - acc: 0.977 - ETA: 3s - loss: 0.0839 - acc: 0.977 - ETA: 3s - loss: 0.0836 - acc: 0.977 - ETA: 3s - loss: 0.0831 - acc: 0.977 - ETA: 3s - loss: 0.0824 - acc: 0.978 - ETA: 3s - loss: 0.0825 - acc: 0.977 - ETA: 3s - loss: 0.0832 - acc: 0.977 - ETA: 3s - loss: 0.0829 - acc: 0.977 - ETA: 3s - loss: 0.0822 - acc: 0.977 - ETA: 3s - loss: 0.0822 - acc: 0.977 - ETA: 3s - loss: 0.0815 - acc: 0.977 - ETA: 2s - loss: 0.0814 - acc: 0.977 - ETA: 2s - loss: 0.0815 - acc: 0.977 - ETA: 2s - loss: 0.0819 - acc: 0.977 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 2s - loss: 0.0805 - acc: 0.977 - ETA: 2s - loss: 0.0801 - acc: 0.977 - ETA: 2s - loss: 0.0799 - acc: 0.977 - ETA: 2s - loss: 0.0794 - acc: 0.978 - ETA: 2s - loss: 0.0791 - acc: 0.978 - ETA: 2s - loss: 0.0787 - acc: 0.978 - ETA: 2s - loss: 0.0786 - acc: 0.977 - ETA: 2s - loss: 0.0783 - acc: 0.977 - ETA: 2s - loss: 0.0786 - acc: 0.977 - ETA: 2s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0791 - acc: 0.977 - ETA: 2s - loss: 0.0787 - acc: 0.977 - ETA: 2s - loss: 0.0792 - acc: 0.977 - ETA: 1s - loss: 0.0787 - acc: 0.977 - ETA: 1s - loss: 0.0781 - acc: 0.977 - ETA: 1s - loss: 0.0778 - acc: 0.977 - ETA: 1s - loss: 0.0777 - acc: 0.977 - ETA: 1s - loss: 0.0772 - acc: 0.977 - ETA: 1s - loss: 0.0774 - acc: 0.977 - ETA: 1s - loss: 0.0769 - acc: 0.977 - ETA: 1s - loss: 0.0772 - acc: 0.977 - ETA: 1s - loss: 0.0770 - acc: 0.977 - ETA: 1s - loss: 0.0771 - acc: 0.977 - ETA: 1s - loss: 0.0768 - acc: 0.977 - ETA: 1s - loss: 0.0763 - acc: 0.977 - ETA: 1s - loss: 0.0764 - acc: 0.977 - ETA: 1s - loss: 0.0760 - acc: 0.977 - ETA: 1s - loss: 0.0756 - acc: 0.977 - ETA: 1s - loss: 0.0761 - acc: 0.977 - ETA: 1s - loss: 0.0757 - acc: 0.977 - ETA: 1s - loss: 0.0753 - acc: 0.978 - ETA: 0s - loss: 0.0757 - acc: 0.977 - ETA: 0s - loss: 0.0777 - acc: 0.977 - ETA: 0s - loss: 0.0790 - acc: 0.977 - ETA: 0s - loss: 0.0788 - acc: 0.977 - ETA: 0s - loss: 0.0784 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.977 - ETA: 0s - loss: 0.0783 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.978 - ETA: 0s - loss: 0.0782 - acc: 0.977 - ETA: 0s - loss: 0.0791 - acc: 0.977 - ETA: 0s - loss: 0.0791 - acc: 0.977 - ETA: 0s - loss: 0.0788 - acc: 0.977 - ETA: 0s - loss: 0.0784 - acc: 0.977 - ETA: 0s - loss: 0.0781 - acc: 0.977 - ETA: 0s - loss: 0.0777 - acc: 0.977 - ETA: 0s - loss: 0.0773 - acc: 0.978 - ETA: 0s - loss: 0.0770 - acc: 0.978 - ETA: 0s - loss: 0.0775 - acc: 0.978 - ETA: 0s - loss: 0.0772 - acc: 0.9781Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0770 - acc: 0.9781 - val_loss: 0.6545 - val_acc: 0.8575\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0357 - acc: 1.000 - ETA: 9s - loss: 0.0740 - acc: 0.983 - ETA: 9s - loss: 0.0459 - acc: 0.990 - ETA: 9s - loss: 0.0761 - acc: 0.971 - ETA: 9s - loss: 0.0886 - acc: 0.972 - ETA: 9s - loss: 0.1477 - acc: 0.972 - ETA: 9s - loss: 0.1435 - acc: 0.969 - ETA: 9s - loss: 0.1367 - acc: 0.966 - ETA: 8s - loss: 0.1242 - acc: 0.970 - ETA: 8s - loss: 0.1121 - acc: 0.973 - ETA: 8s - loss: 0.1017 - acc: 0.976 - ETA: 8s - loss: 0.0948 - acc: 0.978 - ETA: 8s - loss: 0.0874 - acc: 0.980 - ETA: 8s - loss: 0.0850 - acc: 0.977 - ETA: 8s - loss: 0.0824 - acc: 0.977 - ETA: 8s - loss: 0.0783 - acc: 0.979 - ETA: 8s - loss: 0.0834 - acc: 0.975 - ETA: 8s - loss: 0.0796 - acc: 0.977 - ETA: 8s - loss: 0.0761 - acc: 0.978 - ETA: 8s - loss: 0.0724 - acc: 0.979 - ETA: 8s - loss: 0.0701 - acc: 0.980 - ETA: 7s - loss: 0.0700 - acc: 0.980 - ETA: 7s - loss: 0.0674 - acc: 0.981 - ETA: 7s - loss: 0.0659 - acc: 0.981 - ETA: 7s - loss: 0.0641 - acc: 0.982 - ETA: 7s - loss: 0.0701 - acc: 0.982 - ETA: 7s - loss: 0.0717 - acc: 0.982 - ETA: 7s - loss: 0.0693 - acc: 0.982 - ETA: 7s - loss: 0.0671 - acc: 0.983 - ETA: 7s - loss: 0.0734 - acc: 0.982 - ETA: 7s - loss: 0.0716 - acc: 0.982 - ETA: 7s - loss: 0.0701 - acc: 0.983 - ETA: 7s - loss: 0.0766 - acc: 0.981 - ETA: 7s - loss: 0.0767 - acc: 0.980 - ETA: 7s - loss: 0.0767 - acc: 0.979 - ETA: 7s - loss: 0.0755 - acc: 0.979 - ETA: 7s - loss: 0.0739 - acc: 0.980 - ETA: 7s - loss: 0.0721 - acc: 0.980 - ETA: 7s - loss: 0.0715 - acc: 0.980 - ETA: 6s - loss: 0.0712 - acc: 0.980 - ETA: 6s - loss: 0.0717 - acc: 0.980 - ETA: 6s - loss: 0.0710 - acc: 0.980 - ETA: 6s - loss: 0.0700 - acc: 0.980 - ETA: 6s - loss: 0.0686 - acc: 0.980 - ETA: 6s - loss: 0.0704 - acc: 0.980 - ETA: 6s - loss: 0.0692 - acc: 0.980 - ETA: 6s - loss: 0.0688 - acc: 0.980 - ETA: 6s - loss: 0.0691 - acc: 0.980 - ETA: 6s - loss: 0.0679 - acc: 0.980 - ETA: 6s - loss: 0.0666 - acc: 0.980 - ETA: 6s - loss: 0.0689 - acc: 0.980 - ETA: 6s - loss: 0.0677 - acc: 0.980 - ETA: 6s - loss: 0.0685 - acc: 0.980 - ETA: 6s - loss: 0.0703 - acc: 0.979 - ETA: 6s - loss: 0.0695 - acc: 0.980 - ETA: 6s - loss: 0.0724 - acc: 0.979 - ETA: 6s - loss: 0.0748 - acc: 0.978 - ETA: 6s - loss: 0.0738 - acc: 0.979 - ETA: 5s - loss: 0.0726 - acc: 0.979 - ETA: 5s - loss: 0.0719 - acc: 0.979 - ETA: 5s - loss: 0.0709 - acc: 0.979 - ETA: 5s - loss: 0.0700 - acc: 0.980 - ETA: 5s - loss: 0.0690 - acc: 0.980 - ETA: 5s - loss: 0.0689 - acc: 0.980 - ETA: 5s - loss: 0.0686 - acc: 0.980 - ETA: 5s - loss: 0.0679 - acc: 0.980 - ETA: 5s - loss: 0.0674 - acc: 0.980 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0660 - acc: 0.981 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0683 - acc: 0.980 - ETA: 5s - loss: 0.0676 - acc: 0.980 - ETA: 5s - loss: 0.0671 - acc: 0.981 - ETA: 5s - loss: 0.0664 - acc: 0.981 - ETA: 5s - loss: 0.0659 - acc: 0.981 - ETA: 4s - loss: 0.0667 - acc: 0.980 - ETA: 4s - loss: 0.0662 - acc: 0.981 - ETA: 4s - loss: 0.0654 - acc: 0.981 - ETA: 4s - loss: 0.0651 - acc: 0.981 - ETA: 4s - loss: 0.0654 - acc: 0.981 - ETA: 4s - loss: 0.0648 - acc: 0.981 - ETA: 4s - loss: 0.0641 - acc: 0.981 - ETA: 4s - loss: 0.0668 - acc: 0.980 - ETA: 4s - loss: 0.0679 - acc: 0.980 - ETA: 4s - loss: 0.0688 - acc: 0.980 - ETA: 4s - loss: 0.0734 - acc: 0.980 - ETA: 4s - loss: 0.0728 - acc: 0.980 - ETA: 4s - loss: 0.0722 - acc: 0.980 - ETA: 4s - loss: 0.0724 - acc: 0.980 - ETA: 4s - loss: 0.0732 - acc: 0.980 - ETA: 4s - loss: 0.0727 - acc: 0.980 - ETA: 4s - loss: 0.0719 - acc: 0.981 - ETA: 4s - loss: 0.0730 - acc: 0.980 - ETA: 4s - loss: 0.0723 - acc: 0.981 - ETA: 3s - loss: 0.0734 - acc: 0.980 - ETA: 3s - loss: 0.0741 - acc: 0.980 - ETA: 3s - loss: 0.0736 - acc: 0.980 - ETA: 3s - loss: 0.0739 - acc: 0.980 - ETA: 3s - loss: 0.0733 - acc: 0.981 - ETA: 3s - loss: 0.0730 - acc: 0.980 - ETA: 3s - loss: 0.0766 - acc: 0.980 - ETA: 3s - loss: 0.0765 - acc: 0.980 - ETA: 3s - loss: 0.0758 - acc: 0.980 - ETA: 3s - loss: 0.0752 - acc: 0.980 - ETA: 3s - loss: 0.0751 - acc: 0.980 - ETA: 3s - loss: 0.0746 - acc: 0.980 - ETA: 3s - loss: 0.0744 - acc: 0.980 - ETA: 3s - loss: 0.0744 - acc: 0.980 - ETA: 3s - loss: 0.0747 - acc: 0.980 - ETA: 3s - loss: 0.0748 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0757 - acc: 0.979 - ETA: 2s - loss: 0.0764 - acc: 0.979 - ETA: 2s - loss: 0.0778 - acc: 0.978 - ETA: 2s - loss: 0.0772 - acc: 0.978 - ETA: 2s - loss: 0.0767 - acc: 0.979 - ETA: 2s - loss: 0.0770 - acc: 0.979 - ETA: 2s - loss: 0.0785 - acc: 0.978 - ETA: 2s - loss: 0.0780 - acc: 0.979 - ETA: 2s - loss: 0.0785 - acc: 0.979 - ETA: 2s - loss: 0.0781 - acc: 0.979 - ETA: 2s - loss: 0.0776 - acc: 0.979 - ETA: 2s - loss: 0.0773 - acc: 0.979 - ETA: 2s - loss: 0.0768 - acc: 0.979 - ETA: 2s - loss: 0.0777 - acc: 0.979 - ETA: 2s - loss: 0.0775 - acc: 0.979 - ETA: 2s - loss: 0.0772 - acc: 0.979 - ETA: 2s - loss: 0.0778 - acc: 0.978 - ETA: 2s - loss: 0.0776 - acc: 0.978 - ETA: 2s - loss: 0.0771 - acc: 0.979 - ETA: 1s - loss: 0.0770 - acc: 0.978 - ETA: 1s - loss: 0.0783 - acc: 0.978 - ETA: 1s - loss: 0.0784 - acc: 0.978 - ETA: 1s - loss: 0.0785 - acc: 0.978 - ETA: 1s - loss: 0.0779 - acc: 0.978 - ETA: 1s - loss: 0.0775 - acc: 0.978 - ETA: 1s - loss: 0.0770 - acc: 0.978 - ETA: 1s - loss: 0.0767 - acc: 0.978 - ETA: 1s - loss: 0.0762 - acc: 0.978 - ETA: 1s - loss: 0.0760 - acc: 0.978 - ETA: 1s - loss: 0.0755 - acc: 0.979 - ETA: 1s - loss: 0.0751 - acc: 0.979 - ETA: 1s - loss: 0.0747 - acc: 0.979 - ETA: 1s - loss: 0.0757 - acc: 0.979 - ETA: 1s - loss: 0.0755 - acc: 0.979 - ETA: 1s - loss: 0.0762 - acc: 0.978 - ETA: 1s - loss: 0.0761 - acc: 0.978 - ETA: 1s - loss: 0.0763 - acc: 0.978 - ETA: 1s - loss: 0.0759 - acc: 0.978 - ETA: 0s - loss: 0.0754 - acc: 0.978 - ETA: 0s - loss: 0.0751 - acc: 0.978 - ETA: 0s - loss: 0.0747 - acc: 0.979 - ETA: 0s - loss: 0.0743 - acc: 0.979 - ETA: 0s - loss: 0.0739 - acc: 0.979 - ETA: 0s - loss: 0.0736 - acc: 0.979 - ETA: 0s - loss: 0.0740 - acc: 0.979 - ETA: 0s - loss: 0.0741 - acc: 0.978 - ETA: 0s - loss: 0.0746 - acc: 0.978 - ETA: 0s - loss: 0.0742 - acc: 0.978 - ETA: 0s - loss: 0.0743 - acc: 0.978 - ETA: 0s - loss: 0.0742 - acc: 0.978 - ETA: 0s - loss: 0.0738 - acc: 0.978 - ETA: 0s - loss: 0.0734 - acc: 0.979 - ETA: 0s - loss: 0.0730 - acc: 0.979 - ETA: 0s - loss: 0.0727 - acc: 0.979 - ETA: 0s - loss: 0.0723 - acc: 0.979 - ETA: 0s - loss: 0.0723 - acc: 0.9793Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0723 - acc: 0.9792 - val_loss: 0.7066 - val_acc: 0.8515\n"
     ]
    }
   ],
   "source": [
    "results = [with_bottleneck(network) for network in networks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40.550239234449762, 'saved_models/weights.best.VGG19.hdf5'), (79.904306220095691, 'saved_models/weights.best.Resnet50.hdf5'), (80.502392344497608, 'saved_models/weights.best.InceptionV3.hdf5'), (85.167464114832541, 'saved_models/weights.best.Xception.hdf5')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, regularizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_merged_model(network1, network2):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network1))\n",
    "    train_VGG16 = bottleneck_features['train']\n",
    "    valid_VGG16 = bottleneck_features['valid']\n",
    "    test_VGG16 = bottleneck_features['test']\n",
    "\n",
    "    VGG16_model = Sequential()\n",
    "    VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "    \n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network2))\n",
    "    train_network2 = bottleneck_features['train']\n",
    "    valid_network2 = bottleneck_features['valid']\n",
    "    test_network2 = bottleneck_features['test']\n",
    "\n",
    "    model_2 = Sequential()\n",
    "    model_2.add(GlobalAveragePooling2D(input_shape=train_network2.shape[1:]))\n",
    "    \n",
    "    merged = Merge([VGG16_model, model_2], mode='sum')\n",
    "    \n",
    "    merged_model = Sequential()\n",
    "    merged_model.add(merged)\n",
    "    merged_model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    merged_model.add(Dropout(0.7))\n",
    "    merged_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "    merged_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    # merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    merged_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}_{}.hdf5'.format(network1, network2)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    merged_model.fit([train_VGG16, train_network2], train_targets, \n",
    "              validation_data=([valid_VGG16, valid_network2], valid_targets),\n",
    "              epochs=15, batch_size=30, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    merged_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    merged_predictions = [np.argmax(merged_model.predict([np.expand_dims(feature[0], axis=0), np.expand_dims(feature[1], axis=0)])) for feature in zip(test_VGG16, test_network2)]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "    return (test_accuracy, VGG16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6570/6680 [============================>.] - ETA: 0s - loss: 4.5137 - acc: 0.0860Epoch 00000: val_loss improved from inf to 3.29262, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 4.5037 - acc: 0.0870 - val_loss: 3.2926 - val_acc: 0.5054\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0727 - acc: 0.3163Epoch 00001: val_loss improved from 3.29262 to 1.86217, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 3.0686 - acc: 0.3169 - val_loss: 1.8622 - val_acc: 0.6719\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1545 - acc: 0.4778Epoch 00002: val_loss improved from 1.86217 to 1.25696, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 2.1544 - acc: 0.4780 - val_loss: 1.2570 - val_acc: 0.7497\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.6618 - acc: 0.5722Epoch 00003: val_loss improved from 1.25696 to 0.96798, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.6595 - acc: 0.5729 - val_loss: 0.9680 - val_acc: 0.7760\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3628 - acc: 0.6416Epoch 00004: val_loss improved from 0.96798 to 0.80073, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.3609 - acc: 0.6425 - val_loss: 0.8007 - val_acc: 0.8048\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1494 - acc: 0.6896Epoch 00005: val_loss improved from 0.80073 to 0.71062, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.1499 - acc: 0.6891 - val_loss: 0.7106 - val_acc: 0.8168\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0102 - acc: 0.7222Epoch 00006: val_loss improved from 0.71062 to 0.64454, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.0116 - acc: 0.7216 - val_loss: 0.6445 - val_acc: 0.8240\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9233 - acc: 0.7419Epoch 00007: val_loss improved from 0.64454 to 0.60130, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.9228 - acc: 0.7415 - val_loss: 0.6013 - val_acc: 0.8359\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7640Epoch 00008: val_loss improved from 0.60130 to 0.55887, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.8232 - acc: 0.7644 - val_loss: 0.5589 - val_acc: 0.8359\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7776 - acc: 0.7789Epoch 00009: val_loss improved from 0.55887 to 0.53556, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.7763 - acc: 0.7793 - val_loss: 0.5356 - val_acc: 0.8431\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7024 - acc: 0.7961Epoch 00010: val_loss improved from 0.53556 to 0.51437, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.7020 - acc: 0.7961 - val_loss: 0.5144 - val_acc: 0.8491\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6619 - acc: 0.8057Epoch 00011: val_loss improved from 0.51437 to 0.50287, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.6621 - acc: 0.8058 - val_loss: 0.5029 - val_acc: 0.8563\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6151 - acc: 0.8204Epoch 00012: val_loss improved from 0.50287 to 0.48812, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.6155 - acc: 0.8205 - val_loss: 0.4881 - val_acc: 0.8479\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5993 - acc: 0.8275Epoch 00013: val_loss improved from 0.48812 to 0.47834, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.6008 - acc: 0.8274 - val_loss: 0.4783 - val_acc: 0.8623\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.8412Epoch 00014: val_loss improved from 0.47834 to 0.46808, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.5443 - acc: 0.8412 - val_loss: 0.4681 - val_acc: 0.8575\n"
     ]
    }
   ],
   "source": [
    "merged_model = train_merged_model('Xception', 'Resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85.047846889952154, <keras.models.Sequential object at 0x7f0b3ea5e5f8>)\n"
     ]
    }
   ],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "networks = [\"VGG19\", \"Resnet50\", \"InceptionV3\", \"Xception\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, regularizers\n",
    "import numpy as np\n",
    "\n",
    "def create_entry_module(network):\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_network = bottleneck_features['train']\n",
    "    #valid_network = bottleneck_features['valid']\n",
    "    #test_network = bottleneck_features['test']\n",
    "\n",
    "    input_data = Input(shape=train_network.shape[1:])\n",
    "    x = GlobalAveragePooling2D()(input_data)\n",
    "    \n",
    "    return x, bottleneck_features, input_data\n",
    "    \n",
    "def train_merged_model(model_types, merge_type):\n",
    "    ### Obtain Bottleneck Features\n",
    "    print(\"==========models with {} and {}\".format(model_types, merge_type))\n",
    "    entry_modules = [create_entry_module(model_type) for model_type in model_types]\n",
    "    \n",
    "    only_modules = [module[0] for module in entry_modules]\n",
    "    if merge_type == \"add\":\n",
    "        merged = layers.add(only_modules)\n",
    "    elif merge_type == \"concat\":\n",
    "        merged = layers.concatenate(only_modules, axis=1)\n",
    "    elif merge_type == \"multiply\":\n",
    "        merged = layers.multiply(only_modules)\n",
    "    elif merge_type == \"average\":\n",
    "        merged = layers.average(only_modules)\n",
    "    elif merge_type == \"maximum\":\n",
    "        merged = layers.average(only_modules)\n",
    "    \n",
    "    x = Dense(512, kernel_initializer='he_normal')(merged)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #merged_model.add(Dense(512, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    x = Dropout(0.7)(x)\n",
    "    x = Dense(133, activation='softmax')(x)\n",
    "    \n",
    "    merged_model = Model([module[2] for module in entry_modules], x)\n",
    "    merged_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    # merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(\"_\".join(model_types))\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "    \n",
    "    extract_data =lambda phase: [model[1][phase] for model in entry_modules]\n",
    "    merged_model.fit(extract_data('train'), train_targets, \n",
    "              validation_data=(extract_data('valid'), valid_targets),\n",
    "              epochs=15, batch_size=30, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    merged_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    merged_predictions = [np.argmax(merged_model.predict([np.expand_dims(feature, axis=0) for feature in features])) \n",
    "                          for features in zip(*extract_data('test'))]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "    return (test_accuracy, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_145 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_146 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_147 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_153 (Gl (None, 2048)          0           input_145[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_154 (Gl (None, 2048)          0           input_146[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_155 (Gl (None, 2048)          0           input_147[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_36 (Add)                     (None, 2048)          0           global_average_pooling2d_153[0][0\n",
      "                                                                   global_average_pooling2d_154[0][0\n",
      "                                                                   global_average_pooling2d_155[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_117 (Dense)                (None, 512)           1049088     add_36[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 512)           0           dense_117[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 512)           0           activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_118 (Dense)                (None, 133)           68229       dropout_59[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.8997 - acc: 0.0867Epoch 00000: val_loss improved from inf to 2.67718, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 4.8851 - acc: 0.0882 - val_loss: 2.6772 - val_acc: 0.4886\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.9989 - acc: 0.2914- ETA: 0s - loss: 3.0551 - acc:Epoch 00001: val_loss improved from 2.67718 to 1.25171, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.9951 - acc: 0.2921 - val_loss: 1.2517 - val_acc: 0.6838\n",
      "Epoch 3/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.2818 - acc: 0.4159- ETA: 0s - loss: 2.2935 - acc: 0.Epoch 00002: val_loss improved from 1.25171 to 0.87837, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.2808 - acc: 0.4157 - val_loss: 0.8784 - val_acc: 0.7389\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9156 - acc: 0.4841- ETA: 3s - loss: 1.9725 -Epoch 00003: val_loss improved from 0.87837 to 0.73533, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9130 - acc: 0.4847 - val_loss: 0.7353 - val_acc: 0.7820\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7155 - acc: 0.5444Epoch 00004: val_loss improved from 0.73533 to 0.62038, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7118 - acc: 0.5446 - val_loss: 0.6204 - val_acc: 0.8060\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5824 - acc: 0.5611Epoch 00005: val_loss improved from 0.62038 to 0.56888, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.5794 - acc: 0.5615 - val_loss: 0.5689 - val_acc: 0.8311\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5007 - acc: 0.5865Epoch 00006: val_loss improved from 0.56888 to 0.55440, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.4997 - acc: 0.5874 - val_loss: 0.5544 - val_acc: 0.8335\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4384 - acc: 0.6062Epoch 00007: val_loss improved from 0.55440 to 0.53437, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.4347 - acc: 0.6064 - val_loss: 0.5344 - val_acc: 0.8323\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3836 - acc: 0.6236Epoch 00008: val_loss improved from 0.53437 to 0.52345, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3848 - acc: 0.6232 - val_loss: 0.5235 - val_acc: 0.8275\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3422 - acc: 0.6367Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.3467 - acc: 0.6368 - val_loss: 0.5845 - val_acc: 0.8180\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3041 - acc: 0.6461Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.3032 - acc: 0.6457 - val_loss: 0.5845 - val_acc: 0.8299\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2603 - acc: 0.6564- ETA: 0s - loss: 1.2778 - Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2629 - acc: 0.6564 - val_loss: 0.5539 - val_acc: 0.8287\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2166 - acc: 0.6689Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2164 - acc: 0.6687 - val_loss: 0.5289 - val_acc: 0.8419\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2299 - acc: 0.6641- ETA: 3s - ETA: 1s - loss: 1.2374 - ETA: 0s - loss: 1.2357 - acc: 0.Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2277 - acc: 0.6644 - val_loss: 0.5322 - val_acc: 0.8371\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6811Epoch 00014: val_loss improved from 0.52345 to 0.50693, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1939 - acc: 0.6808 - val_loss: 0.5069 - val_acc: 0.8599\n",
      "==========models with ('Resnet50', 'InceptionV3') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_148 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_149 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_156 (Gl (None, 2048)          0           input_148[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_157 (Gl (None, 2048)          0           input_149[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_37 (Add)                     (None, 2048)          0           global_average_pooling2d_156[0][0\n",
      "                                                                   global_average_pooling2d_157[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_119 (Dense)                (None, 512)           1049088     add_37[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 512)           0           dense_119[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 512)           0           activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_120 (Dense)                (None, 133)           68229       dropout_60[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 4.3108 - acc: 0.1418Epoch 00000: val_loss improved from inf to 1.89192, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 4.3023 - acc: 0.1427 - val_loss: 1.8919 - val_acc: 0.6156\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4086 - acc: 0.4107Epoch 00001: val_loss improved from 1.89192 to 0.95637, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 2.4075 - acc: 0.4102 - val_loss: 0.9564 - val_acc: 0.7485\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.8455 - acc: 0.5198Epoch 00002: val_loss improved from 0.95637 to 0.72451, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.8411 - acc: 0.5211 - val_loss: 0.7245 - val_acc: 0.7796\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5538 - acc: 0.5777Epoch 00003: val_loss improved from 0.72451 to 0.60544, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.5516 - acc: 0.5774 - val_loss: 0.6054 - val_acc: 0.8263\n",
      "Epoch 5/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3806 - acc: 0.6228Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.3789 - acc: 0.6234 - val_loss: 0.6086 - val_acc: 0.8000\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3354 - acc: 0.6416Epoch 00005: val_loss improved from 0.60544 to 0.55830, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.3352 - acc: 0.6419 - val_loss: 0.5583 - val_acc: 0.8120\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2283 - acc: 0.6694Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.2295 - acc: 0.6693 - val_loss: 0.5716 - val_acc: 0.8204\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6769Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.1856 - acc: 0.6772 - val_loss: 0.5657 - val_acc: 0.8383\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1504 - acc: 0.6830Epoch 00008: val_loss improved from 0.55830 to 0.45960, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.1520 - acc: 0.6829 - val_loss: 0.4596 - val_acc: 0.8359\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0877 - acc: 0.7024Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0873 - acc: 0.7025 - val_loss: 0.5454 - val_acc: 0.8263\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.7063Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.1077 - acc: 0.7055 - val_loss: 0.4853 - val_acc: 0.8599\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0658 - acc: 0.7095Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0725 - acc: 0.7085 - val_loss: 0.5786 - val_acc: 0.8371\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0404 - acc: 0.7262Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0457 - acc: 0.7256 - val_loss: 0.5415 - val_acc: 0.8359\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9882 - acc: 0.7299Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.9892 - acc: 0.7295 - val_loss: 0.5224 - val_acc: 0.8539\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9997 - acc: 0.7338Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.9976 - acc: 0.7335 - val_loss: 0.5251 - val_acc: 0.8467\n",
      "==========models with ('Resnet50', 'Xception') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_150 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_151 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_158 (Gl (None, 2048)          0           input_150[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_159 (Gl (None, 2048)          0           input_151[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_38 (Add)                     (None, 2048)          0           global_average_pooling2d_158[0][0\n",
      "                                                                   global_average_pooling2d_159[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_121 (Dense)                (None, 512)           1049088     add_38[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 512)           0           dense_121[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 512)           0           activation_69[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_122 (Dense)                (None, 133)           68229       dropout_61[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.9509 - acc: 0.1697Epoch 00000: val_loss improved from inf to 1.70577, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 3.9367 - acc: 0.1713 - val_loss: 1.7058 - val_acc: 0.6180\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.1204 - acc: 0.4502Epoch 00001: val_loss improved from 1.70577 to 0.85347, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.1148 - acc: 0.4510 - val_loss: 0.8535 - val_acc: 0.7461\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5429 - acc: 0.5767Epoch 00002: val_loss improved from 0.85347 to 0.69322, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.5441 - acc: 0.5769 - val_loss: 0.6932 - val_acc: 0.7796\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3264 - acc: 0.6235Epoch 00003: val_loss improved from 0.69322 to 0.58897, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.3246 - acc: 0.6244 - val_loss: 0.5890 - val_acc: 0.8132\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6589Epoch 00004: val_loss improved from 0.58897 to 0.55494, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s - loss: 1.1926 - acc: 0.6587 - val_loss: 0.5549 - val_acc: 0.8311\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0952 - acc: 0.6868Epoch 00005: val_loss improved from 0.55494 to 0.52410, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0934 - acc: 0.6871 - val_loss: 0.5241 - val_acc: 0.8299\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0296 - acc: 0.7023Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 1.0280 - acc: 0.7027 - val_loss: 0.5421 - val_acc: 0.8359\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.7202Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.9808 - acc: 0.7199 - val_loss: 0.5386 - val_acc: 0.8287\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9151 - acc: 0.7376Epoch 00008: val_loss improved from 0.52410 to 0.49290, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9156 - acc: 0.7377 - val_loss: 0.4929 - val_acc: 0.8479\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.7488Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8827 - acc: 0.7488 - val_loss: 0.5808 - val_acc: 0.8359\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.7586Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8455 - acc: 0.7590 - val_loss: 0.4972 - val_acc: 0.8539\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7936 - acc: 0.7824Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7946 - acc: 0.7826 - val_loss: 0.4977 - val_acc: 0.8587\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7676Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8143 - acc: 0.7681 - val_loss: 0.5308 - val_acc: 0.8491\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7792Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7810 - acc: 0.7793 - val_loss: 0.5199 - val_acc: 0.8515\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7781 - acc: 0.7841Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7778 - acc: 0.7837 - val_loss: 0.5036 - val_acc: 0.8563\n",
      "==========models with ('InceptionV3', 'Xception') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_152 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_153 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_160 (Gl (None, 2048)          0           input_152[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_161 (Gl (None, 2048)          0           input_153[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_39 (Add)                     (None, 2048)          0           global_average_pooling2d_160[0][0\n",
      "                                                                   global_average_pooling2d_161[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_123 (Dense)                (None, 512)           1049088     add_39[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_70 (Activation)       (None, 512)           0           dense_123[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 512)           0           activation_70[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_124 (Dense)                (None, 133)           68229       dropout_62[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.4083 - acc: 0.2880- ETA: 1s - loss: 3.8577 - acc: 0. - ETA: 1s - loss: 3.7593  - ETA: 0s - loss: 3.4831 - acc: 0.Epoch 00000: val_loss improved from inf to 1.04773, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 3.3925 - acc: 0.2907 - val_loss: 1.0477 - val_acc: 0.7329\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6788 - acc: 0.5724- ETA: 3s - loss: 1.8288 Epoch 00001: val_loss improved from 1.04773 to 0.70742, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6802 - acc: 0.5723 - val_loss: 0.7074 - val_acc: 0.7725\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3129 - acc: 0.6473- ETA - ETA: 0s - loss: 1.3184 - acc:Epoch 00002: val_loss improved from 0.70742 to 0.58277, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3097 - acc: 0.6487 - val_loss: 0.5828 - val_acc: 0.8180\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1728 - acc: 0.6888Epoch 00003: val_loss improved from 0.58277 to 0.53712, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1688 - acc: 0.6894 - val_loss: 0.5371 - val_acc: 0.8311\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0922 - acc: 0.7086Epoch 00004: val_loss improved from 0.53712 to 0.51356, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0888 - acc: 0.7094 - val_loss: 0.5136 - val_acc: 0.8371\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0047 - acc: 0.7295Epoch 00005: val_loss improved from 0.51356 to 0.50224, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0081 - acc: 0.7293 - val_loss: 0.5022 - val_acc: 0.8479\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9730 - acc: 0.7417- ETA: 1s -Epoch 00006: val_loss improved from 0.50224 to 0.45790, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9708 - acc: 0.7425 - val_loss: 0.4579 - val_acc: 0.8587\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.7592- ETA: 3s - loss: 0.8410 - acc: - ETA: 0s - loss: 0.9177 - acc:Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9184 - acc: 0.7603 - val_loss: 0.5367 - val_acc: 0.8479\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8986 - acc: 0.7617- ETA: 2s - - ETA: 1s - loss: 0.9Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8999 - acc: 0.7621 - val_loss: 0.5036 - val_acc: 0.8599\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.7761Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8711 - acc: 0.7753 - val_loss: 0.5003 - val_acc: 0.8467\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7767Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8470 - acc: 0.7768 - val_loss: 0.5532 - val_acc: 0.8611\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8552 - acc: 0.7833Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8559 - acc: 0.7832 - val_loss: 0.5693 - val_acc: 0.8515\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7923Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8073 - acc: 0.7912 - val_loss: 0.5580 - val_acc: 0.8515\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8126 - acc: 0.7953- ETA: 1s - loss: 0.8 - ETA: 0s - loss: 0.8090 - Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8117 - acc: 0.7955 - val_loss: 0.5373 - val_acc: 0.8539\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.8041- EEpoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.7984 - acc: 0.8034 - val_loss: 0.5627 - val_acc: 0.8611\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_154 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_155 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_156 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_162 (Gl (None, 2048)          0           input_154[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_163 (Gl (None, 2048)          0           input_155[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_164 (Gl (None, 2048)          0           input_156[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 6144)          0           global_average_pooling2d_162[0][0\n",
      "                                                                   global_average_pooling2d_163[0][0\n",
      "                                                                   global_average_pooling2d_164[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_125 (Dense)                (None, 512)           3146240     concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_71 (Activation)       (None, 512)           0           dense_125[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           activation_71[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_126 (Dense)                (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3,214,469\n",
      "Trainable params: 3,214,469\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 3.5563 - acc: 0.2949Epoch 00000: val_loss improved from inf to 0.93777, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 10s - loss: 3.5514 - acc: 0.2957 - val_loss: 0.9378 - val_acc: 0.7341\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7184 - acc: 0.5620Epoch 00001: val_loss improved from 0.93777 to 0.64874, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.7177 - acc: 0.5624 - val_loss: 0.6487 - val_acc: 0.7916\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3785 - acc: 0.6486Epoch 00002: val_loss improved from 0.64874 to 0.50104, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.3782 - acc: 0.6488 - val_loss: 0.5010 - val_acc: 0.8431\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1923 - acc: 0.6884Epoch 00003: val_loss improved from 0.50104 to 0.47235, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.1902 - acc: 0.6886 - val_loss: 0.4724 - val_acc: 0.8707\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0896 - acc: 0.7250Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 1.0863 - acc: 0.7253 - val_loss: 0.4805 - val_acc: 0.8527\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0476 - acc: 0.7397Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 1.0478 - acc: 0.7400 - val_loss: 0.5210 - val_acc: 0.8515\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.7554Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9453 - acc: 0.7557 - val_loss: 0.5269 - val_acc: 0.8575\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9529 - acc: 0.7623Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9518 - acc: 0.7629 - val_loss: 0.4930 - val_acc: 0.8623\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7735Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9149 - acc: 0.7737 - val_loss: 0.5046 - val_acc: 0.8743\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7879Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8842 - acc: 0.7877 - val_loss: 0.5086 - val_acc: 0.8611\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.8027Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8444 - acc: 0.8024 - val_loss: 0.5232 - val_acc: 0.8814\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.8081Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8162 - acc: 0.8085 - val_loss: 0.5007 - val_acc: 0.8790\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8283 - acc: 0.8086Epoch 00012: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 9s - loss: 0.8275 - acc: 0.8084 - val_loss: 0.5629 - val_acc: 0.8683\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8297 - acc: 0.8109Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8288 - acc: 0.8111 - val_loss: 0.5409 - val_acc: 0.8766\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.8201Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.7797 - acc: 0.8199 - val_loss: 0.6123 - val_acc: 0.8754\n",
      "==========models with ('Resnet50', 'InceptionV3') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_157 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_158 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_165 (Gl (None, 2048)          0           input_157[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_166 (Gl (None, 2048)          0           input_158[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 4096)          0           global_average_pooling2d_165[0][0\n",
      "                                                                   global_average_pooling2d_166[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_127 (Dense)                (None, 512)           2097664     concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_72 (Activation)       (None, 512)           0           dense_127[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (None, 512)           0           activation_72[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_128 (Dense)                (None, 133)           68229       dropout_64[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.6149 - acc: 0.2711Epoch 00000: val_loss improved from inf to 1.04501, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 3.6045 - acc: 0.2720 - val_loss: 1.0450 - val_acc: 0.7413\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7775 - acc: 0.5492Epoch 00001: val_loss improved from 1.04501 to 0.61549, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7742 - acc: 0.5506 - val_loss: 0.6155 - val_acc: 0.8120\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3776 - acc: 0.6336Epoch 00002: val_loss improved from 0.61549 to 0.54531, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3736 - acc: 0.6337 - val_loss: 0.5453 - val_acc: 0.8311\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1985 - acc: 0.6868Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.1985 - acc: 0.6865 - val_loss: 0.5761 - val_acc: 0.8335\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.7070Epoch 00004: val_loss improved from 0.54531 to 0.50812, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1021 - acc: 0.7075 - val_loss: 0.5081 - val_acc: 0.8419\n",
      "Epoch 6/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.0383 - acc: 0.7284Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.0362 - acc: 0.7287 - val_loss: 0.5516 - val_acc: 0.8443\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.7438Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9684 - acc: 0.7446 - val_loss: 0.5391 - val_acc: 0.8539\n",
      "Epoch 8/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7674Epoch 00007: val_loss improved from 0.50812 to 0.50195, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9305 - acc: 0.7680 - val_loss: 0.5019 - val_acc: 0.8599\n",
      "Epoch 9/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7725Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9096 - acc: 0.7725 - val_loss: 0.5187 - val_acc: 0.8575\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8381 - acc: 0.7845Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8430 - acc: 0.7843 - val_loss: 0.6567 - val_acc: 0.8407\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8936 - acc: 0.7848Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8906 - acc: 0.7850 - val_loss: 0.5981 - val_acc: 0.8527\n",
      "Epoch 12/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8340 - acc: 0.7977Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8335 - acc: 0.7979 - val_loss: 0.5631 - val_acc: 0.8623\n",
      "Epoch 13/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.8033Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8044 - acc: 0.8036 - val_loss: 0.6673 - val_acc: 0.8515\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.8080Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8218 - acc: 0.8078 - val_loss: 0.6426 - val_acc: 0.8695\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.8106Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.7783 - acc: 0.8102 - val_loss: 0.6262 - val_acc: 0.8563\n",
      "==========models with ('Resnet50', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_159 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_160 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_167 (Gl (None, 2048)          0           input_159[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_168 (Gl (None, 2048)          0           input_160[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 4096)          0           global_average_pooling2d_167[0][0\n",
      "                                                                   global_average_pooling2d_168[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_129 (Dense)                (None, 512)           2097664     concatenate_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, 512)           0           dense_129[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (None, 512)           0           activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_130 (Dense)                (None, 133)           68229       dropout_65[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.2416 - acc: 0.3047Epoch 00000: val_loss improved from inf to 1.01537, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.2344 - acc: 0.3058 - val_loss: 1.0154 - val_acc: 0.7150\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5413 - acc: 0.5872Epoch 00001: val_loss improved from 1.01537 to 0.62521, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.5370 - acc: 0.5883 - val_loss: 0.6252 - val_acc: 0.7964\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2116 - acc: 0.6780Epoch 00002: val_loss improved from 0.62521 to 0.54739, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.2094 - acc: 0.6784 - val_loss: 0.5474 - val_acc: 0.8192\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.7222Epoch 00003: val_loss improved from 0.54739 to 0.48773, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.0058 - acc: 0.7220 - val_loss: 0.4877 - val_acc: 0.8383\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.7478Epoch 00004: val_loss improved from 0.48773 to 0.47498, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.9198 - acc: 0.7479 - val_loss: 0.4750 - val_acc: 0.8491\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7585Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.8754 - acc: 0.7582 - val_loss: 0.4929 - val_acc: 0.8455\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7705 - acc: 0.7861Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.7724 - acc: 0.7862 - val_loss: 0.5014 - val_acc: 0.8491\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.8005Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.7336 - acc: 0.8001 - val_loss: 0.5313 - val_acc: 0.8431\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7377 - acc: 0.8017Epoch 00008: val_loss improved from 0.47498 to 0.43486, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.7393 - acc: 0.8019 - val_loss: 0.4349 - val_acc: 0.8635\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.8158Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6910 - acc: 0.8153 - val_loss: 0.4760 - val_acc: 0.8599\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6799 - acc: 0.8178Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6844 - acc: 0.8174 - val_loss: 0.5402 - val_acc: 0.8587\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.8312Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6383 - acc: 0.8308 - val_loss: 0.5183 - val_acc: 0.8563\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6245 - acc: 0.8407Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6227 - acc: 0.8409 - val_loss: 0.4924 - val_acc: 0.8766\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.8460Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6151 - acc: 0.8452 - val_loss: 0.5941 - val_acc: 0.8587\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8456Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.5760 - acc: 0.8451 - val_loss: 0.5577 - val_acc: 0.8743\n",
      "==========models with ('InceptionV3', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_161 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_162 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_169 (Gl (None, 2048)          0           input_161[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_170 (Gl (None, 2048)          0           input_162[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)     (None, 4096)          0           global_average_pooling2d_169[0][0\n",
      "                                                                   global_average_pooling2d_170[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_131 (Dense)                (None, 512)           2097664     concatenate_17[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, 512)           0           dense_131[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 512)           0           activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_132 (Dense)                (None, 133)           68229       dropout_66[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7659 - acc: 0.4036Epoch 00000: val_loss improved from inf to 0.72083, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.7564 - acc: 0.4055 - val_loss: 0.7208 - val_acc: 0.7844\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3499 - acc: 0.6558Epoch 00001: val_loss improved from 0.72083 to 0.57421, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.3473 - acc: 0.6560 - val_loss: 0.5742 - val_acc: 0.8275\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1002 - acc: 0.7089Epoch 00002: val_loss improved from 0.57421 to 0.50415, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.0988 - acc: 0.7090 - val_loss: 0.5041 - val_acc: 0.8455\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9743 - acc: 0.7480Epoch 00003: val_loss improved from 0.50415 to 0.46462, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.9736 - acc: 0.7479 - val_loss: 0.4646 - val_acc: 0.8539\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7635Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 0.9063 - acc: 0.7633 - val_loss: 0.5096 - val_acc: 0.8563\n",
      "Epoch 6/15\n",
      "1470/6680 [=====>........................] - ETA: 5s - loss: 0.9559 - acc: 0.7707"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d5a64668d4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n\u001b[0;32m----> 2\u001b[0;31m                                                   [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-d5a64668d4a2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                   [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]\n",
      "\u001b[0;32m<ipython-input-59-547bfb7dd80b>\u001b[0m in \u001b[0;36mtrain_merged_model\u001b[0;34m(model_types, merge_type)\u001b[0m\n\u001b[1;32m     56\u001b[0m     merged_model.fit(extract_data('train'), train_targets, \n\u001b[1;32m     57\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m               epochs=15, batch_size=30, callbacks=[checkpointer], verbose=1)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m### Load the Model with the Best Validation Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2229\u001b[0m                               feed_dict=feed_dict)\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muninitialized_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n",
    "                                                  [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glob (None, 2048)          0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glob (None, 2048)          0           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glob (None, 2048)          0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 6144)          0           global_average_pooling2d_3[0][0] \n",
      "                                                                   global_average_pooling2d_4[0][0] \n",
      "                                                                   global_average_pooling2d_5[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           3146240     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 133)           68229       dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,214,469\n",
      "Trainable params: 3,214,469\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.6051 - acc: 0.2880Epoch 00000: val_loss improved from inf to 0.94420, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 3.5888 - acc: 0.2901 - val_loss: 0.9442 - val_acc: 0.7281\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7680 - acc: 0.5632Epoch 00001: val_loss improved from 0.94420 to 0.66351, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7655 - acc: 0.5641 - val_loss: 0.6635 - val_acc: 0.7892\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3377 - acc: 0.6573Epoch 00002: val_loss improved from 0.66351 to 0.55809, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3389 - acc: 0.6564 - val_loss: 0.5581 - val_acc: 0.8263\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2431 - acc: 0.6812Epoch 00003: val_loss improved from 0.55809 to 0.51183, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.2461 - acc: 0.6813 - val_loss: 0.5118 - val_acc: 0.8431\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0973 - acc: 0.7194Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.0949 - acc: 0.7199 - val_loss: 0.5190 - val_acc: 0.8443\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0447 - acc: 0.7429Epoch 00005: val_loss improved from 0.51183 to 0.51057, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0419 - acc: 0.7437 - val_loss: 0.5106 - val_acc: 0.8491\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9503 - acc: 0.7615Epoch 00006: val_loss improved from 0.51057 to 0.50141, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9538 - acc: 0.7600 - val_loss: 0.5014 - val_acc: 0.8539\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9367 - acc: 0.7662Epoch 00007: val_loss improved from 0.50141 to 0.49606, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9335 - acc: 0.7666 - val_loss: 0.4961 - val_acc: 0.8575\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8934 - acc: 0.7785Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8969 - acc: 0.7789 - val_loss: 0.5194 - val_acc: 0.8707\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8542 - acc: 0.7861Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8507 - acc: 0.7868 - val_loss: 0.5806 - val_acc: 0.8659\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8264 - acc: 0.8024Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8260 - acc: 0.8024 - val_loss: 0.5466 - val_acc: 0.8671\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7997Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8412 - acc: 0.7988 - val_loss: 0.6093 - val_acc: 0.8659\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8035 - acc: 0.8074Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8060 - acc: 0.8069 - val_loss: 0.5677 - val_acc: 0.8778\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7757 - acc: 0.8250Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7753 - acc: 0.8253 - val_loss: 0.6559 - val_acc: 0.8671\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.8250Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7927 - acc: 0.8247 - val_loss: 0.5607 - val_acc: 0.8766\n",
      "==========models with ('Resnet50', 'InceptionV3') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glob (None, 2048)          0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glob (None, 2048)          0           input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 4096)          0           global_average_pooling2d_6[0][0] \n",
      "                                                                   global_average_pooling2d_7[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 512)           2097664     concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 512)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 512)           0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 133)           68229       dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.5214 - acc: 0.2647Epoch 00000: val_loss improved from inf to 1.03654, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 3.5076 - acc: 0.2665 - val_loss: 1.0365 - val_acc: 0.7246\n",
      "Epoch 2/15\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 1.7712 - acc: 0.5451Epoch 00001: val_loss improved from 1.03654 to 0.64988, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.7615 - acc: 0.5482 - val_loss: 0.6499 - val_acc: 0.7880\n",
      "Epoch 3/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3906 - acc: 0.6369Epoch 00002: val_loss improved from 0.64988 to 0.57836, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.3919 - acc: 0.6368 - val_loss: 0.5784 - val_acc: 0.8216\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2021 - acc: 0.6723Epoch 00003: val_loss improved from 0.57836 to 0.52255, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1981 - acc: 0.6731 - val_loss: 0.5226 - val_acc: 0.8455\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1153 - acc: 0.6998Epoch 00004: val_loss improved from 0.52255 to 0.51649, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 1.1151 - acc: 0.7004 - val_loss: 0.5165 - val_acc: 0.8527\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0567 - acc: 0.7220Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 1.0603 - acc: 0.7214 - val_loss: 0.5460 - val_acc: 0.8431\n",
      "Epoch 7/15\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.9752 - acc: 0.7405Epoch 00006: val_loss improved from 0.51649 to 0.50435, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 0.9799 - acc: 0.7400 - val_loss: 0.5044 - val_acc: 0.8539\n",
      "Epoch 8/15\n",
      "6570/6680 [============================>.] - ETA: 0s - loss: 0.9167 - acc: 0.7495Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.9203 - acc: 0.7497 - val_loss: 0.5337 - val_acc: 0.8467\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8983 - acc: 0.7685Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.8966 - acc: 0.7684 - val_loss: 0.5444 - val_acc: 0.8491\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8875 - acc: 0.7752Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.8890 - acc: 0.7746 - val_loss: 0.5150 - val_acc: 0.8563\n",
      "Epoch 11/15\n",
      "6570/6680 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7868Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.8695 - acc: 0.7867 - val_loss: 0.5898 - val_acc: 0.8563\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8157 - acc: 0.7860Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.8135 - acc: 0.7862 - val_loss: 0.5659 - val_acc: 0.8635\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7950Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.7709 - acc: 0.7946 - val_loss: 0.5724 - val_acc: 0.8683\n",
      "Epoch 14/15\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.8026Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.8098 - acc: 0.8022 - val_loss: 0.5765 - val_acc: 0.8695\n",
      "Epoch 15/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.7550 - acc: 0.8093Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s - loss: 0.7569 - acc: 0.8087 - val_loss: 0.5746 - val_acc: 0.8683\n",
      "==========models with ('Resnet50', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_7 (InputLayer)             (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glob (None, 2048)          0           input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glob (None, 2048)          0           input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 4096)          0           global_average_pooling2d_8[0][0] \n",
      "                                                                   global_average_pooling2d_9[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 512)           2097664     concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 512)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 512)           0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 133)           68229       dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.2490 - acc: 0.3003Epoch 00000: val_loss improved from inf to 1.02377, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 3.2406 - acc: 0.3021 - val_loss: 1.0238 - val_acc: 0.7162\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5369 - acc: 0.5941Epoch 00001: val_loss improved from 1.02377 to 0.68368, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.5425 - acc: 0.5933 - val_loss: 0.6837 - val_acc: 0.7844\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1800 - acc: 0.6747Epoch 00002: val_loss improved from 0.68368 to 0.60083, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.1810 - acc: 0.6744 - val_loss: 0.6008 - val_acc: 0.8108\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0341 - acc: 0.7166Epoch 00003: val_loss improved from 0.60083 to 0.50155, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.0367 - acc: 0.7156 - val_loss: 0.5016 - val_acc: 0.8407\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8954 - acc: 0.7528Epoch 00004: val_loss improved from 0.50155 to 0.45743, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 3s - loss: 0.8911 - acc: 0.7537 - val_loss: 0.4574 - val_acc: 0.8539\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8481 - acc: 0.7614Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.8500 - acc: 0.7614 - val_loss: 0.4913 - val_acc: 0.8359\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.7842Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.7997 - acc: 0.7834 - val_loss: 0.5577 - val_acc: 0.8204\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.7946Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.7354 - acc: 0.7948 - val_loss: 0.4993 - val_acc: 0.8539\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7005 - acc: 0.8103Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6992 - acc: 0.8105 - val_loss: 0.5023 - val_acc: 0.8467\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6884 - acc: 0.8145Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6881 - acc: 0.8145 - val_loss: 0.5064 - val_acc: 0.8599\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.8246Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6437 - acc: 0.8241 - val_loss: 0.5032 - val_acc: 0.8659\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6375 - acc: 0.8293Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6389 - acc: 0.8292 - val_loss: 0.4966 - val_acc: 0.8683\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.8374Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6283 - acc: 0.8376 - val_loss: 0.5047 - val_acc: 0.8611\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6110 - acc: 0.8412Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.6108 - acc: 0.8407 - val_loss: 0.5605 - val_acc: 0.8754\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.8498Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.5870 - acc: 0.8494 - val_loss: 0.5655 - val_acc: 0.8731\n",
      "==========models with ('InceptionV3', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Glo (None, 2048)          0           input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_11 (Glo (None, 2048)          0           input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 4096)          0           global_average_pooling2d_10[0][0]\n",
      "                                                                   global_average_pooling2d_11[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 512)           2097664     concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 512)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 512)           0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 133)           68229       dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.8073 - acc: 0.4126Epoch 00000: val_loss improved from inf to 0.80698, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.7953 - acc: 0.4148 - val_loss: 0.8070 - val_acc: 0.7713\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3564 - acc: 0.6492Epoch 00001: val_loss improved from 0.80698 to 0.54513, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.3553 - acc: 0.6491 - val_loss: 0.5451 - val_acc: 0.8275\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.7150Epoch 00002: val_loss improved from 0.54513 to 0.48767, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0892 - acc: 0.7147 - val_loss: 0.4877 - val_acc: 0.8443\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.7453Epoch 00003: val_loss improved from 0.48767 to 0.47163, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9806 - acc: 0.7461 - val_loss: 0.4716 - val_acc: 0.8503\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.7589Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.9256 - acc: 0.7593 - val_loss: 0.4940 - val_acc: 0.8515\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8618 - acc: 0.7823Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8632 - acc: 0.7820 - val_loss: 0.4976 - val_acc: 0.8659\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8215 - acc: 0.7862Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8217 - acc: 0.7856 - val_loss: 0.5239 - val_acc: 0.8683\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7745 - acc: 0.7982Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7721 - acc: 0.7982 - val_loss: 0.5469 - val_acc: 0.8695\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.8048Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7772 - acc: 0.8045 - val_loss: 0.5594 - val_acc: 0.8587\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.8136Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7427 - acc: 0.8136 - val_loss: 0.5511 - val_acc: 0.8575\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7321 - acc: 0.8189Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7327 - acc: 0.8183 - val_loss: 0.5263 - val_acc: 0.8695\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7140 - acc: 0.8298Epoch 00011: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s - loss: 0.7097 - acc: 0.8307 - val_loss: 0.5902 - val_acc: 0.8623\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6849 - acc: 0.8412Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.6890 - acc: 0.8407 - val_loss: 0.5492 - val_acc: 0.8695\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6719 - acc: 0.8391Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.6756 - acc: 0.8385 - val_loss: 0.5922 - val_acc: 0.8671\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.8432Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.6735 - acc: 0.8434 - val_loss: 0.6316 - val_acc: 0.8683\n"
     ]
    }
   ],
   "source": [
    "merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"concat\"], \n",
    "                                                  [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('concat', ('InceptionV3', 'Xception')) accuracy 81.45933014354067\n",
      "('concat', ('Resnet50', 'InceptionV3')) accuracy 83.37320574162679\n",
      "('concat', ('Resnet50', 'Xception')) accuracy 84.80861244019138\n",
      "('concat', ['Resnet50', 'InceptionV3', 'Xception']) accuracy 85.16746411483254\n"
     ]
    }
   ],
   "source": [
    "for model in sorted(merged_models, key=lambda x: x[1][0]):\n",
    "    print(\"{} accuracy {}\".format(model[0], model[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, regularizers\n",
    "import numpy as np\n",
    "\n",
    "def create_entry_module(network):\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_network = bottleneck_features['train']\n",
    "    #valid_network = bottleneck_features['valid']\n",
    "    #test_network = bottleneck_features['test']\n",
    "\n",
    "    input_data = Input(shape=train_network.shape[1:])\n",
    "    x = GlobalAveragePooling2D()(input_data)\n",
    "    \n",
    "    return x, bottleneck_features, input_data\n",
    "    \n",
    "def train_merged_model(ker1, l2_1, dr1, ker2, l2_2, dr2):\n",
    "    model_types = ['Resnet50', 'InceptionV3', 'Xception']\n",
    "    \n",
    "    ### Obtain Bottleneck Features\n",
    "    print(\"==========models with {} fileters1 {} l2 {} Dropout1 {} \\n filters2 {} l2 {} Dropout2 {}\".format(\n",
    "        model_types, ker1, l2_1, dr1, ker2, l2_2, dr2))\n",
    "    entry_modules = [create_entry_module(model_type) for model_type in model_types]\n",
    "    \n",
    "    only_modules = [module[0] for module in entry_modules]\n",
    "    \n",
    "    merged = layers.concatenate(only_modules, axis=1)\n",
    "    \n",
    "    x = Dense(ker1, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_1))(merged)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(dr1)(x)\n",
    "    #x = Dense(ker2, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_1))(x)\n",
    "    #x = Activation(\"relu\")(x)\n",
    "    #x = Dropout(dr2)(x)\n",
    "    x = Dense(133, activation='softmax')(x)\n",
    "    \n",
    "    merged_model = Model([module[2] for module in entry_modules], x)\n",
    "    merged_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    # merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(\"_\".join(model_types))\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "    \n",
    "    extract_data =lambda phase: [model[1][phase] for model in entry_modules]\n",
    "    merged_model.fit(extract_data('train'), train_targets, \n",
    "              validation_data=(extract_data('valid'), valid_targets),\n",
    "              epochs=15, batch_size=30, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    merged_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    merged_predictions = [np.argmax(merged_model.predict([np.expand_dims(feature, axis=0) for feature in features])) \n",
    "                          for features in zip(*extract_data('test'))]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "    return (test_accuracy, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_var = [0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_var = [0.8, 0.5, 0.3, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ker_var = [4096, 2048, 1024, 512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_prouct = list(itertools.product(ker_var, l2_var, dr_var, ker_var, l2_var, dr_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ver_product = [var_prouct[np.random.randint(len(var_prouct))] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1024, 1e-05, 0.8, 1024, 0.01, 0.1), (1024, 1e-05, 0.1, 2048, 0.01, 0.1), (2048, 0.01, 0.01, 1024, 1e-05, 0.3), (2048, 0.0001, 0.8, 1024, 1e-05, 0.5), (1024, 0.001, 0.8, 2048, 0.01, 0.5), (256, 0.0001, 0.1, 512, 0.0001, 0.3), (256, 1e-05, 0.5, 4096, 0.01, 0.01), (256, 0.0001, 0.3, 2048, 0.001, 0.8), (1024, 0.01, 0.5, 1024, 0.001, 0.01), (512, 0.01, 0.3, 1024, 0.01, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "print(rand_ver_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 1e-05 Dropout1 0.8 \n",
      " filters2 1024 l2 0.01 Dropout2 0.1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_49 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_50 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_51 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_51 (Glo (None, 2048)          0           input_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_52 (Glo (None, 2048)          0           input_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_53 (Glo (None, 2048)          0           input_51[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)     (None, 6144)          0           global_average_pooling2d_51[0][0]\n",
      "                                                                   global_average_pooling2d_52[0][0]\n",
      "                                                                   global_average_pooling2d_53[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (None, 1024)          6292480     concatenate_18[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 1024)          0           dense_48[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 1024)          0           activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_49 (Dense)                 (None, 133)           136325      dropout_31[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 7.3577 - acc: 0.1716Epoch 00000: val_loss improved from inf to 1.74129, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 7.3330 - acc: 0.1722 - val_loss: 1.7413 - val_acc: 0.5605\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5635 - acc: 0.4477Epoch 00001: val_loss improved from 1.74129 to 0.77915, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.5583 - acc: 0.4488 - val_loss: 0.7792 - val_acc: 0.7808\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.9991 - acc: 0.5548Epoch 00002: val_loss improved from 0.77915 to 0.71690, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.0009 - acc: 0.5543 - val_loss: 0.7169 - val_acc: 0.7749\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7910 - acc: 0.5938Epoch 00003: val_loss improved from 0.71690 to 0.58400, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.7877 - acc: 0.5942 - val_loss: 0.5840 - val_acc: 0.8251\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.6430 - acc: 0.6371Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.6427 - acc: 0.6364 - val_loss: 0.5879 - val_acc: 0.8323\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5126 - acc: 0.6597Epoch 00005: val_loss improved from 0.58400 to 0.56279, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.5132 - acc: 0.6593 - val_loss: 0.5628 - val_acc: 0.8467\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5124 - acc: 0.6680Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.5153 - acc: 0.6678 - val_loss: 0.5753 - val_acc: 0.8515\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4658 - acc: 0.6843Epoch 00007: val_loss improved from 0.56279 to 0.54858, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.4686 - acc: 0.6835 - val_loss: 0.5486 - val_acc: 0.8539\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4191 - acc: 0.6991Epoch 00008: val_loss improved from 0.54858 to 0.51689, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.4231 - acc: 0.6985 - val_loss: 0.5169 - val_acc: 0.8479\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4131 - acc: 0.6941Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.4137 - acc: 0.6942 - val_loss: 0.5463 - val_acc: 0.8611\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4041 - acc: 0.7074Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.4083 - acc: 0.7070 - val_loss: 0.5749 - val_acc: 0.8575\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3958 - acc: 0.7098Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3985 - acc: 0.7091 - val_loss: 0.6152 - val_acc: 0.8575\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3719 - acc: 0.7186Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3678 - acc: 0.7189 - val_loss: 0.5321 - val_acc: 0.8743\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2932 - acc: 0.7296Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.2929 - acc: 0.7292 - val_loss: 0.5870 - val_acc: 0.8623\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3335 - acc: 0.7264Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3326 - acc: 0.7269 - val_loss: 0.6241 - val_acc: 0.8635\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 1e-05 Dropout1 0.1 \n",
      " filters2 2048 l2 0.01 Dropout2 0.1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_52 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_53 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_54 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_54 (Glo (None, 2048)          0           input_52[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_55 (Glo (None, 2048)          0           input_53[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_56 (Glo (None, 2048)          0           input_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)     (None, 6144)          0           global_average_pooling2d_54[0][0]\n",
      "                                                                   global_average_pooling2d_55[0][0]\n",
      "                                                                   global_average_pooling2d_56[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 (None, 1024)          6292480     concatenate_19[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 1024)          0           dense_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 1024)          0           activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 (None, 133)           136325      dropout_32[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.9148 - acc: 0.5317Epoch 00000: val_loss improved from inf to 1.65565, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.8970 - acc: 0.5328 - val_loss: 1.6557 - val_acc: 0.6419\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7956Epoch 00001: val_loss improved from 1.65565 to 1.01342, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.8359 - acc: 0.7958 - val_loss: 1.0134 - val_acc: 0.7725\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8637Epoch 00002: val_loss improved from 1.01342 to 0.93692, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.5774 - acc: 0.8642 - val_loss: 0.9369 - val_acc: 0.8120\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8897Epoch 00003: val_loss improved from 0.93692 to 0.84680, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.4576 - acc: 0.8898 - val_loss: 0.8468 - val_acc: 0.8419\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.9155Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.3586 - acc: 0.9157 - val_loss: 0.9428 - val_acc: 0.8491\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9333Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2875 - acc: 0.9335 - val_loss: 0.9826 - val_acc: 0.8383\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9425Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2624 - acc: 0.9424 - val_loss: 1.0347 - val_acc: 0.8443\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9471Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2395 - acc: 0.9472 - val_loss: 0.9884 - val_acc: 0.8527\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9585Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1984 - acc: 0.9585 - val_loss: 1.0780 - val_acc: 0.8527\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9611Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2046 - acc: 0.9612 - val_loss: 1.1641 - val_acc: 0.8443\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9635Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1755 - acc: 0.9635 - val_loss: 1.2208 - val_acc: 0.8503\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9683Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1642 - acc: 0.9681 - val_loss: 1.1722 - val_acc: 0.8491\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9659Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1750 - acc: 0.9657 - val_loss: 1.2688 - val_acc: 0.8323\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9665Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1852 - acc: 0.9665 - val_loss: 1.1957 - val_acc: 0.8419\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9733Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1435 - acc: 0.9729 - val_loss: 1.1655 - val_acc: 0.8395\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 2048 l2 0.01 Dropout1 0.01 \n",
      " filters2 1024 l2 1e-05 Dropout2 0.3\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_55 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_56 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_57 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_57 (Glo (None, 2048)          0           input_55[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_58 (Glo (None, 2048)          0           input_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_59 (Glo (None, 2048)          0           input_57[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 6144)          0           global_average_pooling2d_57[0][0]\n",
      "                                                                   global_average_pooling2d_58[0][0]\n",
      "                                                                   global_average_pooling2d_59[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_52 (Dense)                 (None, 2048)          12584960    concatenate_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 2048)          0           dense_52[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 2048)          0           activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_53 (Dense)                 (None, 133)           272517      dropout_33[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 12,857,477\n",
      "Trainable params: 12,857,477\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.4318 - acc: 0.3190Epoch 00000: val_loss improved from inf to 4.43211, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 10.3925 - acc: 0.3204 - val_loss: 4.4321 - val_acc: 0.6335\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.9697 - acc: 0.6024Epoch 00001: val_loss improved from 4.43211 to 3.86219, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 3.9710 - acc: 0.6024 - val_loss: 3.8622 - val_acc: 0.5952\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.4520 - acc: 0.6389Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 3.4491 - acc: 0.6394 - val_loss: 4.1148 - val_acc: 0.5725\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.1445 - acc: 0.6665Epoch 00003: val_loss improved from 3.86219 to 3.39953, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 8s - loss: 3.1489 - acc: 0.6653 - val_loss: 3.3995 - val_acc: 0.6419\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0811 - acc: 0.6775- ETA: 0s - loss: 3.0843Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 3.0917 - acc: 0.6763 - val_loss: 4.1267 - val_acc: 0.5317\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.9026 - acc: 0.6973- ETA: 1s - loEpoch 00005: val_loss improved from 3.39953 to 2.87478, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.9006 - acc: 0.6978 - val_loss: 2.8748 - val_acc: 0.6790\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7580 - acc: 0.6991Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 2.7571 - acc: 0.6991 - val_loss: 2.9062 - val_acc: 0.6934\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6513 - acc: 0.7161Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 2.6508 - acc: 0.7159 - val_loss: 3.1201 - val_acc: 0.6323\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5738 - acc: 0.7181Epoch 00008: val_loss improved from 2.87478 to 2.81971, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.5795 - acc: 0.7171 - val_loss: 2.8197 - val_acc: 0.6802\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4761 - acc: 0.7336Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 2.4704 - acc: 0.7346 - val_loss: 2.8879 - val_acc: 0.6802\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3877 - acc: 0.7351Epoch 00010: val_loss improved from 2.81971 to 2.63747, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.3822 - acc: 0.7358 - val_loss: 2.6375 - val_acc: 0.6874\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3093 - acc: 0.7428- ETA: 1s -Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 2.3091 - acc: 0.7425 - val_loss: 3.2330 - val_acc: 0.6108\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1266 - acc: 0.7535Epoch 00012: val_loss improved from 2.63747 to 2.46404, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.1257 - acc: 0.7543 - val_loss: 2.4640 - val_acc: 0.7138\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.0035 - acc: 0.7667Epoch 00013: val_loss improved from 2.46404 to 2.23329, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.9991 - acc: 0.7671 - val_loss: 2.2333 - val_acc: 0.7150\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.9172 - acc: 0.7665Epoch 00014: val_loss improved from 2.23329 to 2.06659, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.9220 - acc: 0.7659 - val_loss: 2.0666 - val_acc: 0.7473\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 2048 l2 0.0001 Dropout1 0.8 \n",
      " filters2 1024 l2 1e-05 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_58 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_59 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_60 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_60 (Glo (None, 2048)          0           input_58[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_61 (Glo (None, 2048)          0           input_59[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_62 (Glo (None, 2048)          0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)     (None, 6144)          0           global_average_pooling2d_60[0][0]\n",
      "                                                                   global_average_pooling2d_61[0][0]\n",
      "                                                                   global_average_pooling2d_62[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_54 (Dense)                 (None, 2048)          12584960    concatenate_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 2048)          0           dense_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)             (None, 2048)          0           activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 133)           272517      dropout_34[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 12,857,477\n",
      "Trainable params: 12,857,477\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 14.5424 - acc: 0.0673Epoch 00000: val_loss improved from inf to 12.52506, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 14.5358 - acc: 0.0675 - val_loss: 12.5251 - val_acc: 0.2060\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 13.2256 - acc: 0.1569Epoch 00001: val_loss improved from 12.52506 to 11.02990, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 13.2244 - acc: 0.1570 - val_loss: 11.0299 - val_acc: 0.3042\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 12.2135 - acc: 0.2261Epoch 00002: val_loss improved from 11.02990 to 10.81168, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 12.1977 - acc: 0.2269 - val_loss: 10.8117 - val_acc: 0.3305\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 11.4813 - acc: 0.2735Epoch 00003: val_loss improved from 10.81168 to 10.21103, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 11.4785 - acc: 0.2737 - val_loss: 10.2110 - val_acc: 0.3701\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 11.2255 - acc: 0.2968Epoch 00004: val_loss improved from 10.21103 to 10.01307, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 11.2111 - acc: 0.2976 - val_loss: 10.0131 - val_acc: 0.3880\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.9829 - acc: 0.3106Epoch 00005: val_loss improved from 10.01307 to 9.86549, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.9749 - acc: 0.3112 - val_loss: 9.8655 - val_acc: 0.3952\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.9587 - acc: 0.3139Epoch 00006: val_loss improved from 9.86549 to 9.68901, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.9754 - acc: 0.3130 - val_loss: 9.6890 - val_acc: 0.4048\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.7662 - acc: 0.3300Epoch 00007: val_loss improved from 9.68901 to 9.65490, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.7667 - acc: 0.3301 - val_loss: 9.6549 - val_acc: 0.4120\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.7091 - acc: 0.3297Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 10.7069 - acc: 0.3298 - val_loss: 9.8334 - val_acc: 0.3964\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.6412 - acc: 0.3391Epoch 00009: val_loss improved from 9.65490 to 9.65273, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.6256 - acc: 0.3401 - val_loss: 9.6527 - val_acc: 0.4132\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.6582 - acc: 0.3359 ETA: 0s - loss: 10.6790 - acc - ETA: 0s - loss: 10.6750 - accEpoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.6604 - acc: 0.3358 - val_loss: 9.6695 - val_acc: 0.4096\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5729 - acc: 0.3410Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 10.5765 - acc: 0.3409 - val_loss: 9.7754 - val_acc: 0.4012\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5763 - acc: 0.3424Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.5587 - acc: 0.3436 - val_loss: 9.6575 - val_acc: 0.4108\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.4717 - acc: 0.3484Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.4687 - acc: 0.3485 - val_loss: 9.9933 - val_acc: 0.3892\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5567 - acc: 0.3428Epoch 00014: val_loss improved from 9.65273 to 9.62009, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.5548 - acc: 0.3430 - val_loss: 9.6201 - val_acc: 0.4120\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 0.001 Dropout1 0.8 \n",
      " filters2 2048 l2 0.01 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_61 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_62 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_63 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_63 (Glo (None, 2048)          0           input_61[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_64 (Glo (None, 2048)          0           input_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_65 (Glo (None, 2048)          0           input_63[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 6144)          0           global_average_pooling2d_63[0][0]\n",
      "                                                                   global_average_pooling2d_64[0][0]\n",
      "                                                                   global_average_pooling2d_65[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 1024)          6292480     concatenate_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 1024)          0           dense_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)             (None, 1024)          0           activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_57 (Dense)                 (None, 133)           136325      dropout_35[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 7.2409 - acc: 0.1732Epoch 00000: val_loss improved from inf to 1.94713, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 7.2164 - acc: 0.1744 - val_loss: 1.9471 - val_acc: 0.6395\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.3619 - acc: 0.4196Epoch 00001: val_loss improved from 1.94713 to 1.40740, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.3596 - acc: 0.4201 - val_loss: 1.4074 - val_acc: 0.7353\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0168 - acc: 0.5006Epoch 00002: val_loss improved from 1.40740 to 1.19659, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.0142 - acc: 0.5006 - val_loss: 1.1966 - val_acc: 0.8048\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8972 - acc: 0.5371Epoch 00003: val_loss improved from 1.19659 to 1.19115, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8965 - acc: 0.5371 - val_loss: 1.1911 - val_acc: 0.8204\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8250 - acc: 0.5626Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.8213 - acc: 0.5629 - val_loss: 1.2372 - val_acc: 0.8012\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8470 - acc: 0.5745Epoch 00005: val_loss improved from 1.19115 to 1.17878, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8481 - acc: 0.5746 - val_loss: 1.1788 - val_acc: 0.8156\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7223 - acc: 0.5872Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7277 - acc: 0.5858 - val_loss: 1.1798 - val_acc: 0.8323\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7698 - acc: 0.5952Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7723 - acc: 0.5955 - val_loss: 1.2908 - val_acc: 0.8311\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6397 - acc: 0.6145Epoch 00008: val_loss improved from 1.17878 to 1.16438, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6316 - acc: 0.6156 - val_loss: 1.1644 - val_acc: 0.8395\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.6017Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7275 - acc: 0.6009 - val_loss: 1.2538 - val_acc: 0.8419\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7256 - acc: 0.6179Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7249 - acc: 0.6187 - val_loss: 1.2336 - val_acc: 0.8359\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6399 - acc: 0.6107Epoch 00011: val_loss improved from 1.16438 to 1.13106, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6356 - acc: 0.6109 - val_loss: 1.1311 - val_acc: 0.8455\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5943 - acc: 0.6222Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5987 - acc: 0.6219 - val_loss: 1.2598 - val_acc: 0.8180\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6367 - acc: 0.6228Epoch 00013: val_loss improved from 1.13106 to 1.11448, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6382 - acc: 0.6231 - val_loss: 1.1145 - val_acc: 0.8479\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5406 - acc: 0.6249Epoch 00014: val_loss improved from 1.11448 to 1.07862, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.5373 - acc: 0.6250 - val_loss: 1.0786 - val_acc: 0.8527\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 0.0001 Dropout1 0.1 \n",
      " filters2 512 l2 0.0001 Dropout2 0.3\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_64 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_65 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_66 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_66 (Glo (None, 2048)          0           input_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_67 (Glo (None, 2048)          0           input_65[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_68 (Glo (None, 2048)          0           input_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 6144)          0           global_average_pooling2d_66[0][0]\n",
      "                                                                   global_average_pooling2d_67[0][0]\n",
      "                                                                   global_average_pooling2d_68[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_58 (Dense)                 (None, 256)           1573120     concatenate_23[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 256)           0           dense_58[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)             (None, 256)           0           activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_59 (Dense)                 (None, 133)           34181       dropout_36[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7181 - acc: 0.6077Epoch 00000: val_loss improved from inf to 0.74406, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7145 - acc: 0.6079 - val_loss: 0.7441 - val_acc: 0.7856\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.8221Epoch 00001: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.6429 - acc: 0.8225 - val_loss: 0.7455 - val_acc: 0.7964\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8727Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4776 - acc: 0.8729 - val_loss: 0.8240 - val_acc: 0.8120\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8970Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3862 - acc: 0.8972 - val_loss: 0.7558 - val_acc: 0.8168\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.9168Epoch 00004: val_loss improved from 0.74406 to 0.69155, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3324 - acc: 0.9169 - val_loss: 0.6915 - val_acc: 0.8599\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9239Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.3169 - acc: 0.9235 - val_loss: 0.8649 - val_acc: 0.8419\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9435Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2784 - acc: 0.9425 - val_loss: 0.9183 - val_acc: 0.8419\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9503Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2606 - acc: 0.9503 - val_loss: 0.8732 - val_acc: 0.8527\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9508Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2593 - acc: 0.9513 - val_loss: 0.9081 - val_acc: 0.8503\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9555Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2399 - acc: 0.9552 - val_loss: 0.9621 - val_acc: 0.8431\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9583Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2385 - acc: 0.9582 - val_loss: 0.9588 - val_acc: 0.8479\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9594Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2465 - acc: 0.9596 - val_loss: 0.9935 - val_acc: 0.8383\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9617Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2392 - acc: 0.9618 - val_loss: 1.0856 - val_acc: 0.8359\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9636Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2265 - acc: 0.9639 - val_loss: 1.0267 - val_acc: 0.8575\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9626Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2390 - acc: 0.9621 - val_loss: 1.2573 - val_acc: 0.8287\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 1e-05 Dropout1 0.5 \n",
      " filters2 4096 l2 0.01 Dropout2 0.01\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_67 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_68 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_69 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_69 (Glo (None, 2048)          0           input_67[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_70 (Glo (None, 2048)          0           input_68[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_71 (Glo (None, 2048)          0           input_69[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)     (None, 6144)          0           global_average_pooling2d_69[0][0]\n",
      "                                                                   global_average_pooling2d_70[0][0]\n",
      "                                                                   global_average_pooling2d_71[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_60 (Dense)                 (None, 256)           1573120     concatenate_24[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 256)           0           dense_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)             (None, 256)           0           activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_61 (Dense)                 (None, 133)           34181       dropout_37[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.7580 - acc: 0.4033Epoch 00000: val_loss improved from inf to 0.76667, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.7435 - acc: 0.4051 - val_loss: 0.7667 - val_acc: 0.7641\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1685 - acc: 0.6845Epoch 00001: val_loss improved from 0.76667 to 0.59458, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1644 - acc: 0.6859 - val_loss: 0.5946 - val_acc: 0.8240\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8607 - acc: 0.7653Epoch 00002: val_loss improved from 0.59458 to 0.57576, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8654 - acc: 0.7647 - val_loss: 0.5758 - val_acc: 0.8275\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7964Epoch 00003: val_loss improved from 0.57576 to 0.50511, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7134 - acc: 0.7964 - val_loss: 0.5051 - val_acc: 0.8695\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6442 - acc: 0.8153Epoch 00004: val_loss improved from 0.50511 to 0.48371, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6418 - acc: 0.8157 - val_loss: 0.4837 - val_acc: 0.8503\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.8421Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.5457 - acc: 0.8416 - val_loss: 0.5041 - val_acc: 0.8683\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8555Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.5173 - acc: 0.8549 - val_loss: 0.5303 - val_acc: 0.8599\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8638Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4641 - acc: 0.8632 - val_loss: 0.5366 - val_acc: 0.8659\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8712Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4466 - acc: 0.8708 - val_loss: 0.4871 - val_acc: 0.8743\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8841Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4127 - acc: 0.8831 - val_loss: 0.6162 - val_acc: 0.8671\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8848Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4112 - acc: 0.8850 - val_loss: 0.6304 - val_acc: 0.8647\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8920Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3827 - acc: 0.8906 - val_loss: 0.5493 - val_acc: 0.8802\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.9089Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3515 - acc: 0.9081 - val_loss: 0.6849 - val_acc: 0.8539\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.9047Epoch 00013: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s - loss: 0.3495 - acc: 0.9045 - val_loss: 0.6030 - val_acc: 0.8731\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.9097Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3475 - acc: 0.9091 - val_loss: 0.6627 - val_acc: 0.8778\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 0.0001 Dropout1 0.3 \n",
      " filters2 2048 l2 0.001 Dropout2 0.8\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_70 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_71 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_72 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_72 (Glo (None, 2048)          0           input_70[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_73 (Glo (None, 2048)          0           input_71[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_74 (Glo (None, 2048)          0           input_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)     (None, 6144)          0           global_average_pooling2d_72[0][0]\n",
      "                                                                   global_average_pooling2d_73[0][0]\n",
      "                                                                   global_average_pooling2d_74[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_62 (Dense)                 (None, 256)           1573120     concatenate_25[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 256)           0           dense_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)             (None, 256)           0           activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_63 (Dense)                 (None, 133)           34181       dropout_38[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.0120 - acc: 0.5541Epoch 00000: val_loss improved from inf to 0.79505, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.0024 - acc: 0.5551 - val_loss: 0.7950 - val_acc: 0.7820\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7779Epoch 00001: val_loss improved from 0.79505 to 0.65269, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8189 - acc: 0.7777 - val_loss: 0.6527 - val_acc: 0.8084\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6414 - acc: 0.8303Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.6464 - acc: 0.8296 - val_loss: 0.6995 - val_acc: 0.8323\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8615Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.5306 - acc: 0.8606 - val_loss: 0.8258 - val_acc: 0.8120\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8782Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4796 - acc: 0.8777 - val_loss: 0.6654 - val_acc: 0.8527\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8923Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4461 - acc: 0.8927 - val_loss: 0.7375 - val_acc: 0.8527\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.9033Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4148 - acc: 0.9025 - val_loss: 0.6975 - val_acc: 0.8563\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.9130Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3975 - acc: 0.9124 - val_loss: 0.7876 - val_acc: 0.8491\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.9202Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3677 - acc: 0.9198 - val_loss: 0.8019 - val_acc: 0.8491\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9262Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3294 - acc: 0.9254 - val_loss: 0.8139 - val_acc: 0.8515\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.9333Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.3352 - acc: 0.9335 - val_loss: 0.7610 - val_acc: 0.8671\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.9323Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3475 - acc: 0.9320 - val_loss: 0.8896 - val_acc: 0.8503\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9347Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3438 - acc: 0.9343 - val_loss: 0.9100 - val_acc: 0.8503\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9402Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3129 - acc: 0.9395 - val_loss: 0.8023 - val_acc: 0.8671\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.9385Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3242 - acc: 0.9388 - val_loss: 0.9591 - val_acc: 0.8659\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 0.01 Dropout1 0.5 \n",
      " filters2 1024 l2 0.001 Dropout2 0.01\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_73 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_74 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_75 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_75 (Glo (None, 2048)          0           input_73[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_76 (Glo (None, 2048)          0           input_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_77 (Glo (None, 2048)          0           input_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)     (None, 6144)          0           global_average_pooling2d_75[0][0]\n",
      "                                                                   global_average_pooling2d_76[0][0]\n",
      "                                                                   global_average_pooling2d_77[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_64 (Dense)                 (None, 1024)          6292480     concatenate_26[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 1024)          0           dense_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)             (None, 1024)          0           activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_65 (Dense)                 (None, 133)           136325      dropout_39[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 6.8735 - acc: 0.4362Epoch 00000: val_loss improved from inf to 3.08689, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 6.8457 - acc: 0.4374 - val_loss: 3.0869 - val_acc: 0.6934\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.4593 - acc: 0.5919Epoch 00001: val_loss improved from 3.08689 to 2.69905, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.4579 - acc: 0.5916 - val_loss: 2.6991 - val_acc: 0.6778\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0576 - acc: 0.6258Epoch 00002: val_loss improved from 2.69905 to 2.42455, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.0575 - acc: 0.6254 - val_loss: 2.4245 - val_acc: 0.7090\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.9800 - acc: 0.6353Epoch 00003: val_loss improved from 2.42455 to 2.32077, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.9867 - acc: 0.6338 - val_loss: 2.3208 - val_acc: 0.7377\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8227 - acc: 0.6483Epoch 00004: val_loss improved from 2.32077 to 1.96411, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8213 - acc: 0.6493 - val_loss: 1.9641 - val_acc: 0.8024\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6496 - acc: 0.6618Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.6531 - acc: 0.6614 - val_loss: 2.0456 - val_acc: 0.7593\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6377 - acc: 0.6602Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.6407 - acc: 0.6594 - val_loss: 2.1363 - val_acc: 0.7509\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5568 - acc: 0.6674Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5640 - acc: 0.6668 - val_loss: 2.2292 - val_acc: 0.7138\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5322 - acc: 0.6689Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5335 - acc: 0.6695 - val_loss: 1.9996 - val_acc: 0.7665\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4412 - acc: 0.6790Epoch 00009: val_loss improved from 1.96411 to 1.93914, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.4392 - acc: 0.6793 - val_loss: 1.9391 - val_acc: 0.7737\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4002 - acc: 0.6753Epoch 00010: val_loss improved from 1.93914 to 1.91448, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.3990 - acc: 0.6756 - val_loss: 1.9145 - val_acc: 0.7749\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3052 - acc: 0.6968Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.3086 - acc: 0.6957 - val_loss: 1.9362 - val_acc: 0.7749\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.2366 - acc: 0.6947Epoch 00012: val_loss improved from 1.91448 to 1.81238, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.2347 - acc: 0.6948 - val_loss: 1.8124 - val_acc: 0.7629\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1765 - acc: 0.6902Epoch 00013: val_loss improved from 1.81238 to 1.71082, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.1799 - acc: 0.6900 - val_loss: 1.7108 - val_acc: 0.7916\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1013 - acc: 0.7012Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.0994 - acc: 0.7013 - val_loss: 1.9551 - val_acc: 0.7174\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 512 l2 0.01 Dropout1 0.3 \n",
      " filters2 1024 l2 0.01 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_76 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_77 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_78 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_78 (Glo (None, 2048)          0           input_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_79 (Glo (None, 2048)          0           input_77[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_80 (Glo (None, 2048)          0           input_78[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)     (None, 6144)          0           global_average_pooling2d_78[0][0]\n",
      "                                                                   global_average_pooling2d_79[0][0]\n",
      "                                                                   global_average_pooling2d_80[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_66 (Dense)                 (None, 512)           3146240     concatenate_27[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 512)           0           dense_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)             (None, 512)           0           activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_67 (Dense)                 (None, 133)           68229       dropout_40[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3,214,469\n",
      "Trainable params: 3,214,469\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.4501 - acc: 0.5208Epoch 00000: val_loss improved from inf to 2.21937, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 4.4302 - acc: 0.5217 - val_loss: 2.2194 - val_acc: 0.6982\n",
      "Epoch 2/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.2780 - acc: 0.6829Epoch 00001: val_loss improved from 2.21937 to 2.08154, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 5s - loss: 2.2775 - acc: 0.6828 - val_loss: 2.0815 - val_acc: 0.7102\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9979 - acc: 0.7171Epoch 00002: val_loss improved from 2.08154 to 1.86843, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9974 - acc: 0.7166 - val_loss: 1.8684 - val_acc: 0.7389\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9117 - acc: 0.7179Epoch 00003: val_loss improved from 1.86843 to 1.70168, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9144 - acc: 0.7174 - val_loss: 1.7017 - val_acc: 0.7437\n",
      "Epoch 5/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.8664 - acc: 0.7263Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.8683 - acc: 0.7260 - val_loss: 1.9851 - val_acc: 0.7269\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7923 - acc: 0.7402Epoch 00005: val_loss improved from 1.70168 to 1.61132, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7907 - acc: 0.7398 - val_loss: 1.6113 - val_acc: 0.7701\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7556 - acc: 0.7368Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.7554 - acc: 0.7370 - val_loss: 1.7491 - val_acc: 0.7485\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7182 - acc: 0.7373Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.7201 - acc: 0.7367 - val_loss: 1.6669 - val_acc: 0.7353\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7004 - acc: 0.7500Epoch 00008: val_loss improved from 1.61132 to 1.48000, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6973 - acc: 0.7499 - val_loss: 1.4800 - val_acc: 0.7940\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6313 - acc: 0.7511Epoch 00009: val_loss improved from 1.48000 to 1.39639, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6291 - acc: 0.7519 - val_loss: 1.3964 - val_acc: 0.7868\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5972 - acc: 0.7502Epoch 00010: val_loss improved from 1.39639 to 1.34935, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.5965 - acc: 0.7504 - val_loss: 1.3494 - val_acc: 0.8096\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5528 - acc: 0.7645Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5522 - acc: 0.7651 - val_loss: 1.6309 - val_acc: 0.7689\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5133 - acc: 0.7639Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5145 - acc: 0.7641 - val_loss: 1.5043 - val_acc: 0.7760\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5224 - acc: 0.7623Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5232 - acc: 0.7629 - val_loss: 1.5055 - val_acc: 0.7689\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4629 - acc: 0.7758Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.4650 - acc: 0.7757 - val_loss: 1.3569 - val_acc: 0.7892\n"
     ]
    }
   ],
   "source": [
    "merged_models = [train_merged_model(*var) for var in rand_ver_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 1e-05, 0.8, 1024, 0.01, 0.1) accuracy 83.85167464114832\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 1e-05, 0.1, 2048, 0.01, 0.1) accuracy 83.73205741626795\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(2048, 0.01, 0.01, 1024, 1e-05, 0.3) accuracy 72.8468899521531\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(2048, 0.0001, 0.8, 1024, 1e-05, 0.5) accuracy 41.985645933014354\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 0.001, 0.8, 2048, 0.01, 0.5) accuracy 84.68899521531101\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 0.0001, 0.1, 512, 0.0001, 0.3) accuracy 84.0909090909091\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 1e-05, 0.5, 4096, 0.01, 0.01) accuracy 84.80861244019138\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 0.0001, 0.3, 2048, 0.001, 0.8) accuracy 80.14354066985646\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 0.01, 0.5, 1024, 0.001, 0.01) accuracy 78.11004784688996\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(512, 0.01, 0.3, 1024, 0.01, 0.5) accuracy 80.14354066985646\n"
     ]
    }
   ],
   "source": [
    "for parms, merged_model in zip(rand_ver_product, merged_models):\n",
    "    print(\"ker1, l2_1, dr1, ker2, l2_2, dr2\")\n",
    "    print(\"{} accuracy {}\".format(parms, merged_model[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('concat', ('InceptionV3', 'Xception')) accuracy 81.45933014354067\n",
      "('concat', ('Resnet50', 'InceptionV3')) accuracy 83.37320574162679\n",
      "('concat', ('Resnet50', 'Xception')) accuracy 84.80861244019138\n",
      "('concat', ['Resnet50', 'InceptionV3', 'Xception']) accuracy 85.16746411483254\n"
     ]
    }
   ],
   "source": [
    "for model in sorted(merged_models, key=lambda x: x[1][0]):\n",
    "    print(\"{} accuracy {}\".format(model[0], model[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_145 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_146 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_147 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_153 (Gl (None, 2048)          0           input_145[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_154 (Gl (None, 2048)          0           input_146[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_155 (Gl (None, 2048)          0           input_147[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_36 (Add)                     (None, 2048)          0           global_average_pooling2d_153[0][0\n",
      "                                                                   global_average_pooling2d_154[0][0\n",
      "                                                                   global_average_pooling2d_155[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_117 (Dense)                (None, 512)           1049088     add_36[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 512)           0           dense_117[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 512)           0           activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_118 (Dense)                (None, 133)           68229       dropout_59[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.8997 - acc: 0.0867Epoch 00000: val_loss improved from inf to 2.67718, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 4.8851 - acc: 0.0882 - val_loss: 2.6772 - val_acc: 0.4886\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.9989 - acc: 0.2914- ETA: 0s - loss: 3.0551 - acc:Epoch 00001: val_loss improved from 2.67718 to 1.25171, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.9951 - acc: 0.2921 - val_loss: 1.2517 - val_acc: 0.6838\n",
      "Epoch 3/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.2818 - acc: 0.4159- ETA: 0s - loss: 2.2935 - acc: 0.Epoch 00002: val_loss improved from 1.25171 to 0.87837, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.2808 - acc: 0.4157 - val_loss: 0.8784 - val_acc: 0.7389\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9156 - acc: 0.4841- ETA: 3s - loss: 1.9725 -Epoch 00003: val_loss improved from 0.87837 to 0.73533, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9130 - acc: 0.4847 - val_loss: 0.7353 - val_acc: 0.7820\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7155 - acc: 0.5444Epoch 00004: val_loss improved from 0.73533 to 0.62038, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7118 - acc: 0.5446 - val_loss: 0.6204 - val_acc: 0.8060\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5824 - acc: 0.5611Epoch 00005: val_loss improved from 0.62038 to 0.56888, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.5794 - acc: 0.5615 - val_loss: 0.5689 - val_acc: 0.8311\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5007 - acc: 0.5865Epoch 00006: val_loss improved from 0.56888 to 0.55440, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.4997 - acc: 0.5874 - val_loss: 0.5544 - val_acc: 0.8335\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4384 - acc: 0.6062Epoch 00007: val_loss improved from 0.55440 to 0.53437, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.4347 - acc: 0.6064 - val_loss: 0.5344 - val_acc: 0.8323\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3836 - acc: 0.6236Epoch 00008: val_loss improved from 0.53437 to 0.52345, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3848 - acc: 0.6232 - val_loss: 0.5235 - val_acc: 0.8275\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3422 - acc: 0.6367Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.3467 - acc: 0.6368 - val_loss: 0.5845 - val_acc: 0.8180\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3041 - acc: 0.6461Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.3032 - acc: 0.6457 - val_loss: 0.5845 - val_acc: 0.8299\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2603 - acc: 0.6564- ETA: 0s - loss: 1.2778 - Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2629 - acc: 0.6564 - val_loss: 0.5539 - val_acc: 0.8287\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2166 - acc: 0.6689Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2164 - acc: 0.6687 - val_loss: 0.5289 - val_acc: 0.8419\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.2299 - acc: 0.6641- ETA: 3s - ETA: 1s - loss: 1.2374 - ETA: 0s - loss: 1.2357 - acc: 0.Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.2277 - acc: 0.6644 - val_loss: 0.5322 - val_acc: 0.8371\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6811Epoch 00014: val_loss improved from 0.52345 to 0.50693, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1939 - acc: 0.6808 - val_loss: 0.5069 - val_acc: 0.8599\n",
      "==========models with ('Resnet50', 'InceptionV3') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_148 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_149 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_156 (Gl (None, 2048)          0           input_148[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_157 (Gl (None, 2048)          0           input_149[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_37 (Add)                     (None, 2048)          0           global_average_pooling2d_156[0][0\n",
      "                                                                   global_average_pooling2d_157[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_119 (Dense)                (None, 512)           1049088     add_37[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 512)           0           dense_119[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 512)           0           activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_120 (Dense)                (None, 133)           68229       dropout_60[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 4.3108 - acc: 0.1418Epoch 00000: val_loss improved from inf to 1.89192, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 4.3023 - acc: 0.1427 - val_loss: 1.8919 - val_acc: 0.6156\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4086 - acc: 0.4107Epoch 00001: val_loss improved from 1.89192 to 0.95637, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 2.4075 - acc: 0.4102 - val_loss: 0.9564 - val_acc: 0.7485\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.8455 - acc: 0.5198Epoch 00002: val_loss improved from 0.95637 to 0.72451, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.8411 - acc: 0.5211 - val_loss: 0.7245 - val_acc: 0.7796\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5538 - acc: 0.5777Epoch 00003: val_loss improved from 0.72451 to 0.60544, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.5516 - acc: 0.5774 - val_loss: 0.6054 - val_acc: 0.8263\n",
      "Epoch 5/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3806 - acc: 0.6228Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.3789 - acc: 0.6234 - val_loss: 0.6086 - val_acc: 0.8000\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3354 - acc: 0.6416Epoch 00005: val_loss improved from 0.60544 to 0.55830, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.3352 - acc: 0.6419 - val_loss: 0.5583 - val_acc: 0.8120\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2283 - acc: 0.6694Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.2295 - acc: 0.6693 - val_loss: 0.5716 - val_acc: 0.8204\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6769Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.1856 - acc: 0.6772 - val_loss: 0.5657 - val_acc: 0.8383\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1504 - acc: 0.6830Epoch 00008: val_loss improved from 0.55830 to 0.45960, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 1.1520 - acc: 0.6829 - val_loss: 0.4596 - val_acc: 0.8359\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0877 - acc: 0.7024Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0873 - acc: 0.7025 - val_loss: 0.5454 - val_acc: 0.8263\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.7063Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.1077 - acc: 0.7055 - val_loss: 0.4853 - val_acc: 0.8599\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0658 - acc: 0.7095Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0725 - acc: 0.7085 - val_loss: 0.5786 - val_acc: 0.8371\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0404 - acc: 0.7262Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 1.0457 - acc: 0.7256 - val_loss: 0.5415 - val_acc: 0.8359\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9882 - acc: 0.7299Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.9892 - acc: 0.7295 - val_loss: 0.5224 - val_acc: 0.8539\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9997 - acc: 0.7338Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.9976 - acc: 0.7335 - val_loss: 0.5251 - val_acc: 0.8467\n",
      "==========models with ('Resnet50', 'Xception') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_150 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_151 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_158 (Gl (None, 2048)          0           input_150[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_159 (Gl (None, 2048)          0           input_151[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_38 (Add)                     (None, 2048)          0           global_average_pooling2d_158[0][0\n",
      "                                                                   global_average_pooling2d_159[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_121 (Dense)                (None, 512)           1049088     add_38[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 512)           0           dense_121[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 512)           0           activation_69[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_122 (Dense)                (None, 133)           68229       dropout_61[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.9509 - acc: 0.1697Epoch 00000: val_loss improved from inf to 1.70577, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 3.9367 - acc: 0.1713 - val_loss: 1.7058 - val_acc: 0.6180\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.1204 - acc: 0.4502Epoch 00001: val_loss improved from 1.70577 to 0.85347, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 2.1148 - acc: 0.4510 - val_loss: 0.8535 - val_acc: 0.7461\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5429 - acc: 0.5767Epoch 00002: val_loss improved from 0.85347 to 0.69322, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.5441 - acc: 0.5769 - val_loss: 0.6932 - val_acc: 0.7796\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3264 - acc: 0.6235Epoch 00003: val_loss improved from 0.69322 to 0.58897, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.3246 - acc: 0.6244 - val_loss: 0.5890 - val_acc: 0.8132\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1910 - acc: 0.6589Epoch 00004: val_loss improved from 0.58897 to 0.55494, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s - loss: 1.1926 - acc: 0.6587 - val_loss: 0.5549 - val_acc: 0.8311\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0952 - acc: 0.6868Epoch 00005: val_loss improved from 0.55494 to 0.52410, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.0934 - acc: 0.6871 - val_loss: 0.5241 - val_acc: 0.8299\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0296 - acc: 0.7023Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 1.0280 - acc: 0.7027 - val_loss: 0.5421 - val_acc: 0.8359\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.7202Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.9808 - acc: 0.7199 - val_loss: 0.5386 - val_acc: 0.8287\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9151 - acc: 0.7376Epoch 00008: val_loss improved from 0.52410 to 0.49290, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.9156 - acc: 0.7377 - val_loss: 0.4929 - val_acc: 0.8479\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.7488Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8827 - acc: 0.7488 - val_loss: 0.5808 - val_acc: 0.8359\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.7586Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8455 - acc: 0.7590 - val_loss: 0.4972 - val_acc: 0.8539\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7936 - acc: 0.7824Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7946 - acc: 0.7826 - val_loss: 0.4977 - val_acc: 0.8587\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7676Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.8143 - acc: 0.7681 - val_loss: 0.5308 - val_acc: 0.8491\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7792Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7810 - acc: 0.7793 - val_loss: 0.5199 - val_acc: 0.8515\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7781 - acc: 0.7841Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.7778 - acc: 0.7837 - val_loss: 0.5036 - val_acc: 0.8563\n",
      "==========models with ('InceptionV3', 'Xception') and add\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_152 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_153 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_160 (Gl (None, 2048)          0           input_152[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_161 (Gl (None, 2048)          0           input_153[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_39 (Add)                     (None, 2048)          0           global_average_pooling2d_160[0][0\n",
      "                                                                   global_average_pooling2d_161[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_123 (Dense)                (None, 512)           1049088     add_39[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_70 (Activation)       (None, 512)           0           dense_123[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 512)           0           activation_70[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_124 (Dense)                (None, 133)           68229       dropout_62[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.4083 - acc: 0.2880- ETA: 1s - loss: 3.8577 - acc: 0. - ETA: 1s - loss: 3.7593  - ETA: 0s - loss: 3.4831 - acc: 0.Epoch 00000: val_loss improved from inf to 1.04773, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 3.3925 - acc: 0.2907 - val_loss: 1.0477 - val_acc: 0.7329\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6788 - acc: 0.5724- ETA: 3s - loss: 1.8288 Epoch 00001: val_loss improved from 1.04773 to 0.70742, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6802 - acc: 0.5723 - val_loss: 0.7074 - val_acc: 0.7725\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3129 - acc: 0.6473- ETA - ETA: 0s - loss: 1.3184 - acc:Epoch 00002: val_loss improved from 0.70742 to 0.58277, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3097 - acc: 0.6487 - val_loss: 0.5828 - val_acc: 0.8180\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1728 - acc: 0.6888Epoch 00003: val_loss improved from 0.58277 to 0.53712, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1688 - acc: 0.6894 - val_loss: 0.5371 - val_acc: 0.8311\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0922 - acc: 0.7086Epoch 00004: val_loss improved from 0.53712 to 0.51356, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0888 - acc: 0.7094 - val_loss: 0.5136 - val_acc: 0.8371\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.0047 - acc: 0.7295Epoch 00005: val_loss improved from 0.51356 to 0.50224, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0081 - acc: 0.7293 - val_loss: 0.5022 - val_acc: 0.8479\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9730 - acc: 0.7417- ETA: 1s -Epoch 00006: val_loss improved from 0.50224 to 0.45790, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9708 - acc: 0.7425 - val_loss: 0.4579 - val_acc: 0.8587\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9188 - acc: 0.7592- ETA: 3s - loss: 0.8410 - acc: - ETA: 0s - loss: 0.9177 - acc:Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9184 - acc: 0.7603 - val_loss: 0.5367 - val_acc: 0.8479\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8986 - acc: 0.7617- ETA: 2s - - ETA: 1s - loss: 0.9Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8999 - acc: 0.7621 - val_loss: 0.5036 - val_acc: 0.8599\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.7761Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8711 - acc: 0.7753 - val_loss: 0.5003 - val_acc: 0.8467\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7767Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8470 - acc: 0.7768 - val_loss: 0.5532 - val_acc: 0.8611\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8552 - acc: 0.7833Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8559 - acc: 0.7832 - val_loss: 0.5693 - val_acc: 0.8515\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7923Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8073 - acc: 0.7912 - val_loss: 0.5580 - val_acc: 0.8515\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8126 - acc: 0.7953- ETA: 1s - loss: 0.8 - ETA: 0s - loss: 0.8090 - Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8117 - acc: 0.7955 - val_loss: 0.5373 - val_acc: 0.8539\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.8041- EEpoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.7984 - acc: 0.8034 - val_loss: 0.5627 - val_acc: 0.8611\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_154 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_155 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_156 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_162 (Gl (None, 2048)          0           input_154[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_163 (Gl (None, 2048)          0           input_155[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_164 (Gl (None, 2048)          0           input_156[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)     (None, 6144)          0           global_average_pooling2d_162[0][0\n",
      "                                                                   global_average_pooling2d_163[0][0\n",
      "                                                                   global_average_pooling2d_164[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_125 (Dense)                (None, 512)           3146240     concatenate_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_71 (Activation)       (None, 512)           0           dense_125[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           activation_71[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_126 (Dense)                (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3,214,469\n",
      "Trainable params: 3,214,469\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 3.5563 - acc: 0.2949Epoch 00000: val_loss improved from inf to 0.93777, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 10s - loss: 3.5514 - acc: 0.2957 - val_loss: 0.9378 - val_acc: 0.7341\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7184 - acc: 0.5620Epoch 00001: val_loss improved from 0.93777 to 0.64874, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.7177 - acc: 0.5624 - val_loss: 0.6487 - val_acc: 0.7916\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3785 - acc: 0.6486Epoch 00002: val_loss improved from 0.64874 to 0.50104, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.3782 - acc: 0.6488 - val_loss: 0.5010 - val_acc: 0.8431\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1923 - acc: 0.6884Epoch 00003: val_loss improved from 0.50104 to 0.47235, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 1.1902 - acc: 0.6886 - val_loss: 0.4724 - val_acc: 0.8707\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0896 - acc: 0.7250Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 1.0863 - acc: 0.7253 - val_loss: 0.4805 - val_acc: 0.8527\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0476 - acc: 0.7397Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 1.0478 - acc: 0.7400 - val_loss: 0.5210 - val_acc: 0.8515\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.7554Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9453 - acc: 0.7557 - val_loss: 0.5269 - val_acc: 0.8575\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9529 - acc: 0.7623Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9518 - acc: 0.7629 - val_loss: 0.4930 - val_acc: 0.8623\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7735Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.9149 - acc: 0.7737 - val_loss: 0.5046 - val_acc: 0.8743\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7879Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8842 - acc: 0.7877 - val_loss: 0.5086 - val_acc: 0.8611\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.8027Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8444 - acc: 0.8024 - val_loss: 0.5232 - val_acc: 0.8814\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.8081Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8162 - acc: 0.8085 - val_loss: 0.5007 - val_acc: 0.8790\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8283 - acc: 0.8086Epoch 00012: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 9s - loss: 0.8275 - acc: 0.8084 - val_loss: 0.5629 - val_acc: 0.8683\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8297 - acc: 0.8109Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.8288 - acc: 0.8111 - val_loss: 0.5409 - val_acc: 0.8766\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.8201Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.7797 - acc: 0.8199 - val_loss: 0.6123 - val_acc: 0.8754\n",
      "==========models with ('Resnet50', 'InceptionV3') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_157 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_158 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_165 (Gl (None, 2048)          0           input_157[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_166 (Gl (None, 2048)          0           input_158[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 4096)          0           global_average_pooling2d_165[0][0\n",
      "                                                                   global_average_pooling2d_166[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_127 (Dense)                (None, 512)           2097664     concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_72 (Activation)       (None, 512)           0           dense_127[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (None, 512)           0           activation_72[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_128 (Dense)                (None, 133)           68229       dropout_64[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 3.6149 - acc: 0.2711Epoch 00000: val_loss improved from inf to 1.04501, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 3.6045 - acc: 0.2720 - val_loss: 1.0450 - val_acc: 0.7413\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7775 - acc: 0.5492Epoch 00001: val_loss improved from 1.04501 to 0.61549, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7742 - acc: 0.5506 - val_loss: 0.6155 - val_acc: 0.8120\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3776 - acc: 0.6336Epoch 00002: val_loss improved from 0.61549 to 0.54531, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.3736 - acc: 0.6337 - val_loss: 0.5453 - val_acc: 0.8311\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1985 - acc: 0.6868Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.1985 - acc: 0.6865 - val_loss: 0.5761 - val_acc: 0.8335\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.7070Epoch 00004: val_loss improved from 0.54531 to 0.50812, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1021 - acc: 0.7075 - val_loss: 0.5081 - val_acc: 0.8419\n",
      "Epoch 6/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.0383 - acc: 0.7284Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.0362 - acc: 0.7287 - val_loss: 0.5516 - val_acc: 0.8443\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.7438Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9684 - acc: 0.7446 - val_loss: 0.5391 - val_acc: 0.8539\n",
      "Epoch 8/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7674Epoch 00007: val_loss improved from 0.50812 to 0.50195, saving model to saved_models/weights.best.Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.9305 - acc: 0.7680 - val_loss: 0.5019 - val_acc: 0.8599\n",
      "Epoch 9/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.9078 - acc: 0.7725Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.9096 - acc: 0.7725 - val_loss: 0.5187 - val_acc: 0.8575\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8381 - acc: 0.7845Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8430 - acc: 0.7843 - val_loss: 0.6567 - val_acc: 0.8407\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8936 - acc: 0.7848Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8906 - acc: 0.7850 - val_loss: 0.5981 - val_acc: 0.8527\n",
      "Epoch 12/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8340 - acc: 0.7977Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8335 - acc: 0.7979 - val_loss: 0.5631 - val_acc: 0.8623\n",
      "Epoch 13/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.8033Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8044 - acc: 0.8036 - val_loss: 0.6673 - val_acc: 0.8515\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.8080Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.8218 - acc: 0.8078 - val_loss: 0.6426 - val_acc: 0.8695\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.8106Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.7783 - acc: 0.8102 - val_loss: 0.6262 - val_acc: 0.8563\n",
      "==========models with ('Resnet50', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_159 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_160 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_167 (Gl (None, 2048)          0           input_159[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_168 (Gl (None, 2048)          0           input_160[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)     (None, 4096)          0           global_average_pooling2d_167[0][0\n",
      "                                                                   global_average_pooling2d_168[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_129 (Dense)                (None, 512)           2097664     concatenate_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, 512)           0           dense_129[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (None, 512)           0           activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_130 (Dense)                (None, 133)           68229       dropout_65[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.2416 - acc: 0.3047Epoch 00000: val_loss improved from inf to 1.01537, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.2344 - acc: 0.3058 - val_loss: 1.0154 - val_acc: 0.7150\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5413 - acc: 0.5872Epoch 00001: val_loss improved from 1.01537 to 0.62521, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.5370 - acc: 0.5883 - val_loss: 0.6252 - val_acc: 0.7964\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2116 - acc: 0.6780Epoch 00002: val_loss improved from 0.62521 to 0.54739, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.2094 - acc: 0.6784 - val_loss: 0.5474 - val_acc: 0.8192\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.7222Epoch 00003: val_loss improved from 0.54739 to 0.48773, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 1.0058 - acc: 0.7220 - val_loss: 0.4877 - val_acc: 0.8383\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.7478Epoch 00004: val_loss improved from 0.48773 to 0.47498, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.9198 - acc: 0.7479 - val_loss: 0.4750 - val_acc: 0.8491\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7585Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.8754 - acc: 0.7582 - val_loss: 0.4929 - val_acc: 0.8455\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7705 - acc: 0.7861Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.7724 - acc: 0.7862 - val_loss: 0.5014 - val_acc: 0.8491\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.8005Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.7336 - acc: 0.8001 - val_loss: 0.5313 - val_acc: 0.8431\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.7377 - acc: 0.8017Epoch 00008: val_loss improved from 0.47498 to 0.43486, saving model to saved_models/weights.best.Resnet50_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.7393 - acc: 0.8019 - val_loss: 0.4349 - val_acc: 0.8635\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.8158Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6910 - acc: 0.8153 - val_loss: 0.4760 - val_acc: 0.8599\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6799 - acc: 0.8178Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6844 - acc: 0.8174 - val_loss: 0.5402 - val_acc: 0.8587\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.8312Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6383 - acc: 0.8308 - val_loss: 0.5183 - val_acc: 0.8563\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6245 - acc: 0.8407Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6227 - acc: 0.8409 - val_loss: 0.4924 - val_acc: 0.8766\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.6108 - acc: 0.8460Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.6151 - acc: 0.8452 - val_loss: 0.5941 - val_acc: 0.8587\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8456Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.5760 - acc: 0.8451 - val_loss: 0.5577 - val_acc: 0.8743\n",
      "==========models with ('InceptionV3', 'Xception') and concat\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_161 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_162 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_169 (Gl (None, 2048)          0           input_161[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_170 (Gl (None, 2048)          0           input_162[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)     (None, 4096)          0           global_average_pooling2d_169[0][0\n",
      "                                                                   global_average_pooling2d_170[0][0\n",
      "____________________________________________________________________________________________________\n",
      "dense_131 (Dense)                (None, 512)           2097664     concatenate_17[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, 512)           0           dense_131[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 512)           0           activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_132 (Dense)                (None, 133)           68229       dropout_66[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,165,893\n",
      "Trainable params: 2,165,893\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7659 - acc: 0.4036Epoch 00000: val_loss improved from inf to 0.72083, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.7564 - acc: 0.4055 - val_loss: 0.7208 - val_acc: 0.7844\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3499 - acc: 0.6558Epoch 00001: val_loss improved from 0.72083 to 0.57421, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.3473 - acc: 0.6560 - val_loss: 0.5742 - val_acc: 0.8275\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.1002 - acc: 0.7089Epoch 00002: val_loss improved from 0.57421 to 0.50415, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.0988 - acc: 0.7090 - val_loss: 0.5041 - val_acc: 0.8455\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9743 - acc: 0.7480Epoch 00003: val_loss improved from 0.50415 to 0.46462, saving model to saved_models/weights.best.InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.9736 - acc: 0.7479 - val_loss: 0.4646 - val_acc: 0.8539\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.9045 - acc: 0.7635Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 0.9063 - acc: 0.7633 - val_loss: 0.5096 - val_acc: 0.8563\n",
      "Epoch 6/15\n",
      "1470/6680 [=====>........................] - ETA: 5s - loss: 0.9559 - acc: 0.7707"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-d5a64668d4a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n\u001b[0;32m----> 2\u001b[0;31m                                                   [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-d5a64668d4a2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                   [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]\n",
      "\u001b[0;32m<ipython-input-59-547bfb7dd80b>\u001b[0m in \u001b[0;36mtrain_merged_model\u001b[0;34m(model_types, merge_type)\u001b[0m\n\u001b[1;32m     56\u001b[0m     merged_model.fit(extract_data('train'), train_targets, \n\u001b[1;32m     57\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m               epochs=15, batch_size=30, callbacks=[checkpointer], verbose=1)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m### Load the Model with the Best Validation Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2229\u001b[0m                               feed_dict=feed_dict)\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muninitialized_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged_models = [(params, train_merged_model(params[1], params[0])) for params in itertools.product([\"add\", \"concat\", \"multiply\", \"average\", \"maximum\"], \n",
    "                                                  [networks[1:]] + list(itertools.combinations(networks[1:], 2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "\n",
    "def train_model(network):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_data = bottleneck_features['train']\n",
    "    valid_data = bottleneck_features['valid']\n",
    "    test_data = bottleneck_features['test']\n",
    "    \n",
    "    img_input = Input(shape=train_data.shape[1:])\n",
    "    \n",
    "    x = img_input\n",
    "        \n",
    "    x = SeparableConv2D(filters=4096, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "\n",
    "    \n",
    "    residual = Conv2D(filters=4096, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Dropout(0.5)(x)\n",
    "\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation=\"relu\", kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(DOG_CLASSES, activation='softmax')(x)\n",
    "    model = Model(img_input, x)\n",
    "    model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(network)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    model.fit(train_data, train_targets, \n",
    "              validation_data=(valid_data, valid_targets),\n",
    "              epochs=30, batch_size=40, callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "    return (test_accuracy, model_file)\n",
    "\n",
    "    # report test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 7, 7, 2048)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_9 (Separabl (None, 7, 7, 4096)        8407040   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 7, 7, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "add_9 (Add)                  (None, 7, 7, 4096)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_9 ( (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 10,589,317.0\n",
      "Trainable params: 10,581,125.0\n",
      "Non-trainable params: 8,192.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 3.6459 - acc: 0.3645Epoch 00000: val_loss improved from inf to 4.14376, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 17s - loss: 3.6397 - acc: 0.3650 - val_loss: 4.1438 - val_acc: 0.6695\n",
      "Epoch 2/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 2.1003 - acc: 0.5590Epoch 00001: val_loss improved from 4.14376 to 2.07865, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 2.0952 - acc: 0.5594 - val_loss: 2.0786 - val_acc: 0.7042\n",
      "Epoch 3/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.8604 - acc: 0.6053Epoch 00002: val_loss improved from 2.07865 to 1.01839, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.8533 - acc: 0.6063 - val_loss: 1.0184 - val_acc: 0.7138\n",
      "Epoch 4/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.6752 - acc: 0.6515Epoch 00003: val_loss improved from 1.01839 to 0.94537, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.6760 - acc: 0.6516 - val_loss: 0.9454 - val_acc: 0.7329\n",
      "Epoch 5/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.6103 - acc: 0.6685Epoch 00004: val_loss improved from 0.94537 to 0.91030, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.6108 - acc: 0.6689 - val_loss: 0.9103 - val_acc: 0.7485\n",
      "Epoch 6/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.5215 - acc: 0.6821Epoch 00005: val_loss improved from 0.91030 to 0.79351, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.5220 - acc: 0.6816 - val_loss: 0.7935 - val_acc: 0.7725\n",
      "Epoch 7/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.4204 - acc: 0.7099Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.4172 - acc: 0.7103 - val_loss: 0.8007 - val_acc: 0.7964\n",
      "Epoch 8/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.3747 - acc: 0.7101Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.3766 - acc: 0.7094 - val_loss: 0.9415 - val_acc: 0.7557\n",
      "Epoch 9/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.3098 - acc: 0.7355Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.3155 - acc: 0.7352 - val_loss: 0.8624 - val_acc: 0.7856\n",
      "Epoch 10/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.2441 - acc: 0.7477Epoch 00009: val_loss improved from 0.79351 to 0.75270, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.2389 - acc: 0.7490 - val_loss: 0.7527 - val_acc: 0.8096\n",
      "Epoch 11/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.3015 - acc: 0.7414Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.3000 - acc: 0.7421 - val_loss: 0.7751 - val_acc: 0.8000\n",
      "Epoch 12/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1628 - acc: 0.7679Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.1636 - acc: 0.7678 - val_loss: 0.8243 - val_acc: 0.7916\n",
      "Epoch 13/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1689 - acc: 0.7617Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.1675 - acc: 0.7620 - val_loss: 0.8137 - val_acc: 0.8096\n",
      "Epoch 14/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.7693Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.1604 - acc: 0.7696 - val_loss: 0.8279 - val_acc: 0.7988\n",
      "Epoch 15/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.1367 - acc: 0.7788Epoch 00014: val_loss improved from 0.75270 to 0.71579, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 16s - loss: 1.1371 - acc: 0.7787 - val_loss: 0.7158 - val_acc: 0.8084\n",
      "Epoch 16/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.0779 - acc: 0.7807Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.0862 - acc: 0.7804 - val_loss: 0.9081 - val_acc: 0.7988\n",
      "Epoch 17/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.0947 - acc: 0.7889Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.0894 - acc: 0.7897 - val_loss: 0.8114 - val_acc: 0.8048\n",
      "Epoch 18/30\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.0854 - acc: 0.7932Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 16s - loss: 1.0836 - acc: 0.7937 - val_loss: 0.9071 - val_acc: 0.7856\n",
      "Epoch 19/30\n",
      "5000/6680 [=====================>........] - ETA: 3s - loss: 1.0662 - acc: 0.7970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c7afeb3a4e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_xception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f7b22f8f4517>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network)\u001b[0m\n\u001b[1;32m     53\u001b[0m     model.fit(train_data, train_targets, \n\u001b[1;32m     54\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m               epochs=30, batch_size=40, callbacks=[checkpointer], verbose=1, shuffle=True)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m### Load the Model with the Best Validation Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind-dog/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_xception = train_model(\"Xception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82.41626794258373, 'saved_models/weights.best.Xception.hdf5')\n"
     ]
    }
   ],
   "source": [
    "print(model_xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Define your architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-924a59c303df>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-924a59c303df>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    extract_Resnet50(path_to_tensor(image_path))]\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "def predict(image_path):\n",
    "    bottleneck_feature = [extract_Xception(path_to_tensor(image_path))\n",
    "                        extract_Resnet50(path_to_tensor(image_path))]\n",
    "    predicted_prob = merged_model[-1].predict(bottleneck_feature)\n",
    "    print(predicted_prob.shape)\n",
    "    print(predicted_prob)\n",
    "    predicted_idx = np.argmax(predicted_prob[0])\n",
    "    print(predicted_idx)\n",
    "    return dog_names[predicted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "my_image_paths = [\"test_images/dog4.jpg\",\n",
    "                \"test_images/philippe_jordan.jpg\", \n",
    "                  \"test_images/eric_clapton.jpg\",\n",
    "                  \"test_images/shibainu.jpg\", \n",
    "                  \"test_images/pag.jpg\", \n",
    "                 \"test_images/dog2.jpg\",\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n",
      "[[ 0.34740329  0.          0.         ...,  0.00185373  0.          0.        ]]\n",
      "160\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-6cfc26d91344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmy_image_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-150-e563d41e0d49>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpredicted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdog_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for image_path in my_image_paths:\n",
    "    print(predict(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Affenpinscher', 'Afghan_hound', 'Airedale_terrier', 'Akita', 'Alaskan_malamute', 'American_eskimo_dog', 'American_foxhound', 'American_staffordshire_terrier', 'American_water_spaniel', 'Anatolian_shepherd_dog', 'Australian_cattle_dog', 'Australian_shepherd', 'Australian_terrier', 'Basenji', 'Basset_hound', 'Beagle', 'Bearded_collie', 'Beauceron', 'Bedlington_terrier', 'Belgian_malinois', 'Belgian_sheepdog', 'Belgian_tervuren', 'Bernese_mountain_dog', 'Bichon_frise', 'Black_and_tan_coonhound', 'Black_russian_terrier', 'Bloodhound', 'Bluetick_coonhound', 'Border_collie', 'Border_terrier', 'Borzoi', 'Boston_terrier', 'Bouvier_des_flandres', 'Boxer', 'Boykin_spaniel', 'Briard', 'Brittany', 'Brussels_griffon', 'Bull_terrier', 'Bulldog', 'Bullmastiff', 'Cairn_terrier', 'Canaan_dog', 'Cane_corso', 'Cardigan_welsh_corgi', 'Cavalier_king_charles_spaniel', 'Chesapeake_bay_retriever', 'Chihuahua', 'Chinese_crested', 'Chinese_shar-pei', 'Chow_chow', 'Clumber_spaniel', 'Cocker_spaniel', 'Collie', 'Curly-coated_retriever', 'Dachshund', 'Dalmatian', 'Dandie_dinmont_terrier', 'Doberman_pinscher', 'Dogue_de_bordeaux', 'English_cocker_spaniel', 'English_setter', 'English_springer_spaniel', 'English_toy_spaniel', 'Entlebucher_mountain_dog', 'Field_spaniel', 'Finnish_spitz', 'Flat-coated_retriever', 'French_bulldog', 'German_pinscher', 'German_shepherd_dog', 'German_shorthaired_pointer', 'German_wirehaired_pointer', 'Giant_schnauzer', 'Glen_of_imaal_terrier', 'Golden_retriever', 'Gordon_setter', 'Great_dane', 'Great_pyrenees', 'Greater_swiss_mountain_dog', 'Greyhound', 'Havanese', 'Ibizan_hound', 'Icelandic_sheepdog', 'Irish_red_and_white_setter', 'Irish_setter', 'Irish_terrier', 'Irish_water_spaniel', 'Irish_wolfhound', 'Italian_greyhound', 'Japanese_chin', 'Keeshond', 'Kerry_blue_terrier', 'Komondor', 'Kuvasz', 'Labrador_retriever', 'Lakeland_terrier', 'Leonberger', 'Lhasa_apso', 'Lowchen', 'Maltese', 'Manchester_terrier', 'Mastiff', 'Miniature_schnauzer', 'Neapolitan_mastiff', 'Newfoundland', 'Norfolk_terrier', 'Norwegian_buhund', 'Norwegian_elkhound', 'Norwegian_lundehund', 'Norwich_terrier', 'Nova_scotia_duck_tolling_retriever', 'Old_english_sheepdog', 'Otterhound', 'Papillon', 'Parson_russell_terrier', 'Pekingese', 'Pembroke_welsh_corgi', 'Petit_basset_griffon_vendeen', 'Pharaoh_hound', 'Plott', 'Pointer', 'Pomeranian', 'Poodle', 'Portuguese_water_dog', 'Saint_bernard', 'Silky_terrier', 'Smooth_fox_terrier', 'Tibetan_mastiff', 'Welsh_springer_spaniel', 'Wirehaired_pointing_griffon', 'Xoloitzcuintli', 'Yorkshire_terrier']\n"
     ]
    }
   ],
   "source": [
    "print(dog_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dog_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
