{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f84bf9e40749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# load color (BGR) image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# convert BGR image to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot call `vectorize` on size 0 inputs unless `otypes` is set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70c350408927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"percentage of the images hava a detected face in human_files_short: {} %\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_face_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files_short\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"percentage of the images hava a detected face in dog_files_short: {} %\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_face_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog_files_short\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-70c350408927>\u001b[0m in \u001b[0;36mcalc_face_detected_proportion\u001b[0;34m(img_paths)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_face_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvectorized_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_detector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvectorized_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2600\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2602\u001b[0;31m                 raise ValueError('cannot call `vectorize` on size 0 inputs '\n\u001b[0m\u001b[1;32m   2603\u001b[0m                                  'unless `otypes` is set')\n\u001b[1;32m   2604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot call `vectorize` on size 0 inputs unless `otypes` is set"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "def calc_face_detected_proportion(img_paths):\n",
    "    vectorized_detector = np.vectorize(face_detector)\n",
    "    return len(img_paths[vectorized_detector(img_paths)]) / len(img_paths)\n",
    "\n",
    "\n",
    "print(\"percentage of the images hava a detected face in human_files_short: {} %\".format(calc_face_detected_proportion(human_files_short) * 100) )\n",
    "print(\"percentage of the images hava a detected face in dog_files_short: {} %\".format(calc_face_detected_proportion(dog_files_short) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "I think it depends on the buisiness purpose. However, supposed to widely used api, it should not pose the limitations on user because u\n",
    "\n",
    "With cnn, \n",
    "\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot call `vectorize` on size 0 inputs unless `otypes` is set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b38578e66ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_dog_detected\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"percentage of the images hava a detected dog in human_files_short: {} %\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_dog_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files_short\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"percentage of the images hava a detected dog in dog_files_short: {} %\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_dog_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog_files_short\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b38578e66ef0>\u001b[0m in \u001b[0;36mcalc_dog_detected_proportion\u001b[0;34m(img_paths)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_dog_detected_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvectorized_doc_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog_detector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnum_dog_detected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvectorized_doc_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum_dog_detected\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2600\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2602\u001b[0;31m                 raise ValueError('cannot call `vectorize` on size 0 inputs '\n\u001b[0m\u001b[1;32m   2603\u001b[0m                                  'unless `otypes` is set')\n\u001b[1;32m   2604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot call `vectorize` on size 0 inputs unless `otypes` is set"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "def calc_dog_detected_proportion(img_paths):\n",
    "    vectorized_doc_detector = np.vectorize(dog_detector)\n",
    "    num_dog_detected = len(img_paths[vectorized_doc_detector(img_paths)])\n",
    "    return num_dog_detected / len(img_paths)\n",
    "\n",
    "print(\"percentage of the images hava a detected dog in human_files_short: {} %\".format(calc_dog_detected_proportion(human_files_short) * 100))\n",
    "print(\"percentage of the images hava a detected dog in dog_files_short: {} %\".format(calc_dog_detected_proportion(dog_files_short) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:53<00:00, 124.44it/s]\n",
      "100%|██████████| 835/835 [00:05<00:00, 140.04it/s]\n",
      "100%|██████████| 836/836 [00:05<00:00, 169.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 224, 224, 64)  1728                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 224, 224, 64)  256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 224, 224, 64)  0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCon (None, 224, 224, 128) 8768                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 224, 224, 128) 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 224, 224, 128) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCon (None, 224, 224, 128) 17536                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 224, 224, 128) 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 224, 224, 128) 8192                                         \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 224, 224, 128) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 224, 224, 128) 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "add_18 (Add)                     (None, 224, 224, 128) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCon (None, 224, 224, 256) 33920                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 224, 224, 256) 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 224, 224, 256) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCon (None, 224, 224, 256) 67840                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 224, 224, 256) 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 224, 224, 256) 32768                                        \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 224, 224, 256) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 224, 224, 256) 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_19 (Add)                     (None, 224, 224, 256) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 133)           34181                                        \n",
      "====================================================================================================\n",
      "Total params: 209,797.0\n",
      "Trainable params: 207,365.0\n",
      "Non-trainable params: 2,432.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "residual = Conv2D(filters=128, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=128, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "residual = Conv2D(filters=256, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax')(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   3/6680 [..............................] - ETA: 719579s - loss: 5.2616 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1d20fe29da2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                   epochs=epochs, validation_steps=valid_tensors.shape[0], callbacks=[checkpointer, history_storer, base_logger], verbose=1)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#history = model.fit(train_tensors, train_targets,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History, BaseLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "RANDOM_SEED=10\n",
    "epochs = 30\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "\n",
    "valid_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "train_generator.fit(train_tensors, seed=RANDOM_SEED)\n",
    "valid_generator.fit(valid_tensors, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "history_storer = History()\n",
    "base_logger = BaseLogger()\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size=20\n",
    "model.fit_generator(train_generator.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_tensors.shape[0],\n",
    "                  validation_data=valid_generator.flow(valid_tensors, valid_targets, batch_size=batch_size),\n",
    "                  epochs=epochs, validation_steps=valid_tensors.shape[0], callbacks=[checkpointer, history_storer, base_logger], verbose=1)\n",
    "\n",
    "#history = model.fit(train_tensors, train_targets, \n",
    "#          validation_data=(valid_tensors, valid_targets),\n",
    "#          epochs=epochs, batch_size=20, callbacks=[checkpointer, history_storer, base_logger], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100 * np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1)) / len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6580/6680 [============================>.] - ETA: 148s - loss: 14.4393 - acc: 0.05 - ETA: 20s - loss: 14.5490 - acc: 0.0188 - ETA: 11s - loss: 14.4552 - acc: 0.023 - ETA: 9s - loss: 14.5233 - acc: 0.023 - ETA: 7s - loss: 14.2980 - acc: 0.03 - ETA: 6s - loss: 14.2529 - acc: 0.03 - ETA: 5s - loss: 14.2113 - acc: 0.03 - ETA: 4s - loss: 14.1709 - acc: 0.03 - ETA: 4s - loss: 14.0686 - acc: 0.04 - ETA: 4s - loss: 13.9183 - acc: 0.04 - ETA: 3s - loss: 13.8582 - acc: 0.04 - ETA: 3s - loss: 13.7835 - acc: 0.05 - ETA: 3s - loss: 13.7094 - acc: 0.05 - ETA: 3s - loss: 13.6460 - acc: 0.05 - ETA: 3s - loss: 13.6196 - acc: 0.05 - ETA: 2s - loss: 13.5635 - acc: 0.05 - ETA: 2s - loss: 13.5402 - acc: 0.05 - ETA: 2s - loss: 13.4613 - acc: 0.06 - ETA: 2s - loss: 13.4469 - acc: 0.06 - ETA: 2s - loss: 13.3886 - acc: 0.06 - ETA: 2s - loss: 13.3197 - acc: 0.06 - ETA: 2s - loss: 13.2838 - acc: 0.07 - ETA: 2s - loss: 13.2448 - acc: 0.07 - ETA: 1s - loss: 13.1960 - acc: 0.07 - ETA: 1s - loss: 13.0754 - acc: 0.07 - ETA: 1s - loss: 13.0476 - acc: 0.07 - ETA: 1s - loss: 12.9930 - acc: 0.08 - ETA: 1s - loss: 12.9255 - acc: 0.08 - ETA: 1s - loss: 12.8542 - acc: 0.08 - ETA: 1s - loss: 12.8286 - acc: 0.08 - ETA: 1s - loss: 12.7661 - acc: 0.09 - ETA: 1s - loss: 12.7339 - acc: 0.09 - ETA: 1s - loss: 12.6976 - acc: 0.09 - ETA: 1s - loss: 12.6459 - acc: 0.09 - ETA: 1s - loss: 12.5538 - acc: 0.10 - ETA: 1s - loss: 12.5098 - acc: 0.10 - ETA: 0s - loss: 12.4834 - acc: 0.10 - ETA: 0s - loss: 12.4200 - acc: 0.10 - ETA: 0s - loss: 12.3607 - acc: 0.10 - ETA: 0s - loss: 12.3212 - acc: 0.11 - ETA: 0s - loss: 12.2731 - acc: 0.11 - ETA: 0s - loss: 12.2582 - acc: 0.11 - ETA: 0s - loss: 12.2195 - acc: 0.11 - ETA: 0s - loss: 12.1752 - acc: 0.11 - ETA: 0s - loss: 12.1475 - acc: 0.12 - ETA: 0s - loss: 12.1003 - acc: 0.12 - ETA: 0s - loss: 12.0741 - acc: 0.12 - ETA: 0s - loss: 12.0473 - acc: 0.12 - ETA: 0s - loss: 11.9993 - acc: 0.12 - ETA: 0s - loss: 11.9650 - acc: 0.13 - ETA: 0s - loss: 11.9343 - acc: 0.1334Epoch 00000: val_loss improved from inf to 10.46817, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 11.9010 - acc: 0.1355 - val_loss: 10.4682 - val_acc: 0.2263\n",
      "Epoch 2/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 11.3439 - acc: 0.15 - ETA: 3s - loss: 10.2317 - acc: 0.28 - ETA: 3s - loss: 10.1631 - acc: 0.26 - ETA: 3s - loss: 10.3988 - acc: 0.25 - ETA: 2s - loss: 10.0310 - acc: 0.26 - ETA: 2s - loss: 10.0438 - acc: 0.27 - ETA: 2s - loss: 10.0230 - acc: 0.27 - ETA: 2s - loss: 9.9733 - acc: 0.2756 - ETA: 2s - loss: 9.9857 - acc: 0.279 - ETA: 2s - loss: 9.9475 - acc: 0.281 - ETA: 2s - loss: 10.0310 - acc: 0.27 - ETA: 2s - loss: 9.9808 - acc: 0.2750 - ETA: 2s - loss: 9.8892 - acc: 0.277 - ETA: 2s - loss: 9.9210 - acc: 0.275 - ETA: 2s - loss: 9.8995 - acc: 0.278 - ETA: 2s - loss: 9.8839 - acc: 0.276 - ETA: 2s - loss: 9.8819 - acc: 0.278 - ETA: 2s - loss: 9.8265 - acc: 0.282 - ETA: 1s - loss: 9.7613 - acc: 0.286 - ETA: 1s - loss: 9.7479 - acc: 0.287 - ETA: 1s - loss: 9.7510 - acc: 0.286 - ETA: 1s - loss: 9.7538 - acc: 0.287 - ETA: 1s - loss: 9.7245 - acc: 0.289 - ETA: 1s - loss: 9.6594 - acc: 0.292 - ETA: 1s - loss: 9.6864 - acc: 0.290 - ETA: 1s - loss: 9.6931 - acc: 0.290 - ETA: 1s - loss: 9.7054 - acc: 0.289 - ETA: 1s - loss: 9.7082 - acc: 0.290 - ETA: 1s - loss: 9.6784 - acc: 0.293 - ETA: 1s - loss: 9.6798 - acc: 0.294 - ETA: 1s - loss: 9.6853 - acc: 0.294 - ETA: 1s - loss: 9.6935 - acc: 0.294 - ETA: 1s - loss: 9.6809 - acc: 0.296 - ETA: 1s - loss: 9.6417 - acc: 0.298 - ETA: 1s - loss: 9.6017 - acc: 0.299 - ETA: 0s - loss: 9.5979 - acc: 0.300 - ETA: 0s - loss: 9.5987 - acc: 0.300 - ETA: 0s - loss: 9.5970 - acc: 0.300 - ETA: 0s - loss: 9.6143 - acc: 0.300 - ETA: 0s - loss: 9.5941 - acc: 0.302 - ETA: 0s - loss: 9.5886 - acc: 0.303 - ETA: 0s - loss: 9.5462 - acc: 0.305 - ETA: 0s - loss: 9.5473 - acc: 0.305 - ETA: 0s - loss: 9.5513 - acc: 0.305 - ETA: 0s - loss: 9.5505 - acc: 0.306 - ETA: 0s - loss: 9.5326 - acc: 0.306 - ETA: 0s - loss: 9.5150 - acc: 0.306 - ETA: 0s - loss: 9.4905 - acc: 0.308 - ETA: 0s - loss: 9.4825 - acc: 0.308 - ETA: 0s - loss: 9.4832 - acc: 0.309 - ETA: 0s - loss: 9.5028 - acc: 0.308 - ETA: 0s - loss: 9.4826 - acc: 0.309 - ETA: 0s - loss: 9.5016 - acc: 0.309 - ETA: 0s - loss: 9.4969 - acc: 0.3103Epoch 00001: val_loss improved from 10.46817 to 9.37262, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4935 - acc: 0.3105 - val_loss: 9.3726 - val_acc: 0.3269\n",
      "Epoch 3/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.9605 - acc: 0.400 - ETA: 3s - loss: 8.8353 - acc: 0.378 - ETA: 2s - loss: 9.4159 - acc: 0.361 - ETA: 2s - loss: 9.1783 - acc: 0.373 - ETA: 2s - loss: 9.0468 - acc: 0.384 - ETA: 2s - loss: 9.2068 - acc: 0.374 - ETA: 2s - loss: 9.1361 - acc: 0.379 - ETA: 2s - loss: 9.1137 - acc: 0.373 - ETA: 2s - loss: 9.0287 - acc: 0.377 - ETA: 2s - loss: 9.1241 - acc: 0.373 - ETA: 2s - loss: 9.0334 - acc: 0.378 - ETA: 2s - loss: 8.9112 - acc: 0.381 - ETA: 2s - loss: 8.9210 - acc: 0.378 - ETA: 2s - loss: 8.9963 - acc: 0.375 - ETA: 2s - loss: 9.0427 - acc: 0.369 - ETA: 2s - loss: 8.9661 - acc: 0.372 - ETA: 2s - loss: 8.9375 - acc: 0.373 - ETA: 2s - loss: 8.8885 - acc: 0.376 - ETA: 1s - loss: 8.8451 - acc: 0.378 - ETA: 1s - loss: 8.8216 - acc: 0.380 - ETA: 1s - loss: 8.8661 - acc: 0.379 - ETA: 1s - loss: 8.8478 - acc: 0.380 - ETA: 1s - loss: 8.8229 - acc: 0.383 - ETA: 1s - loss: 8.8608 - acc: 0.379 - ETA: 1s - loss: 8.8577 - acc: 0.380 - ETA: 1s - loss: 8.8589 - acc: 0.381 - ETA: 1s - loss: 8.8738 - acc: 0.378 - ETA: 1s - loss: 8.8724 - acc: 0.379 - ETA: 1s - loss: 8.8711 - acc: 0.380 - ETA: 1s - loss: 8.9108 - acc: 0.378 - ETA: 1s - loss: 8.8954 - acc: 0.378 - ETA: 1s - loss: 8.8411 - acc: 0.381 - ETA: 1s - loss: 8.8492 - acc: 0.379 - ETA: 1s - loss: 8.8443 - acc: 0.380 - ETA: 1s - loss: 8.8353 - acc: 0.381 - ETA: 1s - loss: 8.8410 - acc: 0.381 - ETA: 0s - loss: 8.8586 - acc: 0.379 - ETA: 0s - loss: 8.8255 - acc: 0.381 - ETA: 0s - loss: 8.8362 - acc: 0.381 - ETA: 0s - loss: 8.8675 - acc: 0.380 - ETA: 0s - loss: 8.8812 - acc: 0.379 - ETA: 0s - loss: 8.8733 - acc: 0.379 - ETA: 0s - loss: 8.8501 - acc: 0.381 - ETA: 0s - loss: 8.8464 - acc: 0.381 - ETA: 0s - loss: 8.8665 - acc: 0.380 - ETA: 0s - loss: 8.8626 - acc: 0.380 - ETA: 0s - loss: 8.8726 - acc: 0.380 - ETA: 0s - loss: 8.8891 - acc: 0.379 - ETA: 0s - loss: 8.8843 - acc: 0.379 - ETA: 0s - loss: 8.8623 - acc: 0.380 - ETA: 0s - loss: 8.8573 - acc: 0.380 - ETA: 0s - loss: 8.8402 - acc: 0.381 - ETA: 0s - loss: 8.8329 - acc: 0.381 - ETA: 0s - loss: 8.8208 - acc: 0.381 - ETA: 0s - loss: 8.8063 - acc: 0.3821Epoch 00002: val_loss improved from 9.37262 to 9.01960, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.8262 - acc: 0.3813 - val_loss: 9.0196 - val_acc: 0.3521\n",
      "Epoch 4/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 5.3237 - acc: 0.600 - ETA: 3s - loss: 7.1220 - acc: 0.507 - ETA: 2s - loss: 7.3717 - acc: 0.496 - ETA: 2s - loss: 8.4736 - acc: 0.431 - ETA: 2s - loss: 8.7189 - acc: 0.414 - ETA: 2s - loss: 8.8358 - acc: 0.408 - ETA: 2s - loss: 8.5364 - acc: 0.423 - ETA: 2s - loss: 8.4849 - acc: 0.421 - ETA: 2s - loss: 8.3415 - acc: 0.431 - ETA: 2s - loss: 8.3891 - acc: 0.428 - ETA: 2s - loss: 8.2625 - acc: 0.438 - ETA: 2s - loss: 8.2255 - acc: 0.439 - ETA: 2s - loss: 8.1427 - acc: 0.443 - ETA: 2s - loss: 8.0838 - acc: 0.444 - ETA: 2s - loss: 8.1855 - acc: 0.438 - ETA: 2s - loss: 8.2372 - acc: 0.436 - ETA: 1s - loss: 8.2657 - acc: 0.435 - ETA: 1s - loss: 8.2471 - acc: 0.435 - ETA: 1s - loss: 8.2455 - acc: 0.434 - ETA: 1s - loss: 8.2371 - acc: 0.432 - ETA: 1s - loss: 8.2651 - acc: 0.430 - ETA: 1s - loss: 8.3193 - acc: 0.426 - ETA: 1s - loss: 8.3409 - acc: 0.424 - ETA: 1s - loss: 8.4076 - acc: 0.421 - ETA: 1s - loss: 8.4390 - acc: 0.419 - ETA: 1s - loss: 8.4817 - acc: 0.416 - ETA: 1s - loss: 8.4930 - acc: 0.416 - ETA: 1s - loss: 8.4874 - acc: 0.417 - ETA: 1s - loss: 8.4706 - acc: 0.418 - ETA: 1s - loss: 8.4739 - acc: 0.417 - ETA: 1s - loss: 8.4571 - acc: 0.419 - ETA: 1s - loss: 8.4532 - acc: 0.419 - ETA: 1s - loss: 8.4446 - acc: 0.420 - ETA: 1s - loss: 8.4694 - acc: 0.419 - ETA: 1s - loss: 8.4866 - acc: 0.418 - ETA: 1s - loss: 8.4713 - acc: 0.419 - ETA: 0s - loss: 8.4670 - acc: 0.420 - ETA: 0s - loss: 8.4651 - acc: 0.419 - ETA: 0s - loss: 8.4688 - acc: 0.419 - ETA: 0s - loss: 8.4627 - acc: 0.419 - ETA: 0s - loss: 8.4608 - acc: 0.419 - ETA: 0s - loss: 8.4735 - acc: 0.418 - ETA: 0s - loss: 8.4998 - acc: 0.416 - ETA: 0s - loss: 8.4985 - acc: 0.416 - ETA: 0s - loss: 8.5191 - acc: 0.415 - ETA: 0s - loss: 8.4988 - acc: 0.416 - ETA: 0s - loss: 8.4695 - acc: 0.418 - ETA: 0s - loss: 8.4595 - acc: 0.418 - ETA: 0s - loss: 8.4962 - acc: 0.416 - ETA: 0s - loss: 8.5077 - acc: 0.415 - ETA: 0s - loss: 8.5033 - acc: 0.415 - ETA: 0s - loss: 8.5056 - acc: 0.415 - ETA: 0s - loss: 8.4939 - acc: 0.416 - ETA: 0s - loss: 8.5064 - acc: 0.4166Epoch 00003: val_loss improved from 9.01960 to 8.87551, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.5072 - acc: 0.4169 - val_loss: 8.8755 - val_acc: 0.3665\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.7305 - acc: 0.350 - ETA: 3s - loss: 8.0499 - acc: 0.466 - ETA: 3s - loss: 8.2876 - acc: 0.445 - ETA: 3s - loss: 8.4324 - acc: 0.429 - ETA: 2s - loss: 8.3660 - acc: 0.439 - ETA: 2s - loss: 8.2938 - acc: 0.446 - ETA: 2s - loss: 8.2847 - acc: 0.445 - ETA: 2s - loss: 8.2611 - acc: 0.447 - ETA: 2s - loss: 8.3435 - acc: 0.442 - ETA: 2s - loss: 8.3644 - acc: 0.438 - ETA: 2s - loss: 8.4946 - acc: 0.432 - ETA: 2s - loss: 8.4310 - acc: 0.437 - ETA: 2s - loss: 8.4968 - acc: 0.434 - ETA: 2s - loss: 8.5064 - acc: 0.431 - ETA: 2s - loss: 8.4344 - acc: 0.436 - ETA: 2s - loss: 8.4922 - acc: 0.432 - ETA: 2s - loss: 8.5164 - acc: 0.431 - ETA: 2s - loss: 8.4297 - acc: 0.436 - ETA: 2s - loss: 8.4066 - acc: 0.437 - ETA: 1s - loss: 8.4033 - acc: 0.438 - ETA: 1s - loss: 8.4564 - acc: 0.436 - ETA: 1s - loss: 8.4379 - acc: 0.437 - ETA: 1s - loss: 8.4295 - acc: 0.437 - ETA: 1s - loss: 8.3866 - acc: 0.440 - ETA: 1s - loss: 8.4043 - acc: 0.440 - ETA: 1s - loss: 8.4229 - acc: 0.440 - ETA: 1s - loss: 8.4047 - acc: 0.441 - ETA: 1s - loss: 8.3823 - acc: 0.442 - ETA: 1s - loss: 8.3570 - acc: 0.444 - ETA: 1s - loss: 8.3383 - acc: 0.445 - ETA: 1s - loss: 8.3757 - acc: 0.443 - ETA: 1s - loss: 8.3778 - acc: 0.443 - ETA: 1s - loss: 8.4256 - acc: 0.441 - ETA: 1s - loss: 8.4048 - acc: 0.442 - ETA: 1s - loss: 8.4423 - acc: 0.440 - ETA: 1s - loss: 8.4278 - acc: 0.440 - ETA: 1s - loss: 8.4177 - acc: 0.441 - ETA: 1s - loss: 8.4707 - acc: 0.439 - ETA: 0s - loss: 8.4689 - acc: 0.438 - ETA: 0s - loss: 8.4752 - acc: 0.437 - ETA: 0s - loss: 8.4786 - acc: 0.437 - ETA: 0s - loss: 8.4920 - acc: 0.436 - ETA: 0s - loss: 8.4651 - acc: 0.437 - ETA: 0s - loss: 8.4421 - acc: 0.439 - ETA: 0s - loss: 8.4364 - acc: 0.439 - ETA: 0s - loss: 8.4233 - acc: 0.439 - ETA: 0s - loss: 8.4089 - acc: 0.440 - ETA: 0s - loss: 8.3937 - acc: 0.441 - ETA: 0s - loss: 8.3927 - acc: 0.442 - ETA: 0s - loss: 8.4207 - acc: 0.440 - ETA: 0s - loss: 8.4118 - acc: 0.441 - ETA: 0s - loss: 8.3988 - acc: 0.441 - ETA: 0s - loss: 8.3896 - acc: 0.442 - ETA: 0s - loss: 8.3831 - acc: 0.442 - ETA: 0s - loss: 8.3800 - acc: 0.442 - ETA: 0s - loss: 8.3790 - acc: 0.4415Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.3737 - acc: 0.4415 - val_loss: 8.9443 - val_acc: 0.3689\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 6.7664 - acc: 0.500 - ETA: 2s - loss: 8.0628 - acc: 0.450 - ETA: 2s - loss: 8.5319 - acc: 0.428 - ETA: 2s - loss: 8.5088 - acc: 0.430 - ETA: 2s - loss: 8.5447 - acc: 0.434 - ETA: 2s - loss: 8.6621 - acc: 0.432 - ETA: 2s - loss: 8.3765 - acc: 0.450 - ETA: 2s - loss: 8.3038 - acc: 0.453 - ETA: 2s - loss: 8.2782 - acc: 0.454 - ETA: 2s - loss: 8.3307 - acc: 0.453 - ETA: 2s - loss: 8.3753 - acc: 0.450 - ETA: 2s - loss: 8.3757 - acc: 0.451 - ETA: 2s - loss: 8.3996 - acc: 0.450 - ETA: 2s - loss: 8.3017 - acc: 0.457 - ETA: 2s - loss: 8.3390 - acc: 0.453 - ETA: 1s - loss: 8.3596 - acc: 0.452 - ETA: 1s - loss: 8.3547 - acc: 0.451 - ETA: 1s - loss: 8.3621 - acc: 0.451 - ETA: 1s - loss: 8.2643 - acc: 0.457 - ETA: 1s - loss: 8.2443 - acc: 0.457 - ETA: 1s - loss: 8.3189 - acc: 0.452 - ETA: 1s - loss: 8.3773 - acc: 0.449 - ETA: 1s - loss: 8.3550 - acc: 0.450 - ETA: 1s - loss: 8.3578 - acc: 0.450 - ETA: 1s - loss: 8.3488 - acc: 0.451 - ETA: 1s - loss: 8.3065 - acc: 0.454 - ETA: 1s - loss: 8.3143 - acc: 0.453 - ETA: 1s - loss: 8.3325 - acc: 0.452 - ETA: 1s - loss: 8.3011 - acc: 0.454 - ETA: 1s - loss: 8.3003 - acc: 0.454 - ETA: 1s - loss: 8.3014 - acc: 0.454 - ETA: 1s - loss: 8.3164 - acc: 0.453 - ETA: 1s - loss: 8.3182 - acc: 0.453 - ETA: 1s - loss: 8.3307 - acc: 0.452 - ETA: 1s - loss: 8.3344 - acc: 0.452 - ETA: 1s - loss: 8.3261 - acc: 0.453 - ETA: 0s - loss: 8.3210 - acc: 0.453 - ETA: 0s - loss: 8.3294 - acc: 0.452 - ETA: 0s - loss: 8.3608 - acc: 0.451 - ETA: 0s - loss: 8.3588 - acc: 0.451 - ETA: 0s - loss: 8.3566 - acc: 0.452 - ETA: 0s - loss: 8.3247 - acc: 0.453 - ETA: 0s - loss: 8.3241 - acc: 0.452 - ETA: 0s - loss: 8.3132 - acc: 0.453 - ETA: 0s - loss: 8.3365 - acc: 0.452 - ETA: 0s - loss: 8.3218 - acc: 0.452 - ETA: 0s - loss: 8.2879 - acc: 0.455 - ETA: 0s - loss: 8.2901 - acc: 0.455 - ETA: 0s - loss: 8.2931 - acc: 0.455 - ETA: 0s - loss: 8.2892 - acc: 0.455 - ETA: 0s - loss: 8.2579 - acc: 0.457 - ETA: 0s - loss: 8.2726 - acc: 0.456 - ETA: 0s - loss: 8.2728 - acc: 0.457 - ETA: 0s - loss: 8.2699 - acc: 0.457 - ETA: 0s - loss: 8.2874 - acc: 0.4566Epoch 00005: val_loss improved from 8.87551 to 8.71168, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.2933 - acc: 0.4564 - val_loss: 8.7117 - val_acc: 0.3916\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 7.3177 - acc: 0.500 - ETA: 3s - loss: 8.7902 - acc: 0.421 - ETA: 2s - loss: 9.2973 - acc: 0.388 - ETA: 2s - loss: 8.7896 - acc: 0.425 - ETA: 2s - loss: 8.6150 - acc: 0.441 - ETA: 2s - loss: 8.4869 - acc: 0.451 - ETA: 2s - loss: 8.2471 - acc: 0.464 - ETA: 2s - loss: 8.3114 - acc: 0.459 - ETA: 2s - loss: 8.2859 - acc: 0.463 - ETA: 2s - loss: 8.2576 - acc: 0.464 - ETA: 2s - loss: 8.2291 - acc: 0.465 - ETA: 2s - loss: 8.1707 - acc: 0.469 - ETA: 2s - loss: 8.1631 - acc: 0.469 - ETA: 2s - loss: 8.1207 - acc: 0.472 - ETA: 2s - loss: 8.1269 - acc: 0.472 - ETA: 2s - loss: 8.2007 - acc: 0.467 - ETA: 2s - loss: 8.1342 - acc: 0.470 - ETA: 1s - loss: 8.1475 - acc: 0.470 - ETA: 1s - loss: 8.1266 - acc: 0.471 - ETA: 1s - loss: 8.1363 - acc: 0.471 - ETA: 1s - loss: 8.0957 - acc: 0.474 - ETA: 1s - loss: 8.1484 - acc: 0.470 - ETA: 1s - loss: 8.1751 - acc: 0.468 - ETA: 1s - loss: 8.1707 - acc: 0.467 - ETA: 1s - loss: 8.1790 - acc: 0.467 - ETA: 1s - loss: 8.1472 - acc: 0.469 - ETA: 1s - loss: 8.0897 - acc: 0.473 - ETA: 1s - loss: 8.1017 - acc: 0.472 - ETA: 1s - loss: 8.1085 - acc: 0.472 - ETA: 1s - loss: 8.1156 - acc: 0.471 - ETA: 1s - loss: 8.1155 - acc: 0.472 - ETA: 1s - loss: 8.1199 - acc: 0.472 - ETA: 1s - loss: 8.1228 - acc: 0.472 - ETA: 1s - loss: 8.0876 - acc: 0.474 - ETA: 1s - loss: 8.0900 - acc: 0.474 - ETA: 1s - loss: 8.1104 - acc: 0.473 - ETA: 0s - loss: 8.1243 - acc: 0.472 - ETA: 0s - loss: 8.0906 - acc: 0.473 - ETA: 0s - loss: 8.1229 - acc: 0.471 - ETA: 0s - loss: 8.1124 - acc: 0.471 - ETA: 0s - loss: 8.1142 - acc: 0.471 - ETA: 0s - loss: 8.1242 - acc: 0.470 - ETA: 0s - loss: 8.1325 - acc: 0.470 - ETA: 0s - loss: 8.1348 - acc: 0.469 - ETA: 0s - loss: 8.1567 - acc: 0.468 - ETA: 0s - loss: 8.1599 - acc: 0.468 - ETA: 0s - loss: 8.1638 - acc: 0.468 - ETA: 0s - loss: 8.1636 - acc: 0.468 - ETA: 0s - loss: 8.1317 - acc: 0.470 - ETA: 0s - loss: 8.1503 - acc: 0.468 - ETA: 0s - loss: 8.1421 - acc: 0.468 - ETA: 0s - loss: 8.1326 - acc: 0.468 - ETA: 0s - loss: 8.1473 - acc: 0.4681Epoch 00006: val_loss improved from 8.71168 to 8.69161, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1615 - acc: 0.4672 - val_loss: 8.6916 - val_acc: 0.3880\n",
      "Epoch 8/20\n",
      "6560/6680 [============================>.] - ETA: 4s - loss: 8.0625 - acc: 0.500 - ETA: 3s - loss: 6.9307 - acc: 0.557 - ETA: 3s - loss: 6.9942 - acc: 0.550 - ETA: 2s - loss: 7.5059 - acc: 0.521 - ETA: 2s - loss: 7.7243 - acc: 0.507 - ETA: 2s - loss: 7.9415 - acc: 0.495 - ETA: 2s - loss: 7.9000 - acc: 0.497 - ETA: 2s - loss: 7.8857 - acc: 0.493 - ETA: 2s - loss: 7.7506 - acc: 0.501 - ETA: 2s - loss: 7.7340 - acc: 0.501 - ETA: 2s - loss: 7.7987 - acc: 0.499 - ETA: 2s - loss: 7.8160 - acc: 0.497 - ETA: 2s - loss: 7.8259 - acc: 0.498 - ETA: 2s - loss: 7.9456 - acc: 0.491 - ETA: 2s - loss: 8.0395 - acc: 0.485 - ETA: 1s - loss: 8.0448 - acc: 0.485 - ETA: 1s - loss: 8.1496 - acc: 0.479 - ETA: 1s - loss: 8.1067 - acc: 0.480 - ETA: 1s - loss: 8.0512 - acc: 0.484 - ETA: 1s - loss: 8.0687 - acc: 0.483 - ETA: 1s - loss: 8.1079 - acc: 0.481 - ETA: 1s - loss: 8.1183 - acc: 0.481 - ETA: 1s - loss: 8.0718 - acc: 0.484 - ETA: 1s - loss: 8.1284 - acc: 0.480 - ETA: 1s - loss: 8.1255 - acc: 0.481 - ETA: 1s - loss: 8.1345 - acc: 0.480 - ETA: 1s - loss: 8.1256 - acc: 0.480 - ETA: 1s - loss: 8.0885 - acc: 0.482 - ETA: 1s - loss: 8.0836 - acc: 0.483 - ETA: 1s - loss: 8.1146 - acc: 0.481 - ETA: 1s - loss: 8.1055 - acc: 0.482 - ETA: 1s - loss: 8.1227 - acc: 0.481 - ETA: 1s - loss: 8.1481 - acc: 0.479 - ETA: 1s - loss: 8.1527 - acc: 0.478 - ETA: 1s - loss: 8.1441 - acc: 0.478 - ETA: 0s - loss: 8.1290 - acc: 0.479 - ETA: 0s - loss: 8.1149 - acc: 0.480 - ETA: 0s - loss: 8.1329 - acc: 0.478 - ETA: 0s - loss: 8.1383 - acc: 0.478 - ETA: 0s - loss: 8.1085 - acc: 0.480 - ETA: 0s - loss: 8.1078 - acc: 0.480 - ETA: 0s - loss: 8.1061 - acc: 0.479 - ETA: 0s - loss: 8.1058 - acc: 0.480 - ETA: 0s - loss: 8.0874 - acc: 0.480 - ETA: 0s - loss: 8.0822 - acc: 0.480 - ETA: 0s - loss: 8.1018 - acc: 0.479 - ETA: 0s - loss: 8.1102 - acc: 0.478 - ETA: 0s - loss: 8.1126 - acc: 0.478 - ETA: 0s - loss: 8.1391 - acc: 0.477 - ETA: 0s - loss: 8.1334 - acc: 0.477 - ETA: 0s - loss: 8.1133 - acc: 0.478 - ETA: 0s - loss: 8.1041 - acc: 0.479 - ETA: 0s - loss: 8.1038 - acc: 0.4791Epoch 00007: val_loss improved from 8.69161 to 8.60822, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0902 - acc: 0.4799 - val_loss: 8.6082 - val_acc: 0.4024\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 4.0661 - acc: 0.700 - ETA: 3s - loss: 7.2257 - acc: 0.535 - ETA: 3s - loss: 7.5119 - acc: 0.525 - ETA: 2s - loss: 7.4731 - acc: 0.530 - ETA: 2s - loss: 7.6414 - acc: 0.516 - ETA: 2s - loss: 7.5592 - acc: 0.520 - ETA: 2s - loss: 7.7839 - acc: 0.506 - ETA: 2s - loss: 7.8440 - acc: 0.502 - ETA: 2s - loss: 7.9425 - acc: 0.495 - ETA: 2s - loss: 7.8985 - acc: 0.498 - ETA: 2s - loss: 7.7897 - acc: 0.505 - ETA: 2s - loss: 7.7552 - acc: 0.508 - ETA: 2s - loss: 7.7702 - acc: 0.506 - ETA: 2s - loss: 7.8182 - acc: 0.502 - ETA: 2s - loss: 7.7519 - acc: 0.506 - ETA: 2s - loss: 7.7147 - acc: 0.507 - ETA: 2s - loss: 7.7126 - acc: 0.506 - ETA: 1s - loss: 7.7690 - acc: 0.502 - ETA: 1s - loss: 7.7842 - acc: 0.502 - ETA: 1s - loss: 7.7606 - acc: 0.504 - ETA: 1s - loss: 7.7861 - acc: 0.502 - ETA: 1s - loss: 7.7662 - acc: 0.504 - ETA: 1s - loss: 7.7111 - acc: 0.506 - ETA: 1s - loss: 7.7150 - acc: 0.507 - ETA: 1s - loss: 7.7148 - acc: 0.507 - ETA: 1s - loss: 7.7530 - acc: 0.504 - ETA: 1s - loss: 7.7746 - acc: 0.504 - ETA: 1s - loss: 7.7976 - acc: 0.502 - ETA: 1s - loss: 7.8645 - acc: 0.498 - ETA: 1s - loss: 7.8706 - acc: 0.497 - ETA: 1s - loss: 7.8980 - acc: 0.495 - ETA: 1s - loss: 7.8786 - acc: 0.496 - ETA: 1s - loss: 7.9065 - acc: 0.494 - ETA: 1s - loss: 7.9359 - acc: 0.492 - ETA: 1s - loss: 7.9589 - acc: 0.491 - ETA: 0s - loss: 7.9287 - acc: 0.494 - ETA: 0s - loss: 7.9187 - acc: 0.494 - ETA: 0s - loss: 7.9169 - acc: 0.494 - ETA: 0s - loss: 7.9008 - acc: 0.495 - ETA: 0s - loss: 7.9155 - acc: 0.494 - ETA: 0s - loss: 7.9045 - acc: 0.494 - ETA: 0s - loss: 7.8947 - acc: 0.495 - ETA: 0s - loss: 7.8603 - acc: 0.497 - ETA: 0s - loss: 7.8929 - acc: 0.495 - ETA: 0s - loss: 7.8665 - acc: 0.497 - ETA: 0s - loss: 7.8477 - acc: 0.498 - ETA: 0s - loss: 7.8540 - acc: 0.498 - ETA: 0s - loss: 7.8417 - acc: 0.499 - ETA: 0s - loss: 7.8549 - acc: 0.498 - ETA: 0s - loss: 7.8628 - acc: 0.497 - ETA: 0s - loss: 7.8908 - acc: 0.495 - ETA: 0s - loss: 7.9032 - acc: 0.495 - ETA: 0s - loss: 7.9311 - acc: 0.493 - ETA: 0s - loss: 7.9516 - acc: 0.492 - ETA: 0s - loss: 7.9685 - acc: 0.4913Epoch 00008: val_loss improved from 8.60822 to 8.60318, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9671 - acc: 0.4913 - val_loss: 8.6032 - val_acc: 0.3940\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.8650 - acc: 0.450 - ETA: 2s - loss: 8.6839 - acc: 0.442 - ETA: 2s - loss: 8.2762 - acc: 0.473 - ETA: 2s - loss: 7.8472 - acc: 0.502 - ETA: 2s - loss: 7.9378 - acc: 0.498 - ETA: 2s - loss: 7.6806 - acc: 0.512 - ETA: 2s - loss: 7.6580 - acc: 0.514 - ETA: 2s - loss: 7.5822 - acc: 0.518 - ETA: 2s - loss: 7.5514 - acc: 0.519 - ETA: 2s - loss: 7.7158 - acc: 0.510 - ETA: 2s - loss: 7.7615 - acc: 0.508 - ETA: 2s - loss: 7.7764 - acc: 0.507 - ETA: 2s - loss: 7.7249 - acc: 0.511 - ETA: 2s - loss: 7.6207 - acc: 0.516 - ETA: 2s - loss: 7.7167 - acc: 0.510 - ETA: 2s - loss: 7.7191 - acc: 0.509 - ETA: 1s - loss: 7.8207 - acc: 0.503 - ETA: 1s - loss: 7.7705 - acc: 0.506 - ETA: 1s - loss: 7.8011 - acc: 0.504 - ETA: 1s - loss: 7.8122 - acc: 0.503 - ETA: 1s - loss: 7.8438 - acc: 0.502 - ETA: 1s - loss: 7.8734 - acc: 0.500 - ETA: 1s - loss: 7.8896 - acc: 0.499 - ETA: 1s - loss: 7.9301 - acc: 0.497 - ETA: 1s - loss: 7.9379 - acc: 0.496 - ETA: 1s - loss: 7.9804 - acc: 0.493 - ETA: 1s - loss: 8.0081 - acc: 0.492 - ETA: 1s - loss: 7.9654 - acc: 0.495 - ETA: 1s - loss: 7.9939 - acc: 0.493 - ETA: 1s - loss: 8.0011 - acc: 0.493 - ETA: 1s - loss: 7.9730 - acc: 0.495 - ETA: 1s - loss: 7.9556 - acc: 0.496 - ETA: 1s - loss: 7.9591 - acc: 0.496 - ETA: 1s - loss: 7.9737 - acc: 0.494 - ETA: 1s - loss: 7.9618 - acc: 0.495 - ETA: 0s - loss: 7.9688 - acc: 0.495 - ETA: 0s - loss: 7.9872 - acc: 0.493 - ETA: 0s - loss: 8.0061 - acc: 0.492 - ETA: 0s - loss: 7.9974 - acc: 0.493 - ETA: 0s - loss: 7.9690 - acc: 0.495 - ETA: 0s - loss: 7.9739 - acc: 0.494 - ETA: 0s - loss: 7.9570 - acc: 0.495 - ETA: 0s - loss: 7.9763 - acc: 0.494 - ETA: 0s - loss: 7.9666 - acc: 0.494 - ETA: 0s - loss: 7.9509 - acc: 0.496 - ETA: 0s - loss: 7.9417 - acc: 0.496 - ETA: 0s - loss: 7.9592 - acc: 0.495 - ETA: 0s - loss: 7.9755 - acc: 0.494 - ETA: 0s - loss: 7.9901 - acc: 0.493 - ETA: 0s - loss: 7.9907 - acc: 0.493 - ETA: 0s - loss: 7.9572 - acc: 0.495 - ETA: 0s - loss: 7.9470 - acc: 0.495 - ETA: 0s - loss: 7.9594 - acc: 0.495 - ETA: 0s - loss: 7.9658 - acc: 0.494 - ETA: 0s - loss: 7.9369 - acc: 0.4961Epoch 00009: val_loss improved from 8.60318 to 8.52814, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9284 - acc: 0.4967 - val_loss: 8.5281 - val_acc: 0.4036\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 6.4753 - acc: 0.600 - ETA: 2s - loss: 8.0408 - acc: 0.492 - ETA: 2s - loss: 7.7725 - acc: 0.510 - ETA: 2s - loss: 7.7966 - acc: 0.509 - ETA: 2s - loss: 7.8259 - acc: 0.509 - ETA: 2s - loss: 7.6773 - acc: 0.518 - ETA: 2s - loss: 7.8284 - acc: 0.507 - ETA: 2s - loss: 7.7973 - acc: 0.507 - ETA: 2s - loss: 7.8276 - acc: 0.506 - ETA: 2s - loss: 7.7553 - acc: 0.511 - ETA: 2s - loss: 7.6622 - acc: 0.515 - ETA: 2s - loss: 7.7079 - acc: 0.513 - ETA: 2s - loss: 7.8032 - acc: 0.507 - ETA: 2s - loss: 7.8985 - acc: 0.501 - ETA: 2s - loss: 7.9279 - acc: 0.500 - ETA: 1s - loss: 7.8880 - acc: 0.502 - ETA: 1s - loss: 7.9313 - acc: 0.500 - ETA: 1s - loss: 7.9165 - acc: 0.500 - ETA: 1s - loss: 7.8733 - acc: 0.503 - ETA: 1s - loss: 7.9109 - acc: 0.501 - ETA: 1s - loss: 7.8660 - acc: 0.504 - ETA: 1s - loss: 7.8285 - acc: 0.506 - ETA: 1s - loss: 7.8437 - acc: 0.504 - ETA: 1s - loss: 7.8375 - acc: 0.504 - ETA: 1s - loss: 7.8467 - acc: 0.504 - ETA: 1s - loss: 7.8241 - acc: 0.506 - ETA: 1s - loss: 7.8252 - acc: 0.505 - ETA: 1s - loss: 7.8310 - acc: 0.504 - ETA: 1s - loss: 7.8590 - acc: 0.502 - ETA: 1s - loss: 7.9053 - acc: 0.499 - ETA: 1s - loss: 7.8959 - acc: 0.499 - ETA: 1s - loss: 7.8829 - acc: 0.500 - ETA: 1s - loss: 7.8671 - acc: 0.501 - ETA: 1s - loss: 7.8786 - acc: 0.500 - ETA: 1s - loss: 7.8638 - acc: 0.500 - ETA: 0s - loss: 7.8558 - acc: 0.500 - ETA: 0s - loss: 7.8634 - acc: 0.500 - ETA: 0s - loss: 7.9031 - acc: 0.497 - ETA: 0s - loss: 7.9386 - acc: 0.495 - ETA: 0s - loss: 7.8997 - acc: 0.497 - ETA: 0s - loss: 7.8795 - acc: 0.498 - ETA: 0s - loss: 7.8634 - acc: 0.499 - ETA: 0s - loss: 7.8535 - acc: 0.500 - ETA: 0s - loss: 7.8629 - acc: 0.499 - ETA: 0s - loss: 7.8572 - acc: 0.499 - ETA: 0s - loss: 7.8186 - acc: 0.502 - ETA: 0s - loss: 7.8203 - acc: 0.501 - ETA: 0s - loss: 7.8057 - acc: 0.502 - ETA: 0s - loss: 7.8350 - acc: 0.500 - ETA: 0s - loss: 7.8526 - acc: 0.499 - ETA: 0s - loss: 7.8618 - acc: 0.499 - ETA: 0s - loss: 7.8685 - acc: 0.498 - ETA: 0s - loss: 7.8611 - acc: 0.498 - ETA: 0s - loss: 7.8550 - acc: 0.499 - ETA: 0s - loss: 7.8588 - acc: 0.4995Epoch 00010: val_loss improved from 8.52814 to 8.50432, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.8737 - acc: 0.4985 - val_loss: 8.5043 - val_acc: 0.4168\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 7.2532 - acc: 0.550 - ETA: 3s - loss: 6.9818 - acc: 0.557 - ETA: 3s - loss: 7.2974 - acc: 0.541 - ETA: 2s - loss: 7.2627 - acc: 0.541 - ETA: 2s - loss: 7.4991 - acc: 0.527 - ETA: 2s - loss: 7.6433 - acc: 0.515 - ETA: 2s - loss: 7.7138 - acc: 0.508 - ETA: 2s - loss: 7.8006 - acc: 0.504 - ETA: 2s - loss: 7.8403 - acc: 0.503 - ETA: 2s - loss: 7.9016 - acc: 0.499 - ETA: 2s - loss: 7.8951 - acc: 0.499 - ETA: 2s - loss: 7.8934 - acc: 0.499 - ETA: 2s - loss: 7.8347 - acc: 0.502 - ETA: 2s - loss: 7.8716 - acc: 0.499 - ETA: 2s - loss: 7.8028 - acc: 0.503 - ETA: 2s - loss: 7.7847 - acc: 0.505 - ETA: 2s - loss: 7.8757 - acc: 0.500 - ETA: 1s - loss: 7.8864 - acc: 0.500 - ETA: 1s - loss: 7.8538 - acc: 0.501 - ETA: 1s - loss: 7.8369 - acc: 0.503 - ETA: 1s - loss: 7.8250 - acc: 0.504 - ETA: 1s - loss: 7.7755 - acc: 0.507 - ETA: 1s - loss: 7.8052 - acc: 0.505 - ETA: 1s - loss: 7.7237 - acc: 0.509 - ETA: 1s - loss: 7.8014 - acc: 0.504 - ETA: 1s - loss: 7.7964 - acc: 0.505 - ETA: 1s - loss: 7.8336 - acc: 0.503 - ETA: 1s - loss: 7.8378 - acc: 0.502 - ETA: 1s - loss: 7.8016 - acc: 0.505 - ETA: 1s - loss: 7.8259 - acc: 0.503 - ETA: 1s - loss: 7.8122 - acc: 0.504 - ETA: 1s - loss: 7.8266 - acc: 0.503 - ETA: 1s - loss: 7.8547 - acc: 0.501 - ETA: 1s - loss: 7.8914 - acc: 0.499 - ETA: 1s - loss: 7.8980 - acc: 0.498 - ETA: 1s - loss: 7.9066 - acc: 0.497 - ETA: 0s - loss: 7.8722 - acc: 0.500 - ETA: 0s - loss: 7.8738 - acc: 0.500 - ETA: 0s - loss: 7.8863 - acc: 0.499 - ETA: 0s - loss: 7.8636 - acc: 0.500 - ETA: 0s - loss: 7.8474 - acc: 0.501 - ETA: 0s - loss: 7.8617 - acc: 0.500 - ETA: 0s - loss: 7.8493 - acc: 0.501 - ETA: 0s - loss: 7.8417 - acc: 0.501 - ETA: 0s - loss: 7.8272 - acc: 0.502 - ETA: 0s - loss: 7.8287 - acc: 0.502 - ETA: 0s - loss: 7.8426 - acc: 0.501 - ETA: 0s - loss: 7.8256 - acc: 0.502 - ETA: 0s - loss: 7.8117 - acc: 0.503 - ETA: 0s - loss: 7.7925 - acc: 0.504 - ETA: 0s - loss: 7.7978 - acc: 0.504 - ETA: 0s - loss: 7.7867 - acc: 0.505 - ETA: 0s - loss: 7.7990 - acc: 0.504 - ETA: 0s - loss: 7.7733 - acc: 0.506 - ETA: 0s - loss: 7.7650 - acc: 0.506 - ETA: 0s - loss: 7.7795 - acc: 0.506 - ETA: 0s - loss: 7.7720 - acc: 0.5065Epoch 00011: val_loss improved from 8.50432 to 8.46542, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.7925 - acc: 0.5054 - val_loss: 8.4654 - val_acc: 0.4096\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6540/6680 [============================>.] - ETA: 2s - loss: 6.4495 - acc: 0.600 - ETA: 2s - loss: 7.8594 - acc: 0.512 - ETA: 2s - loss: 8.1359 - acc: 0.492 - ETA: 2s - loss: 7.3457 - acc: 0.535 - ETA: 2s - loss: 7.3331 - acc: 0.534 - ETA: 2s - loss: 7.5548 - acc: 0.518 - ETA: 2s - loss: 7.5526 - acc: 0.518 - ETA: 2s - loss: 7.6068 - acc: 0.515 - ETA: 2s - loss: 7.5039 - acc: 0.523 - ETA: 2s - loss: 7.4838 - acc: 0.525 - ETA: 2s - loss: 7.5669 - acc: 0.521 - ETA: 2s - loss: 7.4369 - acc: 0.529 - ETA: 2s - loss: 7.5020 - acc: 0.526 - ETA: 2s - loss: 7.5402 - acc: 0.523 - ETA: 2s - loss: 7.4798 - acc: 0.526 - ETA: 2s - loss: 7.5529 - acc: 0.522 - ETA: 2s - loss: 7.5918 - acc: 0.520 - ETA: 2s - loss: 7.6391 - acc: 0.517 - ETA: 2s - loss: 7.6866 - acc: 0.514 - ETA: 2s - loss: 7.6370 - acc: 0.517 - ETA: 2s - loss: 7.6614 - acc: 0.516 - ETA: 2s - loss: 7.6722 - acc: 0.515 - ETA: 2s - loss: 7.6665 - acc: 0.515 - ETA: 2s - loss: 7.6485 - acc: 0.516 - ETA: 1s - loss: 7.5695 - acc: 0.521 - ETA: 1s - loss: 7.5502 - acc: 0.523 - ETA: 1s - loss: 7.5639 - acc: 0.522 - ETA: 1s - loss: 7.6171 - acc: 0.518 - ETA: 1s - loss: 7.6392 - acc: 0.517 - ETA: 1s - loss: 7.6815 - acc: 0.515 - ETA: 1s - loss: 7.6905 - acc: 0.514 - ETA: 1s - loss: 7.6745 - acc: 0.515 - ETA: 1s - loss: 7.6859 - acc: 0.514 - ETA: 1s - loss: 7.6618 - acc: 0.515 - ETA: 1s - loss: 7.6856 - acc: 0.514 - ETA: 1s - loss: 7.6521 - acc: 0.516 - ETA: 1s - loss: 7.6623 - acc: 0.515 - ETA: 1s - loss: 7.6947 - acc: 0.514 - ETA: 1s - loss: 7.7066 - acc: 0.513 - ETA: 1s - loss: 7.7163 - acc: 0.513 - ETA: 1s - loss: 7.7202 - acc: 0.513 - ETA: 0s - loss: 7.7373 - acc: 0.512 - ETA: 0s - loss: 7.7484 - acc: 0.511 - ETA: 0s - loss: 7.7246 - acc: 0.512 - ETA: 0s - loss: 7.7433 - acc: 0.511 - ETA: 0s - loss: 7.7160 - acc: 0.513 - ETA: 0s - loss: 7.7529 - acc: 0.511 - ETA: 0s - loss: 7.7435 - acc: 0.511 - ETA: 0s - loss: 7.7257 - acc: 0.512 - ETA: 0s - loss: 7.7210 - acc: 0.512 - ETA: 0s - loss: 7.7143 - acc: 0.512 - ETA: 0s - loss: 7.7229 - acc: 0.512 - ETA: 0s - loss: 7.7514 - acc: 0.510 - ETA: 0s - loss: 7.7648 - acc: 0.509 - ETA: 0s - loss: 7.7822 - acc: 0.508 - ETA: 0s - loss: 7.7880 - acc: 0.508 - ETA: 0s - loss: 7.7783 - acc: 0.5093Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.7654 - acc: 0.5100 - val_loss: 8.4743 - val_acc: 0.4036\n",
      "Epoch 14/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 7.3460 - acc: 0.5286 - ETA: 2s - loss: 7.6405 - acc: 0.510 - ETA: 2s - loss: 8.0890 - acc: 0.487 - ETA: 2s - loss: 8.0152 - acc: 0.490 - ETA: 2s - loss: 7.8556 - acc: 0.498 - ETA: 2s - loss: 7.7358 - acc: 0.505 - ETA: 2s - loss: 7.6293 - acc: 0.513 - ETA: 2s - loss: 7.7857 - acc: 0.503 - ETA: 2s - loss: 7.9546 - acc: 0.493 - ETA: 2s - loss: 8.0355 - acc: 0.487 - ETA: 2s - loss: 8.0276 - acc: 0.486 - ETA: 2s - loss: 8.0238 - acc: 0.486 - ETA: 2s - loss: 8.0075 - acc: 0.487 - ETA: 2s - loss: 8.0244 - acc: 0.485 - ETA: 2s - loss: 8.0768 - acc: 0.482 - ETA: 1s - loss: 8.0705 - acc: 0.482 - ETA: 1s - loss: 7.9951 - acc: 0.486 - ETA: 1s - loss: 8.0326 - acc: 0.485 - ETA: 1s - loss: 7.9879 - acc: 0.487 - ETA: 1s - loss: 7.9851 - acc: 0.488 - ETA: 1s - loss: 7.9421 - acc: 0.490 - ETA: 1s - loss: 7.8868 - acc: 0.494 - ETA: 1s - loss: 7.9118 - acc: 0.493 - ETA: 1s - loss: 7.9199 - acc: 0.492 - ETA: 1s - loss: 7.8760 - acc: 0.495 - ETA: 1s - loss: 7.9038 - acc: 0.494 - ETA: 1s - loss: 7.8777 - acc: 0.495 - ETA: 1s - loss: 7.8453 - acc: 0.497 - ETA: 1s - loss: 7.8356 - acc: 0.498 - ETA: 1s - loss: 7.8693 - acc: 0.496 - ETA: 1s - loss: 7.8666 - acc: 0.497 - ETA: 1s - loss: 7.8427 - acc: 0.498 - ETA: 1s - loss: 7.8304 - acc: 0.499 - ETA: 1s - loss: 7.7840 - acc: 0.502 - ETA: 1s - loss: 7.7776 - acc: 0.503 - ETA: 0s - loss: 7.7484 - acc: 0.504 - ETA: 0s - loss: 7.7135 - acc: 0.506 - ETA: 0s - loss: 7.7423 - acc: 0.505 - ETA: 0s - loss: 7.7297 - acc: 0.505 - ETA: 0s - loss: 7.7079 - acc: 0.507 - ETA: 0s - loss: 7.6878 - acc: 0.508 - ETA: 0s - loss: 7.6600 - acc: 0.510 - ETA: 0s - loss: 7.6465 - acc: 0.511 - ETA: 0s - loss: 7.6526 - acc: 0.510 - ETA: 0s - loss: 7.6627 - acc: 0.510 - ETA: 0s - loss: 7.6488 - acc: 0.511 - ETA: 0s - loss: 7.6228 - acc: 0.513 - ETA: 0s - loss: 7.6111 - acc: 0.513 - ETA: 0s - loss: 7.6070 - acc: 0.514 - ETA: 0s - loss: 7.6190 - acc: 0.513 - ETA: 0s - loss: 7.6214 - acc: 0.513 - ETA: 0s - loss: 7.6466 - acc: 0.512 - ETA: 0s - loss: 7.6692 - acc: 0.5111Epoch 00013: val_loss improved from 8.46542 to 8.36962, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6757 - acc: 0.5108 - val_loss: 8.3696 - val_acc: 0.4168\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 8.1191 - acc: 0.450 - ETA: 3s - loss: 8.0702 - acc: 0.491 - ETA: 3s - loss: 7.9357 - acc: 0.500 - ETA: 3s - loss: 7.9247 - acc: 0.502 - ETA: 3s - loss: 7.8120 - acc: 0.509 - ETA: 3s - loss: 7.6424 - acc: 0.519 - ETA: 3s - loss: 7.4859 - acc: 0.530 - ETA: 2s - loss: 7.6784 - acc: 0.519 - ETA: 2s - loss: 7.6886 - acc: 0.518 - ETA: 2s - loss: 7.7069 - acc: 0.517 - ETA: 2s - loss: 7.7120 - acc: 0.516 - ETA: 2s - loss: 7.8228 - acc: 0.509 - ETA: 2s - loss: 7.7544 - acc: 0.513 - ETA: 2s - loss: 7.7470 - acc: 0.514 - ETA: 2s - loss: 7.6705 - acc: 0.518 - ETA: 2s - loss: 7.6413 - acc: 0.520 - ETA: 2s - loss: 7.5957 - acc: 0.521 - ETA: 2s - loss: 7.5690 - acc: 0.523 - ETA: 2s - loss: 7.4859 - acc: 0.529 - ETA: 2s - loss: 7.4043 - acc: 0.534 - ETA: 2s - loss: 7.4210 - acc: 0.533 - ETA: 2s - loss: 7.4082 - acc: 0.534 - ETA: 2s - loss: 7.4216 - acc: 0.533 - ETA: 1s - loss: 7.4345 - acc: 0.532 - ETA: 1s - loss: 7.3897 - acc: 0.535 - ETA: 1s - loss: 7.4044 - acc: 0.534 - ETA: 1s - loss: 7.3905 - acc: 0.534 - ETA: 1s - loss: 7.3873 - acc: 0.534 - ETA: 1s - loss: 7.3781 - acc: 0.534 - ETA: 1s - loss: 7.3924 - acc: 0.533 - ETA: 1s - loss: 7.4233 - acc: 0.531 - ETA: 1s - loss: 7.4619 - acc: 0.528 - ETA: 1s - loss: 7.4428 - acc: 0.529 - ETA: 1s - loss: 7.4667 - acc: 0.527 - ETA: 1s - loss: 7.4746 - acc: 0.526 - ETA: 1s - loss: 7.4823 - acc: 0.525 - ETA: 1s - loss: 7.4716 - acc: 0.526 - ETA: 1s - loss: 7.4985 - acc: 0.524 - ETA: 1s - loss: 7.4770 - acc: 0.526 - ETA: 1s - loss: 7.4648 - acc: 0.527 - ETA: 1s - loss: 7.5160 - acc: 0.524 - ETA: 0s - loss: 7.5405 - acc: 0.523 - ETA: 0s - loss: 7.5298 - acc: 0.523 - ETA: 0s - loss: 7.5401 - acc: 0.522 - ETA: 0s - loss: 7.5246 - acc: 0.523 - ETA: 0s - loss: 7.5507 - acc: 0.522 - ETA: 0s - loss: 7.5677 - acc: 0.521 - ETA: 0s - loss: 7.5770 - acc: 0.520 - ETA: 0s - loss: 7.5592 - acc: 0.521 - ETA: 0s - loss: 7.5814 - acc: 0.520 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6050 - acc: 0.519 - ETA: 0s - loss: 7.6259 - acc: 0.518 - ETA: 0s - loss: 7.6211 - acc: 0.518 - ETA: 0s - loss: 7.6039 - acc: 0.519 - ETA: 0s - loss: 7.6143 - acc: 0.518 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6215 - acc: 0.5186Epoch 00014: val_loss improved from 8.36962 to 8.29462, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6171 - acc: 0.5190 - val_loss: 8.2946 - val_acc: 0.4287\n",
      "Epoch 16/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 7.2635 - acc: 0.550 - ETA: 2s - loss: 7.8818 - acc: 0.500 - ETA: 2s - loss: 7.6538 - acc: 0.519 - ETA: 2s - loss: 7.7301 - acc: 0.513 - ETA: 2s - loss: 7.6270 - acc: 0.521 - ETA: 2s - loss: 7.8571 - acc: 0.506 - ETA: 2s - loss: 7.7768 - acc: 0.512 - ETA: 2s - loss: 7.7399 - acc: 0.515 - ETA: 2s - loss: 7.7312 - acc: 0.514 - ETA: 2s - loss: 7.6309 - acc: 0.521 - ETA: 2s - loss: 7.6504 - acc: 0.520 - ETA: 2s - loss: 7.5978 - acc: 0.521 - ETA: 2s - loss: 7.5918 - acc: 0.522 - ETA: 2s - loss: 7.5499 - acc: 0.524 - ETA: 2s - loss: 7.6451 - acc: 0.519 - ETA: 2s - loss: 7.7274 - acc: 0.514 - ETA: 2s - loss: 7.7723 - acc: 0.511 - ETA: 2s - loss: 7.7591 - acc: 0.512 - ETA: 2s - loss: 7.7080 - acc: 0.516 - ETA: 2s - loss: 7.7203 - acc: 0.515 - ETA: 1s - loss: 7.7586 - acc: 0.513 - ETA: 1s - loss: 7.7348 - acc: 0.514 - ETA: 1s - loss: 7.7444 - acc: 0.513 - ETA: 1s - loss: 7.7465 - acc: 0.513 - ETA: 1s - loss: 7.7508 - acc: 0.513 - ETA: 1s - loss: 7.7711 - acc: 0.511 - ETA: 1s - loss: 7.7875 - acc: 0.511 - ETA: 1s - loss: 7.7927 - acc: 0.510 - ETA: 1s - loss: 7.7901 - acc: 0.510 - ETA: 1s - loss: 7.8511 - acc: 0.507 - ETA: 1s - loss: 7.8544 - acc: 0.506 - ETA: 1s - loss: 7.8357 - acc: 0.507 - ETA: 1s - loss: 7.8195 - acc: 0.508 - ETA: 1s - loss: 7.7948 - acc: 0.510 - ETA: 1s - loss: 7.7749 - acc: 0.511 - ETA: 1s - loss: 7.7451 - acc: 0.513 - ETA: 1s - loss: 7.7392 - acc: 0.514 - ETA: 1s - loss: 7.7333 - acc: 0.514 - ETA: 0s - loss: 7.7100 - acc: 0.516 - ETA: 0s - loss: 7.6758 - acc: 0.518 - ETA: 0s - loss: 7.6740 - acc: 0.518 - ETA: 0s - loss: 7.6735 - acc: 0.518 - ETA: 0s - loss: 7.6920 - acc: 0.516 - ETA: 0s - loss: 7.6697 - acc: 0.518 - ETA: 0s - loss: 7.6695 - acc: 0.518 - ETA: 0s - loss: 7.6542 - acc: 0.519 - ETA: 0s - loss: 7.6551 - acc: 0.519 - ETA: 0s - loss: 7.6409 - acc: 0.520 - ETA: 0s - loss: 7.6396 - acc: 0.519 - ETA: 0s - loss: 7.6266 - acc: 0.520 - ETA: 0s - loss: 7.6219 - acc: 0.521 - ETA: 0s - loss: 7.6389 - acc: 0.520 - ETA: 0s - loss: 7.6367 - acc: 0.519 - ETA: 0s - loss: 7.6329 - acc: 0.520 - ETA: 0s - loss: 7.6234 - acc: 0.520 - ETA: 0s - loss: 7.6242 - acc: 0.5207Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.6061 - acc: 0.5217 - val_loss: 8.3678 - val_acc: 0.4180\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 7.2603 - acc: 0.550 - ETA: 3s - loss: 7.3919 - acc: 0.541 - ETA: 3s - loss: 7.2556 - acc: 0.550 - ETA: 3s - loss: 7.4446 - acc: 0.538 - ETA: 3s - loss: 7.7097 - acc: 0.521 - ETA: 2s - loss: 7.4213 - acc: 0.539 - ETA: 2s - loss: 7.6229 - acc: 0.527 - ETA: 2s - loss: 7.6053 - acc: 0.528 - ETA: 2s - loss: 7.5328 - acc: 0.530 - ETA: 2s - loss: 7.5453 - acc: 0.527 - ETA: 2s - loss: 7.4494 - acc: 0.532 - ETA: 2s - loss: 7.4969 - acc: 0.527 - ETA: 2s - loss: 7.4867 - acc: 0.528 - ETA: 2s - loss: 7.5227 - acc: 0.526 - ETA: 2s - loss: 7.4745 - acc: 0.530 - ETA: 2s - loss: 7.4288 - acc: 0.532 - ETA: 2s - loss: 7.4533 - acc: 0.529 - ETA: 2s - loss: 7.4586 - acc: 0.528 - ETA: 2s - loss: 7.4895 - acc: 0.526 - ETA: 1s - loss: 7.4453 - acc: 0.529 - ETA: 1s - loss: 7.3830 - acc: 0.533 - ETA: 1s - loss: 7.3776 - acc: 0.533 - ETA: 1s - loss: 7.3054 - acc: 0.538 - ETA: 1s - loss: 7.3171 - acc: 0.537 - ETA: 1s - loss: 7.3206 - acc: 0.537 - ETA: 1s - loss: 7.3628 - acc: 0.535 - ETA: 1s - loss: 7.3986 - acc: 0.532 - ETA: 1s - loss: 7.3544 - acc: 0.534 - ETA: 1s - loss: 7.3540 - acc: 0.534 - ETA: 1s - loss: 7.3563 - acc: 0.534 - ETA: 1s - loss: 7.3580 - acc: 0.533 - ETA: 1s - loss: 7.4031 - acc: 0.530 - ETA: 1s - loss: 7.4518 - acc: 0.527 - ETA: 1s - loss: 7.4404 - acc: 0.528 - ETA: 1s - loss: 7.4320 - acc: 0.528 - ETA: 1s - loss: 7.4322 - acc: 0.528 - ETA: 1s - loss: 7.4380 - acc: 0.527 - ETA: 1s - loss: 7.4436 - acc: 0.527 - ETA: 0s - loss: 7.4739 - acc: 0.525 - ETA: 0s - loss: 7.5068 - acc: 0.524 - ETA: 0s - loss: 7.4910 - acc: 0.525 - ETA: 0s - loss: 7.4882 - acc: 0.525 - ETA: 0s - loss: 7.4994 - acc: 0.524 - ETA: 0s - loss: 7.4936 - acc: 0.524 - ETA: 0s - loss: 7.4997 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4998 - acc: 0.524 - ETA: 0s - loss: 7.5014 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4958 - acc: 0.524 - ETA: 0s - loss: 7.4885 - acc: 0.524 - ETA: 0s - loss: 7.4867 - acc: 0.524 - ETA: 0s - loss: 7.5016 - acc: 0.523 - ETA: 0s - loss: 7.4819 - acc: 0.524 - ETA: 0s - loss: 7.4759 - acc: 0.525 - ETA: 0s - loss: 7.4787 - acc: 0.524 - ETA: 0s - loss: 7.4916 - acc: 0.5240Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.4977 - acc: 0.5237 - val_loss: 8.3458 - val_acc: 0.4180\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 7.0893 - acc: 0.550 - ETA: 2s - loss: 6.9683 - acc: 0.560 - ETA: 2s - loss: 6.9400 - acc: 0.561 - ETA: 2s - loss: 7.1489 - acc: 0.546 - ETA: 2s - loss: 7.1231 - acc: 0.548 - ETA: 2s - loss: 7.2517 - acc: 0.541 - ETA: 2s - loss: 7.2871 - acc: 0.538 - ETA: 2s - loss: 7.3151 - acc: 0.538 - ETA: 2s - loss: 7.2947 - acc: 0.540 - ETA: 2s - loss: 7.2529 - acc: 0.543 - ETA: 2s - loss: 7.2548 - acc: 0.543 - ETA: 2s - loss: 7.2656 - acc: 0.541 - ETA: 2s - loss: 7.2673 - acc: 0.540 - ETA: 2s - loss: 7.2426 - acc: 0.540 - ETA: 2s - loss: 7.2729 - acc: 0.538 - ETA: 2s - loss: 7.3583 - acc: 0.532 - ETA: 1s - loss: 7.2495 - acc: 0.537 - ETA: 1s - loss: 7.2414 - acc: 0.537 - ETA: 1s - loss: 7.2802 - acc: 0.534 - ETA: 1s - loss: 7.2668 - acc: 0.535 - ETA: 1s - loss: 7.3609 - acc: 0.529 - ETA: 1s - loss: 7.3710 - acc: 0.528 - ETA: 1s - loss: 7.4021 - acc: 0.526 - ETA: 1s - loss: 7.3636 - acc: 0.529 - ETA: 1s - loss: 7.3848 - acc: 0.527 - ETA: 1s - loss: 7.4198 - acc: 0.524 - ETA: 1s - loss: 7.3947 - acc: 0.526 - ETA: 1s - loss: 7.3927 - acc: 0.527 - ETA: 1s - loss: 7.3394 - acc: 0.530 - ETA: 1s - loss: 7.3555 - acc: 0.529 - ETA: 1s - loss: 7.3621 - acc: 0.528 - ETA: 1s - loss: 7.3426 - acc: 0.529 - ETA: 1s - loss: 7.3620 - acc: 0.527 - ETA: 1s - loss: 7.3478 - acc: 0.528 - ETA: 1s - loss: 7.2966 - acc: 0.531 - ETA: 1s - loss: 7.3112 - acc: 0.530 - ETA: 0s - loss: 7.3256 - acc: 0.529 - ETA: 0s - loss: 7.3207 - acc: 0.530 - ETA: 0s - loss: 7.2994 - acc: 0.531 - ETA: 0s - loss: 7.2911 - acc: 0.532 - ETA: 0s - loss: 7.2815 - acc: 0.533 - ETA: 0s - loss: 7.2866 - acc: 0.533 - ETA: 0s - loss: 7.2518 - acc: 0.535 - ETA: 0s - loss: 7.2667 - acc: 0.534 - ETA: 0s - loss: 7.2647 - acc: 0.534 - ETA: 0s - loss: 7.2813 - acc: 0.533 - ETA: 0s - loss: 7.2608 - acc: 0.534 - ETA: 0s - loss: 7.2649 - acc: 0.534 - ETA: 0s - loss: 7.2799 - acc: 0.533 - ETA: 0s - loss: 7.2986 - acc: 0.532 - ETA: 0s - loss: 7.3243 - acc: 0.530 - ETA: 0s - loss: 7.3390 - acc: 0.529 - ETA: 0s - loss: 7.3387 - acc: 0.529 - ETA: 0s - loss: 7.3281 - acc: 0.529 - ETA: 0s - loss: 7.3360 - acc: 0.5292Epoch 00017: val_loss improved from 8.29462 to 8.08558, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.3317 - acc: 0.5295 - val_loss: 8.0856 - val_acc: 0.4335\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 6.8885 - acc: 0.550 - ETA: 2s - loss: 7.2228 - acc: 0.535 - ETA: 2s - loss: 7.3910 - acc: 0.526 - ETA: 2s - loss: 7.4907 - acc: 0.521 - ETA: 2s - loss: 7.3426 - acc: 0.532 - ETA: 2s - loss: 7.3179 - acc: 0.532 - ETA: 2s - loss: 7.2918 - acc: 0.533 - ETA: 2s - loss: 7.3430 - acc: 0.531 - ETA: 2s - loss: 7.4151 - acc: 0.527 - ETA: 2s - loss: 7.3154 - acc: 0.532 - ETA: 2s - loss: 7.2280 - acc: 0.537 - ETA: 2s - loss: 7.2360 - acc: 0.537 - ETA: 2s - loss: 7.2641 - acc: 0.536 - ETA: 2s - loss: 7.2337 - acc: 0.538 - ETA: 2s - loss: 7.2446 - acc: 0.536 - ETA: 2s - loss: 7.2642 - acc: 0.535 - ETA: 2s - loss: 7.2639 - acc: 0.535 - ETA: 2s - loss: 7.2385 - acc: 0.537 - ETA: 1s - loss: 7.2326 - acc: 0.538 - ETA: 1s - loss: 7.2576 - acc: 0.537 - ETA: 1s - loss: 7.2939 - acc: 0.533 - ETA: 1s - loss: 7.2757 - acc: 0.533 - ETA: 1s - loss: 7.3289 - acc: 0.529 - ETA: 1s - loss: 7.2758 - acc: 0.532 - ETA: 1s - loss: 7.2195 - acc: 0.536 - ETA: 1s - loss: 7.1786 - acc: 0.539 - ETA: 1s - loss: 7.1907 - acc: 0.538 - ETA: 1s - loss: 7.1671 - acc: 0.540 - ETA: 1s - loss: 7.1686 - acc: 0.540 - ETA: 1s - loss: 7.1838 - acc: 0.539 - ETA: 1s - loss: 7.1874 - acc: 0.539 - ETA: 1s - loss: 7.1951 - acc: 0.539 - ETA: 1s - loss: 7.2130 - acc: 0.537 - ETA: 1s - loss: 7.1998 - acc: 0.538 - ETA: 1s - loss: 7.2027 - acc: 0.538 - ETA: 1s - loss: 7.2407 - acc: 0.535 - ETA: 1s - loss: 7.2058 - acc: 0.537 - ETA: 1s - loss: 7.2381 - acc: 0.535 - ETA: 0s - loss: 7.2452 - acc: 0.535 - ETA: 0s - loss: 7.2500 - acc: 0.535 - ETA: 0s - loss: 7.2371 - acc: 0.536 - ETA: 0s - loss: 7.2252 - acc: 0.536 - ETA: 0s - loss: 7.2330 - acc: 0.536 - ETA: 0s - loss: 7.2559 - acc: 0.535 - ETA: 0s - loss: 7.2780 - acc: 0.534 - ETA: 0s - loss: 7.2628 - acc: 0.535 - ETA: 0s - loss: 7.2629 - acc: 0.535 - ETA: 0s - loss: 7.2444 - acc: 0.536 - ETA: 0s - loss: 7.2302 - acc: 0.537 - ETA: 0s - loss: 7.2377 - acc: 0.537 - ETA: 0s - loss: 7.2305 - acc: 0.537 - ETA: 0s - loss: 7.2353 - acc: 0.537 - ETA: 0s - loss: 7.2317 - acc: 0.537 - ETA: 0s - loss: 7.2283 - acc: 0.537 - ETA: 0s - loss: 7.2171 - acc: 0.538 - ETA: 0s - loss: 7.2086 - acc: 0.5388Epoch 00018: val_loss improved from 8.08558 to 8.07654, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.2070 - acc: 0.5388 - val_loss: 8.0765 - val_acc: 0.4299\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 8.0597 - acc: 0.500 - ETA: 2s - loss: 8.6445 - acc: 0.450 - ETA: 2s - loss: 7.7636 - acc: 0.510 - ETA: 2s - loss: 7.4556 - acc: 0.530 - ETA: 2s - loss: 7.4778 - acc: 0.528 - ETA: 2s - loss: 7.4375 - acc: 0.531 - ETA: 2s - loss: 7.4656 - acc: 0.530 - ETA: 2s - loss: 7.5119 - acc: 0.527 - ETA: 2s - loss: 7.3846 - acc: 0.536 - ETA: 2s - loss: 7.5269 - acc: 0.525 - ETA: 2s - loss: 7.4774 - acc: 0.528 - ETA: 2s - loss: 7.4726 - acc: 0.528 - ETA: 2s - loss: 7.4169 - acc: 0.531 - ETA: 2s - loss: 7.3852 - acc: 0.533 - ETA: 2s - loss: 7.3618 - acc: 0.534 - ETA: 2s - loss: 7.3575 - acc: 0.534 - ETA: 2s - loss: 7.3638 - acc: 0.533 - ETA: 1s - loss: 7.3835 - acc: 0.531 - ETA: 1s - loss: 7.3778 - acc: 0.531 - ETA: 1s - loss: 7.3376 - acc: 0.534 - ETA: 1s - loss: 7.3365 - acc: 0.534 - ETA: 1s - loss: 7.3058 - acc: 0.535 - ETA: 1s - loss: 7.3157 - acc: 0.535 - ETA: 1s - loss: 7.2926 - acc: 0.536 - ETA: 1s - loss: 7.2798 - acc: 0.538 - ETA: 1s - loss: 7.2591 - acc: 0.539 - ETA: 1s - loss: 7.2075 - acc: 0.542 - ETA: 1s - loss: 7.2032 - acc: 0.542 - ETA: 1s - loss: 7.1955 - acc: 0.543 - ETA: 1s - loss: 7.1500 - acc: 0.545 - ETA: 1s - loss: 7.1603 - acc: 0.544 - ETA: 1s - loss: 7.1489 - acc: 0.545 - ETA: 1s - loss: 7.1580 - acc: 0.544 - ETA: 1s - loss: 7.1693 - acc: 0.544 - ETA: 1s - loss: 7.1396 - acc: 0.546 - ETA: 1s - loss: 7.1629 - acc: 0.545 - ETA: 1s - loss: 7.1425 - acc: 0.546 - ETA: 0s - loss: 7.1432 - acc: 0.545 - ETA: 0s - loss: 7.1497 - acc: 0.545 - ETA: 0s - loss: 7.1651 - acc: 0.544 - ETA: 0s - loss: 7.1317 - acc: 0.546 - ETA: 0s - loss: 7.1152 - acc: 0.547 - ETA: 0s - loss: 7.1272 - acc: 0.546 - ETA: 0s - loss: 7.1364 - acc: 0.546 - ETA: 0s - loss: 7.1539 - acc: 0.545 - ETA: 0s - loss: 7.1809 - acc: 0.543 - ETA: 0s - loss: 7.1697 - acc: 0.544 - ETA: 0s - loss: 7.1756 - acc: 0.543 - ETA: 0s - loss: 7.1549 - acc: 0.545 - ETA: 0s - loss: 7.1626 - acc: 0.544 - ETA: 0s - loss: 7.1839 - acc: 0.543 - ETA: 0s - loss: 7.1940 - acc: 0.542 - ETA: 0s - loss: 7.1900 - acc: 0.542 - ETA: 0s - loss: 7.1768 - acc: 0.543 - ETA: 0s - loss: 7.1672 - acc: 0.543 - ETA: 0s - loss: 7.1493 - acc: 0.5442Epoch 00019: val_loss improved from 8.07654 to 8.01932, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.1519 - acc: 0.5442 - val_loss: 8.0193 - val_acc: 0.4407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b9f5bff98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.5311%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# transfer model search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "networks = [\"VGG19\", \"Resnet50\", \"InceptionV3\", \"Xception\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def with_bottleneck(network):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_VGG16 = bottleneck_features['train']\n",
    "    valid_VGG16 = bottleneck_features['valid']\n",
    "    test_VGG16 = bottleneck_features['test']\n",
    "\n",
    "    VGG16_model = Sequential()\n",
    "    VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "    VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "    VGG16_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(network)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    VGG16_model.fit(train_VGG16, train_targets, \n",
    "              validation_data=(valid_VGG16, valid_targets),\n",
    "              epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    VGG16_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "    return (test_accuracy, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 119s - loss: 15.3378 - acc: 0.0000e+ - ETA: 22s - loss: 14.8794 - acc: 0.0083     - ETA: 12s - loss: 14.8448 - acc: 0.008 - ETA: 9s - loss: 14.8060 - acc: 0.013 - ETA: 7s - loss: 14.7613 - acc: 0.01 - ETA: 6s - loss: 14.5223 - acc: 0.02 - ETA: 5s - loss: 14.4560 - acc: 0.02 - ETA: 5s - loss: 14.3702 - acc: 0.02 - ETA: 4s - loss: 14.2390 - acc: 0.03 - ETA: 4s - loss: 14.1351 - acc: 0.03 - ETA: 4s - loss: 14.0882 - acc: 0.03 - ETA: 3s - loss: 14.1000 - acc: 0.03 - ETA: 3s - loss: 14.0135 - acc: 0.04 - ETA: 3s - loss: 13.9707 - acc: 0.04 - ETA: 3s - loss: 13.9578 - acc: 0.04 - ETA: 3s - loss: 13.8934 - acc: 0.04 - ETA: 2s - loss: 13.8196 - acc: 0.04 - ETA: 2s - loss: 13.7468 - acc: 0.05 - ETA: 2s - loss: 13.7028 - acc: 0.05 - ETA: 2s - loss: 13.6698 - acc: 0.05 - ETA: 2s - loss: 13.5930 - acc: 0.05 - ETA: 2s - loss: 13.5048 - acc: 0.06 - ETA: 2s - loss: 13.4538 - acc: 0.06 - ETA: 2s - loss: 13.3880 - acc: 0.06 - ETA: 1s - loss: 13.3324 - acc: 0.07 - ETA: 1s - loss: 13.2950 - acc: 0.07 - ETA: 1s - loss: 13.2452 - acc: 0.07 - ETA: 1s - loss: 13.1816 - acc: 0.08 - ETA: 1s - loss: 13.1186 - acc: 0.08 - ETA: 1s - loss: 13.0426 - acc: 0.08 - ETA: 1s - loss: 13.0197 - acc: 0.08 - ETA: 1s - loss: 12.9883 - acc: 0.09 - ETA: 1s - loss: 12.9821 - acc: 0.09 - ETA: 1s - loss: 12.9661 - acc: 0.09 - ETA: 1s - loss: 12.9151 - acc: 0.09 - ETA: 1s - loss: 12.8892 - acc: 0.09 - ETA: 1s - loss: 12.8532 - acc: 0.09 - ETA: 1s - loss: 12.8240 - acc: 0.09 - ETA: 0s - loss: 12.7832 - acc: 0.10 - ETA: 0s - loss: 12.7569 - acc: 0.10 - ETA: 0s - loss: 12.7169 - acc: 0.10 - ETA: 0s - loss: 12.6837 - acc: 0.10 - ETA: 0s - loss: 12.6612 - acc: 0.10 - ETA: 0s - loss: 12.6482 - acc: 0.11 - ETA: 0s - loss: 12.6152 - acc: 0.11 - ETA: 0s - loss: 12.5806 - acc: 0.11 - ETA: 0s - loss: 12.5312 - acc: 0.11 - ETA: 0s - loss: 12.5114 - acc: 0.11 - ETA: 0s - loss: 12.4869 - acc: 0.12 - ETA: 0s - loss: 12.4891 - acc: 0.12 - ETA: 0s - loss: 12.4839 - acc: 0.12 - ETA: 0s - loss: 12.4433 - acc: 0.12 - ETA: 0s - loss: 12.4455 - acc: 0.12 - ETA: 0s - loss: 12.4001 - acc: 0.1264Epoch 00000: val_loss improved from inf to 11.20366, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 12.3916 - acc: 0.1268 - val_loss: 11.2037 - val_acc: 0.1880\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 12.6304 - acc: 0.05 - ETA: 2s - loss: 11.1897 - acc: 0.19 - ETA: 3s - loss: 10.7371 - acc: 0.21 - ETA: 3s - loss: 11.4056 - acc: 0.18 - ETA: 2s - loss: 11.1842 - acc: 0.20 - ETA: 2s - loss: 11.1154 - acc: 0.21 - ETA: 2s - loss: 11.1633 - acc: 0.21 - ETA: 2s - loss: 11.1499 - acc: 0.21 - ETA: 2s - loss: 11.1169 - acc: 0.22 - ETA: 2s - loss: 10.9961 - acc: 0.22 - ETA: 2s - loss: 10.8755 - acc: 0.23 - ETA: 2s - loss: 10.7873 - acc: 0.24 - ETA: 2s - loss: 10.7824 - acc: 0.24 - ETA: 2s - loss: 10.7082 - acc: 0.24 - ETA: 2s - loss: 10.6844 - acc: 0.24 - ETA: 2s - loss: 10.7100 - acc: 0.24 - ETA: 2s - loss: 10.6580 - acc: 0.24 - ETA: 2s - loss: 10.6291 - acc: 0.25 - ETA: 1s - loss: 10.6975 - acc: 0.24 - ETA: 1s - loss: 10.7312 - acc: 0.24 - ETA: 1s - loss: 10.7471 - acc: 0.24 - ETA: 1s - loss: 10.6925 - acc: 0.25 - ETA: 1s - loss: 10.6359 - acc: 0.25 - ETA: 1s - loss: 10.6257 - acc: 0.25 - ETA: 1s - loss: 10.6420 - acc: 0.25 - ETA: 1s - loss: 10.6545 - acc: 0.25 - ETA: 1s - loss: 10.6545 - acc: 0.25 - ETA: 1s - loss: 10.6358 - acc: 0.25 - ETA: 1s - loss: 10.6749 - acc: 0.25 - ETA: 1s - loss: 10.6797 - acc: 0.25 - ETA: 1s - loss: 10.6594 - acc: 0.25 - ETA: 1s - loss: 10.6743 - acc: 0.25 - ETA: 1s - loss: 10.6683 - acc: 0.25 - ETA: 1s - loss: 10.6808 - acc: 0.25 - ETA: 1s - loss: 10.6933 - acc: 0.25 - ETA: 1s - loss: 10.7075 - acc: 0.25 - ETA: 0s - loss: 10.6865 - acc: 0.25 - ETA: 0s - loss: 10.6908 - acc: 0.25 - ETA: 0s - loss: 10.7035 - acc: 0.25 - ETA: 0s - loss: 10.6864 - acc: 0.25 - ETA: 0s - loss: 10.7060 - acc: 0.25 - ETA: 0s - loss: 10.7131 - acc: 0.25 - ETA: 0s - loss: 10.7153 - acc: 0.25 - ETA: 0s - loss: 10.7105 - acc: 0.25 - ETA: 0s - loss: 10.7011 - acc: 0.25 - ETA: 0s - loss: 10.6917 - acc: 0.25 - ETA: 0s - loss: 10.6854 - acc: 0.26 - ETA: 0s - loss: 10.6827 - acc: 0.26 - ETA: 0s - loss: 10.6839 - acc: 0.26 - ETA: 0s - loss: 10.6830 - acc: 0.26 - ETA: 0s - loss: 10.7021 - acc: 0.25 - ETA: 0s - loss: 10.6870 - acc: 0.26 - ETA: 0s - loss: 10.6843 - acc: 0.26 - ETA: 0s - loss: 10.6635 - acc: 0.26 - ETA: 0s - loss: 10.6646 - acc: 0.26 - ETA: 0s - loss: 10.6620 - acc: 0.2639Epoch 00001: val_loss improved from 11.20366 to 10.60583, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.6619 - acc: 0.2638 - val_loss: 10.6058 - val_acc: 0.2575\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 9.9278 - acc: 0.300 - ETA: 3s - loss: 9.8332 - acc: 0.342 - ETA: 2s - loss: 9.5782 - acc: 0.353 - ETA: 2s - loss: 9.7026 - acc: 0.337 - ETA: 2s - loss: 9.9625 - acc: 0.326 - ETA: 2s - loss: 10.1623 - acc: 0.31 - ETA: 2s - loss: 10.2761 - acc: 0.30 - ETA: 2s - loss: 10.3732 - acc: 0.30 - ETA: 2s - loss: 10.3296 - acc: 0.30 - ETA: 2s - loss: 10.3033 - acc: 0.30 - ETA: 2s - loss: 10.2733 - acc: 0.30 - ETA: 2s - loss: 10.3185 - acc: 0.30 - ETA: 2s - loss: 10.2494 - acc: 0.31 - ETA: 2s - loss: 10.2330 - acc: 0.31 - ETA: 2s - loss: 10.2637 - acc: 0.31 - ETA: 2s - loss: 10.2983 - acc: 0.31 - ETA: 2s - loss: 10.3812 - acc: 0.30 - ETA: 2s - loss: 10.3730 - acc: 0.30 - ETA: 1s - loss: 10.3610 - acc: 0.30 - ETA: 1s - loss: 10.3566 - acc: 0.30 - ETA: 1s - loss: 10.3887 - acc: 0.30 - ETA: 1s - loss: 10.3422 - acc: 0.30 - ETA: 1s - loss: 10.3461 - acc: 0.30 - ETA: 1s - loss: 10.3165 - acc: 0.30 - ETA: 1s - loss: 10.3125 - acc: 0.30 - ETA: 1s - loss: 10.3254 - acc: 0.30 - ETA: 1s - loss: 10.3242 - acc: 0.30 - ETA: 1s - loss: 10.3000 - acc: 0.31 - ETA: 1s - loss: 10.3263 - acc: 0.30 - ETA: 1s - loss: 10.3142 - acc: 0.30 - ETA: 1s - loss: 10.3370 - acc: 0.30 - ETA: 1s - loss: 10.3661 - acc: 0.30 - ETA: 1s - loss: 10.3747 - acc: 0.30 - ETA: 1s - loss: 10.4048 - acc: 0.30 - ETA: 1s - loss: 10.4185 - acc: 0.30 - ETA: 1s - loss: 10.4025 - acc: 0.30 - ETA: 1s - loss: 10.3829 - acc: 0.30 - ETA: 0s - loss: 10.3721 - acc: 0.30 - ETA: 0s - loss: 10.3585 - acc: 0.30 - ETA: 0s - loss: 10.3217 - acc: 0.30 - ETA: 0s - loss: 10.3444 - acc: 0.30 - ETA: 0s - loss: 10.3524 - acc: 0.30 - ETA: 0s - loss: 10.3569 - acc: 0.30 - ETA: 0s - loss: 10.3663 - acc: 0.30 - ETA: 0s - loss: 10.3660 - acc: 0.30 - ETA: 0s - loss: 10.3768 - acc: 0.30 - ETA: 0s - loss: 10.3806 - acc: 0.30 - ETA: 0s - loss: 10.4042 - acc: 0.30 - ETA: 0s - loss: 10.3798 - acc: 0.30 - ETA: 0s - loss: 10.3903 - acc: 0.30 - ETA: 0s - loss: 10.3871 - acc: 0.30 - ETA: 0s - loss: 10.3606 - acc: 0.30 - ETA: 0s - loss: 10.3563 - acc: 0.30 - ETA: 0s - loss: 10.3461 - acc: 0.31 - ETA: 0s - loss: 10.3243 - acc: 0.31 - ETA: 0s - loss: 10.3138 - acc: 0.3130Epoch 00002: val_loss improved from 10.60583 to 10.55068, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.3108 - acc: 0.3130 - val_loss: 10.5507 - val_acc: 0.2790\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 2s - loss: 11.2897 - acc: 0.30 - ETA: 2s - loss: 10.4424 - acc: 0.31 - ETA: 2s - loss: 10.3723 - acc: 0.31 - ETA: 2s - loss: 10.3209 - acc: 0.32 - ETA: 2s - loss: 10.1619 - acc: 0.33 - ETA: 2s - loss: 10.2264 - acc: 0.32 - ETA: 2s - loss: 10.1690 - acc: 0.33 - ETA: 2s - loss: 10.2242 - acc: 0.32 - ETA: 2s - loss: 10.2998 - acc: 0.32 - ETA: 2s - loss: 10.4210 - acc: 0.31 - ETA: 2s - loss: 10.4479 - acc: 0.31 - ETA: 2s - loss: 10.4183 - acc: 0.31 - ETA: 2s - loss: 10.4434 - acc: 0.31 - ETA: 2s - loss: 10.3837 - acc: 0.31 - ETA: 2s - loss: 10.2909 - acc: 0.32 - ETA: 2s - loss: 10.2704 - acc: 0.32 - ETA: 2s - loss: 10.2554 - acc: 0.32 - ETA: 2s - loss: 10.2601 - acc: 0.32 - ETA: 2s - loss: 10.2778 - acc: 0.32 - ETA: 2s - loss: 10.3072 - acc: 0.32 - ETA: 1s - loss: 10.3053 - acc: 0.32 - ETA: 1s - loss: 10.2451 - acc: 0.32 - ETA: 1s - loss: 10.1917 - acc: 0.33 - ETA: 1s - loss: 10.2108 - acc: 0.33 - ETA: 1s - loss: 10.1918 - acc: 0.33 - ETA: 1s - loss: 10.0906 - acc: 0.33 - ETA: 1s - loss: 10.0931 - acc: 0.33 - ETA: 1s - loss: 10.1006 - acc: 0.33 - ETA: 1s - loss: 10.0801 - acc: 0.33 - ETA: 1s - loss: 10.0279 - acc: 0.34 - ETA: 1s - loss: 10.0680 - acc: 0.33 - ETA: 1s - loss: 10.0951 - acc: 0.33 - ETA: 1s - loss: 10.1069 - acc: 0.33 - ETA: 1s - loss: 10.1102 - acc: 0.33 - ETA: 1s - loss: 10.1094 - acc: 0.33 - ETA: 1s - loss: 10.1253 - acc: 0.33 - ETA: 1s - loss: 10.1460 - acc: 0.33 - ETA: 0s - loss: 10.1751 - acc: 0.33 - ETA: 0s - loss: 10.1822 - acc: 0.33 - ETA: 0s - loss: 10.1569 - acc: 0.33 - ETA: 0s - loss: 10.1238 - acc: 0.33 - ETA: 0s - loss: 10.1362 - acc: 0.33 - ETA: 0s - loss: 10.1135 - acc: 0.33 - ETA: 0s - loss: 10.1101 - acc: 0.33 - ETA: 0s - loss: 10.0935 - acc: 0.33 - ETA: 0s - loss: 10.1049 - acc: 0.33 - ETA: 0s - loss: 10.1110 - acc: 0.33 - ETA: 0s - loss: 10.1079 - acc: 0.33 - ETA: 0s - loss: 10.0990 - acc: 0.33 - ETA: 0s - loss: 10.0894 - acc: 0.33 - ETA: 0s - loss: 10.0872 - acc: 0.33 - ETA: 0s - loss: 10.0702 - acc: 0.33 - ETA: 0s - loss: 10.0827 - acc: 0.33 - ETA: 0s - loss: 10.0901 - acc: 0.33 - ETA: 0s - loss: 10.0625 - acc: 0.3389Epoch 00003: val_loss improved from 10.55068 to 10.24227, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 10.0572 - acc: 0.3392 - val_loss: 10.2423 - val_acc: 0.2946\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 9.6709 - acc: 0.400 - ETA: 2s - loss: 9.5695 - acc: 0.371 - ETA: 3s - loss: 9.6627 - acc: 0.366 - ETA: 3s - loss: 9.7529 - acc: 0.361 - ETA: 2s - loss: 9.4875 - acc: 0.383 - ETA: 2s - loss: 9.5860 - acc: 0.381 - ETA: 2s - loss: 9.5463 - acc: 0.383 - ETA: 2s - loss: 9.4656 - acc: 0.384 - ETA: 2s - loss: 9.5628 - acc: 0.376 - ETA: 2s - loss: 9.4249 - acc: 0.385 - ETA: 2s - loss: 9.6042 - acc: 0.374 - ETA: 2s - loss: 9.7478 - acc: 0.364 - ETA: 2s - loss: 9.6892 - acc: 0.369 - ETA: 2s - loss: 9.7913 - acc: 0.363 - ETA: 2s - loss: 9.7640 - acc: 0.365 - ETA: 2s - loss: 9.7146 - acc: 0.369 - ETA: 2s - loss: 9.7380 - acc: 0.366 - ETA: 2s - loss: 9.6818 - acc: 0.369 - ETA: 2s - loss: 9.6397 - acc: 0.372 - ETA: 2s - loss: 9.7075 - acc: 0.369 - ETA: 1s - loss: 9.7295 - acc: 0.368 - ETA: 1s - loss: 9.7461 - acc: 0.367 - ETA: 1s - loss: 9.7559 - acc: 0.367 - ETA: 1s - loss: 9.8179 - acc: 0.364 - ETA: 1s - loss: 9.8121 - acc: 0.364 - ETA: 1s - loss: 9.7812 - acc: 0.365 - ETA: 1s - loss: 9.7889 - acc: 0.364 - ETA: 1s - loss: 9.7882 - acc: 0.364 - ETA: 1s - loss: 9.7894 - acc: 0.365 - ETA: 1s - loss: 9.7647 - acc: 0.366 - ETA: 1s - loss: 9.7775 - acc: 0.365 - ETA: 1s - loss: 9.7815 - acc: 0.365 - ETA: 1s - loss: 9.7634 - acc: 0.366 - ETA: 1s - loss: 9.7957 - acc: 0.364 - ETA: 1s - loss: 9.8072 - acc: 0.364 - ETA: 1s - loss: 9.8121 - acc: 0.362 - ETA: 1s - loss: 9.8241 - acc: 0.361 - ETA: 0s - loss: 9.8158 - acc: 0.362 - ETA: 0s - loss: 9.8351 - acc: 0.361 - ETA: 0s - loss: 9.8442 - acc: 0.360 - ETA: 0s - loss: 9.8467 - acc: 0.360 - ETA: 0s - loss: 9.8426 - acc: 0.360 - ETA: 0s - loss: 9.8311 - acc: 0.360 - ETA: 0s - loss: 9.8317 - acc: 0.361 - ETA: 0s - loss: 9.8293 - acc: 0.361 - ETA: 0s - loss: 9.8140 - acc: 0.362 - ETA: 0s - loss: 9.8209 - acc: 0.362 - ETA: 0s - loss: 9.8090 - acc: 0.362 - ETA: 0s - loss: 9.7979 - acc: 0.362 - ETA: 0s - loss: 9.7925 - acc: 0.362 - ETA: 0s - loss: 9.7922 - acc: 0.361 - ETA: 0s - loss: 9.8030 - acc: 0.360 - ETA: 0s - loss: 9.8245 - acc: 0.359 - ETA: 0s - loss: 9.8285 - acc: 0.358 - ETA: 0s - loss: 9.8005 - acc: 0.360 - ETA: 0s - loss: 9.8162 - acc: 0.3591Epoch 00004: val_loss improved from 10.24227 to 10.01084, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.8157 - acc: 0.3590 - val_loss: 10.0108 - val_acc: 0.3078\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.8974 - acc: 0.350 - ETA: 2s - loss: 9.3096 - acc: 0.381 - ETA: 2s - loss: 9.6258 - acc: 0.373 - ETA: 2s - loss: 9.7100 - acc: 0.365 - ETA: 2s - loss: 9.5335 - acc: 0.373 - ETA: 2s - loss: 9.5676 - acc: 0.367 - ETA: 2s - loss: 9.6860 - acc: 0.363 - ETA: 2s - loss: 9.4614 - acc: 0.378 - ETA: 2s - loss: 9.5849 - acc: 0.374 - ETA: 2s - loss: 9.5137 - acc: 0.380 - ETA: 2s - loss: 9.5565 - acc: 0.379 - ETA: 2s - loss: 9.5174 - acc: 0.381 - ETA: 2s - loss: 9.4171 - acc: 0.388 - ETA: 2s - loss: 9.4501 - acc: 0.385 - ETA: 2s - loss: 9.4404 - acc: 0.387 - ETA: 2s - loss: 9.4612 - acc: 0.385 - ETA: 2s - loss: 9.4803 - acc: 0.384 - ETA: 1s - loss: 9.4852 - acc: 0.384 - ETA: 1s - loss: 9.4500 - acc: 0.386 - ETA: 1s - loss: 9.4613 - acc: 0.386 - ETA: 1s - loss: 9.5312 - acc: 0.382 - ETA: 1s - loss: 9.5936 - acc: 0.378 - ETA: 1s - loss: 9.5697 - acc: 0.380 - ETA: 1s - loss: 9.5948 - acc: 0.378 - ETA: 1s - loss: 9.5634 - acc: 0.380 - ETA: 1s - loss: 9.5650 - acc: 0.380 - ETA: 1s - loss: 9.5769 - acc: 0.378 - ETA: 1s - loss: 9.5647 - acc: 0.378 - ETA: 1s - loss: 9.6021 - acc: 0.376 - ETA: 1s - loss: 9.5811 - acc: 0.377 - ETA: 1s - loss: 9.5880 - acc: 0.376 - ETA: 1s - loss: 9.5478 - acc: 0.379 - ETA: 1s - loss: 9.5425 - acc: 0.380 - ETA: 1s - loss: 9.5057 - acc: 0.381 - ETA: 1s - loss: 9.5174 - acc: 0.382 - ETA: 1s - loss: 9.5119 - acc: 0.382 - ETA: 0s - loss: 9.5188 - acc: 0.382 - ETA: 0s - loss: 9.5260 - acc: 0.382 - ETA: 0s - loss: 9.5177 - acc: 0.382 - ETA: 0s - loss: 9.5032 - acc: 0.383 - ETA: 0s - loss: 9.4906 - acc: 0.384 - ETA: 0s - loss: 9.4690 - acc: 0.385 - ETA: 0s - loss: 9.4791 - acc: 0.385 - ETA: 0s - loss: 9.4787 - acc: 0.385 - ETA: 0s - loss: 9.4715 - acc: 0.385 - ETA: 0s - loss: 9.4508 - acc: 0.386 - ETA: 0s - loss: 9.4302 - acc: 0.387 - ETA: 0s - loss: 9.4396 - acc: 0.386 - ETA: 0s - loss: 9.4447 - acc: 0.386 - ETA: 0s - loss: 9.4679 - acc: 0.385 - ETA: 0s - loss: 9.4851 - acc: 0.385 - ETA: 0s - loss: 9.5050 - acc: 0.383 - ETA: 0s - loss: 9.5252 - acc: 0.382 - ETA: 0s - loss: 9.5314 - acc: 0.381 - ETA: 0s - loss: 9.5270 - acc: 0.3820Epoch 00005: val_loss improved from 10.01084 to 9.90151, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.5435 - acc: 0.3810 - val_loss: 9.9015 - val_acc: 0.3198\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 2s - loss: 9.6719 - acc: 0.400 - ETA: 2s - loss: 9.2036 - acc: 0.425 - ETA: 2s - loss: 9.6489 - acc: 0.396 - ETA: 2s - loss: 9.5534 - acc: 0.395 - ETA: 2s - loss: 9.3719 - acc: 0.405 - ETA: 2s - loss: 9.4437 - acc: 0.400 - ETA: 2s - loss: 9.5243 - acc: 0.396 - ETA: 2s - loss: 9.4389 - acc: 0.401 - ETA: 2s - loss: 9.3461 - acc: 0.405 - ETA: 2s - loss: 9.3702 - acc: 0.403 - ETA: 2s - loss: 9.4574 - acc: 0.399 - ETA: 2s - loss: 9.4674 - acc: 0.398 - ETA: 2s - loss: 9.5198 - acc: 0.395 - ETA: 2s - loss: 9.4318 - acc: 0.400 - ETA: 2s - loss: 9.4213 - acc: 0.400 - ETA: 2s - loss: 9.4033 - acc: 0.401 - ETA: 1s - loss: 9.4310 - acc: 0.399 - ETA: 1s - loss: 9.3726 - acc: 0.403 - ETA: 1s - loss: 9.3732 - acc: 0.402 - ETA: 1s - loss: 9.3981 - acc: 0.401 - ETA: 1s - loss: 9.3757 - acc: 0.402 - ETA: 1s - loss: 9.4459 - acc: 0.397 - ETA: 1s - loss: 9.4671 - acc: 0.395 - ETA: 1s - loss: 9.4657 - acc: 0.394 - ETA: 1s - loss: 9.4800 - acc: 0.393 - ETA: 1s - loss: 9.4894 - acc: 0.392 - ETA: 1s - loss: 9.4578 - acc: 0.395 - ETA: 1s - loss: 9.4667 - acc: 0.395 - ETA: 1s - loss: 9.4787 - acc: 0.394 - ETA: 1s - loss: 9.4830 - acc: 0.394 - ETA: 1s - loss: 9.4850 - acc: 0.394 - ETA: 1s - loss: 9.5133 - acc: 0.393 - ETA: 1s - loss: 9.5070 - acc: 0.393 - ETA: 1s - loss: 9.4932 - acc: 0.394 - ETA: 1s - loss: 9.4904 - acc: 0.394 - ETA: 0s - loss: 9.4956 - acc: 0.393 - ETA: 0s - loss: 9.5125 - acc: 0.392 - ETA: 0s - loss: 9.5172 - acc: 0.392 - ETA: 0s - loss: 9.4943 - acc: 0.394 - ETA: 0s - loss: 9.5255 - acc: 0.391 - ETA: 0s - loss: 9.4925 - acc: 0.393 - ETA: 0s - loss: 9.4931 - acc: 0.393 - ETA: 0s - loss: 9.4537 - acc: 0.396 - ETA: 0s - loss: 9.4308 - acc: 0.397 - ETA: 0s - loss: 9.4483 - acc: 0.396 - ETA: 0s - loss: 9.4578 - acc: 0.396 - ETA: 0s - loss: 9.4653 - acc: 0.396 - ETA: 0s - loss: 9.4808 - acc: 0.395 - ETA: 0s - loss: 9.4879 - acc: 0.394 - ETA: 0s - loss: 9.4663 - acc: 0.396 - ETA: 0s - loss: 9.4710 - acc: 0.395 - ETA: 0s - loss: 9.4606 - acc: 0.395 - ETA: 0s - loss: 9.4389 - acc: 0.396 - ETA: 0s - loss: 9.4587 - acc: 0.3956Epoch 00006: val_loss improved from 9.90151 to 9.81552, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4580 - acc: 0.3957 - val_loss: 9.8155 - val_acc: 0.3317\n",
      "Epoch 8/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 10.4831 - acc: 0.35 - ETA: 2s - loss: 9.7451 - acc: 0.3786 - ETA: 2s - loss: 10.0430 - acc: 0.36 - ETA: 2s - loss: 10.3393 - acc: 0.34 - ETA: 2s - loss: 10.5632 - acc: 0.33 - ETA: 2s - loss: 10.4290 - acc: 0.34 - ETA: 2s - loss: 10.3791 - acc: 0.34 - ETA: 2s - loss: 10.2534 - acc: 0.35 - ETA: 2s - loss: 10.0805 - acc: 0.36 - ETA: 2s - loss: 10.1713 - acc: 0.35 - ETA: 2s - loss: 9.9523 - acc: 0.3706 - ETA: 2s - loss: 9.8392 - acc: 0.378 - ETA: 2s - loss: 9.7432 - acc: 0.384 - ETA: 2s - loss: 9.7054 - acc: 0.387 - ETA: 2s - loss: 9.6764 - acc: 0.389 - ETA: 2s - loss: 9.6603 - acc: 0.390 - ETA: 2s - loss: 9.5997 - acc: 0.394 - ETA: 1s - loss: 9.5584 - acc: 0.397 - ETA: 1s - loss: 9.5723 - acc: 0.396 - ETA: 1s - loss: 9.5794 - acc: 0.396 - ETA: 1s - loss: 9.5261 - acc: 0.399 - ETA: 1s - loss: 9.4900 - acc: 0.401 - ETA: 1s - loss: 9.5113 - acc: 0.400 - ETA: 1s - loss: 9.4370 - acc: 0.404 - ETA: 1s - loss: 9.4413 - acc: 0.404 - ETA: 1s - loss: 9.4623 - acc: 0.403 - ETA: 1s - loss: 9.4673 - acc: 0.402 - ETA: 1s - loss: 9.4614 - acc: 0.402 - ETA: 1s - loss: 9.4988 - acc: 0.400 - ETA: 1s - loss: 9.4856 - acc: 0.400 - ETA: 1s - loss: 9.4741 - acc: 0.401 - ETA: 1s - loss: 9.4387 - acc: 0.404 - ETA: 1s - loss: 9.4344 - acc: 0.404 - ETA: 1s - loss: 9.4849 - acc: 0.401 - ETA: 1s - loss: 9.4694 - acc: 0.402 - ETA: 1s - loss: 9.4629 - acc: 0.401 - ETA: 0s - loss: 9.4677 - acc: 0.401 - ETA: 0s - loss: 9.4635 - acc: 0.401 - ETA: 0s - loss: 9.4841 - acc: 0.400 - ETA: 0s - loss: 9.4629 - acc: 0.400 - ETA: 0s - loss: 9.4551 - acc: 0.401 - ETA: 0s - loss: 9.4873 - acc: 0.399 - ETA: 0s - loss: 9.4883 - acc: 0.398 - ETA: 0s - loss: 9.4828 - acc: 0.398 - ETA: 0s - loss: 9.4883 - acc: 0.398 - ETA: 0s - loss: 9.4771 - acc: 0.399 - ETA: 0s - loss: 9.4623 - acc: 0.400 - ETA: 0s - loss: 9.4637 - acc: 0.399 - ETA: 0s - loss: 9.4570 - acc: 0.400 - ETA: 0s - loss: 9.4598 - acc: 0.400 - ETA: 0s - loss: 9.4498 - acc: 0.400 - ETA: 0s - loss: 9.4205 - acc: 0.402 - ETA: 0s - loss: 9.4459 - acc: 0.401 - ETA: 0s - loss: 9.4351 - acc: 0.402 - ETA: 0s - loss: 9.4256 - acc: 0.4027Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.4166 - acc: 0.4033 - val_loss: 9.8341 - val_acc: 0.3246\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 9.6709 - acc: 0.400 - ETA: 3s - loss: 9.2475 - acc: 0.407 - ETA: 2s - loss: 9.1155 - acc: 0.425 - ETA: 2s - loss: 9.2934 - acc: 0.415 - ETA: 2s - loss: 9.3192 - acc: 0.415 - ETA: 2s - loss: 9.0744 - acc: 0.428 - ETA: 2s - loss: 8.9575 - acc: 0.436 - ETA: 2s - loss: 9.0370 - acc: 0.433 - ETA: 2s - loss: 9.0517 - acc: 0.432 - ETA: 2s - loss: 9.0452 - acc: 0.430 - ETA: 2s - loss: 9.1085 - acc: 0.426 - ETA: 2s - loss: 9.0728 - acc: 0.429 - ETA: 2s - loss: 9.1218 - acc: 0.427 - ETA: 2s - loss: 9.1927 - acc: 0.423 - ETA: 2s - loss: 9.1147 - acc: 0.427 - ETA: 2s - loss: 9.1079 - acc: 0.428 - ETA: 2s - loss: 9.0878 - acc: 0.429 - ETA: 1s - loss: 9.1180 - acc: 0.427 - ETA: 1s - loss: 9.1429 - acc: 0.426 - ETA: 1s - loss: 9.2086 - acc: 0.421 - ETA: 1s - loss: 9.2318 - acc: 0.420 - ETA: 1s - loss: 9.2090 - acc: 0.421 - ETA: 1s - loss: 9.2472 - acc: 0.419 - ETA: 1s - loss: 9.3128 - acc: 0.414 - ETA: 1s - loss: 9.3219 - acc: 0.414 - ETA: 1s - loss: 9.3461 - acc: 0.413 - ETA: 1s - loss: 9.3159 - acc: 0.414 - ETA: 1s - loss: 9.3103 - acc: 0.414 - ETA: 1s - loss: 9.3418 - acc: 0.413 - ETA: 1s - loss: 9.3989 - acc: 0.409 - ETA: 1s - loss: 9.4052 - acc: 0.408 - ETA: 1s - loss: 9.3844 - acc: 0.409 - ETA: 1s - loss: 9.4272 - acc: 0.407 - ETA: 1s - loss: 9.4588 - acc: 0.405 - ETA: 1s - loss: 9.4232 - acc: 0.407 - ETA: 1s - loss: 9.4004 - acc: 0.409 - ETA: 0s - loss: 9.3929 - acc: 0.410 - ETA: 0s - loss: 9.3935 - acc: 0.410 - ETA: 0s - loss: 9.3966 - acc: 0.409 - ETA: 0s - loss: 9.3635 - acc: 0.411 - ETA: 0s - loss: 9.3973 - acc: 0.410 - ETA: 0s - loss: 9.4106 - acc: 0.409 - ETA: 0s - loss: 9.4150 - acc: 0.408 - ETA: 0s - loss: 9.4258 - acc: 0.408 - ETA: 0s - loss: 9.4073 - acc: 0.409 - ETA: 0s - loss: 9.4174 - acc: 0.408 - ETA: 0s - loss: 9.4331 - acc: 0.407 - ETA: 0s - loss: 9.4229 - acc: 0.408 - ETA: 0s - loss: 9.3980 - acc: 0.410 - ETA: 0s - loss: 9.3836 - acc: 0.411 - ETA: 0s - loss: 9.3727 - acc: 0.411 - ETA: 0s - loss: 9.3615 - acc: 0.412 - ETA: 0s - loss: 9.3639 - acc: 0.411 - ETA: 0s - loss: 9.4020 - acc: 0.409 - ETA: 0s - loss: 9.3944 - acc: 0.4094Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.3906 - acc: 0.4097 - val_loss: 9.8160 - val_acc: 0.3269\n",
      "Epoch 10/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 15.3122 - acc: 0.05 - ETA: 2s - loss: 10.3707 - acc: 0.35 - ETA: 2s - loss: 9.7129 - acc: 0.3962 - ETA: 2s - loss: 9.5146 - acc: 0.405 - ETA: 2s - loss: 9.2000 - acc: 0.416 - ETA: 2s - loss: 9.3234 - acc: 0.407 - ETA: 2s - loss: 9.2947 - acc: 0.411 - ETA: 2s - loss: 9.2923 - acc: 0.412 - ETA: 2s - loss: 9.4415 - acc: 0.402 - ETA: 2s - loss: 9.5099 - acc: 0.398 - ETA: 2s - loss: 9.4031 - acc: 0.401 - ETA: 2s - loss: 9.3220 - acc: 0.406 - ETA: 2s - loss: 9.3410 - acc: 0.405 - ETA: 2s - loss: 9.3577 - acc: 0.404 - ETA: 2s - loss: 9.2284 - acc: 0.411 - ETA: 2s - loss: 9.2859 - acc: 0.408 - ETA: 2s - loss: 9.3541 - acc: 0.402 - ETA: 1s - loss: 9.3763 - acc: 0.401 - ETA: 1s - loss: 9.3593 - acc: 0.402 - ETA: 1s - loss: 9.3600 - acc: 0.403 - ETA: 1s - loss: 9.3516 - acc: 0.403 - ETA: 1s - loss: 9.3139 - acc: 0.405 - ETA: 1s - loss: 9.2931 - acc: 0.406 - ETA: 1s - loss: 9.2814 - acc: 0.406 - ETA: 1s - loss: 9.2576 - acc: 0.406 - ETA: 1s - loss: 9.2242 - acc: 0.408 - ETA: 1s - loss: 9.2374 - acc: 0.407 - ETA: 1s - loss: 9.2303 - acc: 0.408 - ETA: 1s - loss: 9.2163 - acc: 0.408 - ETA: 1s - loss: 9.2366 - acc: 0.407 - ETA: 1s - loss: 9.2419 - acc: 0.407 - ETA: 1s - loss: 9.2543 - acc: 0.407 - ETA: 1s - loss: 9.2782 - acc: 0.405 - ETA: 1s - loss: 9.2767 - acc: 0.405 - ETA: 1s - loss: 9.2484 - acc: 0.408 - ETA: 1s - loss: 9.2227 - acc: 0.409 - ETA: 0s - loss: 9.2125 - acc: 0.409 - ETA: 0s - loss: 9.2101 - acc: 0.409 - ETA: 0s - loss: 9.2061 - acc: 0.409 - ETA: 0s - loss: 9.1956 - acc: 0.410 - ETA: 0s - loss: 9.1805 - acc: 0.410 - ETA: 0s - loss: 9.1705 - acc: 0.410 - ETA: 0s - loss: 9.1891 - acc: 0.409 - ETA: 0s - loss: 9.1887 - acc: 0.408 - ETA: 0s - loss: 9.1910 - acc: 0.408 - ETA: 0s - loss: 9.1905 - acc: 0.408 - ETA: 0s - loss: 9.1922 - acc: 0.407 - ETA: 0s - loss: 9.1677 - acc: 0.408 - ETA: 0s - loss: 9.1672 - acc: 0.408 - ETA: 0s - loss: 9.1698 - acc: 0.408 - ETA: 0s - loss: 9.1637 - acc: 0.408 - ETA: 0s - loss: 9.1811 - acc: 0.408 - ETA: 0s - loss: 9.1989 - acc: 0.406 - ETA: 0s - loss: 9.2113 - acc: 0.405 - ETA: 0s - loss: 9.2134 - acc: 0.4062Epoch 00009: val_loss improved from 9.81552 to 9.59339, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.2159 - acc: 0.4060 - val_loss: 9.5934 - val_acc: 0.3377\n",
      "Epoch 11/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 9.6729 - acc: 0.400 - ETA: 2s - loss: 8.1780 - acc: 0.475 - ETA: 2s - loss: 8.5988 - acc: 0.450 - ETA: 2s - loss: 8.9805 - acc: 0.423 - ETA: 2s - loss: 9.1092 - acc: 0.414 - ETA: 2s - loss: 8.9469 - acc: 0.427 - ETA: 2s - loss: 8.9880 - acc: 0.425 - ETA: 2s - loss: 9.0795 - acc: 0.422 - ETA: 2s - loss: 9.1501 - acc: 0.419 - ETA: 2s - loss: 9.0962 - acc: 0.424 - ETA: 2s - loss: 9.0242 - acc: 0.427 - ETA: 2s - loss: 8.9904 - acc: 0.429 - ETA: 2s - loss: 9.0679 - acc: 0.424 - ETA: 2s - loss: 9.0963 - acc: 0.422 - ETA: 2s - loss: 9.0754 - acc: 0.423 - ETA: 2s - loss: 9.1418 - acc: 0.420 - ETA: 2s - loss: 9.1269 - acc: 0.421 - ETA: 1s - loss: 9.1435 - acc: 0.420 - ETA: 1s - loss: 9.1375 - acc: 0.420 - ETA: 1s - loss: 9.0908 - acc: 0.422 - ETA: 1s - loss: 9.0493 - acc: 0.425 - ETA: 1s - loss: 9.0284 - acc: 0.426 - ETA: 1s - loss: 8.9866 - acc: 0.428 - ETA: 1s - loss: 8.9987 - acc: 0.428 - ETA: 1s - loss: 8.9823 - acc: 0.429 - ETA: 1s - loss: 8.9499 - acc: 0.431 - ETA: 1s - loss: 8.9476 - acc: 0.431 - ETA: 1s - loss: 8.9677 - acc: 0.429 - ETA: 1s - loss: 9.0217 - acc: 0.426 - ETA: 1s - loss: 9.0001 - acc: 0.428 - ETA: 1s - loss: 9.0482 - acc: 0.425 - ETA: 1s - loss: 9.0517 - acc: 0.424 - ETA: 1s - loss: 9.0659 - acc: 0.423 - ETA: 1s - loss: 9.0588 - acc: 0.424 - ETA: 1s - loss: 9.0503 - acc: 0.425 - ETA: 1s - loss: 9.0341 - acc: 0.426 - ETA: 1s - loss: 9.0184 - acc: 0.427 - ETA: 0s - loss: 9.0181 - acc: 0.428 - ETA: 0s - loss: 9.0825 - acc: 0.424 - ETA: 0s - loss: 9.0672 - acc: 0.425 - ETA: 0s - loss: 9.0594 - acc: 0.425 - ETA: 0s - loss: 9.0626 - acc: 0.425 - ETA: 0s - loss: 9.0365 - acc: 0.427 - ETA: 0s - loss: 9.0339 - acc: 0.427 - ETA: 0s - loss: 9.0465 - acc: 0.426 - ETA: 0s - loss: 9.0560 - acc: 0.426 - ETA: 0s - loss: 9.0667 - acc: 0.425 - ETA: 0s - loss: 9.0653 - acc: 0.425 - ETA: 0s - loss: 9.0722 - acc: 0.425 - ETA: 0s - loss: 9.0602 - acc: 0.426 - ETA: 0s - loss: 9.0502 - acc: 0.426 - ETA: 0s - loss: 9.0526 - acc: 0.426 - ETA: 0s - loss: 9.0477 - acc: 0.426 - ETA: 0s - loss: 9.0589 - acc: 0.426 - ETA: 0s - loss: 9.0559 - acc: 0.426 - ETA: 0s - loss: 9.0707 - acc: 0.4252Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 9.0726 - acc: 0.4251 - val_loss: 9.6434 - val_acc: 0.3353\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 11.2832 - acc: 0.30 - ETA: 2s - loss: 9.3679 - acc: 0.4000 - ETA: 2s - loss: 9.8187 - acc: 0.380 - ETA: 2s - loss: 9.3336 - acc: 0.413 - ETA: 2s - loss: 9.1478 - acc: 0.424 - ETA: 2s - loss: 8.8391 - acc: 0.443 - ETA: 2s - loss: 8.9523 - acc: 0.437 - ETA: 2s - loss: 9.0529 - acc: 0.432 - ETA: 2s - loss: 9.0654 - acc: 0.431 - ETA: 2s - loss: 9.0440 - acc: 0.433 - ETA: 2s - loss: 9.0128 - acc: 0.435 - ETA: 2s - loss: 9.0042 - acc: 0.435 - ETA: 2s - loss: 9.0319 - acc: 0.433 - ETA: 2s - loss: 9.0873 - acc: 0.430 - ETA: 2s - loss: 9.0719 - acc: 0.430 - ETA: 2s - loss: 9.0274 - acc: 0.431 - ETA: 1s - loss: 8.9770 - acc: 0.432 - ETA: 1s - loss: 9.0558 - acc: 0.428 - ETA: 1s - loss: 9.0622 - acc: 0.427 - ETA: 1s - loss: 9.0565 - acc: 0.428 - ETA: 1s - loss: 9.0480 - acc: 0.428 - ETA: 1s - loss: 9.0038 - acc: 0.431 - ETA: 1s - loss: 9.0131 - acc: 0.430 - ETA: 1s - loss: 8.9807 - acc: 0.432 - ETA: 1s - loss: 9.0034 - acc: 0.431 - ETA: 1s - loss: 9.0191 - acc: 0.430 - ETA: 1s - loss: 9.0417 - acc: 0.429 - ETA: 1s - loss: 9.0723 - acc: 0.427 - ETA: 1s - loss: 9.0534 - acc: 0.428 - ETA: 1s - loss: 9.0625 - acc: 0.427 - ETA: 1s - loss: 9.0324 - acc: 0.429 - ETA: 1s - loss: 9.0188 - acc: 0.430 - ETA: 1s - loss: 8.9914 - acc: 0.431 - ETA: 1s - loss: 9.0026 - acc: 0.430 - ETA: 1s - loss: 9.0055 - acc: 0.429 - ETA: 0s - loss: 8.9669 - acc: 0.432 - ETA: 0s - loss: 8.9433 - acc: 0.433 - ETA: 0s - loss: 8.9942 - acc: 0.430 - ETA: 0s - loss: 8.9981 - acc: 0.430 - ETA: 0s - loss: 8.9958 - acc: 0.430 - ETA: 0s - loss: 8.9993 - acc: 0.430 - ETA: 0s - loss: 9.0152 - acc: 0.429 - ETA: 0s - loss: 9.0232 - acc: 0.428 - ETA: 0s - loss: 9.0130 - acc: 0.429 - ETA: 0s - loss: 9.0189 - acc: 0.429 - ETA: 0s - loss: 9.0074 - acc: 0.429 - ETA: 0s - loss: 8.9879 - acc: 0.431 - ETA: 0s - loss: 8.9977 - acc: 0.430 - ETA: 0s - loss: 9.0062 - acc: 0.429 - ETA: 0s - loss: 8.9882 - acc: 0.431 - ETA: 0s - loss: 8.9784 - acc: 0.431 - ETA: 0s - loss: 8.9739 - acc: 0.432 - ETA: 0s - loss: 8.9550 - acc: 0.433 - ETA: 0s - loss: 8.9641 - acc: 0.432 - ETA: 0s - loss: 8.9633 - acc: 0.4324Epoch 00011: val_loss improved from 9.59339 to 9.37930, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.9660 - acc: 0.4322 - val_loss: 9.3793 - val_acc: 0.3605\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 8.5643 - acc: 0.468 - ETA: 2s - loss: 8.8176 - acc: 0.450 - ETA: 2s - loss: 8.6339 - acc: 0.460 - ETA: 2s - loss: 8.6205 - acc: 0.459 - ETA: 2s - loss: 8.5849 - acc: 0.457 - ETA: 2s - loss: 8.7978 - acc: 0.444 - ETA: 2s - loss: 8.8261 - acc: 0.444 - ETA: 2s - loss: 8.9265 - acc: 0.439 - ETA: 2s - loss: 9.0919 - acc: 0.428 - ETA: 2s - loss: 9.0181 - acc: 0.432 - ETA: 2s - loss: 9.0580 - acc: 0.428 - ETA: 2s - loss: 9.0194 - acc: 0.430 - ETA: 2s - loss: 9.1042 - acc: 0.424 - ETA: 2s - loss: 9.1194 - acc: 0.422 - ETA: 2s - loss: 9.0962 - acc: 0.424 - ETA: 2s - loss: 9.1048 - acc: 0.424 - ETA: 2s - loss: 9.0497 - acc: 0.427 - ETA: 1s - loss: 9.0213 - acc: 0.430 - ETA: 1s - loss: 9.0873 - acc: 0.426 - ETA: 1s - loss: 9.0284 - acc: 0.429 - ETA: 1s - loss: 8.9423 - acc: 0.434 - ETA: 1s - loss: 8.9717 - acc: 0.431 - ETA: 1s - loss: 8.9499 - acc: 0.433 - ETA: 1s - loss: 8.9358 - acc: 0.433 - ETA: 1s - loss: 8.9311 - acc: 0.434 - ETA: 1s - loss: 8.9061 - acc: 0.435 - ETA: 1s - loss: 8.8573 - acc: 0.437 - ETA: 1s - loss: 8.8834 - acc: 0.435 - ETA: 1s - loss: 8.8919 - acc: 0.434 - ETA: 1s - loss: 8.8643 - acc: 0.436 - ETA: 1s - loss: 8.8410 - acc: 0.437 - ETA: 1s - loss: 8.8602 - acc: 0.436 - ETA: 1s - loss: 8.8636 - acc: 0.435 - ETA: 1s - loss: 8.9027 - acc: 0.433 - ETA: 0s - loss: 8.9023 - acc: 0.432 - ETA: 0s - loss: 8.8992 - acc: 0.433 - ETA: 0s - loss: 8.9080 - acc: 0.432 - ETA: 0s - loss: 8.8913 - acc: 0.433 - ETA: 0s - loss: 8.8892 - acc: 0.433 - ETA: 0s - loss: 8.8688 - acc: 0.434 - ETA: 0s - loss: 8.8697 - acc: 0.434 - ETA: 0s - loss: 8.8497 - acc: 0.435 - ETA: 0s - loss: 8.8513 - acc: 0.435 - ETA: 0s - loss: 8.8428 - acc: 0.435 - ETA: 0s - loss: 8.8711 - acc: 0.433 - ETA: 0s - loss: 8.8629 - acc: 0.433 - ETA: 0s - loss: 8.8420 - acc: 0.435 - ETA: 0s - loss: 8.8444 - acc: 0.434 - ETA: 0s - loss: 8.8419 - acc: 0.434 - ETA: 0s - loss: 8.8400 - acc: 0.434 - ETA: 0s - loss: 8.8251 - acc: 0.434 - ETA: 0s - loss: 8.8137 - acc: 0.435 - ETA: 0s - loss: 8.8080 - acc: 0.4358Epoch 00012: val_loss improved from 9.37930 to 9.25013, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.7967 - acc: 0.4367 - val_loss: 9.2501 - val_acc: 0.3509\n",
      "Epoch 14/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 10.6388 - acc: 0.30 - ETA: 2s - loss: 9.3647 - acc: 0.3857 - ETA: 2s - loss: 8.4519 - acc: 0.446 - ETA: 2s - loss: 7.9125 - acc: 0.478 - ETA: 2s - loss: 8.3519 - acc: 0.454 - ETA: 2s - loss: 8.1343 - acc: 0.464 - ETA: 2s - loss: 8.2461 - acc: 0.462 - ETA: 2s - loss: 8.1726 - acc: 0.466 - ETA: 2s - loss: 8.2201 - acc: 0.464 - ETA: 2s - loss: 8.2886 - acc: 0.461 - ETA: 2s - loss: 8.4000 - acc: 0.453 - ETA: 2s - loss: 8.4134 - acc: 0.453 - ETA: 2s - loss: 8.4772 - acc: 0.450 - ETA: 2s - loss: 8.4880 - acc: 0.450 - ETA: 2s - loss: 8.5087 - acc: 0.448 - ETA: 1s - loss: 8.5076 - acc: 0.447 - ETA: 1s - loss: 8.4709 - acc: 0.451 - ETA: 1s - loss: 8.4954 - acc: 0.450 - ETA: 1s - loss: 8.4752 - acc: 0.452 - ETA: 1s - loss: 8.4297 - acc: 0.455 - ETA: 1s - loss: 8.4916 - acc: 0.451 - ETA: 1s - loss: 8.4371 - acc: 0.455 - ETA: 1s - loss: 8.4735 - acc: 0.453 - ETA: 1s - loss: 8.4795 - acc: 0.452 - ETA: 1s - loss: 8.4852 - acc: 0.452 - ETA: 1s - loss: 8.4813 - acc: 0.453 - ETA: 1s - loss: 8.4900 - acc: 0.453 - ETA: 1s - loss: 8.5187 - acc: 0.451 - ETA: 1s - loss: 8.5075 - acc: 0.452 - ETA: 1s - loss: 8.4614 - acc: 0.454 - ETA: 1s - loss: 8.4567 - acc: 0.455 - ETA: 1s - loss: 8.4668 - acc: 0.455 - ETA: 1s - loss: 8.4762 - acc: 0.454 - ETA: 1s - loss: 8.4900 - acc: 0.453 - ETA: 1s - loss: 8.4913 - acc: 0.454 - ETA: 0s - loss: 8.5143 - acc: 0.452 - ETA: 0s - loss: 8.4867 - acc: 0.454 - ETA: 0s - loss: 8.4906 - acc: 0.454 - ETA: 0s - loss: 8.4679 - acc: 0.456 - ETA: 0s - loss: 8.4863 - acc: 0.454 - ETA: 0s - loss: 8.4844 - acc: 0.454 - ETA: 0s - loss: 8.4863 - acc: 0.454 - ETA: 0s - loss: 8.4574 - acc: 0.456 - ETA: 0s - loss: 8.4644 - acc: 0.455 - ETA: 0s - loss: 8.4504 - acc: 0.456 - ETA: 0s - loss: 8.4760 - acc: 0.454 - ETA: 0s - loss: 8.4796 - acc: 0.454 - ETA: 0s - loss: 8.4769 - acc: 0.455 - ETA: 0s - loss: 8.4983 - acc: 0.454 - ETA: 0s - loss: 8.5217 - acc: 0.452 - ETA: 0s - loss: 8.5176 - acc: 0.453 - ETA: 0s - loss: 8.5410 - acc: 0.451 - ETA: 0s - loss: 8.5700 - acc: 0.450 - ETA: 0s - loss: 8.5690 - acc: 0.4503Epoch 00013: val_loss improved from 9.25013 to 9.05533, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 8.5607 - acc: 0.4507 - val_loss: 9.0553 - val_acc: 0.3784\n",
      "Epoch 15/20\n",
      "6580/6680 [============================>.] - ETA: 2s - loss: 8.8666 - acc: 0.450 - ETA: 2s - loss: 8.4875 - acc: 0.471 - ETA: 2s - loss: 8.3549 - acc: 0.480 - ETA: 2s - loss: 8.4903 - acc: 0.463 - ETA: 2s - loss: 8.3206 - acc: 0.470 - ETA: 2s - loss: 8.2671 - acc: 0.472 - ETA: 2s - loss: 8.3220 - acc: 0.468 - ETA: 2s - loss: 8.4224 - acc: 0.462 - ETA: 2s - loss: 8.4279 - acc: 0.464 - ETA: 2s - loss: 8.4319 - acc: 0.463 - ETA: 2s - loss: 8.5666 - acc: 0.452 - ETA: 2s - loss: 8.5099 - acc: 0.457 - ETA: 2s - loss: 8.4957 - acc: 0.459 - ETA: 2s - loss: 8.4451 - acc: 0.461 - ETA: 2s - loss: 8.3282 - acc: 0.469 - ETA: 2s - loss: 8.3322 - acc: 0.469 - ETA: 2s - loss: 8.3915 - acc: 0.466 - ETA: 1s - loss: 8.3013 - acc: 0.471 - ETA: 1s - loss: 8.3407 - acc: 0.469 - ETA: 1s - loss: 8.4236 - acc: 0.465 - ETA: 1s - loss: 8.4167 - acc: 0.465 - ETA: 1s - loss: 8.3695 - acc: 0.468 - ETA: 1s - loss: 8.3713 - acc: 0.468 - ETA: 1s - loss: 8.4282 - acc: 0.464 - ETA: 1s - loss: 8.4745 - acc: 0.462 - ETA: 1s - loss: 8.5276 - acc: 0.458 - ETA: 1s - loss: 8.5166 - acc: 0.459 - ETA: 1s - loss: 8.5066 - acc: 0.459 - ETA: 1s - loss: 8.4483 - acc: 0.463 - ETA: 1s - loss: 8.4576 - acc: 0.463 - ETA: 1s - loss: 8.4494 - acc: 0.464 - ETA: 1s - loss: 8.4417 - acc: 0.464 - ETA: 1s - loss: 8.4276 - acc: 0.465 - ETA: 1s - loss: 8.4050 - acc: 0.466 - ETA: 1s - loss: 8.4207 - acc: 0.465 - ETA: 1s - loss: 8.3956 - acc: 0.467 - ETA: 0s - loss: 8.3714 - acc: 0.469 - ETA: 0s - loss: 8.3582 - acc: 0.469 - ETA: 0s - loss: 8.3685 - acc: 0.469 - ETA: 0s - loss: 8.3892 - acc: 0.467 - ETA: 0s - loss: 8.4001 - acc: 0.466 - ETA: 0s - loss: 8.3884 - acc: 0.467 - ETA: 0s - loss: 8.3875 - acc: 0.467 - ETA: 0s - loss: 8.3997 - acc: 0.466 - ETA: 0s - loss: 8.4223 - acc: 0.465 - ETA: 0s - loss: 8.4342 - acc: 0.465 - ETA: 0s - loss: 8.4467 - acc: 0.464 - ETA: 0s - loss: 8.4416 - acc: 0.465 - ETA: 0s - loss: 8.4651 - acc: 0.463 - ETA: 0s - loss: 8.4725 - acc: 0.462 - ETA: 0s - loss: 8.4840 - acc: 0.462 - ETA: 0s - loss: 8.4904 - acc: 0.461 - ETA: 0s - loss: 8.5210 - acc: 0.459 - ETA: 0s - loss: 8.5167 - acc: 0.459 - ETA: 0s - loss: 8.4982 - acc: 0.4606Epoch 00014: val_loss improved from 9.05533 to 9.02061, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.4799 - acc: 0.4618 - val_loss: 9.0206 - val_acc: 0.3796\n",
      "Epoch 16/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 9.6958 - acc: 0.400 - ETA: 2s - loss: 9.5613 - acc: 0.407 - ETA: 2s - loss: 9.1863 - acc: 0.426 - ETA: 2s - loss: 8.9119 - acc: 0.444 - ETA: 2s - loss: 8.8640 - acc: 0.448 - ETA: 2s - loss: 8.7904 - acc: 0.453 - ETA: 2s - loss: 8.8554 - acc: 0.447 - ETA: 2s - loss: 8.8401 - acc: 0.447 - ETA: 2s - loss: 8.7201 - acc: 0.454 - ETA: 2s - loss: 8.6949 - acc: 0.455 - ETA: 2s - loss: 8.6860 - acc: 0.456 - ETA: 2s - loss: 8.6443 - acc: 0.458 - ETA: 2s - loss: 8.5118 - acc: 0.465 - ETA: 2s - loss: 8.4431 - acc: 0.470 - ETA: 2s - loss: 8.4524 - acc: 0.469 - ETA: 2s - loss: 8.4624 - acc: 0.469 - ETA: 2s - loss: 8.5378 - acc: 0.464 - ETA: 1s - loss: 8.5494 - acc: 0.463 - ETA: 1s - loss: 8.5374 - acc: 0.464 - ETA: 1s - loss: 8.5066 - acc: 0.466 - ETA: 1s - loss: 8.5865 - acc: 0.461 - ETA: 1s - loss: 8.6431 - acc: 0.457 - ETA: 1s - loss: 8.5753 - acc: 0.460 - ETA: 1s - loss: 8.5564 - acc: 0.461 - ETA: 1s - loss: 8.5201 - acc: 0.462 - ETA: 1s - loss: 8.5394 - acc: 0.460 - ETA: 1s - loss: 8.5831 - acc: 0.458 - ETA: 1s - loss: 8.5658 - acc: 0.458 - ETA: 1s - loss: 8.5609 - acc: 0.458 - ETA: 1s - loss: 8.5251 - acc: 0.459 - ETA: 1s - loss: 8.5187 - acc: 0.459 - ETA: 1s - loss: 8.4930 - acc: 0.461 - ETA: 1s - loss: 8.4598 - acc: 0.463 - ETA: 1s - loss: 8.4196 - acc: 0.465 - ETA: 1s - loss: 8.4059 - acc: 0.466 - ETA: 0s - loss: 8.3915 - acc: 0.467 - ETA: 0s - loss: 8.3472 - acc: 0.469 - ETA: 0s - loss: 8.3575 - acc: 0.469 - ETA: 0s - loss: 8.3420 - acc: 0.470 - ETA: 0s - loss: 8.3458 - acc: 0.470 - ETA: 0s - loss: 8.3567 - acc: 0.469 - ETA: 0s - loss: 8.3675 - acc: 0.467 - ETA: 0s - loss: 8.3624 - acc: 0.467 - ETA: 0s - loss: 8.3866 - acc: 0.466 - ETA: 0s - loss: 8.3670 - acc: 0.466 - ETA: 0s - loss: 8.3524 - acc: 0.467 - ETA: 0s - loss: 8.3549 - acc: 0.467 - ETA: 0s - loss: 8.3265 - acc: 0.469 - ETA: 0s - loss: 8.3096 - acc: 0.469 - ETA: 0s - loss: 8.3025 - acc: 0.470 - ETA: 0s - loss: 8.2876 - acc: 0.471 - ETA: 0s - loss: 8.3094 - acc: 0.469 - ETA: 0s - loss: 8.3322 - acc: 0.468 - ETA: 0s - loss: 8.3507 - acc: 0.4674Epoch 00015: val_loss improved from 9.02061 to 8.86007, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.3547 - acc: 0.4668 - val_loss: 8.8601 - val_acc: 0.3880\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 10.4867 - acc: 0.35 - ETA: 2s - loss: 8.3029 - acc: 0.4750 - ETA: 2s - loss: 8.3235 - acc: 0.475 - ETA: 2s - loss: 8.3367 - acc: 0.472 - ETA: 2s - loss: 8.6455 - acc: 0.455 - ETA: 2s - loss: 8.4866 - acc: 0.467 - ETA: 2s - loss: 8.6238 - acc: 0.457 - ETA: 2s - loss: 8.4091 - acc: 0.471 - ETA: 2s - loss: 8.2916 - acc: 0.477 - ETA: 2s - loss: 8.1941 - acc: 0.482 - ETA: 2s - loss: 8.1651 - acc: 0.482 - ETA: 2s - loss: 8.0815 - acc: 0.487 - ETA: 2s - loss: 8.0454 - acc: 0.490 - ETA: 2s - loss: 8.0983 - acc: 0.486 - ETA: 2s - loss: 8.1509 - acc: 0.484 - ETA: 2s - loss: 8.1012 - acc: 0.486 - ETA: 1s - loss: 8.0910 - acc: 0.487 - ETA: 1s - loss: 8.0321 - acc: 0.491 - ETA: 1s - loss: 8.1058 - acc: 0.487 - ETA: 1s - loss: 8.1807 - acc: 0.482 - ETA: 1s - loss: 8.2062 - acc: 0.480 - ETA: 1s - loss: 8.1815 - acc: 0.481 - ETA: 1s - loss: 8.1528 - acc: 0.483 - ETA: 1s - loss: 8.1723 - acc: 0.481 - ETA: 1s - loss: 8.2141 - acc: 0.479 - ETA: 1s - loss: 8.2149 - acc: 0.479 - ETA: 1s - loss: 8.2043 - acc: 0.480 - ETA: 1s - loss: 8.1619 - acc: 0.482 - ETA: 1s - loss: 8.2102 - acc: 0.480 - ETA: 1s - loss: 8.1921 - acc: 0.481 - ETA: 1s - loss: 8.1656 - acc: 0.483 - ETA: 1s - loss: 8.1591 - acc: 0.483 - ETA: 1s - loss: 8.1277 - acc: 0.485 - ETA: 1s - loss: 8.1273 - acc: 0.485 - ETA: 1s - loss: 8.1641 - acc: 0.483 - ETA: 0s - loss: 8.1402 - acc: 0.484 - ETA: 0s - loss: 8.1332 - acc: 0.484 - ETA: 0s - loss: 8.1296 - acc: 0.484 - ETA: 0s - loss: 8.1256 - acc: 0.483 - ETA: 0s - loss: 8.1413 - acc: 0.482 - ETA: 0s - loss: 8.1348 - acc: 0.483 - ETA: 0s - loss: 8.1477 - acc: 0.482 - ETA: 0s - loss: 8.1205 - acc: 0.483 - ETA: 0s - loss: 8.1595 - acc: 0.480 - ETA: 0s - loss: 8.1932 - acc: 0.479 - ETA: 0s - loss: 8.2057 - acc: 0.478 - ETA: 0s - loss: 8.2023 - acc: 0.478 - ETA: 0s - loss: 8.1947 - acc: 0.479 - ETA: 0s - loss: 8.1983 - acc: 0.478 - ETA: 0s - loss: 8.2016 - acc: 0.478 - ETA: 0s - loss: 8.2302 - acc: 0.477 - ETA: 0s - loss: 8.2371 - acc: 0.476 - ETA: 0s - loss: 8.2211 - acc: 0.477 - ETA: 0s - loss: 8.1909 - acc: 0.479 - ETA: 0s - loss: 8.1894 - acc: 0.4796Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1880 - acc: 0.4795 - val_loss: 8.8805 - val_acc: 0.3784\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 9.0137 - acc: 0.4286 - ETA: 3s - loss: 8.9559 - acc: 0.433 - ETA: 2s - loss: 8.4061 - acc: 0.466 - ETA: 2s - loss: 8.3709 - acc: 0.468 - ETA: 2s - loss: 8.4160 - acc: 0.468 - ETA: 2s - loss: 8.4494 - acc: 0.465 - ETA: 2s - loss: 8.3571 - acc: 0.472 - ETA: 2s - loss: 8.4544 - acc: 0.467 - ETA: 2s - loss: 8.4704 - acc: 0.467 - ETA: 2s - loss: 8.4563 - acc: 0.469 - ETA: 2s - loss: 8.5063 - acc: 0.466 - ETA: 2s - loss: 8.4804 - acc: 0.468 - ETA: 2s - loss: 8.4842 - acc: 0.467 - ETA: 2s - loss: 8.4721 - acc: 0.467 - ETA: 2s - loss: 8.4760 - acc: 0.468 - ETA: 2s - loss: 8.4421 - acc: 0.470 - ETA: 2s - loss: 8.3599 - acc: 0.475 - ETA: 1s - loss: 8.3795 - acc: 0.473 - ETA: 1s - loss: 8.3338 - acc: 0.476 - ETA: 1s - loss: 8.3321 - acc: 0.475 - ETA: 1s - loss: 8.3737 - acc: 0.472 - ETA: 1s - loss: 8.3414 - acc: 0.475 - ETA: 1s - loss: 8.3552 - acc: 0.474 - ETA: 1s - loss: 8.3100 - acc: 0.476 - ETA: 1s - loss: 8.3375 - acc: 0.475 - ETA: 1s - loss: 8.3078 - acc: 0.477 - ETA: 1s - loss: 8.3006 - acc: 0.477 - ETA: 1s - loss: 8.2544 - acc: 0.480 - ETA: 1s - loss: 8.2569 - acc: 0.480 - ETA: 1s - loss: 8.2742 - acc: 0.479 - ETA: 1s - loss: 8.2367 - acc: 0.482 - ETA: 1s - loss: 8.2452 - acc: 0.481 - ETA: 1s - loss: 8.2595 - acc: 0.480 - ETA: 1s - loss: 8.2565 - acc: 0.480 - ETA: 1s - loss: 8.2584 - acc: 0.480 - ETA: 1s - loss: 8.2550 - acc: 0.480 - ETA: 0s - loss: 8.2390 - acc: 0.481 - ETA: 0s - loss: 8.2522 - acc: 0.480 - ETA: 0s - loss: 8.2103 - acc: 0.483 - ETA: 0s - loss: 8.1609 - acc: 0.486 - ETA: 0s - loss: 8.1867 - acc: 0.484 - ETA: 0s - loss: 8.1750 - acc: 0.485 - ETA: 0s - loss: 8.1962 - acc: 0.484 - ETA: 0s - loss: 8.1681 - acc: 0.485 - ETA: 0s - loss: 8.1544 - acc: 0.486 - ETA: 0s - loss: 8.1496 - acc: 0.486 - ETA: 0s - loss: 8.1540 - acc: 0.486 - ETA: 0s - loss: 8.1566 - acc: 0.486 - ETA: 0s - loss: 8.1575 - acc: 0.486 - ETA: 0s - loss: 8.1670 - acc: 0.485 - ETA: 0s - loss: 8.1571 - acc: 0.486 - ETA: 0s - loss: 8.1681 - acc: 0.485 - ETA: 0s - loss: 8.1692 - acc: 0.485 - ETA: 0s - loss: 8.1565 - acc: 0.486 - ETA: 0s - loss: 8.1402 - acc: 0.4872Epoch 00017: val_loss improved from 8.86007 to 8.78440, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1470 - acc: 0.4868 - val_loss: 8.7844 - val_acc: 0.3820\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 2s - loss: 9.6757 - acc: 0.400 - ETA: 3s - loss: 9.7202 - acc: 0.383 - ETA: 3s - loss: 9.4795 - acc: 0.404 - ETA: 3s - loss: 9.4059 - acc: 0.411 - ETA: 2s - loss: 8.8094 - acc: 0.450 - ETA: 2s - loss: 8.7746 - acc: 0.450 - ETA: 2s - loss: 8.8171 - acc: 0.445 - ETA: 2s - loss: 8.5293 - acc: 0.464 - ETA: 2s - loss: 8.5895 - acc: 0.461 - ETA: 2s - loss: 8.4699 - acc: 0.468 - ETA: 2s - loss: 8.4027 - acc: 0.472 - ETA: 2s - loss: 8.4619 - acc: 0.468 - ETA: 2s - loss: 8.3753 - acc: 0.473 - ETA: 2s - loss: 8.2609 - acc: 0.479 - ETA: 2s - loss: 8.1046 - acc: 0.488 - ETA: 2s - loss: 8.1485 - acc: 0.485 - ETA: 2s - loss: 8.2412 - acc: 0.480 - ETA: 2s - loss: 8.2948 - acc: 0.477 - ETA: 1s - loss: 8.2617 - acc: 0.479 - ETA: 1s - loss: 8.2513 - acc: 0.480 - ETA: 1s - loss: 8.2154 - acc: 0.482 - ETA: 1s - loss: 8.1697 - acc: 0.485 - ETA: 1s - loss: 8.1344 - acc: 0.488 - ETA: 1s - loss: 8.1609 - acc: 0.486 - ETA: 1s - loss: 8.1627 - acc: 0.486 - ETA: 1s - loss: 8.1587 - acc: 0.487 - ETA: 1s - loss: 8.1344 - acc: 0.488 - ETA: 1s - loss: 8.1965 - acc: 0.485 - ETA: 1s - loss: 8.1744 - acc: 0.486 - ETA: 1s - loss: 8.1347 - acc: 0.488 - ETA: 1s - loss: 8.1648 - acc: 0.486 - ETA: 1s - loss: 8.1652 - acc: 0.486 - ETA: 1s - loss: 8.1333 - acc: 0.488 - ETA: 1s - loss: 8.1314 - acc: 0.488 - ETA: 1s - loss: 8.1531 - acc: 0.487 - ETA: 1s - loss: 8.1773 - acc: 0.486 - ETA: 1s - loss: 8.1562 - acc: 0.487 - ETA: 0s - loss: 8.1502 - acc: 0.487 - ETA: 0s - loss: 8.1381 - acc: 0.488 - ETA: 0s - loss: 8.1370 - acc: 0.488 - ETA: 0s - loss: 8.1403 - acc: 0.488 - ETA: 0s - loss: 8.1396 - acc: 0.488 - ETA: 0s - loss: 8.1233 - acc: 0.489 - ETA: 0s - loss: 8.1141 - acc: 0.489 - ETA: 0s - loss: 8.0801 - acc: 0.491 - ETA: 0s - loss: 8.0702 - acc: 0.492 - ETA: 0s - loss: 8.0644 - acc: 0.492 - ETA: 0s - loss: 8.0730 - acc: 0.492 - ETA: 0s - loss: 8.1063 - acc: 0.490 - ETA: 0s - loss: 8.0945 - acc: 0.490 - ETA: 0s - loss: 8.0779 - acc: 0.491 - ETA: 0s - loss: 8.1020 - acc: 0.490 - ETA: 0s - loss: 8.1071 - acc: 0.490 - ETA: 0s - loss: 8.1086 - acc: 0.490 - ETA: 0s - loss: 8.1202 - acc: 0.489 - ETA: 0s - loss: 8.1341 - acc: 0.4886Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.1356 - acc: 0.4885 - val_loss: 8.8578 - val_acc: 0.3868\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 4.0343 - acc: 0.750 - ETA: 3s - loss: 7.1817 - acc: 0.542 - ETA: 2s - loss: 7.8353 - acc: 0.507 - ETA: 2s - loss: 8.0645 - acc: 0.492 - ETA: 2s - loss: 7.9990 - acc: 0.498 - ETA: 2s - loss: 7.9084 - acc: 0.503 - ETA: 2s - loss: 8.0428 - acc: 0.495 - ETA: 2s - loss: 8.0451 - acc: 0.496 - ETA: 2s - loss: 7.9977 - acc: 0.500 - ETA: 2s - loss: 7.9157 - acc: 0.503 - ETA: 2s - loss: 7.9689 - acc: 0.500 - ETA: 2s - loss: 8.0068 - acc: 0.497 - ETA: 2s - loss: 8.0433 - acc: 0.496 - ETA: 2s - loss: 8.0573 - acc: 0.494 - ETA: 2s - loss: 8.0196 - acc: 0.497 - ETA: 2s - loss: 8.0709 - acc: 0.493 - ETA: 2s - loss: 8.0981 - acc: 0.491 - ETA: 1s - loss: 8.0977 - acc: 0.491 - ETA: 1s - loss: 8.0816 - acc: 0.492 - ETA: 1s - loss: 8.0859 - acc: 0.492 - ETA: 1s - loss: 8.0911 - acc: 0.491 - ETA: 1s - loss: 8.0859 - acc: 0.491 - ETA: 1s - loss: 8.1023 - acc: 0.490 - ETA: 1s - loss: 8.1246 - acc: 0.488 - ETA: 1s - loss: 8.1450 - acc: 0.487 - ETA: 1s - loss: 8.0880 - acc: 0.490 - ETA: 1s - loss: 8.0688 - acc: 0.490 - ETA: 1s - loss: 8.0833 - acc: 0.489 - ETA: 1s - loss: 8.0594 - acc: 0.491 - ETA: 1s - loss: 8.0735 - acc: 0.490 - ETA: 1s - loss: 8.0868 - acc: 0.489 - ETA: 1s - loss: 8.0350 - acc: 0.493 - ETA: 1s - loss: 8.0424 - acc: 0.493 - ETA: 1s - loss: 8.0742 - acc: 0.491 - ETA: 1s - loss: 8.0860 - acc: 0.490 - ETA: 1s - loss: 8.0796 - acc: 0.490 - ETA: 0s - loss: 8.0469 - acc: 0.492 - ETA: 0s - loss: 8.0688 - acc: 0.491 - ETA: 0s - loss: 8.0719 - acc: 0.490 - ETA: 0s - loss: 8.0693 - acc: 0.490 - ETA: 0s - loss: 8.0889 - acc: 0.489 - ETA: 0s - loss: 8.0927 - acc: 0.489 - ETA: 0s - loss: 8.1016 - acc: 0.489 - ETA: 0s - loss: 8.1072 - acc: 0.488 - ETA: 0s - loss: 8.1106 - acc: 0.488 - ETA: 0s - loss: 8.0805 - acc: 0.490 - ETA: 0s - loss: 8.0762 - acc: 0.490 - ETA: 0s - loss: 8.1069 - acc: 0.488 - ETA: 0s - loss: 8.1146 - acc: 0.487 - ETA: 0s - loss: 8.0974 - acc: 0.488 - ETA: 0s - loss: 8.0840 - acc: 0.489 - ETA: 0s - loss: 8.0629 - acc: 0.490 - ETA: 0s - loss: 8.0656 - acc: 0.490 - ETA: 0s - loss: 8.0865 - acc: 0.489 - ETA: 0s - loss: 8.0848 - acc: 0.4889Epoch 00019: val_loss improved from 8.78440 to 8.66561, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0812 - acc: 0.4889 - val_loss: 8.6656 - val_acc: 0.3928\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 122s - loss: 5.8515 - acc: 0.0000e+0 - ETA: 27s - loss: 6.1988 - acc: 0.0000e+0 - ETA: 17s - loss: 5.8245 - acc: 0.0222   - ETA: 12s - loss: 5.6034 - acc: 0.03 - ETA: 10s - loss: 5.4419 - acc: 0.05 - ETA: 9s - loss: 5.1984 - acc: 0.0714 - ETA: 8s - loss: 4.9812 - acc: 0.092 - ETA: 7s - loss: 4.7915 - acc: 0.126 - ETA: 7s - loss: 4.6228 - acc: 0.136 - ETA: 6s - loss: 4.4714 - acc: 0.153 - ETA: 6s - loss: 4.3205 - acc: 0.167 - ETA: 5s - loss: 4.1549 - acc: 0.189 - ETA: 5s - loss: 4.0175 - acc: 0.209 - ETA: 5s - loss: 3.9111 - acc: 0.221 - ETA: 4s - loss: 3.7832 - acc: 0.240 - ETA: 4s - loss: 3.6722 - acc: 0.259 - ETA: 4s - loss: 3.5549 - acc: 0.278 - ETA: 4s - loss: 3.4681 - acc: 0.288 - ETA: 4s - loss: 3.3967 - acc: 0.301 - ETA: 4s - loss: 3.3168 - acc: 0.312 - ETA: 3s - loss: 3.2410 - acc: 0.324 - ETA: 3s - loss: 3.1849 - acc: 0.333 - ETA: 3s - loss: 3.1267 - acc: 0.339 - ETA: 3s - loss: 3.0504 - acc: 0.351 - ETA: 3s - loss: 3.0005 - acc: 0.358 - ETA: 3s - loss: 2.9558 - acc: 0.364 - ETA: 3s - loss: 2.8869 - acc: 0.378 - ETA: 3s - loss: 2.8367 - acc: 0.387 - ETA: 3s - loss: 2.7853 - acc: 0.396 - ETA: 3s - loss: 2.7355 - acc: 0.405 - ETA: 3s - loss: 2.6888 - acc: 0.411 - ETA: 2s - loss: 2.6485 - acc: 0.416 - ETA: 2s - loss: 2.6080 - acc: 0.421 - ETA: 2s - loss: 2.5546 - acc: 0.432 - ETA: 2s - loss: 2.5275 - acc: 0.437 - ETA: 2s - loss: 2.4978 - acc: 0.442 - ETA: 2s - loss: 2.4680 - acc: 0.447 - ETA: 2s - loss: 2.4331 - acc: 0.455 - ETA: 2s - loss: 2.4000 - acc: 0.461 - ETA: 2s - loss: 2.3703 - acc: 0.464 - ETA: 2s - loss: 2.3410 - acc: 0.468 - ETA: 2s - loss: 2.3180 - acc: 0.471 - ETA: 2s - loss: 2.2929 - acc: 0.476 - ETA: 2s - loss: 2.2640 - acc: 0.481 - ETA: 2s - loss: 2.2412 - acc: 0.485 - ETA: 2s - loss: 2.2150 - acc: 0.488 - ETA: 1s - loss: 2.1906 - acc: 0.493 - ETA: 1s - loss: 2.1654 - acc: 0.498 - ETA: 1s - loss: 2.1443 - acc: 0.500 - ETA: 1s - loss: 2.1227 - acc: 0.504 - ETA: 1s - loss: 2.0975 - acc: 0.509 - ETA: 1s - loss: 2.0776 - acc: 0.512 - ETA: 1s - loss: 2.0587 - acc: 0.516 - ETA: 1s - loss: 2.0396 - acc: 0.519 - ETA: 1s - loss: 2.0205 - acc: 0.522 - ETA: 1s - loss: 2.0037 - acc: 0.525 - ETA: 1s - loss: 1.9840 - acc: 0.529 - ETA: 1s - loss: 1.9682 - acc: 0.531 - ETA: 1s - loss: 1.9470 - acc: 0.536 - ETA: 1s - loss: 1.9307 - acc: 0.538 - ETA: 1s - loss: 1.9089 - acc: 0.542 - ETA: 1s - loss: 1.8948 - acc: 0.545 - ETA: 1s - loss: 1.8803 - acc: 0.548 - ETA: 0s - loss: 1.8658 - acc: 0.550 - ETA: 0s - loss: 1.8472 - acc: 0.554 - ETA: 0s - loss: 1.8333 - acc: 0.556 - ETA: 0s - loss: 1.8220 - acc: 0.558 - ETA: 0s - loss: 1.8045 - acc: 0.562 - ETA: 0s - loss: 1.7884 - acc: 0.566 - ETA: 0s - loss: 1.7788 - acc: 0.568 - ETA: 0s - loss: 1.7642 - acc: 0.570 - ETA: 0s - loss: 1.7517 - acc: 0.572 - ETA: 0s - loss: 1.7349 - acc: 0.575 - ETA: 0s - loss: 1.7192 - acc: 0.578 - ETA: 0s - loss: 1.7115 - acc: 0.580 - ETA: 0s - loss: 1.6991 - acc: 0.583 - ETA: 0s - loss: 1.6930 - acc: 0.583 - ETA: 0s - loss: 1.6830 - acc: 0.585 - ETA: 0s - loss: 1.6723 - acc: 0.587 - ETA: 0s - loss: 1.6632 - acc: 0.5891Epoch 00000: val_loss improved from inf to 0.83818, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 1.6494 - acc: 0.5918 - val_loss: 0.8382 - val_acc: 0.7485\n",
      "Epoch 2/20\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.2988 - acc: 0.900 - ETA: 3s - loss: 0.3833 - acc: 0.883 - ETA: 3s - loss: 0.3547 - acc: 0.890 - ETA: 3s - loss: 0.3911 - acc: 0.881 - ETA: 3s - loss: 0.4145 - acc: 0.876 - ETA: 3s - loss: 0.3961 - acc: 0.880 - ETA: 3s - loss: 0.4207 - acc: 0.869 - ETA: 3s - loss: 0.4147 - acc: 0.872 - ETA: 3s - loss: 0.4198 - acc: 0.868 - ETA: 3s - loss: 0.4213 - acc: 0.864 - ETA: 3s - loss: 0.4302 - acc: 0.862 - ETA: 3s - loss: 0.4372 - acc: 0.861 - ETA: 3s - loss: 0.4333 - acc: 0.862 - ETA: 3s - loss: 0.4379 - acc: 0.862 - ETA: 3s - loss: 0.4316 - acc: 0.864 - ETA: 3s - loss: 0.4280 - acc: 0.866 - ETA: 2s - loss: 0.4300 - acc: 0.865 - ETA: 2s - loss: 0.4341 - acc: 0.864 - ETA: 2s - loss: 0.4324 - acc: 0.866 - ETA: 2s - loss: 0.4362 - acc: 0.865 - ETA: 2s - loss: 0.4391 - acc: 0.864 - ETA: 2s - loss: 0.4367 - acc: 0.864 - ETA: 2s - loss: 0.4421 - acc: 0.863 - ETA: 2s - loss: 0.4433 - acc: 0.862 - ETA: 2s - loss: 0.4409 - acc: 0.862 - ETA: 2s - loss: 0.4383 - acc: 0.863 - ETA: 2s - loss: 0.4376 - acc: 0.863 - ETA: 2s - loss: 0.4345 - acc: 0.864 - ETA: 2s - loss: 0.4374 - acc: 0.863 - ETA: 2s - loss: 0.4371 - acc: 0.865 - ETA: 2s - loss: 0.4428 - acc: 0.863 - ETA: 2s - loss: 0.4421 - acc: 0.861 - ETA: 2s - loss: 0.4457 - acc: 0.860 - ETA: 2s - loss: 0.4434 - acc: 0.861 - ETA: 1s - loss: 0.4434 - acc: 0.862 - ETA: 1s - loss: 0.4432 - acc: 0.862 - ETA: 1s - loss: 0.4430 - acc: 0.862 - ETA: 1s - loss: 0.4455 - acc: 0.861 - ETA: 1s - loss: 0.4477 - acc: 0.861 - ETA: 1s - loss: 0.4463 - acc: 0.861 - ETA: 1s - loss: 0.4453 - acc: 0.863 - ETA: 1s - loss: 0.4431 - acc: 0.864 - ETA: 1s - loss: 0.4428 - acc: 0.865 - ETA: 1s - loss: 0.4455 - acc: 0.863 - ETA: 1s - loss: 0.4476 - acc: 0.862 - ETA: 1s - loss: 0.4492 - acc: 0.862 - ETA: 1s - loss: 0.4458 - acc: 0.863 - ETA: 1s - loss: 0.4438 - acc: 0.864 - ETA: 1s - loss: 0.4462 - acc: 0.863 - ETA: 1s - loss: 0.4452 - acc: 0.863 - ETA: 1s - loss: 0.4464 - acc: 0.863 - ETA: 1s - loss: 0.4475 - acc: 0.862 - ETA: 0s - loss: 0.4513 - acc: 0.862 - ETA: 0s - loss: 0.4488 - acc: 0.862 - ETA: 0s - loss: 0.4485 - acc: 0.862 - ETA: 0s - loss: 0.4481 - acc: 0.863 - ETA: 0s - loss: 0.4479 - acc: 0.864 - ETA: 0s - loss: 0.4475 - acc: 0.863 - ETA: 0s - loss: 0.4448 - acc: 0.864 - ETA: 0s - loss: 0.4439 - acc: 0.864 - ETA: 0s - loss: 0.4447 - acc: 0.864 - ETA: 0s - loss: 0.4433 - acc: 0.864 - ETA: 0s - loss: 0.4442 - acc: 0.863 - ETA: 0s - loss: 0.4435 - acc: 0.863 - ETA: 0s - loss: 0.4426 - acc: 0.863 - ETA: 0s - loss: 0.4413 - acc: 0.864 - ETA: 0s - loss: 0.4417 - acc: 0.863 - ETA: 0s - loss: 0.4424 - acc: 0.863 - ETA: 0s - loss: 0.4425 - acc: 0.8641Epoch 00001: val_loss improved from 0.83818 to 0.74855, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 0.4435 - acc: 0.8639 - val_loss: 0.7485 - val_acc: 0.7701\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.2466 - acc: 0.950 - ETA: 4s - loss: 0.2049 - acc: 0.940 - ETA: 4s - loss: 0.1993 - acc: 0.945 - ETA: 4s - loss: 0.1883 - acc: 0.946 - ETA: 3s - loss: 0.1889 - acc: 0.944 - ETA: 3s - loss: 0.1787 - acc: 0.950 - ETA: 3s - loss: 0.1909 - acc: 0.946 - ETA: 3s - loss: 0.2062 - acc: 0.939 - ETA: 3s - loss: 0.2167 - acc: 0.935 - ETA: 3s - loss: 0.2234 - acc: 0.934 - ETA: 3s - loss: 0.2292 - acc: 0.932 - ETA: 3s - loss: 0.2277 - acc: 0.934 - ETA: 3s - loss: 0.2232 - acc: 0.935 - ETA: 3s - loss: 0.2263 - acc: 0.935 - ETA: 3s - loss: 0.2300 - acc: 0.931 - ETA: 3s - loss: 0.2268 - acc: 0.931 - ETA: 3s - loss: 0.2315 - acc: 0.930 - ETA: 3s - loss: 0.2327 - acc: 0.929 - ETA: 3s - loss: 0.2330 - acc: 0.928 - ETA: 3s - loss: 0.2307 - acc: 0.929 - ETA: 2s - loss: 0.2354 - acc: 0.927 - ETA: 2s - loss: 0.2353 - acc: 0.926 - ETA: 2s - loss: 0.2366 - acc: 0.926 - ETA: 2s - loss: 0.2390 - acc: 0.922 - ETA: 2s - loss: 0.2387 - acc: 0.922 - ETA: 2s - loss: 0.2426 - acc: 0.920 - ETA: 2s - loss: 0.2407 - acc: 0.922 - ETA: 2s - loss: 0.2466 - acc: 0.920 - ETA: 2s - loss: 0.2464 - acc: 0.920 - ETA: 2s - loss: 0.2426 - acc: 0.921 - ETA: 2s - loss: 0.2414 - acc: 0.921 - ETA: 2s - loss: 0.2412 - acc: 0.922 - ETA: 2s - loss: 0.2414 - acc: 0.921 - ETA: 2s - loss: 0.2465 - acc: 0.920 - ETA: 2s - loss: 0.2459 - acc: 0.920 - ETA: 2s - loss: 0.2452 - acc: 0.919 - ETA: 2s - loss: 0.2449 - acc: 0.919 - ETA: 1s - loss: 0.2473 - acc: 0.918 - ETA: 1s - loss: 0.2451 - acc: 0.919 - ETA: 1s - loss: 0.2475 - acc: 0.919 - ETA: 1s - loss: 0.2472 - acc: 0.919 - ETA: 1s - loss: 0.2467 - acc: 0.919 - ETA: 1s - loss: 0.2450 - acc: 0.921 - ETA: 1s - loss: 0.2463 - acc: 0.920 - ETA: 1s - loss: 0.2447 - acc: 0.920 - ETA: 1s - loss: 0.2479 - acc: 0.920 - ETA: 1s - loss: 0.2483 - acc: 0.920 - ETA: 1s - loss: 0.2492 - acc: 0.920 - ETA: 1s - loss: 0.2504 - acc: 0.920 - ETA: 1s - loss: 0.2495 - acc: 0.920 - ETA: 1s - loss: 0.2512 - acc: 0.919 - ETA: 1s - loss: 0.2514 - acc: 0.919 - ETA: 1s - loss: 0.2540 - acc: 0.919 - ETA: 1s - loss: 0.2523 - acc: 0.919 - ETA: 1s - loss: 0.2542 - acc: 0.918 - ETA: 0s - loss: 0.2595 - acc: 0.917 - ETA: 0s - loss: 0.2582 - acc: 0.917 - ETA: 0s - loss: 0.2604 - acc: 0.916 - ETA: 0s - loss: 0.2606 - acc: 0.917 - ETA: 0s - loss: 0.2621 - acc: 0.916 - ETA: 0s - loss: 0.2636 - acc: 0.916 - ETA: 0s - loss: 0.2632 - acc: 0.916 - ETA: 0s - loss: 0.2625 - acc: 0.916 - ETA: 0s - loss: 0.2617 - acc: 0.916 - ETA: 0s - loss: 0.2655 - acc: 0.915 - ETA: 0s - loss: 0.2675 - acc: 0.914 - ETA: 0s - loss: 0.2670 - acc: 0.915 - ETA: 0s - loss: 0.2671 - acc: 0.914 - ETA: 0s - loss: 0.2686 - acc: 0.914 - ETA: 0s - loss: 0.2690 - acc: 0.914 - ETA: 0s - loss: 0.2687 - acc: 0.914 - ETA: 0s - loss: 0.2699 - acc: 0.9139Epoch 00002: val_loss improved from 0.74855 to 0.66068, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.2690 - acc: 0.9142 - val_loss: 0.6607 - val_acc: 0.7976\n",
      "Epoch 4/20\n",
      "6580/6680 [============================>.] - ETA: 4s - loss: 0.1637 - acc: 0.900 - ETA: 3s - loss: 0.2387 - acc: 0.925 - ETA: 3s - loss: 0.1835 - acc: 0.945 - ETA: 3s - loss: 0.1493 - acc: 0.956 - ETA: 3s - loss: 0.1524 - acc: 0.960 - ETA: 3s - loss: 0.1506 - acc: 0.958 - ETA: 3s - loss: 0.1391 - acc: 0.958 - ETA: 3s - loss: 0.1324 - acc: 0.960 - ETA: 3s - loss: 0.1330 - acc: 0.957 - ETA: 3s - loss: 0.1364 - acc: 0.955 - ETA: 3s - loss: 0.1383 - acc: 0.956 - ETA: 3s - loss: 0.1394 - acc: 0.955 - ETA: 3s - loss: 0.1372 - acc: 0.957 - ETA: 3s - loss: 0.1375 - acc: 0.956 - ETA: 3s - loss: 0.1406 - acc: 0.956 - ETA: 3s - loss: 0.1434 - acc: 0.955 - ETA: 3s - loss: 0.1411 - acc: 0.956 - ETA: 3s - loss: 0.1392 - acc: 0.956 - ETA: 2s - loss: 0.1458 - acc: 0.953 - ETA: 2s - loss: 0.1540 - acc: 0.951 - ETA: 2s - loss: 0.1545 - acc: 0.950 - ETA: 2s - loss: 0.1582 - acc: 0.948 - ETA: 2s - loss: 0.1556 - acc: 0.949 - ETA: 2s - loss: 0.1607 - acc: 0.947 - ETA: 2s - loss: 0.1634 - acc: 0.945 - ETA: 2s - loss: 0.1649 - acc: 0.944 - ETA: 2s - loss: 0.1662 - acc: 0.944 - ETA: 2s - loss: 0.1659 - acc: 0.945 - ETA: 2s - loss: 0.1652 - acc: 0.945 - ETA: 2s - loss: 0.1657 - acc: 0.945 - ETA: 2s - loss: 0.1645 - acc: 0.945 - ETA: 2s - loss: 0.1629 - acc: 0.947 - ETA: 2s - loss: 0.1614 - acc: 0.947 - ETA: 2s - loss: 0.1588 - acc: 0.948 - ETA: 2s - loss: 0.1580 - acc: 0.948 - ETA: 1s - loss: 0.1598 - acc: 0.948 - ETA: 1s - loss: 0.1615 - acc: 0.947 - ETA: 1s - loss: 0.1607 - acc: 0.947 - ETA: 1s - loss: 0.1588 - acc: 0.948 - ETA: 1s - loss: 0.1608 - acc: 0.947 - ETA: 1s - loss: 0.1610 - acc: 0.947 - ETA: 1s - loss: 0.1616 - acc: 0.947 - ETA: 1s - loss: 0.1618 - acc: 0.947 - ETA: 1s - loss: 0.1618 - acc: 0.947 - ETA: 1s - loss: 0.1637 - acc: 0.947 - ETA: 1s - loss: 0.1645 - acc: 0.947 - ETA: 1s - loss: 0.1635 - acc: 0.947 - ETA: 1s - loss: 0.1619 - acc: 0.947 - ETA: 1s - loss: 0.1671 - acc: 0.946 - ETA: 1s - loss: 0.1679 - acc: 0.947 - ETA: 1s - loss: 0.1696 - acc: 0.946 - ETA: 1s - loss: 0.1723 - acc: 0.945 - ETA: 1s - loss: 0.1717 - acc: 0.946 - ETA: 0s - loss: 0.1726 - acc: 0.945 - ETA: 0s - loss: 0.1719 - acc: 0.945 - ETA: 0s - loss: 0.1718 - acc: 0.945 - ETA: 0s - loss: 0.1757 - acc: 0.944 - ETA: 0s - loss: 0.1751 - acc: 0.945 - ETA: 0s - loss: 0.1746 - acc: 0.945 - ETA: 0s - loss: 0.1741 - acc: 0.945 - ETA: 0s - loss: 0.1748 - acc: 0.945 - ETA: 0s - loss: 0.1745 - acc: 0.945 - ETA: 0s - loss: 0.1758 - acc: 0.944 - ETA: 0s - loss: 0.1785 - acc: 0.943 - ETA: 0s - loss: 0.1785 - acc: 0.942 - ETA: 0s - loss: 0.1802 - acc: 0.942 - ETA: 0s - loss: 0.1810 - acc: 0.941 - ETA: 0s - loss: 0.1810 - acc: 0.941 - ETA: 0s - loss: 0.1826 - acc: 0.941 - ETA: 0s - loss: 0.1820 - acc: 0.941 - ETA: 0s - loss: 0.1814 - acc: 0.9413Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1821 - acc: 0.9410 - val_loss: 0.7072 - val_acc: 0.7952\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 5s - loss: 0.0548 - acc: 1.000 - ETA: 4s - loss: 0.0791 - acc: 0.983 - ETA: 4s - loss: 0.0861 - acc: 0.975 - ETA: 3s - loss: 0.1004 - acc: 0.970 - ETA: 3s - loss: 0.1015 - acc: 0.971 - ETA: 3s - loss: 0.0937 - acc: 0.972 - ETA: 3s - loss: 0.0956 - acc: 0.969 - ETA: 3s - loss: 0.0955 - acc: 0.969 - ETA: 3s - loss: 0.0956 - acc: 0.968 - ETA: 3s - loss: 0.0917 - acc: 0.970 - ETA: 3s - loss: 0.0983 - acc: 0.967 - ETA: 3s - loss: 0.1020 - acc: 0.964 - ETA: 3s - loss: 0.1015 - acc: 0.964 - ETA: 3s - loss: 0.1032 - acc: 0.963 - ETA: 3s - loss: 0.1021 - acc: 0.965 - ETA: 3s - loss: 0.1002 - acc: 0.965 - ETA: 3s - loss: 0.1051 - acc: 0.964 - ETA: 3s - loss: 0.1057 - acc: 0.964 - ETA: 3s - loss: 0.1040 - acc: 0.965 - ETA: 3s - loss: 0.1034 - acc: 0.965 - ETA: 3s - loss: 0.1058 - acc: 0.965 - ETA: 3s - loss: 0.1055 - acc: 0.964 - ETA: 2s - loss: 0.1065 - acc: 0.964 - ETA: 2s - loss: 0.1103 - acc: 0.965 - ETA: 2s - loss: 0.1140 - acc: 0.963 - ETA: 2s - loss: 0.1154 - acc: 0.962 - ETA: 2s - loss: 0.1177 - acc: 0.960 - ETA: 2s - loss: 0.1231 - acc: 0.959 - ETA: 2s - loss: 0.1217 - acc: 0.959 - ETA: 2s - loss: 0.1205 - acc: 0.959 - ETA: 2s - loss: 0.1196 - acc: 0.959 - ETA: 2s - loss: 0.1183 - acc: 0.960 - ETA: 2s - loss: 0.1211 - acc: 0.960 - ETA: 2s - loss: 0.1207 - acc: 0.959 - ETA: 2s - loss: 0.1192 - acc: 0.960 - ETA: 2s - loss: 0.1178 - acc: 0.960 - ETA: 2s - loss: 0.1176 - acc: 0.960 - ETA: 2s - loss: 0.1166 - acc: 0.960 - ETA: 2s - loss: 0.1151 - acc: 0.961 - ETA: 2s - loss: 0.1181 - acc: 0.961 - ETA: 2s - loss: 0.1183 - acc: 0.961 - ETA: 2s - loss: 0.1185 - acc: 0.961 - ETA: 2s - loss: 0.1171 - acc: 0.962 - ETA: 1s - loss: 0.1178 - acc: 0.961 - ETA: 1s - loss: 0.1162 - acc: 0.962 - ETA: 1s - loss: 0.1155 - acc: 0.962 - ETA: 1s - loss: 0.1153 - acc: 0.962 - ETA: 1s - loss: 0.1152 - acc: 0.962 - ETA: 1s - loss: 0.1143 - acc: 0.963 - ETA: 1s - loss: 0.1143 - acc: 0.963 - ETA: 1s - loss: 0.1144 - acc: 0.962 - ETA: 1s - loss: 0.1133 - acc: 0.963 - ETA: 1s - loss: 0.1153 - acc: 0.962 - ETA: 1s - loss: 0.1141 - acc: 0.963 - ETA: 1s - loss: 0.1130 - acc: 0.963 - ETA: 1s - loss: 0.1125 - acc: 0.963 - ETA: 1s - loss: 0.1124 - acc: 0.963 - ETA: 1s - loss: 0.1128 - acc: 0.963 - ETA: 1s - loss: 0.1154 - acc: 0.963 - ETA: 1s - loss: 0.1162 - acc: 0.962 - ETA: 1s - loss: 0.1168 - acc: 0.962 - ETA: 1s - loss: 0.1165 - acc: 0.962 - ETA: 1s - loss: 0.1192 - acc: 0.962 - ETA: 1s - loss: 0.1192 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1204 - acc: 0.961 - ETA: 0s - loss: 0.1201 - acc: 0.962 - ETA: 0s - loss: 0.1199 - acc: 0.962 - ETA: 0s - loss: 0.1226 - acc: 0.961 - ETA: 0s - loss: 0.1221 - acc: 0.961 - ETA: 0s - loss: 0.1239 - acc: 0.960 - ETA: 0s - loss: 0.1239 - acc: 0.960 - ETA: 0s - loss: 0.1242 - acc: 0.960 - ETA: 0s - loss: 0.1252 - acc: 0.960 - ETA: 0s - loss: 0.1245 - acc: 0.960 - ETA: 0s - loss: 0.1238 - acc: 0.961 - ETA: 0s - loss: 0.1254 - acc: 0.960 - ETA: 0s - loss: 0.1259 - acc: 0.960 - ETA: 0s - loss: 0.1251 - acc: 0.961 - ETA: 0s - loss: 0.1247 - acc: 0.9614Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1240 - acc: 0.9615 - val_loss: 0.6658 - val_acc: 0.8192\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0396 - acc: 1.000 - ETA: 5s - loss: 0.0387 - acc: 0.990 - ETA: 4s - loss: 0.0483 - acc: 0.985 - ETA: 4s - loss: 0.0687 - acc: 0.983 - ETA: 3s - loss: 0.0688 - acc: 0.977 - ETA: 3s - loss: 0.0688 - acc: 0.978 - ETA: 3s - loss: 0.0684 - acc: 0.980 - ETA: 3s - loss: 0.0639 - acc: 0.982 - ETA: 3s - loss: 0.0704 - acc: 0.979 - ETA: 3s - loss: 0.0734 - acc: 0.979 - ETA: 3s - loss: 0.0776 - acc: 0.979 - ETA: 3s - loss: 0.0810 - acc: 0.978 - ETA: 3s - loss: 0.0864 - acc: 0.976 - ETA: 3s - loss: 0.0867 - acc: 0.975 - ETA: 3s - loss: 0.0869 - acc: 0.974 - ETA: 3s - loss: 0.0863 - acc: 0.974 - ETA: 3s - loss: 0.0863 - acc: 0.974 - ETA: 2s - loss: 0.0835 - acc: 0.975 - ETA: 2s - loss: 0.0830 - acc: 0.975 - ETA: 2s - loss: 0.0815 - acc: 0.975 - ETA: 2s - loss: 0.0804 - acc: 0.975 - ETA: 2s - loss: 0.0788 - acc: 0.976 - ETA: 2s - loss: 0.0796 - acc: 0.975 - ETA: 2s - loss: 0.0800 - acc: 0.975 - ETA: 2s - loss: 0.0794 - acc: 0.976 - ETA: 2s - loss: 0.0785 - acc: 0.976 - ETA: 2s - loss: 0.0831 - acc: 0.975 - ETA: 2s - loss: 0.0819 - acc: 0.976 - ETA: 2s - loss: 0.0820 - acc: 0.976 - ETA: 2s - loss: 0.0841 - acc: 0.976 - ETA: 2s - loss: 0.0829 - acc: 0.976 - ETA: 2s - loss: 0.0823 - acc: 0.976 - ETA: 2s - loss: 0.0817 - acc: 0.976 - ETA: 2s - loss: 0.0809 - acc: 0.977 - ETA: 2s - loss: 0.0808 - acc: 0.976 - ETA: 2s - loss: 0.0813 - acc: 0.976 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.978 - ETA: 1s - loss: 0.0794 - acc: 0.977 - ETA: 1s - loss: 0.0781 - acc: 0.978 - ETA: 1s - loss: 0.0800 - acc: 0.978 - ETA: 1s - loss: 0.0795 - acc: 0.978 - ETA: 1s - loss: 0.0810 - acc: 0.977 - ETA: 1s - loss: 0.0803 - acc: 0.977 - ETA: 1s - loss: 0.0816 - acc: 0.977 - ETA: 1s - loss: 0.0808 - acc: 0.977 - ETA: 1s - loss: 0.0801 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.977 - ETA: 1s - loss: 0.0794 - acc: 0.977 - ETA: 1s - loss: 0.0789 - acc: 0.977 - ETA: 1s - loss: 0.0792 - acc: 0.976 - ETA: 1s - loss: 0.0806 - acc: 0.976 - ETA: 1s - loss: 0.0811 - acc: 0.976 - ETA: 1s - loss: 0.0803 - acc: 0.976 - ETA: 1s - loss: 0.0822 - acc: 0.976 - ETA: 1s - loss: 0.0832 - acc: 0.975 - ETA: 0s - loss: 0.0838 - acc: 0.975 - ETA: 0s - loss: 0.0842 - acc: 0.975 - ETA: 0s - loss: 0.0872 - acc: 0.975 - ETA: 0s - loss: 0.0868 - acc: 0.975 - ETA: 0s - loss: 0.0874 - acc: 0.974 - ETA: 0s - loss: 0.0884 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0887 - acc: 0.974 - ETA: 0s - loss: 0.0895 - acc: 0.973 - ETA: 0s - loss: 0.0896 - acc: 0.973 - ETA: 0s - loss: 0.0893 - acc: 0.973 - ETA: 0s - loss: 0.0900 - acc: 0.973 - ETA: 0s - loss: 0.0892 - acc: 0.973 - ETA: 0s - loss: 0.0890 - acc: 0.973 - ETA: 0s - loss: 0.0896 - acc: 0.973 - ETA: 0s - loss: 0.0898 - acc: 0.973 - ETA: 0s - loss: 0.0892 - acc: 0.9734Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0897 - acc: 0.9734 - val_loss: 0.6728 - val_acc: 0.8156\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0064 - acc: 1.000 - ETA: 4s - loss: 0.0284 - acc: 0.991 - ETA: 4s - loss: 0.0268 - acc: 0.995 - ETA: 3s - loss: 0.0402 - acc: 0.986 - ETA: 3s - loss: 0.0437 - acc: 0.982 - ETA: 3s - loss: 0.0422 - acc: 0.984 - ETA: 3s - loss: 0.0379 - acc: 0.986 - ETA: 3s - loss: 0.0421 - acc: 0.987 - ETA: 3s - loss: 0.0393 - acc: 0.988 - ETA: 3s - loss: 0.0376 - acc: 0.990 - ETA: 3s - loss: 0.0373 - acc: 0.990 - ETA: 3s - loss: 0.0358 - acc: 0.990 - ETA: 3s - loss: 0.0413 - acc: 0.989 - ETA: 3s - loss: 0.0412 - acc: 0.990 - ETA: 3s - loss: 0.0438 - acc: 0.989 - ETA: 3s - loss: 0.0451 - acc: 0.989 - ETA: 3s - loss: 0.0453 - acc: 0.989 - ETA: 3s - loss: 0.0456 - acc: 0.988 - ETA: 3s - loss: 0.0451 - acc: 0.988 - ETA: 2s - loss: 0.0476 - acc: 0.988 - ETA: 2s - loss: 0.0469 - acc: 0.988 - ETA: 2s - loss: 0.0460 - acc: 0.988 - ETA: 2s - loss: 0.0475 - acc: 0.987 - ETA: 2s - loss: 0.0503 - acc: 0.986 - ETA: 2s - loss: 0.0507 - acc: 0.986 - ETA: 2s - loss: 0.0516 - acc: 0.985 - ETA: 2s - loss: 0.0512 - acc: 0.985 - ETA: 2s - loss: 0.0502 - acc: 0.986 - ETA: 2s - loss: 0.0506 - acc: 0.986 - ETA: 2s - loss: 0.0502 - acc: 0.986 - ETA: 2s - loss: 0.0537 - acc: 0.985 - ETA: 2s - loss: 0.0557 - acc: 0.985 - ETA: 2s - loss: 0.0569 - acc: 0.985 - ETA: 2s - loss: 0.0571 - acc: 0.984 - ETA: 2s - loss: 0.0568 - acc: 0.984 - ETA: 2s - loss: 0.0571 - acc: 0.984 - ETA: 2s - loss: 0.0564 - acc: 0.984 - ETA: 2s - loss: 0.0558 - acc: 0.985 - ETA: 1s - loss: 0.0557 - acc: 0.985 - ETA: 1s - loss: 0.0549 - acc: 0.985 - ETA: 1s - loss: 0.0546 - acc: 0.985 - ETA: 1s - loss: 0.0560 - acc: 0.984 - ETA: 1s - loss: 0.0550 - acc: 0.984 - ETA: 1s - loss: 0.0555 - acc: 0.984 - ETA: 1s - loss: 0.0572 - acc: 0.984 - ETA: 1s - loss: 0.0576 - acc: 0.984 - ETA: 1s - loss: 0.0573 - acc: 0.984 - ETA: 1s - loss: 0.0574 - acc: 0.983 - ETA: 1s - loss: 0.0581 - acc: 0.983 - ETA: 1s - loss: 0.0586 - acc: 0.983 - ETA: 1s - loss: 0.0608 - acc: 0.983 - ETA: 1s - loss: 0.0627 - acc: 0.982 - ETA: 1s - loss: 0.0637 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0635 - acc: 0.982 - ETA: 1s - loss: 0.0631 - acc: 0.982 - ETA: 0s - loss: 0.0626 - acc: 0.982 - ETA: 0s - loss: 0.0626 - acc: 0.982 - ETA: 0s - loss: 0.0628 - acc: 0.982 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0636 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.981 - ETA: 0s - loss: 0.0624 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0627 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.9818Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0644 - acc: 0.9817 - val_loss: 0.7585 - val_acc: 0.8132\n",
      "Epoch 8/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0064 - acc: 1.000 - ETA: 3s - loss: 0.0121 - acc: 1.000 - ETA: 3s - loss: 0.0127 - acc: 1.000 - ETA: 3s - loss: 0.0150 - acc: 0.996 - ETA: 3s - loss: 0.0173 - acc: 0.997 - ETA: 3s - loss: 0.0207 - acc: 0.994 - ETA: 3s - loss: 0.0218 - acc: 0.995 - ETA: 3s - loss: 0.0218 - acc: 0.995 - ETA: 3s - loss: 0.0207 - acc: 0.996 - ETA: 3s - loss: 0.0206 - acc: 0.996 - ETA: 3s - loss: 0.0238 - acc: 0.995 - ETA: 3s - loss: 0.0263 - acc: 0.994 - ETA: 3s - loss: 0.0266 - acc: 0.995 - ETA: 3s - loss: 0.0272 - acc: 0.994 - ETA: 3s - loss: 0.0300 - acc: 0.993 - ETA: 2s - loss: 0.0333 - acc: 0.992 - ETA: 2s - loss: 0.0337 - acc: 0.992 - ETA: 2s - loss: 0.0330 - acc: 0.992 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0350 - acc: 0.991 - ETA: 2s - loss: 0.0357 - acc: 0.990 - ETA: 2s - loss: 0.0351 - acc: 0.991 - ETA: 2s - loss: 0.0368 - acc: 0.989 - ETA: 2s - loss: 0.0361 - acc: 0.990 - ETA: 2s - loss: 0.0365 - acc: 0.990 - ETA: 2s - loss: 0.0358 - acc: 0.990 - ETA: 2s - loss: 0.0358 - acc: 0.990 - ETA: 2s - loss: 0.0354 - acc: 0.990 - ETA: 2s - loss: 0.0355 - acc: 0.990 - ETA: 2s - loss: 0.0372 - acc: 0.989 - ETA: 2s - loss: 0.0362 - acc: 0.989 - ETA: 2s - loss: 0.0356 - acc: 0.989 - ETA: 2s - loss: 0.0359 - acc: 0.989 - ETA: 2s - loss: 0.0353 - acc: 0.990 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0344 - acc: 0.990 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0365 - acc: 0.990 - ETA: 1s - loss: 0.0369 - acc: 0.989 - ETA: 1s - loss: 0.0391 - acc: 0.989 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0394 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.988 - ETA: 1s - loss: 0.0415 - acc: 0.988 - ETA: 1s - loss: 0.0409 - acc: 0.988 - ETA: 1s - loss: 0.0411 - acc: 0.988 - ETA: 1s - loss: 0.0420 - acc: 0.988 - ETA: 1s - loss: 0.0423 - acc: 0.988 - ETA: 1s - loss: 0.0422 - acc: 0.988 - ETA: 1s - loss: 0.0417 - acc: 0.988 - ETA: 1s - loss: 0.0417 - acc: 0.988 - ETA: 0s - loss: 0.0426 - acc: 0.988 - ETA: 0s - loss: 0.0422 - acc: 0.988 - ETA: 0s - loss: 0.0418 - acc: 0.988 - ETA: 0s - loss: 0.0418 - acc: 0.988 - ETA: 0s - loss: 0.0416 - acc: 0.988 - ETA: 0s - loss: 0.0415 - acc: 0.988 - ETA: 0s - loss: 0.0416 - acc: 0.988 - ETA: 0s - loss: 0.0434 - acc: 0.988 - ETA: 0s - loss: 0.0442 - acc: 0.987 - ETA: 0s - loss: 0.0438 - acc: 0.987 - ETA: 0s - loss: 0.0446 - acc: 0.987 - ETA: 0s - loss: 0.0447 - acc: 0.987 - ETA: 0s - loss: 0.0443 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0471 - acc: 0.986 - ETA: 0s - loss: 0.0476 - acc: 0.986 - ETA: 0s - loss: 0.0473 - acc: 0.986 - ETA: 0s - loss: 0.0470 - acc: 0.986 - ETA: 0s - loss: 0.0470 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0469 - acc: 0.9868 - val_loss: 0.7041 - val_acc: 0.8072\n",
      "Epoch 9/20\n",
      "6660/6680 [============================>.] - ETA: 3s - loss: 0.0036 - acc: 1.000 - ETA: 3s - loss: 0.0173 - acc: 1.000 - ETA: 3s - loss: 0.0217 - acc: 0.995 - ETA: 3s - loss: 0.0253 - acc: 0.996 - ETA: 3s - loss: 0.0226 - acc: 0.997 - ETA: 3s - loss: 0.0347 - acc: 0.995 - ETA: 3s - loss: 0.0313 - acc: 0.996 - ETA: 3s - loss: 0.0331 - acc: 0.994 - ETA: 3s - loss: 0.0308 - acc: 0.994 - ETA: 3s - loss: 0.0298 - acc: 0.994 - ETA: 3s - loss: 0.0315 - acc: 0.993 - ETA: 3s - loss: 0.0299 - acc: 0.994 - ETA: 3s - loss: 0.0312 - acc: 0.994 - ETA: 3s - loss: 0.0303 - acc: 0.994 - ETA: 3s - loss: 0.0298 - acc: 0.994 - ETA: 3s - loss: 0.0317 - acc: 0.993 - ETA: 2s - loss: 0.0313 - acc: 0.994 - ETA: 2s - loss: 0.0304 - acc: 0.994 - ETA: 2s - loss: 0.0293 - acc: 0.994 - ETA: 2s - loss: 0.0283 - acc: 0.995 - ETA: 2s - loss: 0.0285 - acc: 0.994 - ETA: 2s - loss: 0.0282 - acc: 0.995 - ETA: 2s - loss: 0.0278 - acc: 0.994 - ETA: 2s - loss: 0.0273 - acc: 0.995 - ETA: 2s - loss: 0.0274 - acc: 0.994 - ETA: 2s - loss: 0.0273 - acc: 0.994 - ETA: 2s - loss: 0.0284 - acc: 0.994 - ETA: 2s - loss: 0.0280 - acc: 0.994 - ETA: 2s - loss: 0.0276 - acc: 0.994 - ETA: 2s - loss: 0.0269 - acc: 0.994 - ETA: 2s - loss: 0.0272 - acc: 0.994 - ETA: 2s - loss: 0.0281 - acc: 0.993 - ETA: 2s - loss: 0.0286 - acc: 0.993 - ETA: 2s - loss: 0.0285 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.992 - ETA: 1s - loss: 0.0282 - acc: 0.993 - ETA: 1s - loss: 0.0287 - acc: 0.992 - ETA: 1s - loss: 0.0286 - acc: 0.993 - ETA: 1s - loss: 0.0283 - acc: 0.993 - ETA: 1s - loss: 0.0288 - acc: 0.992 - ETA: 1s - loss: 0.0285 - acc: 0.993 - ETA: 1s - loss: 0.0282 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0280 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0284 - acc: 0.993 - ETA: 1s - loss: 0.0280 - acc: 0.993 - ETA: 1s - loss: 0.0277 - acc: 0.993 - ETA: 1s - loss: 0.0278 - acc: 0.993 - ETA: 1s - loss: 0.0279 - acc: 0.993 - ETA: 1s - loss: 0.0279 - acc: 0.993 - ETA: 0s - loss: 0.0285 - acc: 0.993 - ETA: 0s - loss: 0.0283 - acc: 0.993 - ETA: 0s - loss: 0.0288 - acc: 0.992 - ETA: 0s - loss: 0.0294 - acc: 0.992 - ETA: 0s - loss: 0.0305 - acc: 0.992 - ETA: 0s - loss: 0.0305 - acc: 0.992 - ETA: 0s - loss: 0.0304 - acc: 0.992 - ETA: 0s - loss: 0.0310 - acc: 0.992 - ETA: 0s - loss: 0.0315 - acc: 0.992 - ETA: 0s - loss: 0.0321 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0322 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0326 - acc: 0.991 - ETA: 0s - loss: 0.0327 - acc: 0.991 - ETA: 0s - loss: 0.0330 - acc: 0.991 - ETA: 0s - loss: 0.0329 - acc: 0.991 - ETA: 0s - loss: 0.0336 - acc: 0.9917Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7821 - val_acc: 0.8096\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0033 - acc: 1.000 - ETA: 3s - loss: 0.0183 - acc: 0.991 - ETA: 3s - loss: 0.0136 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.993 - ETA: 3s - loss: 0.0179 - acc: 0.992 - ETA: 3s - loss: 0.0174 - acc: 0.994 - ETA: 3s - loss: 0.0172 - acc: 0.995 - ETA: 3s - loss: 0.0162 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 3s - loss: 0.0167 - acc: 0.995 - ETA: 3s - loss: 0.0174 - acc: 0.995 - ETA: 3s - loss: 0.0170 - acc: 0.994 - ETA: 3s - loss: 0.0160 - acc: 0.995 - ETA: 3s - loss: 0.0160 - acc: 0.994 - ETA: 3s - loss: 0.0172 - acc: 0.994 - ETA: 2s - loss: 0.0175 - acc: 0.994 - ETA: 2s - loss: 0.0179 - acc: 0.994 - ETA: 2s - loss: 0.0174 - acc: 0.994 - ETA: 2s - loss: 0.0177 - acc: 0.994 - ETA: 2s - loss: 0.0171 - acc: 0.994 - ETA: 2s - loss: 0.0183 - acc: 0.994 - ETA: 2s - loss: 0.0216 - acc: 0.994 - ETA: 2s - loss: 0.0213 - acc: 0.994 - ETA: 2s - loss: 0.0210 - acc: 0.994 - ETA: 2s - loss: 0.0243 - acc: 0.993 - ETA: 2s - loss: 0.0238 - acc: 0.994 - ETA: 2s - loss: 0.0232 - acc: 0.994 - ETA: 2s - loss: 0.0237 - acc: 0.994 - ETA: 2s - loss: 0.0239 - acc: 0.993 - ETA: 2s - loss: 0.0234 - acc: 0.994 - ETA: 2s - loss: 0.0233 - acc: 0.993 - ETA: 2s - loss: 0.0233 - acc: 0.993 - ETA: 2s - loss: 0.0231 - acc: 0.993 - ETA: 1s - loss: 0.0229 - acc: 0.994 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0246 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.994 - ETA: 1s - loss: 0.0241 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0240 - acc: 0.994 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0250 - acc: 0.993 - ETA: 1s - loss: 0.0247 - acc: 0.993 - ETA: 1s - loss: 0.0246 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0241 - acc: 0.993 - ETA: 1s - loss: 0.0238 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 1s - loss: 0.0244 - acc: 0.993 - ETA: 1s - loss: 0.0242 - acc: 0.993 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0245 - acc: 0.993 - ETA: 1s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0247 - acc: 0.993 - ETA: 0s - loss: 0.0243 - acc: 0.993 - ETA: 0s - loss: 0.0244 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0257 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0252 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0253 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.9929Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0259 - acc: 0.9927 - val_loss: 0.7659 - val_acc: 0.8192\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0168 - acc: 1.000 - ETA: 3s - loss: 0.0128 - acc: 0.991 - ETA: 3s - loss: 0.0143 - acc: 0.995 - ETA: 3s - loss: 0.0107 - acc: 0.996 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0117 - acc: 0.996 - ETA: 3s - loss: 0.0111 - acc: 0.996 - ETA: 3s - loss: 0.0100 - acc: 0.997 - ETA: 3s - loss: 0.0121 - acc: 0.996 - ETA: 3s - loss: 0.0118 - acc: 0.996 - ETA: 3s - loss: 0.0115 - acc: 0.997 - ETA: 3s - loss: 0.0132 - acc: 0.995 - ETA: 3s - loss: 0.0190 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.994 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.994 - ETA: 2s - loss: 0.0176 - acc: 0.994 - ETA: 2s - loss: 0.0193 - acc: 0.994 - ETA: 2s - loss: 0.0194 - acc: 0.994 - ETA: 2s - loss: 0.0186 - acc: 0.994 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0173 - acc: 0.995 - ETA: 2s - loss: 0.0191 - acc: 0.994 - ETA: 2s - loss: 0.0189 - acc: 0.994 - ETA: 2s - loss: 0.0185 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0177 - acc: 0.995 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0195 - acc: 0.994 - ETA: 2s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.994 - ETA: 1s - loss: 0.0192 - acc: 0.994 - ETA: 1s - loss: 0.0191 - acc: 0.994 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.994 - ETA: 1s - loss: 0.0199 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.994 - ETA: 1s - loss: 0.0206 - acc: 0.994 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0216 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0205 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0204 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0219 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0220 - acc: 0.994 - ETA: 0s - loss: 0.0223 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.9938Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0222 - acc: 0.9937 - val_loss: 0.7574 - val_acc: 0.8251\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0038 - acc: 1.000 - ETA: 3s - loss: 0.0093 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0060 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0077 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.999 - ETA: 3s - loss: 0.0066 - acc: 0.999 - ETA: 3s - loss: 0.0064 - acc: 0.999 - ETA: 3s - loss: 0.0070 - acc: 0.999 - ETA: 3s - loss: 0.0071 - acc: 0.999 - ETA: 3s - loss: 0.0072 - acc: 0.999 - ETA: 3s - loss: 0.0076 - acc: 0.999 - ETA: 3s - loss: 0.0078 - acc: 0.999 - ETA: 3s - loss: 0.0078 - acc: 0.999 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.999 - ETA: 2s - loss: 0.0100 - acc: 0.999 - ETA: 2s - loss: 0.0099 - acc: 0.999 - ETA: 2s - loss: 0.0096 - acc: 0.999 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 2s - loss: 0.0130 - acc: 0.997 - ETA: 2s - loss: 0.0129 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 2s - loss: 0.0124 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0122 - acc: 0.997 - ETA: 2s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0142 - acc: 0.997 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0146 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0144 - acc: 0.996 - ETA: 1s - loss: 0.0142 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0145 - acc: 0.996 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 1s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0140 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0155 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.996 - ETA: 0s - loss: 0.0160 - acc: 0.996 - ETA: 0s - loss: 0.0165 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.9956Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0175 - acc: 0.9955 - val_loss: 0.7644 - val_acc: 0.8240\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0016 - acc: 1.000 - ETA: 3s - loss: 0.0094 - acc: 0.995 - ETA: 3s - loss: 0.0085 - acc: 0.996 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0102 - acc: 0.996 - ETA: 3s - loss: 0.0093 - acc: 0.996 - ETA: 3s - loss: 0.0082 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0080 - acc: 0.997 - ETA: 3s - loss: 0.0076 - acc: 0.997 - ETA: 3s - loss: 0.0078 - acc: 0.997 - ETA: 3s - loss: 0.0073 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 2s - loss: 0.0081 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.996 - ETA: 2s - loss: 0.0090 - acc: 0.996 - ETA: 2s - loss: 0.0088 - acc: 0.996 - ETA: 2s - loss: 0.0087 - acc: 0.996 - ETA: 2s - loss: 0.0103 - acc: 0.995 - ETA: 2s - loss: 0.0101 - acc: 0.995 - ETA: 2s - loss: 0.0102 - acc: 0.995 - ETA: 2s - loss: 0.0105 - acc: 0.995 - ETA: 2s - loss: 0.0104 - acc: 0.995 - ETA: 2s - loss: 0.0103 - acc: 0.995 - ETA: 2s - loss: 0.0101 - acc: 0.995 - ETA: 2s - loss: 0.0114 - acc: 0.995 - ETA: 2s - loss: 0.0113 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.995 - ETA: 1s - loss: 0.0108 - acc: 0.995 - ETA: 1s - loss: 0.0107 - acc: 0.996 - ETA: 1s - loss: 0.0105 - acc: 0.996 - ETA: 1s - loss: 0.0103 - acc: 0.996 - ETA: 1s - loss: 0.0118 - acc: 0.995 - ETA: 1s - loss: 0.0132 - acc: 0.995 - ETA: 1s - loss: 0.0131 - acc: 0.995 - ETA: 1s - loss: 0.0151 - acc: 0.995 - ETA: 1s - loss: 0.0148 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0144 - acc: 0.995 - ETA: 1s - loss: 0.0141 - acc: 0.995 - ETA: 1s - loss: 0.0140 - acc: 0.995 - ETA: 1s - loss: 0.0145 - acc: 0.995 - ETA: 1s - loss: 0.0146 - acc: 0.995 - ETA: 1s - loss: 0.0149 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.995 - ETA: 1s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.995 - ETA: 0s - loss: 0.0152 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0149 - acc: 0.995 - ETA: 0s - loss: 0.0150 - acc: 0.995 - ETA: 0s - loss: 0.0148 - acc: 0.995 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0145 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.995 - ETA: 0s - loss: 0.0142 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.995 - ETA: 0s - loss: 0.0151 - acc: 0.9958Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0152 - acc: 0.9958 - val_loss: 0.8223 - val_acc: 0.8156\n",
      "Epoch 14/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0290 - acc: 0.991 - ETA: 3s - loss: 0.0162 - acc: 0.995 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0105 - acc: 0.997 - ETA: 3s - loss: 0.0113 - acc: 0.998 - ETA: 3s - loss: 0.0106 - acc: 0.998 - ETA: 3s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0128 - acc: 0.997 - ETA: 3s - loss: 0.0123 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0114 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 3s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0103 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0093 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 2s - loss: 0.0102 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.996 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0123 - acc: 0.997 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0118 - acc: 0.997 - ETA: 2s - loss: 0.0120 - acc: 0.997 - ETA: 2s - loss: 0.0119 - acc: 0.997 - ETA: 1s - loss: 0.0117 - acc: 0.997 - ETA: 1s - loss: 0.0116 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0105 - acc: 0.997 - ETA: 1s - loss: 0.0104 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0102 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0110 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.9973Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0111 - acc: 0.9973 - val_loss: 0.8462 - val_acc: 0.8144\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6580/6680 [============================>.] - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0049 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 0.996 - ETA: 3s - loss: 0.0052 - acc: 0.997 - ETA: 3s - loss: 0.0044 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0036 - acc: 0.998 - ETA: 3s - loss: 0.0034 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 3s - loss: 0.0034 - acc: 0.999 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0080 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0080 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0077 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0076 - acc: 0.998 - ETA: 0s - loss: 0.0075 - acc: 0.998 - ETA: 0s - loss: 0.0074 - acc: 0.998 - ETA: 0s - loss: 0.0079 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.998 - ETA: 0s - loss: 0.0091 - acc: 0.9982Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0102 - acc: 0.9981 - val_loss: 0.8691 - val_acc: 0.8096\n",
      "Epoch 16/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.1485 - acc: 0.950 - ETA: 4s - loss: 0.0405 - acc: 0.990 - ETA: 4s - loss: 0.0227 - acc: 0.994 - ETA: 4s - loss: 0.0160 - acc: 0.996 - ETA: 4s - loss: 0.0128 - acc: 0.997 - ETA: 4s - loss: 0.0105 - acc: 0.997 - ETA: 4s - loss: 0.0089 - acc: 0.998 - ETA: 3s - loss: 0.0081 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 3s - loss: 0.0065 - acc: 0.998 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0070 - acc: 0.998 - ETA: 3s - loss: 0.0072 - acc: 0.997 - ETA: 3s - loss: 0.0068 - acc: 0.997 - ETA: 3s - loss: 0.0065 - acc: 0.997 - ETA: 3s - loss: 0.0062 - acc: 0.997 - ETA: 3s - loss: 0.0058 - acc: 0.997 - ETA: 3s - loss: 0.0056 - acc: 0.998 - ETA: 3s - loss: 0.0055 - acc: 0.998 - ETA: 3s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0053 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0046 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.9983Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0069 - acc: 0.9984 - val_loss: 0.9158 - val_acc: 0.8180\n",
      "Epoch 17/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0150 - acc: 1.000 - ETA: 3s - loss: 0.0374 - acc: 0.991 - ETA: 3s - loss: 0.0205 - acc: 0.995 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0122 - acc: 0.997 - ETA: 3s - loss: 0.0100 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.998 - ETA: 3s - loss: 0.0064 - acc: 0.998 - ETA: 3s - loss: 0.0060 - acc: 0.999 - ETA: 3s - loss: 0.0058 - acc: 0.999 - ETA: 3s - loss: 0.0087 - acc: 0.998 - ETA: 3s - loss: 0.0082 - acc: 0.998 - ETA: 3s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0068 - acc: 0.998 - ETA: 3s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0058 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0059 - acc: 0.998 - ETA: 2s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.997 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.9977Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 0.0077 - acc: 0.9978 - val_loss: 0.9053 - val_acc: 0.8156\n",
      "Epoch 18/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 1.3594e-04 - acc: 1.000 - ETA: 4s - loss: 5.2051e-04 - acc: 1.000 - ETA: 4s - loss: 0.0125 - acc: 0.9944    - ETA: 4s - loss: 0.0089 - acc: 0.996 - ETA: 4s - loss: 0.0068 - acc: 0.997 - ETA: 4s - loss: 0.0179 - acc: 0.995 - ETA: 3s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0115 - acc: 0.997 - ETA: 3s - loss: 0.0104 - acc: 0.997 - ETA: 3s - loss: 0.0115 - acc: 0.996 - ETA: 3s - loss: 0.0103 - acc: 0.996 - ETA: 3s - loss: 0.0097 - acc: 0.997 - ETA: 3s - loss: 0.0090 - acc: 0.997 - ETA: 3s - loss: 0.0088 - acc: 0.997 - ETA: 3s - loss: 0.0083 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0075 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.998 - ETA: 3s - loss: 0.0067 - acc: 0.998 - ETA: 3s - loss: 0.0073 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0068 - acc: 0.997 - ETA: 3s - loss: 0.0091 - acc: 0.997 - ETA: 3s - loss: 0.0088 - acc: 0.997 - ETA: 2s - loss: 0.0085 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.998 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 2s - loss: 0.0080 - acc: 0.997 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0095 - acc: 0.997 - ETA: 2s - loss: 0.0093 - acc: 0.997 - ETA: 2s - loss: 0.0091 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.998 - ETA: 1s - loss: 0.0079 - acc: 0.998 - ETA: 1s - loss: 0.0078 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0076 - acc: 0.998 - ETA: 1s - loss: 0.0075 - acc: 0.998 - ETA: 1s - loss: 0.0074 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0071 - acc: 0.998 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.9982Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0067 - acc: 0.9982 - val_loss: 0.9179 - val_acc: 0.8096\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 4s - loss: 2.2950e-04 - acc: 1.000 - ETA: 4s - loss: 0.0014 - acc: 1.0000    - ETA: 4s - loss: 9.4197e-04 - acc: 1.000 - ETA: 4s - loss: 8.0986e-04 - acc: 1.000 - ETA: 4s - loss: 8.4522e-04 - acc: 1.000 - ETA: 4s - loss: 7.2525e-04 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.0000    - ETA: 4s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 9.8181e-04 - acc: 1.000 - ETA: 3s - loss: 0.0011 - acc: 1.0000    - ETA: 3s - loss: 0.0010 - acc: 1.000 - ETA: 3s - loss: 9.3605e-04 - acc: 1.000 - ETA: 3s - loss: 8.9276e-04 - acc: 1.000 - ETA: 3s - loss: 8.7360e-04 - acc: 1.000 - ETA: 3s - loss: 8.3012e-04 - acc: 1.000 - ETA: 3s - loss: 8.0113e-04 - acc: 1.000 - ETA: 3s - loss: 7.6481e-04 - acc: 1.000 - ETA: 3s - loss: 7.5926e-04 - acc: 1.000 - ETA: 3s - loss: 7.7357e-04 - acc: 1.000 - ETA: 3s - loss: 7.6125e-04 - acc: 1.000 - ETA: 3s - loss: 7.7173e-04 - acc: 1.000 - ETA: 3s - loss: 7.9504e-04 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 0.9994    - ETA: 3s - loss: 0.0029 - acc: 0.999 - ETA: 3s - loss: 0.0028 - acc: 0.999 - ETA: 3s - loss: 0.0028 - acc: 0.999 - ETA: 3s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0047 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0038 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0037 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0039 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0068 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - ETA: 0s - loss: 0.0065 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.9985Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0064 - acc: 0.9985 - val_loss: 0.9253 - val_acc: 0.8192\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 7.9375e-05 - acc: 1.000 - ETA: 3s - loss: 0.0245 - acc: 0.9917    - ETA: 4s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0110 - acc: 0.996 - ETA: 4s - loss: 0.0087 - acc: 0.997 - ETA: 4s - loss: 0.0122 - acc: 0.995 - ETA: 3s - loss: 0.0105 - acc: 0.996 - ETA: 3s - loss: 0.0091 - acc: 0.996 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 3s - loss: 0.0149 - acc: 0.996 - ETA: 3s - loss: 0.0140 - acc: 0.996 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0130 - acc: 0.996 - ETA: 3s - loss: 0.0123 - acc: 0.996 - ETA: 3s - loss: 0.0131 - acc: 0.995 - ETA: 3s - loss: 0.0124 - acc: 0.996 - ETA: 3s - loss: 0.0118 - acc: 0.996 - ETA: 3s - loss: 0.0113 - acc: 0.996 - ETA: 3s - loss: 0.0108 - acc: 0.996 - ETA: 3s - loss: 0.0104 - acc: 0.996 - ETA: 3s - loss: 0.0100 - acc: 0.996 - ETA: 3s - loss: 0.0096 - acc: 0.997 - ETA: 3s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 2s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0084 - acc: 0.997 - ETA: 2s - loss: 0.0083 - acc: 0.997 - ETA: 2s - loss: 0.0081 - acc: 0.997 - ETA: 2s - loss: 0.0078 - acc: 0.997 - ETA: 2s - loss: 0.0076 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.997 - ETA: 2s - loss: 0.0072 - acc: 0.997 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0072 - acc: 0.997 - ETA: 2s - loss: 0.0070 - acc: 0.997 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0055 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0048 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.9985Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0062 - acc: 0.9984 - val_loss: 0.9970 - val_acc: 0.8096\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_5 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 131s - loss: 5.0593 - acc: 0.050 - ETA: 38s - loss: 5.8533 - acc: 0.075 - ETA: 24s - loss: 5.7769 - acc: 0.06 - ETA: 19s - loss: 5.4237 - acc: 0.08 - ETA: 16s - loss: 5.1456 - acc: 0.11 - ETA: 14s - loss: 4.7881 - acc: 0.15 - ETA: 12s - loss: 4.5018 - acc: 0.20 - ETA: 11s - loss: 4.3151 - acc: 0.23 - ETA: 10s - loss: 4.0944 - acc: 0.27 - ETA: 10s - loss: 3.9208 - acc: 0.29 - ETA: 9s - loss: 3.7574 - acc: 0.3274 - ETA: 9s - loss: 3.6136 - acc: 0.338 - ETA: 8s - loss: 3.5159 - acc: 0.351 - ETA: 8s - loss: 3.4118 - acc: 0.358 - ETA: 8s - loss: 3.2685 - acc: 0.382 - ETA: 7s - loss: 3.1548 - acc: 0.398 - ETA: 7s - loss: 3.0538 - acc: 0.417 - ETA: 7s - loss: 2.9727 - acc: 0.426 - ETA: 7s - loss: 2.8786 - acc: 0.440 - ETA: 7s - loss: 2.8054 - acc: 0.448 - ETA: 6s - loss: 2.7337 - acc: 0.459 - ETA: 6s - loss: 2.6503 - acc: 0.471 - ETA: 6s - loss: 2.5782 - acc: 0.482 - ETA: 6s - loss: 2.5258 - acc: 0.490 - ETA: 6s - loss: 2.4660 - acc: 0.502 - ETA: 6s - loss: 2.4159 - acc: 0.507 - ETA: 6s - loss: 2.3813 - acc: 0.514 - ETA: 5s - loss: 2.3319 - acc: 0.519 - ETA: 5s - loss: 2.2833 - acc: 0.526 - ETA: 5s - loss: 2.2362 - acc: 0.533 - ETA: 5s - loss: 2.1978 - acc: 0.538 - ETA: 5s - loss: 2.1556 - acc: 0.546 - ETA: 5s - loss: 2.1132 - acc: 0.552 - ETA: 5s - loss: 2.0791 - acc: 0.555 - ETA: 5s - loss: 2.0423 - acc: 0.560 - ETA: 5s - loss: 2.0071 - acc: 0.565 - ETA: 5s - loss: 1.9725 - acc: 0.570 - ETA: 4s - loss: 1.9408 - acc: 0.576 - ETA: 4s - loss: 1.9081 - acc: 0.583 - ETA: 4s - loss: 1.8847 - acc: 0.585 - ETA: 4s - loss: 1.8630 - acc: 0.588 - ETA: 4s - loss: 1.8331 - acc: 0.592 - ETA: 4s - loss: 1.8016 - acc: 0.598 - ETA: 4s - loss: 1.7967 - acc: 0.598 - ETA: 4s - loss: 1.7745 - acc: 0.603 - ETA: 4s - loss: 1.7519 - acc: 0.606 - ETA: 4s - loss: 1.7263 - acc: 0.611 - ETA: 4s - loss: 1.7180 - acc: 0.611 - ETA: 4s - loss: 1.7056 - acc: 0.613 - ETA: 3s - loss: 1.6878 - acc: 0.616 - ETA: 3s - loss: 1.6663 - acc: 0.620 - ETA: 3s - loss: 1.6461 - acc: 0.622 - ETA: 3s - loss: 1.6318 - acc: 0.625 - ETA: 3s - loss: 1.6145 - acc: 0.628 - ETA: 3s - loss: 1.6003 - acc: 0.629 - ETA: 3s - loss: 1.5865 - acc: 0.631 - ETA: 3s - loss: 1.5727 - acc: 0.634 - ETA: 3s - loss: 1.5561 - acc: 0.638 - ETA: 3s - loss: 1.5454 - acc: 0.639 - ETA: 3s - loss: 1.5271 - acc: 0.643 - ETA: 3s - loss: 1.5077 - acc: 0.646 - ETA: 3s - loss: 1.4978 - acc: 0.648 - ETA: 3s - loss: 1.4904 - acc: 0.649 - ETA: 2s - loss: 1.4815 - acc: 0.651 - ETA: 2s - loss: 1.4673 - acc: 0.652 - ETA: 2s - loss: 1.4527 - acc: 0.656 - ETA: 2s - loss: 1.4424 - acc: 0.657 - ETA: 2s - loss: 1.4368 - acc: 0.658 - ETA: 2s - loss: 1.4262 - acc: 0.660 - ETA: 2s - loss: 1.4153 - acc: 0.661 - ETA: 2s - loss: 1.4104 - acc: 0.662 - ETA: 2s - loss: 1.4027 - acc: 0.664 - ETA: 2s - loss: 1.3912 - acc: 0.667 - ETA: 2s - loss: 1.3833 - acc: 0.668 - ETA: 2s - loss: 1.3734 - acc: 0.670 - ETA: 2s - loss: 1.3678 - acc: 0.671 - ETA: 2s - loss: 1.3628 - acc: 0.671 - ETA: 2s - loss: 1.3548 - acc: 0.673 - ETA: 2s - loss: 1.3421 - acc: 0.676 - ETA: 1s - loss: 1.3334 - acc: 0.677 - ETA: 1s - loss: 1.3229 - acc: 0.679 - ETA: 1s - loss: 1.3170 - acc: 0.680 - ETA: 1s - loss: 1.3084 - acc: 0.681 - ETA: 1s - loss: 1.3004 - acc: 0.682 - ETA: 1s - loss: 1.2945 - acc: 0.682 - ETA: 1s - loss: 1.2868 - acc: 0.683 - ETA: 1s - loss: 1.2793 - acc: 0.685 - ETA: 1s - loss: 1.2765 - acc: 0.685 - ETA: 1s - loss: 1.2699 - acc: 0.687 - ETA: 1s - loss: 1.2619 - acc: 0.688 - ETA: 1s - loss: 1.2579 - acc: 0.689 - ETA: 1s - loss: 1.2512 - acc: 0.690 - ETA: 1s - loss: 1.2440 - acc: 0.691 - ETA: 1s - loss: 1.2380 - acc: 0.692 - ETA: 1s - loss: 1.2315 - acc: 0.694 - ETA: 0s - loss: 1.2241 - acc: 0.695 - ETA: 0s - loss: 1.2195 - acc: 0.696 - ETA: 0s - loss: 1.2098 - acc: 0.698 - ETA: 0s - loss: 1.2052 - acc: 0.699 - ETA: 0s - loss: 1.1981 - acc: 0.701 - ETA: 0s - loss: 1.1934 - acc: 0.702 - ETA: 0s - loss: 1.1909 - acc: 0.702 - ETA: 0s - loss: 1.1828 - acc: 0.704 - ETA: 0s - loss: 1.1758 - acc: 0.706 - ETA: 0s - loss: 1.1691 - acc: 0.707 - ETA: 0s - loss: 1.1660 - acc: 0.707 - ETA: 0s - loss: 1.1608 - acc: 0.708 - ETA: 0s - loss: 1.1554 - acc: 0.709 - ETA: 0s - loss: 1.1525 - acc: 0.710 - ETA: 0s - loss: 1.1490 - acc: 0.710 - ETA: 0s - loss: 1.1461 - acc: 0.711 - ETA: 0s - loss: 1.1402 - acc: 0.7130Epoch 00000: val_loss improved from inf to 0.66297, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.1385 - acc: 0.7129 - val_loss: 0.6630 - val_acc: 0.8060\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.3795 - acc: 0.800 - ETA: 6s - loss: 0.4965 - acc: 0.812 - ETA: 5s - loss: 0.5998 - acc: 0.814 - ETA: 5s - loss: 0.4872 - acc: 0.845 - ETA: 5s - loss: 0.5233 - acc: 0.838 - ETA: 5s - loss: 0.5207 - acc: 0.840 - ETA: 5s - loss: 0.5079 - acc: 0.847 - ETA: 5s - loss: 0.5053 - acc: 0.847 - ETA: 5s - loss: 0.4895 - acc: 0.850 - ETA: 5s - loss: 0.4719 - acc: 0.857 - ETA: 5s - loss: 0.4616 - acc: 0.861 - ETA: 5s - loss: 0.4443 - acc: 0.866 - ETA: 5s - loss: 0.4365 - acc: 0.868 - ETA: 5s - loss: 0.4337 - acc: 0.870 - ETA: 5s - loss: 0.4442 - acc: 0.867 - ETA: 5s - loss: 0.4644 - acc: 0.860 - ETA: 5s - loss: 0.4595 - acc: 0.864 - ETA: 5s - loss: 0.4541 - acc: 0.862 - ETA: 5s - loss: 0.4453 - acc: 0.867 - ETA: 5s - loss: 0.4389 - acc: 0.867 - ETA: 5s - loss: 0.4460 - acc: 0.864 - ETA: 5s - loss: 0.4427 - acc: 0.866 - ETA: 4s - loss: 0.4566 - acc: 0.862 - ETA: 4s - loss: 0.4685 - acc: 0.858 - ETA: 4s - loss: 0.4667 - acc: 0.858 - ETA: 4s - loss: 0.4668 - acc: 0.858 - ETA: 4s - loss: 0.4714 - acc: 0.858 - ETA: 4s - loss: 0.4804 - acc: 0.854 - ETA: 4s - loss: 0.4753 - acc: 0.855 - ETA: 4s - loss: 0.4702 - acc: 0.856 - ETA: 4s - loss: 0.4706 - acc: 0.857 - ETA: 4s - loss: 0.4738 - acc: 0.856 - ETA: 4s - loss: 0.4708 - acc: 0.858 - ETA: 4s - loss: 0.4702 - acc: 0.858 - ETA: 4s - loss: 0.4723 - acc: 0.858 - ETA: 4s - loss: 0.4704 - acc: 0.859 - ETA: 4s - loss: 0.4721 - acc: 0.858 - ETA: 4s - loss: 0.4754 - acc: 0.858 - ETA: 4s - loss: 0.4712 - acc: 0.859 - ETA: 4s - loss: 0.4764 - acc: 0.858 - ETA: 4s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4878 - acc: 0.856 - ETA: 3s - loss: 0.4843 - acc: 0.857 - ETA: 3s - loss: 0.4776 - acc: 0.859 - ETA: 3s - loss: 0.4757 - acc: 0.858 - ETA: 3s - loss: 0.4789 - acc: 0.858 - ETA: 3s - loss: 0.4786 - acc: 0.858 - ETA: 3s - loss: 0.4811 - acc: 0.857 - ETA: 3s - loss: 0.4799 - acc: 0.858 - ETA: 3s - loss: 0.4808 - acc: 0.859 - ETA: 3s - loss: 0.4815 - acc: 0.858 - ETA: 3s - loss: 0.4820 - acc: 0.857 - ETA: 3s - loss: 0.4806 - acc: 0.857 - ETA: 3s - loss: 0.4775 - acc: 0.859 - ETA: 3s - loss: 0.4821 - acc: 0.856 - ETA: 3s - loss: 0.4850 - acc: 0.856 - ETA: 3s - loss: 0.4896 - acc: 0.855 - ETA: 3s - loss: 0.4839 - acc: 0.855 - ETA: 2s - loss: 0.4796 - acc: 0.856 - ETA: 2s - loss: 0.4784 - acc: 0.856 - ETA: 2s - loss: 0.4794 - acc: 0.856 - ETA: 2s - loss: 0.4820 - acc: 0.855 - ETA: 2s - loss: 0.4863 - acc: 0.855 - ETA: 2s - loss: 0.4846 - acc: 0.856 - ETA: 2s - loss: 0.4835 - acc: 0.856 - ETA: 2s - loss: 0.4847 - acc: 0.857 - ETA: 2s - loss: 0.4834 - acc: 0.857 - ETA: 2s - loss: 0.4827 - acc: 0.857 - ETA: 2s - loss: 0.4839 - acc: 0.857 - ETA: 2s - loss: 0.4831 - acc: 0.856 - ETA: 2s - loss: 0.4834 - acc: 0.856 - ETA: 2s - loss: 0.4790 - acc: 0.857 - ETA: 2s - loss: 0.4811 - acc: 0.857 - ETA: 2s - loss: 0.4792 - acc: 0.857 - ETA: 2s - loss: 0.4805 - acc: 0.857 - ETA: 2s - loss: 0.4784 - acc: 0.858 - ETA: 1s - loss: 0.4812 - acc: 0.857 - ETA: 1s - loss: 0.4809 - acc: 0.857 - ETA: 1s - loss: 0.4780 - acc: 0.857 - ETA: 1s - loss: 0.4781 - acc: 0.857 - ETA: 1s - loss: 0.4792 - acc: 0.857 - ETA: 1s - loss: 0.4808 - acc: 0.857 - ETA: 1s - loss: 0.4818 - acc: 0.856 - ETA: 1s - loss: 0.4815 - acc: 0.856 - ETA: 1s - loss: 0.4836 - acc: 0.855 - ETA: 1s - loss: 0.4811 - acc: 0.856 - ETA: 1s - loss: 0.4837 - acc: 0.855 - ETA: 1s - loss: 0.4850 - acc: 0.855 - ETA: 1s - loss: 0.4826 - acc: 0.856 - ETA: 1s - loss: 0.4798 - acc: 0.856 - ETA: 1s - loss: 0.4798 - acc: 0.857 - ETA: 1s - loss: 0.4786 - acc: 0.857 - ETA: 1s - loss: 0.4763 - acc: 0.858 - ETA: 0s - loss: 0.4767 - acc: 0.858 - ETA: 0s - loss: 0.4744 - acc: 0.858 - ETA: 0s - loss: 0.4715 - acc: 0.859 - ETA: 0s - loss: 0.4719 - acc: 0.859 - ETA: 0s - loss: 0.4728 - acc: 0.859 - ETA: 0s - loss: 0.4737 - acc: 0.858 - ETA: 0s - loss: 0.4750 - acc: 0.858 - ETA: 0s - loss: 0.4723 - acc: 0.859 - ETA: 0s - loss: 0.4731 - acc: 0.858 - ETA: 0s - loss: 0.4725 - acc: 0.858 - ETA: 0s - loss: 0.4728 - acc: 0.858 - ETA: 0s - loss: 0.4729 - acc: 0.858 - ETA: 0s - loss: 0.4734 - acc: 0.858 - ETA: 0s - loss: 0.4732 - acc: 0.858 - ETA: 0s - loss: 0.4746 - acc: 0.858 - ETA: 0s - loss: 0.4759 - acc: 0.857 - ETA: 0s - loss: 0.4742 - acc: 0.858 - ETA: 0s - loss: 0.4742 - acc: 0.857 - ETA: 0s - loss: 0.4743 - acc: 0.8571Epoch 00001: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.4749 - acc: 0.8569 - val_loss: 0.7624 - val_acc: 0.8000\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.2205 - acc: 0.950 - ETA: 7s - loss: 0.2452 - acc: 0.937 - ETA: 7s - loss: 0.2715 - acc: 0.933 - ETA: 7s - loss: 0.2429 - acc: 0.927 - ETA: 7s - loss: 0.2949 - acc: 0.920 - ETA: 6s - loss: 0.2964 - acc: 0.920 - ETA: 6s - loss: 0.3023 - acc: 0.916 - ETA: 6s - loss: 0.2794 - acc: 0.919 - ETA: 6s - loss: 0.2947 - acc: 0.908 - ETA: 6s - loss: 0.3240 - acc: 0.900 - ETA: 6s - loss: 0.3127 - acc: 0.901 - ETA: 5s - loss: 0.3121 - acc: 0.900 - ETA: 5s - loss: 0.3070 - acc: 0.902 - ETA: 5s - loss: 0.3228 - acc: 0.898 - ETA: 5s - loss: 0.3378 - acc: 0.900 - ETA: 5s - loss: 0.3497 - acc: 0.897 - ETA: 5s - loss: 0.3503 - acc: 0.895 - ETA: 5s - loss: 0.3397 - acc: 0.899 - ETA: 5s - loss: 0.3409 - acc: 0.899 - ETA: 5s - loss: 0.3327 - acc: 0.900 - ETA: 5s - loss: 0.3328 - acc: 0.899 - ETA: 5s - loss: 0.3370 - acc: 0.897 - ETA: 5s - loss: 0.3308 - acc: 0.899 - ETA: 5s - loss: 0.3263 - acc: 0.900 - ETA: 4s - loss: 0.3177 - acc: 0.902 - ETA: 4s - loss: 0.3154 - acc: 0.900 - ETA: 4s - loss: 0.3175 - acc: 0.898 - ETA: 4s - loss: 0.3177 - acc: 0.897 - ETA: 4s - loss: 0.3246 - acc: 0.895 - ETA: 4s - loss: 0.3278 - acc: 0.896 - ETA: 4s - loss: 0.3302 - acc: 0.895 - ETA: 4s - loss: 0.3370 - acc: 0.893 - ETA: 4s - loss: 0.3378 - acc: 0.893 - ETA: 4s - loss: 0.3359 - acc: 0.892 - ETA: 4s - loss: 0.3355 - acc: 0.893 - ETA: 4s - loss: 0.3322 - acc: 0.894 - ETA: 4s - loss: 0.3335 - acc: 0.895 - ETA: 4s - loss: 0.3305 - acc: 0.895 - ETA: 4s - loss: 0.3267 - acc: 0.896 - ETA: 4s - loss: 0.3254 - acc: 0.895 - ETA: 3s - loss: 0.3214 - acc: 0.896 - ETA: 3s - loss: 0.3197 - acc: 0.897 - ETA: 3s - loss: 0.3220 - acc: 0.896 - ETA: 3s - loss: 0.3257 - acc: 0.895 - ETA: 3s - loss: 0.3290 - acc: 0.893 - ETA: 3s - loss: 0.3280 - acc: 0.894 - ETA: 3s - loss: 0.3305 - acc: 0.894 - ETA: 3s - loss: 0.3348 - acc: 0.892 - ETA: 3s - loss: 0.3365 - acc: 0.893 - ETA: 3s - loss: 0.3348 - acc: 0.893 - ETA: 3s - loss: 0.3353 - acc: 0.894 - ETA: 3s - loss: 0.3410 - acc: 0.893 - ETA: 3s - loss: 0.3376 - acc: 0.893 - ETA: 3s - loss: 0.3430 - acc: 0.892 - ETA: 3s - loss: 0.3426 - acc: 0.892 - ETA: 3s - loss: 0.3431 - acc: 0.892 - ETA: 3s - loss: 0.3461 - acc: 0.891 - ETA: 3s - loss: 0.3518 - acc: 0.890 - ETA: 2s - loss: 0.3526 - acc: 0.890 - ETA: 2s - loss: 0.3478 - acc: 0.891 - ETA: 2s - loss: 0.3488 - acc: 0.892 - ETA: 2s - loss: 0.3445 - acc: 0.893 - ETA: 2s - loss: 0.3475 - acc: 0.892 - ETA: 2s - loss: 0.3444 - acc: 0.893 - ETA: 2s - loss: 0.3437 - acc: 0.893 - ETA: 2s - loss: 0.3457 - acc: 0.892 - ETA: 2s - loss: 0.3446 - acc: 0.892 - ETA: 2s - loss: 0.3482 - acc: 0.891 - ETA: 2s - loss: 0.3472 - acc: 0.891 - ETA: 2s - loss: 0.3505 - acc: 0.891 - ETA: 2s - loss: 0.3515 - acc: 0.890 - ETA: 2s - loss: 0.3537 - acc: 0.889 - ETA: 2s - loss: 0.3544 - acc: 0.889 - ETA: 2s - loss: 0.3528 - acc: 0.889 - ETA: 2s - loss: 0.3512 - acc: 0.890 - ETA: 2s - loss: 0.3506 - acc: 0.890 - ETA: 2s - loss: 0.3508 - acc: 0.891 - ETA: 1s - loss: 0.3522 - acc: 0.891 - ETA: 1s - loss: 0.3515 - acc: 0.891 - ETA: 1s - loss: 0.3508 - acc: 0.891 - ETA: 1s - loss: 0.3517 - acc: 0.891 - ETA: 1s - loss: 0.3511 - acc: 0.891 - ETA: 1s - loss: 0.3528 - acc: 0.892 - ETA: 1s - loss: 0.3564 - acc: 0.891 - ETA: 1s - loss: 0.3572 - acc: 0.890 - ETA: 1s - loss: 0.3587 - acc: 0.890 - ETA: 1s - loss: 0.3564 - acc: 0.891 - ETA: 1s - loss: 0.3553 - acc: 0.891 - ETA: 1s - loss: 0.3553 - acc: 0.891 - ETA: 1s - loss: 0.3580 - acc: 0.891 - ETA: 1s - loss: 0.3580 - acc: 0.890 - ETA: 1s - loss: 0.3591 - acc: 0.890 - ETA: 1s - loss: 0.3592 - acc: 0.890 - ETA: 1s - loss: 0.3595 - acc: 0.890 - ETA: 0s - loss: 0.3600 - acc: 0.890 - ETA: 0s - loss: 0.3586 - acc: 0.890 - ETA: 0s - loss: 0.3596 - acc: 0.889 - ETA: 0s - loss: 0.3604 - acc: 0.889 - ETA: 0s - loss: 0.3585 - acc: 0.890 - ETA: 0s - loss: 0.3577 - acc: 0.890 - ETA: 0s - loss: 0.3581 - acc: 0.890 - ETA: 0s - loss: 0.3597 - acc: 0.889 - ETA: 0s - loss: 0.3619 - acc: 0.889 - ETA: 0s - loss: 0.3618 - acc: 0.889 - ETA: 0s - loss: 0.3631 - acc: 0.889 - ETA: 0s - loss: 0.3616 - acc: 0.889 - ETA: 0s - loss: 0.3612 - acc: 0.889 - ETA: 0s - loss: 0.3624 - acc: 0.889 - ETA: 0s - loss: 0.3622 - acc: 0.889 - ETA: 0s - loss: 0.3636 - acc: 0.889 - ETA: 0s - loss: 0.3626 - acc: 0.890 - ETA: 0s - loss: 0.3625 - acc: 0.8901Epoch 00002: val_loss improved from 0.66297 to 0.64605, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 0.3618 - acc: 0.8901 - val_loss: 0.6461 - val_acc: 0.8539\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.4420 - acc: 0.900 - ETA: 6s - loss: 0.3406 - acc: 0.887 - ETA: 6s - loss: 0.2695 - acc: 0.900 - ETA: 6s - loss: 0.3057 - acc: 0.885 - ETA: 6s - loss: 0.2815 - acc: 0.884 - ETA: 6s - loss: 0.2758 - acc: 0.893 - ETA: 6s - loss: 0.2677 - acc: 0.900 - ETA: 5s - loss: 0.2514 - acc: 0.904 - ETA: 5s - loss: 0.2734 - acc: 0.904 - ETA: 5s - loss: 0.2700 - acc: 0.907 - ETA: 5s - loss: 0.2570 - acc: 0.911 - ETA: 5s - loss: 0.2507 - acc: 0.913 - ETA: 5s - loss: 0.2461 - acc: 0.912 - ETA: 5s - loss: 0.2421 - acc: 0.912 - ETA: 5s - loss: 0.2413 - acc: 0.914 - ETA: 5s - loss: 0.2383 - acc: 0.914 - ETA: 5s - loss: 0.2347 - acc: 0.915 - ETA: 5s - loss: 0.2402 - acc: 0.915 - ETA: 5s - loss: 0.2519 - acc: 0.912 - ETA: 5s - loss: 0.2464 - acc: 0.914 - ETA: 5s - loss: 0.2500 - acc: 0.913 - ETA: 5s - loss: 0.2505 - acc: 0.914 - ETA: 4s - loss: 0.2527 - acc: 0.914 - ETA: 4s - loss: 0.2514 - acc: 0.915 - ETA: 4s - loss: 0.2587 - acc: 0.914 - ETA: 4s - loss: 0.2612 - acc: 0.912 - ETA: 4s - loss: 0.2569 - acc: 0.913 - ETA: 4s - loss: 0.2595 - acc: 0.912 - ETA: 4s - loss: 0.2638 - acc: 0.912 - ETA: 4s - loss: 0.2688 - acc: 0.911 - ETA: 4s - loss: 0.2629 - acc: 0.913 - ETA: 4s - loss: 0.2665 - acc: 0.912 - ETA: 4s - loss: 0.2724 - acc: 0.911 - ETA: 4s - loss: 0.2747 - acc: 0.911 - ETA: 4s - loss: 0.2744 - acc: 0.910 - ETA: 4s - loss: 0.2743 - acc: 0.911 - ETA: 4s - loss: 0.2744 - acc: 0.911 - ETA: 4s - loss: 0.2874 - acc: 0.908 - ETA: 4s - loss: 0.2837 - acc: 0.909 - ETA: 4s - loss: 0.2827 - acc: 0.908 - ETA: 3s - loss: 0.2895 - acc: 0.907 - ETA: 3s - loss: 0.2877 - acc: 0.907 - ETA: 3s - loss: 0.2838 - acc: 0.908 - ETA: 3s - loss: 0.2840 - acc: 0.908 - ETA: 3s - loss: 0.2824 - acc: 0.909 - ETA: 3s - loss: 0.2825 - acc: 0.908 - ETA: 3s - loss: 0.2818 - acc: 0.909 - ETA: 3s - loss: 0.2822 - acc: 0.908 - ETA: 3s - loss: 0.2811 - acc: 0.908 - ETA: 3s - loss: 0.2879 - acc: 0.906 - ETA: 3s - loss: 0.2943 - acc: 0.906 - ETA: 3s - loss: 0.2975 - acc: 0.906 - ETA: 3s - loss: 0.3000 - acc: 0.906 - ETA: 3s - loss: 0.2972 - acc: 0.907 - ETA: 3s - loss: 0.2969 - acc: 0.906 - ETA: 3s - loss: 0.2985 - acc: 0.905 - ETA: 3s - loss: 0.2996 - acc: 0.905 - ETA: 3s - loss: 0.3019 - acc: 0.903 - ETA: 2s - loss: 0.3049 - acc: 0.903 - ETA: 2s - loss: 0.3064 - acc: 0.903 - ETA: 2s - loss: 0.3061 - acc: 0.903 - ETA: 2s - loss: 0.3070 - acc: 0.903 - ETA: 2s - loss: 0.3068 - acc: 0.903 - ETA: 2s - loss: 0.3049 - acc: 0.903 - ETA: 2s - loss: 0.3062 - acc: 0.902 - ETA: 2s - loss: 0.3084 - acc: 0.902 - ETA: 2s - loss: 0.3073 - acc: 0.903 - ETA: 2s - loss: 0.3084 - acc: 0.902 - ETA: 2s - loss: 0.3069 - acc: 0.902 - ETA: 2s - loss: 0.3044 - acc: 0.903 - ETA: 2s - loss: 0.3038 - acc: 0.903 - ETA: 2s - loss: 0.3056 - acc: 0.903 - ETA: 2s - loss: 0.3042 - acc: 0.903 - ETA: 2s - loss: 0.3012 - acc: 0.904 - ETA: 2s - loss: 0.3015 - acc: 0.903 - ETA: 2s - loss: 0.2997 - acc: 0.904 - ETA: 1s - loss: 0.2975 - acc: 0.904 - ETA: 1s - loss: 0.2979 - acc: 0.904 - ETA: 1s - loss: 0.2958 - acc: 0.905 - ETA: 1s - loss: 0.2948 - acc: 0.905 - ETA: 1s - loss: 0.2940 - acc: 0.905 - ETA: 1s - loss: 0.2943 - acc: 0.906 - ETA: 1s - loss: 0.2964 - acc: 0.905 - ETA: 1s - loss: 0.2947 - acc: 0.906 - ETA: 1s - loss: 0.2948 - acc: 0.906 - ETA: 1s - loss: 0.2949 - acc: 0.906 - ETA: 1s - loss: 0.2964 - acc: 0.906 - ETA: 1s - loss: 0.2950 - acc: 0.906 - ETA: 1s - loss: 0.2943 - acc: 0.906 - ETA: 1s - loss: 0.2940 - acc: 0.906 - ETA: 1s - loss: 0.2936 - acc: 0.906 - ETA: 1s - loss: 0.2924 - acc: 0.906 - ETA: 1s - loss: 0.2937 - acc: 0.906 - ETA: 1s - loss: 0.2955 - acc: 0.905 - ETA: 0s - loss: 0.2943 - acc: 0.905 - ETA: 0s - loss: 0.2940 - acc: 0.906 - ETA: 0s - loss: 0.2938 - acc: 0.906 - ETA: 0s - loss: 0.2927 - acc: 0.906 - ETA: 0s - loss: 0.2917 - acc: 0.906 - ETA: 0s - loss: 0.2946 - acc: 0.905 - ETA: 0s - loss: 0.2962 - acc: 0.904 - ETA: 0s - loss: 0.2962 - acc: 0.904 - ETA: 0s - loss: 0.2956 - acc: 0.904 - ETA: 0s - loss: 0.2949 - acc: 0.905 - ETA: 0s - loss: 0.2934 - acc: 0.905 - ETA: 0s - loss: 0.2937 - acc: 0.905 - ETA: 0s - loss: 0.2923 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.905 - ETA: 0s - loss: 0.2920 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.905 - ETA: 0s - loss: 0.2911 - acc: 0.905 - ETA: 0s - loss: 0.2916 - acc: 0.9056Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2907 - acc: 0.9055 - val_loss: 0.6553 - val_acc: 0.8491\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0627 - acc: 1.000 - ETA: 7s - loss: 0.1806 - acc: 0.937 - ETA: 7s - loss: 0.1449 - acc: 0.950 - ETA: 7s - loss: 0.1139 - acc: 0.961 - ETA: 6s - loss: 0.1074 - acc: 0.966 - ETA: 6s - loss: 0.1166 - acc: 0.960 - ETA: 6s - loss: 0.1225 - acc: 0.952 - ETA: 6s - loss: 0.1325 - acc: 0.954 - ETA: 6s - loss: 0.1496 - acc: 0.952 - ETA: 6s - loss: 0.1520 - acc: 0.951 - ETA: 6s - loss: 0.1477 - acc: 0.955 - ETA: 5s - loss: 0.1666 - acc: 0.948 - ETA: 5s - loss: 0.1701 - acc: 0.947 - ETA: 5s - loss: 0.1695 - acc: 0.946 - ETA: 5s - loss: 0.1678 - acc: 0.946 - ETA: 5s - loss: 0.1613 - acc: 0.948 - ETA: 5s - loss: 0.1678 - acc: 0.945 - ETA: 5s - loss: 0.1783 - acc: 0.946 - ETA: 5s - loss: 0.1805 - acc: 0.943 - ETA: 5s - loss: 0.1853 - acc: 0.943 - ETA: 5s - loss: 0.1857 - acc: 0.941 - ETA: 5s - loss: 0.1882 - acc: 0.942 - ETA: 5s - loss: 0.1835 - acc: 0.942 - ETA: 5s - loss: 0.1843 - acc: 0.942 - ETA: 4s - loss: 0.1860 - acc: 0.941 - ETA: 4s - loss: 0.1829 - acc: 0.942 - ETA: 4s - loss: 0.1865 - acc: 0.940 - ETA: 4s - loss: 0.1895 - acc: 0.940 - ETA: 4s - loss: 0.1980 - acc: 0.936 - ETA: 4s - loss: 0.1960 - acc: 0.936 - ETA: 4s - loss: 0.1968 - acc: 0.936 - ETA: 4s - loss: 0.1998 - acc: 0.935 - ETA: 4s - loss: 0.2025 - acc: 0.935 - ETA: 4s - loss: 0.2031 - acc: 0.935 - ETA: 4s - loss: 0.2025 - acc: 0.934 - ETA: 4s - loss: 0.2026 - acc: 0.934 - ETA: 4s - loss: 0.2054 - acc: 0.933 - ETA: 4s - loss: 0.2086 - acc: 0.931 - ETA: 4s - loss: 0.2055 - acc: 0.932 - ETA: 4s - loss: 0.2084 - acc: 0.932 - ETA: 4s - loss: 0.2075 - acc: 0.931 - ETA: 3s - loss: 0.2105 - acc: 0.931 - ETA: 3s - loss: 0.2095 - acc: 0.931 - ETA: 3s - loss: 0.2135 - acc: 0.930 - ETA: 3s - loss: 0.2119 - acc: 0.931 - ETA: 3s - loss: 0.2142 - acc: 0.932 - ETA: 3s - loss: 0.2175 - acc: 0.931 - ETA: 3s - loss: 0.2165 - acc: 0.931 - ETA: 3s - loss: 0.2196 - acc: 0.931 - ETA: 3s - loss: 0.2193 - acc: 0.931 - ETA: 3s - loss: 0.2160 - acc: 0.932 - ETA: 3s - loss: 0.2136 - acc: 0.933 - ETA: 3s - loss: 0.2163 - acc: 0.932 - ETA: 3s - loss: 0.2146 - acc: 0.933 - ETA: 3s - loss: 0.2122 - acc: 0.933 - ETA: 3s - loss: 0.2133 - acc: 0.933 - ETA: 3s - loss: 0.2119 - acc: 0.934 - ETA: 3s - loss: 0.2103 - acc: 0.934 - ETA: 2s - loss: 0.2101 - acc: 0.934 - ETA: 2s - loss: 0.2096 - acc: 0.935 - ETA: 2s - loss: 0.2150 - acc: 0.933 - ETA: 2s - loss: 0.2141 - acc: 0.933 - ETA: 2s - loss: 0.2126 - acc: 0.933 - ETA: 2s - loss: 0.2153 - acc: 0.933 - ETA: 2s - loss: 0.2185 - acc: 0.932 - ETA: 2s - loss: 0.2170 - acc: 0.932 - ETA: 2s - loss: 0.2153 - acc: 0.933 - ETA: 2s - loss: 0.2200 - acc: 0.931 - ETA: 2s - loss: 0.2200 - acc: 0.931 - ETA: 2s - loss: 0.2220 - acc: 0.930 - ETA: 2s - loss: 0.2239 - acc: 0.929 - ETA: 2s - loss: 0.2252 - acc: 0.929 - ETA: 2s - loss: 0.2286 - acc: 0.928 - ETA: 2s - loss: 0.2300 - acc: 0.928 - ETA: 2s - loss: 0.2316 - acc: 0.928 - ETA: 2s - loss: 0.2340 - acc: 0.927 - ETA: 2s - loss: 0.2332 - acc: 0.928 - ETA: 1s - loss: 0.2351 - acc: 0.927 - ETA: 1s - loss: 0.2361 - acc: 0.926 - ETA: 1s - loss: 0.2382 - acc: 0.925 - ETA: 1s - loss: 0.2376 - acc: 0.925 - ETA: 1s - loss: 0.2385 - acc: 0.925 - ETA: 1s - loss: 0.2413 - acc: 0.925 - ETA: 1s - loss: 0.2410 - acc: 0.925 - ETA: 1s - loss: 0.2398 - acc: 0.925 - ETA: 1s - loss: 0.2393 - acc: 0.925 - ETA: 1s - loss: 0.2409 - acc: 0.925 - ETA: 1s - loss: 0.2419 - acc: 0.925 - ETA: 1s - loss: 0.2414 - acc: 0.925 - ETA: 1s - loss: 0.2419 - acc: 0.925 - ETA: 1s - loss: 0.2421 - acc: 0.924 - ETA: 1s - loss: 0.2430 - acc: 0.924 - ETA: 1s - loss: 0.2415 - acc: 0.924 - ETA: 1s - loss: 0.2408 - acc: 0.924 - ETA: 1s - loss: 0.2419 - acc: 0.924 - ETA: 0s - loss: 0.2419 - acc: 0.925 - ETA: 0s - loss: 0.2423 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.923 - ETA: 0s - loss: 0.2428 - acc: 0.924 - ETA: 0s - loss: 0.2424 - acc: 0.924 - ETA: 0s - loss: 0.2428 - acc: 0.924 - ETA: 0s - loss: 0.2416 - acc: 0.924 - ETA: 0s - loss: 0.2414 - acc: 0.924 - ETA: 0s - loss: 0.2421 - acc: 0.925 - ETA: 0s - loss: 0.2451 - acc: 0.923 - ETA: 0s - loss: 0.2438 - acc: 0.924 - ETA: 0s - loss: 0.2460 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2430 - acc: 0.925 - ETA: 0s - loss: 0.2417 - acc: 0.925 - ETA: 0s - loss: 0.2418 - acc: 0.925 - ETA: 0s - loss: 0.2413 - acc: 0.9257Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2407 - acc: 0.9260 - val_loss: 0.7276 - val_acc: 0.8311\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0258 - acc: 1.000 - ETA: 7s - loss: 0.1264 - acc: 0.962 - ETA: 6s - loss: 0.2514 - acc: 0.928 - ETA: 7s - loss: 0.2308 - acc: 0.927 - ETA: 6s - loss: 0.2065 - acc: 0.933 - ETA: 6s - loss: 0.2444 - acc: 0.930 - ETA: 6s - loss: 0.2217 - acc: 0.938 - ETA: 6s - loss: 0.2135 - acc: 0.940 - ETA: 6s - loss: 0.2245 - acc: 0.939 - ETA: 6s - loss: 0.2070 - acc: 0.944 - ETA: 5s - loss: 0.1907 - acc: 0.950 - ETA: 5s - loss: 0.1795 - acc: 0.953 - ETA: 5s - loss: 0.1797 - acc: 0.952 - ETA: 5s - loss: 0.1688 - acc: 0.956 - ETA: 5s - loss: 0.1770 - acc: 0.952 - ETA: 5s - loss: 0.1775 - acc: 0.950 - ETA: 5s - loss: 0.1828 - acc: 0.949 - ETA: 5s - loss: 0.1835 - acc: 0.948 - ETA: 5s - loss: 0.1854 - acc: 0.948 - ETA: 5s - loss: 0.1829 - acc: 0.948 - ETA: 5s - loss: 0.1932 - acc: 0.945 - ETA: 5s - loss: 0.1969 - acc: 0.947 - ETA: 5s - loss: 0.1973 - acc: 0.947 - ETA: 5s - loss: 0.1943 - acc: 0.947 - ETA: 4s - loss: 0.1938 - acc: 0.947 - ETA: 4s - loss: 0.1879 - acc: 0.948 - ETA: 4s - loss: 0.1907 - acc: 0.947 - ETA: 4s - loss: 0.1875 - acc: 0.948 - ETA: 4s - loss: 0.1843 - acc: 0.949 - ETA: 4s - loss: 0.1847 - acc: 0.950 - ETA: 4s - loss: 0.1865 - acc: 0.948 - ETA: 4s - loss: 0.1848 - acc: 0.948 - ETA: 4s - loss: 0.1824 - acc: 0.949 - ETA: 4s - loss: 0.1827 - acc: 0.948 - ETA: 4s - loss: 0.1831 - acc: 0.948 - ETA: 4s - loss: 0.1825 - acc: 0.948 - ETA: 4s - loss: 0.1791 - acc: 0.949 - ETA: 4s - loss: 0.1805 - acc: 0.949 - ETA: 4s - loss: 0.1798 - acc: 0.948 - ETA: 4s - loss: 0.1783 - acc: 0.949 - ETA: 4s - loss: 0.1793 - acc: 0.948 - ETA: 3s - loss: 0.1793 - acc: 0.948 - ETA: 3s - loss: 0.1813 - acc: 0.948 - ETA: 3s - loss: 0.1811 - acc: 0.947 - ETA: 3s - loss: 0.1823 - acc: 0.946 - ETA: 3s - loss: 0.1795 - acc: 0.947 - ETA: 3s - loss: 0.1809 - acc: 0.946 - ETA: 3s - loss: 0.1781 - acc: 0.947 - ETA: 3s - loss: 0.1794 - acc: 0.946 - ETA: 3s - loss: 0.1771 - acc: 0.946 - ETA: 3s - loss: 0.1790 - acc: 0.946 - ETA: 3s - loss: 0.1768 - acc: 0.947 - ETA: 3s - loss: 0.1825 - acc: 0.945 - ETA: 3s - loss: 0.1816 - acc: 0.946 - ETA: 3s - loss: 0.1828 - acc: 0.945 - ETA: 3s - loss: 0.1842 - acc: 0.944 - ETA: 3s - loss: 0.1837 - acc: 0.944 - ETA: 3s - loss: 0.1836 - acc: 0.944 - ETA: 3s - loss: 0.1829 - acc: 0.944 - ETA: 2s - loss: 0.1819 - acc: 0.944 - ETA: 2s - loss: 0.1828 - acc: 0.943 - ETA: 2s - loss: 0.1806 - acc: 0.944 - ETA: 2s - loss: 0.1802 - acc: 0.944 - ETA: 2s - loss: 0.1797 - acc: 0.944 - ETA: 2s - loss: 0.1794 - acc: 0.945 - ETA: 2s - loss: 0.1780 - acc: 0.945 - ETA: 2s - loss: 0.1789 - acc: 0.945 - ETA: 2s - loss: 0.1783 - acc: 0.945 - ETA: 2s - loss: 0.1798 - acc: 0.944 - ETA: 2s - loss: 0.1792 - acc: 0.944 - ETA: 2s - loss: 0.1786 - acc: 0.945 - ETA: 2s - loss: 0.1781 - acc: 0.945 - ETA: 2s - loss: 0.1777 - acc: 0.944 - ETA: 2s - loss: 0.1761 - acc: 0.945 - ETA: 2s - loss: 0.1763 - acc: 0.945 - ETA: 2s - loss: 0.1795 - acc: 0.944 - ETA: 1s - loss: 0.1812 - acc: 0.943 - ETA: 1s - loss: 0.1821 - acc: 0.942 - ETA: 1s - loss: 0.1829 - acc: 0.942 - ETA: 1s - loss: 0.1830 - acc: 0.942 - ETA: 1s - loss: 0.1817 - acc: 0.942 - ETA: 1s - loss: 0.1855 - acc: 0.941 - ETA: 1s - loss: 0.1844 - acc: 0.941 - ETA: 1s - loss: 0.1850 - acc: 0.941 - ETA: 1s - loss: 0.1882 - acc: 0.940 - ETA: 1s - loss: 0.1885 - acc: 0.940 - ETA: 1s - loss: 0.1894 - acc: 0.940 - ETA: 1s - loss: 0.1900 - acc: 0.940 - ETA: 1s - loss: 0.1890 - acc: 0.940 - ETA: 1s - loss: 0.1883 - acc: 0.940 - ETA: 1s - loss: 0.1879 - acc: 0.940 - ETA: 1s - loss: 0.1874 - acc: 0.941 - ETA: 1s - loss: 0.1867 - acc: 0.941 - ETA: 1s - loss: 0.1874 - acc: 0.941 - ETA: 0s - loss: 0.1890 - acc: 0.940 - ETA: 0s - loss: 0.1888 - acc: 0.940 - ETA: 0s - loss: 0.1893 - acc: 0.940 - ETA: 0s - loss: 0.1910 - acc: 0.940 - ETA: 0s - loss: 0.1921 - acc: 0.940 - ETA: 0s - loss: 0.1920 - acc: 0.940 - ETA: 0s - loss: 0.1946 - acc: 0.940 - ETA: 0s - loss: 0.1948 - acc: 0.940 - ETA: 0s - loss: 0.1948 - acc: 0.939 - ETA: 0s - loss: 0.1947 - acc: 0.939 - ETA: 0s - loss: 0.1952 - acc: 0.939 - ETA: 0s - loss: 0.1958 - acc: 0.939 - ETA: 0s - loss: 0.1957 - acc: 0.939 - ETA: 0s - loss: 0.1943 - acc: 0.940 - ETA: 0s - loss: 0.1949 - acc: 0.940 - ETA: 0s - loss: 0.1953 - acc: 0.939 - ETA: 0s - loss: 0.1941 - acc: 0.940 - ETA: 0s - loss: 0.1985 - acc: 0.9392Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1995 - acc: 0.9391 - val_loss: 0.6971 - val_acc: 0.8443\n",
      "Epoch 7/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.1057 - acc: 0.900 - ETA: 6s - loss: 0.1497 - acc: 0.937 - ETA: 6s - loss: 0.1308 - acc: 0.942 - ETA: 6s - loss: 0.1089 - acc: 0.950 - ETA: 6s - loss: 0.1133 - acc: 0.946 - ETA: 6s - loss: 0.1390 - acc: 0.943 - ETA: 6s - loss: 0.1502 - acc: 0.942 - ETA: 6s - loss: 0.1469 - acc: 0.945 - ETA: 6s - loss: 0.1428 - acc: 0.942 - ETA: 5s - loss: 0.1374 - acc: 0.944 - ETA: 5s - loss: 0.1376 - acc: 0.946 - ETA: 5s - loss: 0.1303 - acc: 0.950 - ETA: 5s - loss: 0.1359 - acc: 0.950 - ETA: 5s - loss: 0.1572 - acc: 0.946 - ETA: 5s - loss: 0.1484 - acc: 0.950 - ETA: 5s - loss: 0.1535 - acc: 0.951 - ETA: 5s - loss: 0.1479 - acc: 0.953 - ETA: 5s - loss: 0.1514 - acc: 0.952 - ETA: 5s - loss: 0.1477 - acc: 0.954 - ETA: 5s - loss: 0.1477 - acc: 0.955 - ETA: 5s - loss: 0.1455 - acc: 0.954 - ETA: 5s - loss: 0.1418 - acc: 0.955 - ETA: 5s - loss: 0.1449 - acc: 0.955 - ETA: 4s - loss: 0.1438 - acc: 0.955 - ETA: 4s - loss: 0.1400 - acc: 0.956 - ETA: 4s - loss: 0.1384 - acc: 0.957 - ETA: 4s - loss: 0.1397 - acc: 0.957 - ETA: 4s - loss: 0.1475 - acc: 0.955 - ETA: 4s - loss: 0.1473 - acc: 0.954 - ETA: 4s - loss: 0.1433 - acc: 0.956 - ETA: 4s - loss: 0.1400 - acc: 0.957 - ETA: 4s - loss: 0.1373 - acc: 0.958 - ETA: 4s - loss: 0.1342 - acc: 0.959 - ETA: 4s - loss: 0.1334 - acc: 0.959 - ETA: 4s - loss: 0.1325 - acc: 0.958 - ETA: 4s - loss: 0.1319 - acc: 0.959 - ETA: 4s - loss: 0.1285 - acc: 0.960 - ETA: 4s - loss: 0.1279 - acc: 0.960 - ETA: 4s - loss: 0.1273 - acc: 0.960 - ETA: 4s - loss: 0.1259 - acc: 0.960 - ETA: 4s - loss: 0.1279 - acc: 0.959 - ETA: 3s - loss: 0.1288 - acc: 0.959 - ETA: 3s - loss: 0.1289 - acc: 0.960 - ETA: 3s - loss: 0.1295 - acc: 0.959 - ETA: 3s - loss: 0.1346 - acc: 0.958 - ETA: 3s - loss: 0.1322 - acc: 0.959 - ETA: 3s - loss: 0.1321 - acc: 0.959 - ETA: 3s - loss: 0.1308 - acc: 0.959 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1345 - acc: 0.958 - ETA: 3s - loss: 0.1356 - acc: 0.957 - ETA: 3s - loss: 0.1344 - acc: 0.958 - ETA: 3s - loss: 0.1415 - acc: 0.957 - ETA: 3s - loss: 0.1432 - acc: 0.956 - ETA: 3s - loss: 0.1462 - acc: 0.955 - ETA: 3s - loss: 0.1487 - acc: 0.955 - ETA: 3s - loss: 0.1483 - acc: 0.955 - ETA: 3s - loss: 0.1487 - acc: 0.954 - ETA: 2s - loss: 0.1483 - acc: 0.954 - ETA: 2s - loss: 0.1479 - acc: 0.955 - ETA: 2s - loss: 0.1478 - acc: 0.955 - ETA: 2s - loss: 0.1481 - acc: 0.954 - ETA: 2s - loss: 0.1500 - acc: 0.954 - ETA: 2s - loss: 0.1543 - acc: 0.953 - ETA: 2s - loss: 0.1561 - acc: 0.952 - ETA: 2s - loss: 0.1555 - acc: 0.952 - ETA: 2s - loss: 0.1567 - acc: 0.952 - ETA: 2s - loss: 0.1563 - acc: 0.952 - ETA: 2s - loss: 0.1573 - acc: 0.952 - ETA: 2s - loss: 0.1592 - acc: 0.951 - ETA: 2s - loss: 0.1584 - acc: 0.951 - ETA: 2s - loss: 0.1580 - acc: 0.952 - ETA: 2s - loss: 0.1582 - acc: 0.952 - ETA: 2s - loss: 0.1617 - acc: 0.951 - ETA: 2s - loss: 0.1610 - acc: 0.951 - ETA: 2s - loss: 0.1601 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.951 - ETA: 1s - loss: 0.1581 - acc: 0.951 - ETA: 1s - loss: 0.1580 - acc: 0.951 - ETA: 1s - loss: 0.1572 - acc: 0.951 - ETA: 1s - loss: 0.1574 - acc: 0.951 - ETA: 1s - loss: 0.1590 - acc: 0.950 - ETA: 1s - loss: 0.1606 - acc: 0.950 - ETA: 1s - loss: 0.1616 - acc: 0.950 - ETA: 1s - loss: 0.1602 - acc: 0.950 - ETA: 1s - loss: 0.1593 - acc: 0.950 - ETA: 1s - loss: 0.1594 - acc: 0.951 - ETA: 1s - loss: 0.1619 - acc: 0.950 - ETA: 1s - loss: 0.1607 - acc: 0.950 - ETA: 1s - loss: 0.1615 - acc: 0.950 - ETA: 1s - loss: 0.1612 - acc: 0.950 - ETA: 1s - loss: 0.1607 - acc: 0.950 - ETA: 1s - loss: 0.1605 - acc: 0.950 - ETA: 0s - loss: 0.1599 - acc: 0.950 - ETA: 0s - loss: 0.1603 - acc: 0.949 - ETA: 0s - loss: 0.1625 - acc: 0.949 - ETA: 0s - loss: 0.1630 - acc: 0.949 - ETA: 0s - loss: 0.1635 - acc: 0.949 - ETA: 0s - loss: 0.1625 - acc: 0.949 - ETA: 0s - loss: 0.1627 - acc: 0.949 - ETA: 0s - loss: 0.1634 - acc: 0.949 - ETA: 0s - loss: 0.1628 - acc: 0.949 - ETA: 0s - loss: 0.1677 - acc: 0.948 - ETA: 0s - loss: 0.1679 - acc: 0.948 - ETA: 0s - loss: 0.1676 - acc: 0.948 - ETA: 0s - loss: 0.1670 - acc: 0.948 - ETA: 0s - loss: 0.1677 - acc: 0.948 - ETA: 0s - loss: 0.1678 - acc: 0.948 - ETA: 0s - loss: 0.1683 - acc: 0.947 - ETA: 0s - loss: 0.1675 - acc: 0.948 - ETA: 0s - loss: 0.1672 - acc: 0.9485Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1670 - acc: 0.9484 - val_loss: 0.6871 - val_acc: 0.8611\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0203 - acc: 1.000 - ETA: 7s - loss: 0.0616 - acc: 0.987 - ETA: 7s - loss: 0.0816 - acc: 0.985 - ETA: 6s - loss: 0.0848 - acc: 0.985 - ETA: 6s - loss: 0.0827 - acc: 0.980 - ETA: 6s - loss: 0.0689 - acc: 0.984 - ETA: 6s - loss: 0.0679 - acc: 0.981 - ETA: 6s - loss: 0.0675 - acc: 0.981 - ETA: 6s - loss: 0.0832 - acc: 0.974 - ETA: 6s - loss: 0.0871 - acc: 0.973 - ETA: 5s - loss: 0.1165 - acc: 0.969 - ETA: 5s - loss: 0.1177 - acc: 0.966 - ETA: 5s - loss: 0.1104 - acc: 0.967 - ETA: 5s - loss: 0.1212 - acc: 0.965 - ETA: 5s - loss: 0.1155 - acc: 0.966 - ETA: 5s - loss: 0.1120 - acc: 0.967 - ETA: 5s - loss: 0.1075 - acc: 0.969 - ETA: 5s - loss: 0.1053 - acc: 0.970 - ETA: 5s - loss: 0.1052 - acc: 0.970 - ETA: 5s - loss: 0.1098 - acc: 0.969 - ETA: 5s - loss: 0.1067 - acc: 0.970 - ETA: 5s - loss: 0.1220 - acc: 0.968 - ETA: 5s - loss: 0.1311 - acc: 0.966 - ETA: 5s - loss: 0.1327 - acc: 0.964 - ETA: 5s - loss: 0.1276 - acc: 0.966 - ETA: 5s - loss: 0.1351 - acc: 0.964 - ETA: 4s - loss: 0.1341 - acc: 0.963 - ETA: 4s - loss: 0.1393 - acc: 0.961 - ETA: 4s - loss: 0.1366 - acc: 0.962 - ETA: 4s - loss: 0.1381 - acc: 0.962 - ETA: 4s - loss: 0.1349 - acc: 0.963 - ETA: 4s - loss: 0.1322 - acc: 0.964 - ETA: 4s - loss: 0.1299 - acc: 0.964 - ETA: 4s - loss: 0.1272 - acc: 0.964 - ETA: 4s - loss: 0.1283 - acc: 0.964 - ETA: 4s - loss: 0.1263 - acc: 0.965 - ETA: 4s - loss: 0.1257 - acc: 0.964 - ETA: 4s - loss: 0.1248 - acc: 0.964 - ETA: 4s - loss: 0.1233 - acc: 0.964 - ETA: 4s - loss: 0.1249 - acc: 0.964 - ETA: 4s - loss: 0.1250 - acc: 0.964 - ETA: 4s - loss: 0.1263 - acc: 0.963 - ETA: 3s - loss: 0.1261 - acc: 0.963 - ETA: 3s - loss: 0.1251 - acc: 0.963 - ETA: 3s - loss: 0.1234 - acc: 0.963 - ETA: 3s - loss: 0.1212 - acc: 0.964 - ETA: 3s - loss: 0.1222 - acc: 0.963 - ETA: 3s - loss: 0.1251 - acc: 0.962 - ETA: 3s - loss: 0.1235 - acc: 0.962 - ETA: 3s - loss: 0.1232 - acc: 0.962 - ETA: 3s - loss: 0.1232 - acc: 0.962 - ETA: 3s - loss: 0.1222 - acc: 0.963 - ETA: 3s - loss: 0.1212 - acc: 0.963 - ETA: 3s - loss: 0.1198 - acc: 0.963 - ETA: 3s - loss: 0.1201 - acc: 0.963 - ETA: 3s - loss: 0.1182 - acc: 0.964 - ETA: 3s - loss: 0.1165 - acc: 0.964 - ETA: 3s - loss: 0.1168 - acc: 0.964 - ETA: 3s - loss: 0.1177 - acc: 0.964 - ETA: 2s - loss: 0.1170 - acc: 0.965 - ETA: 2s - loss: 0.1161 - acc: 0.965 - ETA: 2s - loss: 0.1180 - acc: 0.964 - ETA: 2s - loss: 0.1208 - acc: 0.964 - ETA: 2s - loss: 0.1240 - acc: 0.963 - ETA: 2s - loss: 0.1242 - acc: 0.962 - ETA: 2s - loss: 0.1254 - acc: 0.961 - ETA: 2s - loss: 0.1241 - acc: 0.962 - ETA: 2s - loss: 0.1231 - acc: 0.961 - ETA: 2s - loss: 0.1248 - acc: 0.961 - ETA: 2s - loss: 0.1261 - acc: 0.960 - ETA: 2s - loss: 0.1270 - acc: 0.960 - ETA: 2s - loss: 0.1277 - acc: 0.960 - ETA: 2s - loss: 0.1277 - acc: 0.960 - ETA: 2s - loss: 0.1278 - acc: 0.959 - ETA: 2s - loss: 0.1299 - acc: 0.959 - ETA: 2s - loss: 0.1290 - acc: 0.959 - ETA: 2s - loss: 0.1286 - acc: 0.959 - ETA: 1s - loss: 0.1288 - acc: 0.959 - ETA: 1s - loss: 0.1281 - acc: 0.959 - ETA: 1s - loss: 0.1300 - acc: 0.958 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 1s - loss: 0.1308 - acc: 0.959 - ETA: 1s - loss: 0.1313 - acc: 0.959 - ETA: 1s - loss: 0.1313 - acc: 0.958 - ETA: 1s - loss: 0.1331 - acc: 0.958 - ETA: 1s - loss: 0.1327 - acc: 0.958 - ETA: 1s - loss: 0.1324 - acc: 0.958 - ETA: 1s - loss: 0.1314 - acc: 0.958 - ETA: 1s - loss: 0.1313 - acc: 0.958 - ETA: 1s - loss: 0.1309 - acc: 0.958 - ETA: 1s - loss: 0.1318 - acc: 0.958 - ETA: 1s - loss: 0.1323 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 0s - loss: 0.1315 - acc: 0.958 - ETA: 0s - loss: 0.1317 - acc: 0.958 - ETA: 0s - loss: 0.1325 - acc: 0.958 - ETA: 0s - loss: 0.1335 - acc: 0.957 - ETA: 0s - loss: 0.1346 - acc: 0.957 - ETA: 0s - loss: 0.1345 - acc: 0.956 - ETA: 0s - loss: 0.1341 - acc: 0.956 - ETA: 0s - loss: 0.1333 - acc: 0.957 - ETA: 0s - loss: 0.1330 - acc: 0.957 - ETA: 0s - loss: 0.1359 - acc: 0.957 - ETA: 0s - loss: 0.1354 - acc: 0.957 - ETA: 0s - loss: 0.1360 - acc: 0.957 - ETA: 0s - loss: 0.1364 - acc: 0.956 - ETA: 0s - loss: 0.1360 - acc: 0.956 - ETA: 0s - loss: 0.1376 - acc: 0.956 - ETA: 0s - loss: 0.1401 - acc: 0.955 - ETA: 0s - loss: 0.1396 - acc: 0.955 - ETA: 0s - loss: 0.1392 - acc: 0.9557Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1408 - acc: 0.9552 - val_loss: 0.7830 - val_acc: 0.8491\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.1602 - acc: 0.900 - ETA: 5s - loss: 0.1880 - acc: 0.912 - ETA: 5s - loss: 0.1562 - acc: 0.921 - ETA: 5s - loss: 0.1632 - acc: 0.925 - ETA: 5s - loss: 0.1357 - acc: 0.934 - ETA: 5s - loss: 0.1534 - acc: 0.937 - ETA: 5s - loss: 0.1615 - acc: 0.934 - ETA: 5s - loss: 0.1467 - acc: 0.940 - ETA: 5s - loss: 0.1361 - acc: 0.944 - ETA: 5s - loss: 0.1375 - acc: 0.944 - ETA: 5s - loss: 0.1287 - acc: 0.946 - ETA: 5s - loss: 0.1218 - acc: 0.950 - ETA: 5s - loss: 0.1172 - acc: 0.952 - ETA: 5s - loss: 0.1170 - acc: 0.952 - ETA: 5s - loss: 0.1114 - acc: 0.954 - ETA: 5s - loss: 0.1105 - acc: 0.955 - ETA: 5s - loss: 0.1114 - acc: 0.954 - ETA: 5s - loss: 0.1243 - acc: 0.952 - ETA: 5s - loss: 0.1209 - acc: 0.953 - ETA: 5s - loss: 0.1157 - acc: 0.956 - ETA: 5s - loss: 0.1124 - acc: 0.957 - ETA: 4s - loss: 0.1156 - acc: 0.954 - ETA: 4s - loss: 0.1164 - acc: 0.956 - ETA: 4s - loss: 0.1142 - acc: 0.956 - ETA: 4s - loss: 0.1153 - acc: 0.955 - ETA: 4s - loss: 0.1204 - acc: 0.954 - ETA: 4s - loss: 0.1217 - acc: 0.953 - ETA: 4s - loss: 0.1205 - acc: 0.954 - ETA: 4s - loss: 0.1229 - acc: 0.954 - ETA: 4s - loss: 0.1225 - acc: 0.954 - ETA: 4s - loss: 0.1258 - acc: 0.954 - ETA: 4s - loss: 0.1245 - acc: 0.955 - ETA: 4s - loss: 0.1230 - acc: 0.955 - ETA: 4s - loss: 0.1240 - acc: 0.954 - ETA: 4s - loss: 0.1212 - acc: 0.955 - ETA: 4s - loss: 0.1197 - acc: 0.956 - ETA: 4s - loss: 0.1207 - acc: 0.956 - ETA: 4s - loss: 0.1237 - acc: 0.955 - ETA: 4s - loss: 0.1236 - acc: 0.955 - ETA: 4s - loss: 0.1235 - acc: 0.955 - ETA: 4s - loss: 0.1246 - acc: 0.955 - ETA: 3s - loss: 0.1220 - acc: 0.956 - ETA: 3s - loss: 0.1209 - acc: 0.957 - ETA: 3s - loss: 0.1257 - acc: 0.956 - ETA: 3s - loss: 0.1267 - acc: 0.956 - ETA: 3s - loss: 0.1274 - acc: 0.955 - ETA: 3s - loss: 0.1251 - acc: 0.956 - ETA: 3s - loss: 0.1233 - acc: 0.957 - ETA: 3s - loss: 0.1225 - acc: 0.956 - ETA: 3s - loss: 0.1209 - acc: 0.957 - ETA: 3s - loss: 0.1194 - acc: 0.958 - ETA: 3s - loss: 0.1204 - acc: 0.957 - ETA: 3s - loss: 0.1188 - acc: 0.958 - ETA: 3s - loss: 0.1171 - acc: 0.959 - ETA: 3s - loss: 0.1158 - acc: 0.959 - ETA: 3s - loss: 0.1148 - acc: 0.960 - ETA: 3s - loss: 0.1161 - acc: 0.959 - ETA: 3s - loss: 0.1172 - acc: 0.959 - ETA: 2s - loss: 0.1170 - acc: 0.960 - ETA: 2s - loss: 0.1171 - acc: 0.960 - ETA: 2s - loss: 0.1157 - acc: 0.960 - ETA: 2s - loss: 0.1168 - acc: 0.961 - ETA: 2s - loss: 0.1154 - acc: 0.961 - ETA: 2s - loss: 0.1150 - acc: 0.961 - ETA: 2s - loss: 0.1144 - acc: 0.961 - ETA: 2s - loss: 0.1150 - acc: 0.961 - ETA: 2s - loss: 0.1147 - acc: 0.961 - ETA: 2s - loss: 0.1138 - acc: 0.961 - ETA: 2s - loss: 0.1127 - acc: 0.962 - ETA: 2s - loss: 0.1114 - acc: 0.962 - ETA: 2s - loss: 0.1145 - acc: 0.962 - ETA: 2s - loss: 0.1161 - acc: 0.962 - ETA: 2s - loss: 0.1158 - acc: 0.962 - ETA: 2s - loss: 0.1163 - acc: 0.962 - ETA: 2s - loss: 0.1157 - acc: 0.962 - ETA: 2s - loss: 0.1171 - acc: 0.962 - ETA: 1s - loss: 0.1183 - acc: 0.962 - ETA: 1s - loss: 0.1188 - acc: 0.961 - ETA: 1s - loss: 0.1194 - acc: 0.961 - ETA: 1s - loss: 0.1192 - acc: 0.961 - ETA: 1s - loss: 0.1199 - acc: 0.961 - ETA: 1s - loss: 0.1200 - acc: 0.961 - ETA: 1s - loss: 0.1220 - acc: 0.960 - ETA: 1s - loss: 0.1255 - acc: 0.960 - ETA: 1s - loss: 0.1246 - acc: 0.960 - ETA: 1s - loss: 0.1239 - acc: 0.960 - ETA: 1s - loss: 0.1251 - acc: 0.960 - ETA: 1s - loss: 0.1246 - acc: 0.960 - ETA: 1s - loss: 0.1238 - acc: 0.960 - ETA: 1s - loss: 0.1245 - acc: 0.960 - ETA: 1s - loss: 0.1268 - acc: 0.960 - ETA: 1s - loss: 0.1262 - acc: 0.960 - ETA: 1s - loss: 0.1253 - acc: 0.960 - ETA: 1s - loss: 0.1242 - acc: 0.961 - ETA: 0s - loss: 0.1235 - acc: 0.961 - ETA: 0s - loss: 0.1224 - acc: 0.961 - ETA: 0s - loss: 0.1219 - acc: 0.961 - ETA: 0s - loss: 0.1218 - acc: 0.961 - ETA: 0s - loss: 0.1257 - acc: 0.960 - ETA: 0s - loss: 0.1266 - acc: 0.960 - ETA: 0s - loss: 0.1267 - acc: 0.960 - ETA: 0s - loss: 0.1263 - acc: 0.960 - ETA: 0s - loss: 0.1278 - acc: 0.960 - ETA: 0s - loss: 0.1272 - acc: 0.960 - ETA: 0s - loss: 0.1262 - acc: 0.961 - ETA: 0s - loss: 0.1260 - acc: 0.960 - ETA: 0s - loss: 0.1256 - acc: 0.960 - ETA: 0s - loss: 0.1249 - acc: 0.960 - ETA: 0s - loss: 0.1247 - acc: 0.961 - ETA: 0s - loss: 0.1259 - acc: 0.960 - ETA: 0s - loss: 0.1253 - acc: 0.960 - ETA: 0s - loss: 0.1249 - acc: 0.9605Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1253 - acc: 0.9605 - val_loss: 0.7596 - val_acc: 0.8503\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0931 - acc: 0.950 - ETA: 6s - loss: 0.0472 - acc: 0.975 - ETA: 6s - loss: 0.0523 - acc: 0.978 - ETA: 6s - loss: 0.0480 - acc: 0.980 - ETA: 6s - loss: 0.0650 - acc: 0.973 - ETA: 6s - loss: 0.0559 - acc: 0.978 - ETA: 6s - loss: 0.0613 - acc: 0.978 - ETA: 6s - loss: 0.0608 - acc: 0.977 - ETA: 5s - loss: 0.0641 - acc: 0.976 - ETA: 5s - loss: 0.0674 - acc: 0.975 - ETA: 5s - loss: 0.0650 - acc: 0.975 - ETA: 5s - loss: 0.0704 - acc: 0.975 - ETA: 5s - loss: 0.0680 - acc: 0.975 - ETA: 5s - loss: 0.0691 - acc: 0.975 - ETA: 5s - loss: 0.0656 - acc: 0.976 - ETA: 5s - loss: 0.0667 - acc: 0.976 - ETA: 5s - loss: 0.0729 - acc: 0.972 - ETA: 5s - loss: 0.0776 - acc: 0.970 - ETA: 5s - loss: 0.0803 - acc: 0.968 - ETA: 5s - loss: 0.0803 - acc: 0.969 - ETA: 5s - loss: 0.0781 - acc: 0.970 - ETA: 5s - loss: 0.0754 - acc: 0.971 - ETA: 5s - loss: 0.0724 - acc: 0.973 - ETA: 4s - loss: 0.0746 - acc: 0.972 - ETA: 4s - loss: 0.0720 - acc: 0.974 - ETA: 4s - loss: 0.0712 - acc: 0.974 - ETA: 4s - loss: 0.0717 - acc: 0.974 - ETA: 4s - loss: 0.0720 - acc: 0.973 - ETA: 4s - loss: 0.0753 - acc: 0.973 - ETA: 4s - loss: 0.0746 - acc: 0.973 - ETA: 4s - loss: 0.0732 - acc: 0.974 - ETA: 4s - loss: 0.0799 - acc: 0.973 - ETA: 4s - loss: 0.0786 - acc: 0.973 - ETA: 4s - loss: 0.0776 - acc: 0.974 - ETA: 4s - loss: 0.0826 - acc: 0.973 - ETA: 4s - loss: 0.0833 - acc: 0.974 - ETA: 4s - loss: 0.0825 - acc: 0.973 - ETA: 4s - loss: 0.0865 - acc: 0.972 - ETA: 4s - loss: 0.0860 - acc: 0.973 - ETA: 4s - loss: 0.0844 - acc: 0.973 - ETA: 3s - loss: 0.0830 - acc: 0.974 - ETA: 3s - loss: 0.0851 - acc: 0.972 - ETA: 3s - loss: 0.0843 - acc: 0.972 - ETA: 3s - loss: 0.0841 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.971 - ETA: 3s - loss: 0.0843 - acc: 0.971 - ETA: 3s - loss: 0.0831 - acc: 0.971 - ETA: 3s - loss: 0.0851 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.971 - ETA: 3s - loss: 0.0836 - acc: 0.972 - ETA: 3s - loss: 0.0864 - acc: 0.971 - ETA: 3s - loss: 0.0857 - acc: 0.971 - ETA: 3s - loss: 0.0873 - acc: 0.971 - ETA: 3s - loss: 0.0897 - acc: 0.970 - ETA: 3s - loss: 0.0883 - acc: 0.970 - ETA: 3s - loss: 0.0878 - acc: 0.971 - ETA: 3s - loss: 0.0902 - acc: 0.971 - ETA: 2s - loss: 0.0906 - acc: 0.970 - ETA: 2s - loss: 0.0904 - acc: 0.970 - ETA: 2s - loss: 0.0939 - acc: 0.970 - ETA: 2s - loss: 0.0970 - acc: 0.969 - ETA: 2s - loss: 0.0964 - acc: 0.969 - ETA: 2s - loss: 0.0983 - acc: 0.969 - ETA: 2s - loss: 0.0981 - acc: 0.969 - ETA: 2s - loss: 0.0973 - acc: 0.969 - ETA: 2s - loss: 0.0966 - acc: 0.969 - ETA: 2s - loss: 0.0960 - acc: 0.969 - ETA: 2s - loss: 0.0955 - acc: 0.969 - ETA: 2s - loss: 0.0972 - acc: 0.968 - ETA: 2s - loss: 0.0970 - acc: 0.969 - ETA: 2s - loss: 0.0978 - acc: 0.968 - ETA: 2s - loss: 0.0981 - acc: 0.968 - ETA: 2s - loss: 0.0977 - acc: 0.968 - ETA: 2s - loss: 0.0966 - acc: 0.968 - ETA: 2s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0962 - acc: 0.968 - ETA: 1s - loss: 0.0970 - acc: 0.968 - ETA: 1s - loss: 0.0971 - acc: 0.968 - ETA: 1s - loss: 0.0965 - acc: 0.968 - ETA: 1s - loss: 0.0955 - acc: 0.969 - ETA: 1s - loss: 0.0945 - acc: 0.969 - ETA: 1s - loss: 0.0940 - acc: 0.969 - ETA: 1s - loss: 0.0951 - acc: 0.969 - ETA: 1s - loss: 0.0951 - acc: 0.969 - ETA: 1s - loss: 0.0946 - acc: 0.970 - ETA: 1s - loss: 0.0956 - acc: 0.969 - ETA: 1s - loss: 0.0979 - acc: 0.969 - ETA: 1s - loss: 0.0986 - acc: 0.969 - ETA: 1s - loss: 0.0988 - acc: 0.969 - ETA: 1s - loss: 0.0980 - acc: 0.969 - ETA: 1s - loss: 0.0978 - acc: 0.969 - ETA: 1s - loss: 0.0973 - acc: 0.969 - ETA: 1s - loss: 0.0982 - acc: 0.969 - ETA: 1s - loss: 0.0980 - acc: 0.969 - ETA: 0s - loss: 0.0998 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.0988 - acc: 0.969 - ETA: 0s - loss: 0.0981 - acc: 0.969 - ETA: 0s - loss: 0.0996 - acc: 0.968 - ETA: 0s - loss: 0.1007 - acc: 0.968 - ETA: 0s - loss: 0.0999 - acc: 0.968 - ETA: 0s - loss: 0.1030 - acc: 0.967 - ETA: 0s - loss: 0.1022 - acc: 0.967 - ETA: 0s - loss: 0.1015 - acc: 0.968 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1019 - acc: 0.967 - ETA: 0s - loss: 0.1017 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.968 - ETA: 0s - loss: 0.1017 - acc: 0.9678Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1017 - acc: 0.9677 - val_loss: 0.8152 - val_acc: 0.8419\n",
      "Epoch 11/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0886 - acc: 0.950 - ETA: 6s - loss: 0.0577 - acc: 0.962 - ETA: 6s - loss: 0.0709 - acc: 0.957 - ETA: 6s - loss: 0.0561 - acc: 0.970 - ETA: 6s - loss: 0.0452 - acc: 0.976 - ETA: 6s - loss: 0.0398 - acc: 0.981 - ETA: 6s - loss: 0.0394 - acc: 0.981 - ETA: 6s - loss: 0.0490 - acc: 0.977 - ETA: 5s - loss: 0.0536 - acc: 0.978 - ETA: 5s - loss: 0.0749 - acc: 0.976 - ETA: 5s - loss: 0.0705 - acc: 0.979 - ETA: 5s - loss: 0.0699 - acc: 0.977 - ETA: 5s - loss: 0.0676 - acc: 0.978 - ETA: 5s - loss: 0.0692 - acc: 0.977 - ETA: 5s - loss: 0.0682 - acc: 0.977 - ETA: 5s - loss: 0.0680 - acc: 0.978 - ETA: 5s - loss: 0.0670 - acc: 0.978 - ETA: 5s - loss: 0.0657 - acc: 0.977 - ETA: 5s - loss: 0.0646 - acc: 0.977 - ETA: 5s - loss: 0.0639 - acc: 0.977 - ETA: 5s - loss: 0.0616 - acc: 0.978 - ETA: 5s - loss: 0.0608 - acc: 0.978 - ETA: 5s - loss: 0.0611 - acc: 0.977 - ETA: 4s - loss: 0.0593 - acc: 0.978 - ETA: 4s - loss: 0.0637 - acc: 0.976 - ETA: 4s - loss: 0.0630 - acc: 0.977 - ETA: 4s - loss: 0.0619 - acc: 0.977 - ETA: 4s - loss: 0.0604 - acc: 0.978 - ETA: 4s - loss: 0.0590 - acc: 0.978 - ETA: 4s - loss: 0.0578 - acc: 0.978 - ETA: 4s - loss: 0.0561 - acc: 0.979 - ETA: 4s - loss: 0.0557 - acc: 0.979 - ETA: 4s - loss: 0.0561 - acc: 0.978 - ETA: 4s - loss: 0.0547 - acc: 0.979 - ETA: 4s - loss: 0.0553 - acc: 0.979 - ETA: 4s - loss: 0.0576 - acc: 0.979 - ETA: 4s - loss: 0.0589 - acc: 0.978 - ETA: 4s - loss: 0.0641 - acc: 0.977 - ETA: 4s - loss: 0.0627 - acc: 0.978 - ETA: 4s - loss: 0.0633 - acc: 0.977 - ETA: 4s - loss: 0.0627 - acc: 0.977 - ETA: 4s - loss: 0.0616 - acc: 0.977 - ETA: 3s - loss: 0.0613 - acc: 0.977 - ETA: 3s - loss: 0.0632 - acc: 0.976 - ETA: 3s - loss: 0.0623 - acc: 0.976 - ETA: 3s - loss: 0.0635 - acc: 0.976 - ETA: 3s - loss: 0.0630 - acc: 0.976 - ETA: 3s - loss: 0.0653 - acc: 0.975 - ETA: 3s - loss: 0.0664 - acc: 0.975 - ETA: 3s - loss: 0.0651 - acc: 0.975 - ETA: 3s - loss: 0.0651 - acc: 0.976 - ETA: 3s - loss: 0.0653 - acc: 0.975 - ETA: 3s - loss: 0.0650 - acc: 0.976 - ETA: 3s - loss: 0.0666 - acc: 0.975 - ETA: 3s - loss: 0.0662 - acc: 0.975 - ETA: 3s - loss: 0.0671 - acc: 0.974 - ETA: 3s - loss: 0.0669 - acc: 0.975 - ETA: 3s - loss: 0.0672 - acc: 0.974 - ETA: 3s - loss: 0.0666 - acc: 0.975 - ETA: 2s - loss: 0.0675 - acc: 0.974 - ETA: 2s - loss: 0.0695 - acc: 0.974 - ETA: 2s - loss: 0.0717 - acc: 0.973 - ETA: 2s - loss: 0.0741 - acc: 0.973 - ETA: 2s - loss: 0.0744 - acc: 0.973 - ETA: 2s - loss: 0.0737 - acc: 0.973 - ETA: 2s - loss: 0.0735 - acc: 0.973 - ETA: 2s - loss: 0.0736 - acc: 0.973 - ETA: 2s - loss: 0.0739 - acc: 0.972 - ETA: 2s - loss: 0.0738 - acc: 0.972 - ETA: 2s - loss: 0.0740 - acc: 0.972 - ETA: 2s - loss: 0.0740 - acc: 0.972 - ETA: 2s - loss: 0.0751 - acc: 0.972 - ETA: 2s - loss: 0.0766 - acc: 0.971 - ETA: 2s - loss: 0.0788 - acc: 0.971 - ETA: 2s - loss: 0.0804 - acc: 0.970 - ETA: 2s - loss: 0.0809 - acc: 0.970 - ETA: 2s - loss: 0.0806 - acc: 0.970 - ETA: 2s - loss: 0.0798 - acc: 0.970 - ETA: 1s - loss: 0.0813 - acc: 0.970 - ETA: 1s - loss: 0.0813 - acc: 0.971 - ETA: 1s - loss: 0.0807 - acc: 0.971 - ETA: 1s - loss: 0.0798 - acc: 0.971 - ETA: 1s - loss: 0.0795 - acc: 0.971 - ETA: 1s - loss: 0.0804 - acc: 0.971 - ETA: 1s - loss: 0.0822 - acc: 0.970 - ETA: 1s - loss: 0.0815 - acc: 0.970 - ETA: 1s - loss: 0.0811 - acc: 0.971 - ETA: 1s - loss: 0.0803 - acc: 0.971 - ETA: 1s - loss: 0.0826 - acc: 0.971 - ETA: 1s - loss: 0.0824 - acc: 0.971 - ETA: 1s - loss: 0.0841 - acc: 0.970 - ETA: 1s - loss: 0.0843 - acc: 0.970 - ETA: 1s - loss: 0.0838 - acc: 0.970 - ETA: 1s - loss: 0.0849 - acc: 0.970 - ETA: 1s - loss: 0.0849 - acc: 0.970 - ETA: 1s - loss: 0.0847 - acc: 0.970 - ETA: 0s - loss: 0.0845 - acc: 0.970 - ETA: 0s - loss: 0.0844 - acc: 0.970 - ETA: 0s - loss: 0.0842 - acc: 0.970 - ETA: 0s - loss: 0.0841 - acc: 0.970 - ETA: 0s - loss: 0.0847 - acc: 0.970 - ETA: 0s - loss: 0.0855 - acc: 0.970 - ETA: 0s - loss: 0.0849 - acc: 0.970 - ETA: 0s - loss: 0.0848 - acc: 0.970 - ETA: 0s - loss: 0.0857 - acc: 0.970 - ETA: 0s - loss: 0.0883 - acc: 0.969 - ETA: 0s - loss: 0.0903 - acc: 0.969 - ETA: 0s - loss: 0.0899 - acc: 0.969 - ETA: 0s - loss: 0.0900 - acc: 0.969 - ETA: 0s - loss: 0.0903 - acc: 0.969 - ETA: 0s - loss: 0.0897 - acc: 0.969 - ETA: 0s - loss: 0.0897 - acc: 0.969 - ETA: 0s - loss: 0.0895 - acc: 0.9695Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0890 - acc: 0.9698 - val_loss: 0.7739 - val_acc: 0.8479\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0764 - acc: 0.950 - ETA: 5s - loss: 0.0404 - acc: 0.987 - ETA: 5s - loss: 0.0294 - acc: 0.992 - ETA: 5s - loss: 0.0209 - acc: 0.995 - ETA: 5s - loss: 0.0224 - acc: 0.992 - ETA: 5s - loss: 0.0234 - acc: 0.990 - ETA: 5s - loss: 0.0343 - acc: 0.986 - ETA: 5s - loss: 0.0400 - acc: 0.984 - ETA: 5s - loss: 0.0418 - acc: 0.984 - ETA: 5s - loss: 0.0442 - acc: 0.983 - ETA: 5s - loss: 0.0404 - acc: 0.985 - ETA: 5s - loss: 0.0384 - acc: 0.986 - ETA: 5s - loss: 0.0379 - acc: 0.986 - ETA: 5s - loss: 0.0370 - acc: 0.986 - ETA: 5s - loss: 0.0390 - acc: 0.984 - ETA: 5s - loss: 0.0390 - acc: 0.984 - ETA: 5s - loss: 0.0388 - acc: 0.984 - ETA: 5s - loss: 0.0435 - acc: 0.983 - ETA: 5s - loss: 0.0441 - acc: 0.983 - ETA: 5s - loss: 0.0435 - acc: 0.983 - ETA: 5s - loss: 0.0464 - acc: 0.982 - ETA: 4s - loss: 0.0452 - acc: 0.983 - ETA: 4s - loss: 0.0443 - acc: 0.984 - ETA: 4s - loss: 0.0436 - acc: 0.984 - ETA: 4s - loss: 0.0463 - acc: 0.984 - ETA: 4s - loss: 0.0500 - acc: 0.983 - ETA: 4s - loss: 0.0491 - acc: 0.983 - ETA: 4s - loss: 0.0483 - acc: 0.983 - ETA: 4s - loss: 0.0499 - acc: 0.983 - ETA: 4s - loss: 0.0560 - acc: 0.982 - ETA: 4s - loss: 0.0557 - acc: 0.982 - ETA: 4s - loss: 0.0541 - acc: 0.983 - ETA: 4s - loss: 0.0541 - acc: 0.982 - ETA: 4s - loss: 0.0535 - acc: 0.983 - ETA: 4s - loss: 0.0537 - acc: 0.982 - ETA: 4s - loss: 0.0554 - acc: 0.982 - ETA: 4s - loss: 0.0571 - acc: 0.982 - ETA: 4s - loss: 0.0565 - acc: 0.982 - ETA: 4s - loss: 0.0590 - acc: 0.980 - ETA: 4s - loss: 0.0625 - acc: 0.980 - ETA: 4s - loss: 0.0621 - acc: 0.980 - ETA: 3s - loss: 0.0627 - acc: 0.980 - ETA: 3s - loss: 0.0620 - acc: 0.980 - ETA: 3s - loss: 0.0622 - acc: 0.980 - ETA: 3s - loss: 0.0624 - acc: 0.980 - ETA: 3s - loss: 0.0645 - acc: 0.979 - ETA: 3s - loss: 0.0635 - acc: 0.980 - ETA: 3s - loss: 0.0653 - acc: 0.979 - ETA: 3s - loss: 0.0658 - acc: 0.979 - ETA: 3s - loss: 0.0659 - acc: 0.979 - ETA: 3s - loss: 0.0650 - acc: 0.979 - ETA: 3s - loss: 0.0683 - acc: 0.979 - ETA: 3s - loss: 0.0684 - acc: 0.979 - ETA: 3s - loss: 0.0679 - acc: 0.979 - ETA: 3s - loss: 0.0691 - acc: 0.978 - ETA: 3s - loss: 0.0695 - acc: 0.978 - ETA: 3s - loss: 0.0697 - acc: 0.978 - ETA: 3s - loss: 0.0707 - acc: 0.978 - ETA: 2s - loss: 0.0717 - acc: 0.977 - ETA: 2s - loss: 0.0709 - acc: 0.977 - ETA: 2s - loss: 0.0717 - acc: 0.977 - ETA: 2s - loss: 0.0709 - acc: 0.977 - ETA: 2s - loss: 0.0704 - acc: 0.978 - ETA: 2s - loss: 0.0707 - acc: 0.977 - ETA: 2s - loss: 0.0702 - acc: 0.977 - ETA: 2s - loss: 0.0700 - acc: 0.977 - ETA: 2s - loss: 0.0694 - acc: 0.978 - ETA: 2s - loss: 0.0704 - acc: 0.977 - ETA: 2s - loss: 0.0701 - acc: 0.977 - ETA: 2s - loss: 0.0693 - acc: 0.978 - ETA: 2s - loss: 0.0693 - acc: 0.977 - ETA: 2s - loss: 0.0689 - acc: 0.977 - ETA: 2s - loss: 0.0692 - acc: 0.977 - ETA: 2s - loss: 0.0712 - acc: 0.976 - ETA: 2s - loss: 0.0706 - acc: 0.977 - ETA: 2s - loss: 0.0712 - acc: 0.977 - ETA: 1s - loss: 0.0721 - acc: 0.977 - ETA: 1s - loss: 0.0717 - acc: 0.977 - ETA: 1s - loss: 0.0709 - acc: 0.977 - ETA: 1s - loss: 0.0703 - acc: 0.977 - ETA: 1s - loss: 0.0698 - acc: 0.977 - ETA: 1s - loss: 0.0697 - acc: 0.977 - ETA: 1s - loss: 0.0713 - acc: 0.977 - ETA: 1s - loss: 0.0718 - acc: 0.977 - ETA: 1s - loss: 0.0721 - acc: 0.977 - ETA: 1s - loss: 0.0716 - acc: 0.977 - ETA: 1s - loss: 0.0731 - acc: 0.976 - ETA: 1s - loss: 0.0728 - acc: 0.976 - ETA: 1s - loss: 0.0744 - acc: 0.976 - ETA: 1s - loss: 0.0744 - acc: 0.976 - ETA: 1s - loss: 0.0741 - acc: 0.976 - ETA: 1s - loss: 0.0749 - acc: 0.976 - ETA: 1s - loss: 0.0746 - acc: 0.976 - ETA: 1s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0743 - acc: 0.976 - ETA: 0s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0748 - acc: 0.976 - ETA: 0s - loss: 0.0745 - acc: 0.976 - ETA: 0s - loss: 0.0744 - acc: 0.976 - ETA: 0s - loss: 0.0741 - acc: 0.976 - ETA: 0s - loss: 0.0743 - acc: 0.976 - ETA: 0s - loss: 0.0753 - acc: 0.975 - ETA: 0s - loss: 0.0756 - acc: 0.975 - ETA: 0s - loss: 0.0759 - acc: 0.975 - ETA: 0s - loss: 0.0770 - acc: 0.974 - ETA: 0s - loss: 0.0770 - acc: 0.974 - ETA: 0s - loss: 0.0768 - acc: 0.974 - ETA: 0s - loss: 0.0765 - acc: 0.974 - ETA: 0s - loss: 0.0767 - acc: 0.974 - ETA: 0s - loss: 0.0761 - acc: 0.974 - ETA: 0s - loss: 0.0773 - acc: 0.974 - ETA: 0s - loss: 0.0787 - acc: 0.9745Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0786 - acc: 0.9744 - val_loss: 0.8104 - val_acc: 0.8635\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 5s - loss: 0.0423 - acc: 0.950 - ETA: 5s - loss: 0.0221 - acc: 0.987 - ETA: 5s - loss: 0.1039 - acc: 0.964 - ETA: 5s - loss: 0.0802 - acc: 0.975 - ETA: 5s - loss: 0.0725 - acc: 0.976 - ETA: 5s - loss: 0.0694 - acc: 0.978 - ETA: 5s - loss: 0.0640 - acc: 0.978 - ETA: 5s - loss: 0.0775 - acc: 0.975 - ETA: 5s - loss: 0.0760 - acc: 0.976 - ETA: 5s - loss: 0.0702 - acc: 0.978 - ETA: 5s - loss: 0.0652 - acc: 0.980 - ETA: 5s - loss: 0.0646 - acc: 0.979 - ETA: 5s - loss: 0.0639 - acc: 0.979 - ETA: 5s - loss: 0.0624 - acc: 0.980 - ETA: 5s - loss: 0.0600 - acc: 0.980 - ETA: 5s - loss: 0.0578 - acc: 0.981 - ETA: 5s - loss: 0.0564 - acc: 0.982 - ETA: 5s - loss: 0.0543 - acc: 0.982 - ETA: 5s - loss: 0.0529 - acc: 0.982 - ETA: 5s - loss: 0.0520 - acc: 0.982 - ETA: 5s - loss: 0.0516 - acc: 0.982 - ETA: 5s - loss: 0.0498 - acc: 0.983 - ETA: 5s - loss: 0.0516 - acc: 0.982 - ETA: 5s - loss: 0.0513 - acc: 0.982 - ETA: 5s - loss: 0.0500 - acc: 0.982 - ETA: 4s - loss: 0.0488 - acc: 0.983 - ETA: 4s - loss: 0.0512 - acc: 0.982 - ETA: 4s - loss: 0.0521 - acc: 0.982 - ETA: 4s - loss: 0.0548 - acc: 0.981 - ETA: 4s - loss: 0.0536 - acc: 0.981 - ETA: 4s - loss: 0.0565 - acc: 0.981 - ETA: 4s - loss: 0.0556 - acc: 0.981 - ETA: 4s - loss: 0.0545 - acc: 0.982 - ETA: 4s - loss: 0.0535 - acc: 0.982 - ETA: 4s - loss: 0.0541 - acc: 0.982 - ETA: 4s - loss: 0.0547 - acc: 0.982 - ETA: 4s - loss: 0.0544 - acc: 0.982 - ETA: 4s - loss: 0.0577 - acc: 0.982 - ETA: 4s - loss: 0.0565 - acc: 0.982 - ETA: 4s - loss: 0.0555 - acc: 0.982 - ETA: 4s - loss: 0.0554 - acc: 0.982 - ETA: 4s - loss: 0.0551 - acc: 0.982 - ETA: 4s - loss: 0.0564 - acc: 0.982 - ETA: 3s - loss: 0.0588 - acc: 0.982 - ETA: 3s - loss: 0.0589 - acc: 0.982 - ETA: 3s - loss: 0.0588 - acc: 0.982 - ETA: 3s - loss: 0.0581 - acc: 0.982 - ETA: 3s - loss: 0.0607 - acc: 0.980 - ETA: 3s - loss: 0.0598 - acc: 0.981 - ETA: 3s - loss: 0.0602 - acc: 0.980 - ETA: 3s - loss: 0.0602 - acc: 0.980 - ETA: 3s - loss: 0.0616 - acc: 0.980 - ETA: 3s - loss: 0.0606 - acc: 0.980 - ETA: 3s - loss: 0.0599 - acc: 0.980 - ETA: 3s - loss: 0.0606 - acc: 0.980 - ETA: 3s - loss: 0.0615 - acc: 0.980 - ETA: 3s - loss: 0.0626 - acc: 0.979 - ETA: 3s - loss: 0.0618 - acc: 0.980 - ETA: 3s - loss: 0.0617 - acc: 0.979 - ETA: 3s - loss: 0.0614 - acc: 0.979 - ETA: 2s - loss: 0.0611 - acc: 0.979 - ETA: 2s - loss: 0.0607 - acc: 0.979 - ETA: 2s - loss: 0.0616 - acc: 0.980 - ETA: 2s - loss: 0.0609 - acc: 0.980 - ETA: 2s - loss: 0.0603 - acc: 0.980 - ETA: 2s - loss: 0.0607 - acc: 0.980 - ETA: 2s - loss: 0.0618 - acc: 0.980 - ETA: 2s - loss: 0.0625 - acc: 0.980 - ETA: 2s - loss: 0.0626 - acc: 0.980 - ETA: 2s - loss: 0.0620 - acc: 0.980 - ETA: 2s - loss: 0.0621 - acc: 0.980 - ETA: 2s - loss: 0.0621 - acc: 0.980 - ETA: 2s - loss: 0.0616 - acc: 0.980 - ETA: 2s - loss: 0.0611 - acc: 0.980 - ETA: 2s - loss: 0.0630 - acc: 0.980 - ETA: 2s - loss: 0.0634 - acc: 0.980 - ETA: 2s - loss: 0.0630 - acc: 0.980 - ETA: 1s - loss: 0.0633 - acc: 0.980 - ETA: 1s - loss: 0.0626 - acc: 0.980 - ETA: 1s - loss: 0.0625 - acc: 0.980 - ETA: 1s - loss: 0.0622 - acc: 0.980 - ETA: 1s - loss: 0.0630 - acc: 0.980 - ETA: 1s - loss: 0.0636 - acc: 0.980 - ETA: 1s - loss: 0.0642 - acc: 0.980 - ETA: 1s - loss: 0.0643 - acc: 0.980 - ETA: 1s - loss: 0.0641 - acc: 0.980 - ETA: 1s - loss: 0.0645 - acc: 0.980 - ETA: 1s - loss: 0.0658 - acc: 0.980 - ETA: 1s - loss: 0.0652 - acc: 0.980 - ETA: 1s - loss: 0.0661 - acc: 0.979 - ETA: 1s - loss: 0.0663 - acc: 0.979 - ETA: 1s - loss: 0.0674 - acc: 0.979 - ETA: 1s - loss: 0.0670 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.979 - ETA: 1s - loss: 0.0664 - acc: 0.979 - ETA: 0s - loss: 0.0666 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0672 - acc: 0.979 - ETA: 0s - loss: 0.0667 - acc: 0.979 - ETA: 0s - loss: 0.0667 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0670 - acc: 0.979 - ETA: 0s - loss: 0.0668 - acc: 0.979 - ETA: 0s - loss: 0.0678 - acc: 0.979 - ETA: 0s - loss: 0.0674 - acc: 0.979 - ETA: 0s - loss: 0.0673 - acc: 0.979 - ETA: 0s - loss: 0.0673 - acc: 0.979 - ETA: 0s - loss: 0.0671 - acc: 0.979 - ETA: 0s - loss: 0.0671 - acc: 0.979 - ETA: 0s - loss: 0.0683 - acc: 0.979 - ETA: 0s - loss: 0.0695 - acc: 0.979 - ETA: 0s - loss: 0.0694 - acc: 0.9791Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0692 - acc: 0.9792 - val_loss: 0.8291 - val_acc: 0.8623\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0258 - acc: 1.000 - ETA: 6s - loss: 0.0255 - acc: 0.987 - ETA: 6s - loss: 0.0213 - acc: 0.985 - ETA: 5s - loss: 0.0183 - acc: 0.990 - ETA: 5s - loss: 0.0355 - acc: 0.984 - ETA: 5s - loss: 0.0412 - acc: 0.981 - ETA: 5s - loss: 0.0466 - acc: 0.978 - ETA: 5s - loss: 0.0415 - acc: 0.981 - ETA: 5s - loss: 0.0396 - acc: 0.982 - ETA: 5s - loss: 0.0370 - acc: 0.983 - ETA: 5s - loss: 0.0348 - acc: 0.985 - ETA: 5s - loss: 0.0366 - acc: 0.985 - ETA: 5s - loss: 0.0371 - acc: 0.985 - ETA: 5s - loss: 0.0353 - acc: 0.986 - ETA: 5s - loss: 0.0332 - acc: 0.987 - ETA: 5s - loss: 0.0428 - acc: 0.985 - ETA: 5s - loss: 0.0438 - acc: 0.985 - ETA: 5s - loss: 0.0422 - acc: 0.986 - ETA: 5s - loss: 0.0485 - acc: 0.985 - ETA: 5s - loss: 0.0507 - acc: 0.984 - ETA: 5s - loss: 0.0512 - acc: 0.983 - ETA: 5s - loss: 0.0497 - acc: 0.984 - ETA: 4s - loss: 0.0483 - acc: 0.985 - ETA: 4s - loss: 0.0505 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0476 - acc: 0.985 - ETA: 4s - loss: 0.0465 - acc: 0.986 - ETA: 4s - loss: 0.0454 - acc: 0.986 - ETA: 4s - loss: 0.0457 - acc: 0.985 - ETA: 4s - loss: 0.0443 - acc: 0.986 - ETA: 4s - loss: 0.0451 - acc: 0.985 - ETA: 4s - loss: 0.0458 - acc: 0.985 - ETA: 4s - loss: 0.0446 - acc: 0.986 - ETA: 4s - loss: 0.0476 - acc: 0.986 - ETA: 4s - loss: 0.0466 - acc: 0.986 - ETA: 4s - loss: 0.0464 - acc: 0.986 - ETA: 4s - loss: 0.0467 - acc: 0.986 - ETA: 4s - loss: 0.0460 - acc: 0.986 - ETA: 4s - loss: 0.0453 - acc: 0.987 - ETA: 4s - loss: 0.0467 - acc: 0.986 - ETA: 3s - loss: 0.0457 - acc: 0.987 - ETA: 3s - loss: 0.0496 - acc: 0.986 - ETA: 3s - loss: 0.0491 - acc: 0.986 - ETA: 3s - loss: 0.0484 - acc: 0.986 - ETA: 3s - loss: 0.0489 - acc: 0.986 - ETA: 3s - loss: 0.0518 - acc: 0.985 - ETA: 3s - loss: 0.0513 - acc: 0.985 - ETA: 3s - loss: 0.0536 - acc: 0.984 - ETA: 3s - loss: 0.0532 - acc: 0.984 - ETA: 3s - loss: 0.0526 - acc: 0.984 - ETA: 3s - loss: 0.0523 - acc: 0.985 - ETA: 3s - loss: 0.0523 - acc: 0.985 - ETA: 3s - loss: 0.0543 - acc: 0.984 - ETA: 3s - loss: 0.0533 - acc: 0.984 - ETA: 3s - loss: 0.0555 - acc: 0.984 - ETA: 3s - loss: 0.0565 - acc: 0.984 - ETA: 3s - loss: 0.0593 - acc: 0.983 - ETA: 3s - loss: 0.0587 - acc: 0.983 - ETA: 3s - loss: 0.0583 - acc: 0.983 - ETA: 2s - loss: 0.0575 - acc: 0.984 - ETA: 2s - loss: 0.0601 - acc: 0.983 - ETA: 2s - loss: 0.0597 - acc: 0.984 - ETA: 2s - loss: 0.0591 - acc: 0.984 - ETA: 2s - loss: 0.0594 - acc: 0.984 - ETA: 2s - loss: 0.0596 - acc: 0.983 - ETA: 2s - loss: 0.0594 - acc: 0.983 - ETA: 2s - loss: 0.0588 - acc: 0.983 - ETA: 2s - loss: 0.0600 - acc: 0.983 - ETA: 2s - loss: 0.0592 - acc: 0.983 - ETA: 2s - loss: 0.0587 - acc: 0.983 - ETA: 2s - loss: 0.0590 - acc: 0.983 - ETA: 2s - loss: 0.0587 - acc: 0.983 - ETA: 2s - loss: 0.0585 - acc: 0.983 - ETA: 2s - loss: 0.0580 - acc: 0.983 - ETA: 2s - loss: 0.0583 - acc: 0.983 - ETA: 2s - loss: 0.0581 - acc: 0.983 - ETA: 1s - loss: 0.0575 - acc: 0.983 - ETA: 1s - loss: 0.0588 - acc: 0.983 - ETA: 1s - loss: 0.0589 - acc: 0.983 - ETA: 1s - loss: 0.0598 - acc: 0.982 - ETA: 1s - loss: 0.0601 - acc: 0.982 - ETA: 1s - loss: 0.0600 - acc: 0.982 - ETA: 1s - loss: 0.0619 - acc: 0.982 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 1s - loss: 0.0610 - acc: 0.982 - ETA: 1s - loss: 0.0610 - acc: 0.982 - ETA: 1s - loss: 0.0631 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0649 - acc: 0.981 - ETA: 1s - loss: 0.0647 - acc: 0.981 - ETA: 1s - loss: 0.0641 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0635 - acc: 0.981 - ETA: 1s - loss: 0.0628 - acc: 0.982 - ETA: 0s - loss: 0.0638 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0632 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0624 - acc: 0.981 - ETA: 0s - loss: 0.0620 - acc: 0.981 - ETA: 0s - loss: 0.0617 - acc: 0.981 - ETA: 0s - loss: 0.0613 - acc: 0.981 - ETA: 0s - loss: 0.0620 - acc: 0.981 - ETA: 0s - loss: 0.0617 - acc: 0.981 - ETA: 0s - loss: 0.0622 - acc: 0.981 - ETA: 0s - loss: 0.0626 - acc: 0.981 - ETA: 0s - loss: 0.0623 - acc: 0.981 - ETA: 0s - loss: 0.0618 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0630 - acc: 0.981 - ETA: 0s - loss: 0.0627 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.9817Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0626 - acc: 0.9817 - val_loss: 0.8523 - val_acc: 0.8479\n",
      "Epoch 15/20\n",
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0046 - acc: 1.000 - ETA: 5s - loss: 0.0417 - acc: 0.975 - ETA: 5s - loss: 0.0360 - acc: 0.978 - ETA: 5s - loss: 0.0319 - acc: 0.980 - ETA: 5s - loss: 0.0396 - acc: 0.980 - ETA: 5s - loss: 0.0328 - acc: 0.984 - ETA: 5s - loss: 0.0353 - acc: 0.984 - ETA: 5s - loss: 0.0483 - acc: 0.981 - ETA: 5s - loss: 0.0455 - acc: 0.982 - ETA: 5s - loss: 0.0421 - acc: 0.983 - ETA: 5s - loss: 0.0384 - acc: 0.985 - ETA: 5s - loss: 0.0445 - acc: 0.985 - ETA: 5s - loss: 0.0499 - acc: 0.983 - ETA: 5s - loss: 0.0528 - acc: 0.983 - ETA: 5s - loss: 0.0500 - acc: 0.984 - ETA: 5s - loss: 0.0477 - acc: 0.985 - ETA: 5s - loss: 0.0495 - acc: 0.983 - ETA: 5s - loss: 0.0509 - acc: 0.983 - ETA: 5s - loss: 0.0495 - acc: 0.984 - ETA: 5s - loss: 0.0527 - acc: 0.984 - ETA: 5s - loss: 0.0512 - acc: 0.984 - ETA: 5s - loss: 0.0506 - acc: 0.985 - ETA: 4s - loss: 0.0485 - acc: 0.985 - ETA: 4s - loss: 0.0514 - acc: 0.984 - ETA: 4s - loss: 0.0498 - acc: 0.984 - ETA: 4s - loss: 0.0536 - acc: 0.982 - ETA: 4s - loss: 0.0527 - acc: 0.983 - ETA: 4s - loss: 0.0508 - acc: 0.984 - ETA: 4s - loss: 0.0504 - acc: 0.984 - ETA: 4s - loss: 0.0502 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0506 - acc: 0.983 - ETA: 4s - loss: 0.0507 - acc: 0.983 - ETA: 4s - loss: 0.0494 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0491 - acc: 0.984 - ETA: 4s - loss: 0.0486 - acc: 0.983 - ETA: 4s - loss: 0.0519 - acc: 0.982 - ETA: 4s - loss: 0.0508 - acc: 0.983 - ETA: 3s - loss: 0.0502 - acc: 0.983 - ETA: 3s - loss: 0.0498 - acc: 0.983 - ETA: 3s - loss: 0.0487 - acc: 0.983 - ETA: 3s - loss: 0.0479 - acc: 0.983 - ETA: 3s - loss: 0.0475 - acc: 0.983 - ETA: 3s - loss: 0.0469 - acc: 0.984 - ETA: 3s - loss: 0.0463 - acc: 0.984 - ETA: 3s - loss: 0.0458 - acc: 0.984 - ETA: 3s - loss: 0.0451 - acc: 0.985 - ETA: 3s - loss: 0.0449 - acc: 0.985 - ETA: 3s - loss: 0.0445 - acc: 0.985 - ETA: 3s - loss: 0.0438 - acc: 0.985 - ETA: 3s - loss: 0.0431 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.985 - ETA: 3s - loss: 0.0425 - acc: 0.986 - ETA: 3s - loss: 0.0421 - acc: 0.986 - ETA: 3s - loss: 0.0414 - acc: 0.986 - ETA: 3s - loss: 0.0430 - acc: 0.986 - ETA: 2s - loss: 0.0424 - acc: 0.986 - ETA: 2s - loss: 0.0431 - acc: 0.986 - ETA: 2s - loss: 0.0427 - acc: 0.986 - ETA: 2s - loss: 0.0421 - acc: 0.987 - ETA: 2s - loss: 0.0429 - acc: 0.986 - ETA: 2s - loss: 0.0433 - acc: 0.986 - ETA: 2s - loss: 0.0434 - acc: 0.986 - ETA: 2s - loss: 0.0433 - acc: 0.986 - ETA: 2s - loss: 0.0430 - acc: 0.986 - ETA: 2s - loss: 0.0425 - acc: 0.986 - ETA: 2s - loss: 0.0451 - acc: 0.985 - ETA: 2s - loss: 0.0448 - acc: 0.986 - ETA: 2s - loss: 0.0445 - acc: 0.986 - ETA: 2s - loss: 0.0450 - acc: 0.986 - ETA: 2s - loss: 0.0451 - acc: 0.985 - ETA: 2s - loss: 0.0455 - acc: 0.985 - ETA: 2s - loss: 0.0452 - acc: 0.985 - ETA: 2s - loss: 0.0448 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0458 - acc: 0.985 - ETA: 1s - loss: 0.0463 - acc: 0.985 - ETA: 1s - loss: 0.0458 - acc: 0.985 - ETA: 1s - loss: 0.0453 - acc: 0.985 - ETA: 1s - loss: 0.0457 - acc: 0.985 - ETA: 1s - loss: 0.0457 - acc: 0.985 - ETA: 1s - loss: 0.0455 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0480 - acc: 0.985 - ETA: 1s - loss: 0.0476 - acc: 0.985 - ETA: 1s - loss: 0.0487 - acc: 0.985 - ETA: 1s - loss: 0.0489 - acc: 0.985 - ETA: 1s - loss: 0.0493 - acc: 0.984 - ETA: 1s - loss: 0.0492 - acc: 0.984 - ETA: 1s - loss: 0.0496 - acc: 0.984 - ETA: 1s - loss: 0.0494 - acc: 0.984 - ETA: 0s - loss: 0.0503 - acc: 0.984 - ETA: 0s - loss: 0.0510 - acc: 0.984 - ETA: 0s - loss: 0.0509 - acc: 0.984 - ETA: 0s - loss: 0.0514 - acc: 0.984 - ETA: 0s - loss: 0.0511 - acc: 0.984 - ETA: 0s - loss: 0.0511 - acc: 0.984 - ETA: 0s - loss: 0.0509 - acc: 0.984 - ETA: 0s - loss: 0.0516 - acc: 0.984 - ETA: 0s - loss: 0.0516 - acc: 0.984 - ETA: 0s - loss: 0.0514 - acc: 0.984 - ETA: 0s - loss: 0.0518 - acc: 0.984 - ETA: 0s - loss: 0.0525 - acc: 0.983 - ETA: 0s - loss: 0.0526 - acc: 0.983 - ETA: 0s - loss: 0.0524 - acc: 0.983 - ETA: 0s - loss: 0.0523 - acc: 0.983 - ETA: 0s - loss: 0.0519 - acc: 0.983 - ETA: 0s - loss: 0.0517 - acc: 0.9838Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0530 - acc: 0.9837 - val_loss: 0.8981 - val_acc: 0.8467\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0989 - acc: 0.950 - ETA: 6s - loss: 0.0273 - acc: 0.987 - ETA: 6s - loss: 0.0496 - acc: 0.978 - ETA: 6s - loss: 0.0381 - acc: 0.985 - ETA: 6s - loss: 0.0318 - acc: 0.988 - ETA: 6s - loss: 0.0261 - acc: 0.990 - ETA: 6s - loss: 0.0239 - acc: 0.992 - ETA: 6s - loss: 0.0220 - acc: 0.993 - ETA: 5s - loss: 0.0197 - acc: 0.994 - ETA: 5s - loss: 0.0186 - acc: 0.994 - ETA: 5s - loss: 0.0219 - acc: 0.993 - ETA: 5s - loss: 0.0278 - acc: 0.991 - ETA: 5s - loss: 0.0265 - acc: 0.991 - ETA: 5s - loss: 0.0267 - acc: 0.992 - ETA: 5s - loss: 0.0287 - acc: 0.991 - ETA: 5s - loss: 0.0305 - acc: 0.991 - ETA: 5s - loss: 0.0303 - acc: 0.991 - ETA: 5s - loss: 0.0316 - acc: 0.991 - ETA: 5s - loss: 0.0312 - acc: 0.990 - ETA: 5s - loss: 0.0304 - acc: 0.991 - ETA: 5s - loss: 0.0318 - acc: 0.990 - ETA: 5s - loss: 0.0318 - acc: 0.989 - ETA: 5s - loss: 0.0308 - acc: 0.990 - ETA: 5s - loss: 0.0365 - acc: 0.987 - ETA: 4s - loss: 0.0368 - acc: 0.987 - ETA: 4s - loss: 0.0364 - acc: 0.988 - ETA: 4s - loss: 0.0368 - acc: 0.987 - ETA: 4s - loss: 0.0380 - acc: 0.987 - ETA: 4s - loss: 0.0392 - acc: 0.986 - ETA: 4s - loss: 0.0387 - acc: 0.986 - ETA: 4s - loss: 0.0406 - acc: 0.986 - ETA: 4s - loss: 0.0400 - acc: 0.986 - ETA: 4s - loss: 0.0391 - acc: 0.986 - ETA: 4s - loss: 0.0383 - acc: 0.987 - ETA: 4s - loss: 0.0386 - acc: 0.986 - ETA: 4s - loss: 0.0380 - acc: 0.986 - ETA: 4s - loss: 0.0371 - acc: 0.987 - ETA: 4s - loss: 0.0372 - acc: 0.987 - ETA: 4s - loss: 0.0375 - acc: 0.987 - ETA: 4s - loss: 0.0397 - acc: 0.986 - ETA: 4s - loss: 0.0388 - acc: 0.986 - ETA: 3s - loss: 0.0382 - acc: 0.986 - ETA: 3s - loss: 0.0375 - acc: 0.987 - ETA: 3s - loss: 0.0381 - acc: 0.986 - ETA: 3s - loss: 0.0384 - acc: 0.986 - ETA: 3s - loss: 0.0380 - acc: 0.986 - ETA: 3s - loss: 0.0375 - acc: 0.987 - ETA: 3s - loss: 0.0372 - acc: 0.987 - ETA: 3s - loss: 0.0403 - acc: 0.986 - ETA: 3s - loss: 0.0431 - acc: 0.986 - ETA: 3s - loss: 0.0426 - acc: 0.986 - ETA: 3s - loss: 0.0440 - acc: 0.986 - ETA: 3s - loss: 0.0434 - acc: 0.986 - ETA: 3s - loss: 0.0432 - acc: 0.986 - ETA: 3s - loss: 0.0428 - acc: 0.986 - ETA: 3s - loss: 0.0460 - acc: 0.986 - ETA: 3s - loss: 0.0463 - acc: 0.985 - ETA: 3s - loss: 0.0456 - acc: 0.986 - ETA: 2s - loss: 0.0472 - acc: 0.985 - ETA: 2s - loss: 0.0465 - acc: 0.986 - ETA: 2s - loss: 0.0460 - acc: 0.986 - ETA: 2s - loss: 0.0460 - acc: 0.985 - ETA: 2s - loss: 0.0482 - acc: 0.985 - ETA: 2s - loss: 0.0484 - acc: 0.985 - ETA: 2s - loss: 0.0483 - acc: 0.985 - ETA: 2s - loss: 0.0476 - acc: 0.985 - ETA: 2s - loss: 0.0486 - acc: 0.985 - ETA: 2s - loss: 0.0489 - acc: 0.985 - ETA: 2s - loss: 0.0488 - acc: 0.985 - ETA: 2s - loss: 0.0487 - acc: 0.985 - ETA: 2s - loss: 0.0480 - acc: 0.985 - ETA: 2s - loss: 0.0475 - acc: 0.985 - ETA: 2s - loss: 0.0478 - acc: 0.985 - ETA: 2s - loss: 0.0473 - acc: 0.985 - ETA: 2s - loss: 0.0481 - acc: 0.985 - ETA: 2s - loss: 0.0475 - acc: 0.985 - ETA: 1s - loss: 0.0469 - acc: 0.985 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.986 - ETA: 1s - loss: 0.0458 - acc: 0.986 - ETA: 1s - loss: 0.0453 - acc: 0.986 - ETA: 1s - loss: 0.0456 - acc: 0.986 - ETA: 1s - loss: 0.0455 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0455 - acc: 0.986 - ETA: 1s - loss: 0.0454 - acc: 0.986 - ETA: 1s - loss: 0.0456 - acc: 0.986 - ETA: 1s - loss: 0.0452 - acc: 0.986 - ETA: 1s - loss: 0.0467 - acc: 0.986 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0464 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.985 - ETA: 1s - loss: 0.0459 - acc: 0.986 - ETA: 1s - loss: 0.0454 - acc: 0.986 - ETA: 0s - loss: 0.0458 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - ETA: 0s - loss: 0.0459 - acc: 0.986 - ETA: 0s - loss: 0.0465 - acc: 0.985 - ETA: 0s - loss: 0.0464 - acc: 0.985 - ETA: 0s - loss: 0.0462 - acc: 0.985 - ETA: 0s - loss: 0.0469 - acc: 0.985 - ETA: 0s - loss: 0.0468 - acc: 0.985 - ETA: 0s - loss: 0.0465 - acc: 0.985 - ETA: 0s - loss: 0.0461 - acc: 0.986 - ETA: 0s - loss: 0.0463 - acc: 0.985 - ETA: 0s - loss: 0.0480 - acc: 0.985 - ETA: 0s - loss: 0.0492 - acc: 0.985 - ETA: 0s - loss: 0.0488 - acc: 0.985 - ETA: 0s - loss: 0.0489 - acc: 0.985 - ETA: 0s - loss: 0.0495 - acc: 0.984 - ETA: 0s - loss: 0.0504 - acc: 0.9847Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0504 - acc: 0.9847 - val_loss: 0.8671 - val_acc: 0.8539\n",
      "Epoch 17/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.0200 - acc: 1.000 - ETA: 7s - loss: 0.0132 - acc: 1.000 - ETA: 7s - loss: 0.0109 - acc: 1.000 - ETA: 6s - loss: 0.0081 - acc: 1.000 - ETA: 6s - loss: 0.0081 - acc: 1.000 - ETA: 6s - loss: 0.0070 - acc: 1.000 - ETA: 6s - loss: 0.0088 - acc: 1.000 - ETA: 6s - loss: 0.0111 - acc: 0.997 - ETA: 6s - loss: 0.0103 - acc: 0.997 - ETA: 6s - loss: 0.0165 - acc: 0.996 - ETA: 5s - loss: 0.0154 - acc: 0.996 - ETA: 5s - loss: 0.0199 - acc: 0.995 - ETA: 5s - loss: 0.0216 - acc: 0.995 - ETA: 5s - loss: 0.0215 - acc: 0.994 - ETA: 5s - loss: 0.0206 - acc: 0.995 - ETA: 5s - loss: 0.0203 - acc: 0.995 - ETA: 5s - loss: 0.0225 - acc: 0.994 - ETA: 5s - loss: 0.0212 - acc: 0.995 - ETA: 5s - loss: 0.0237 - acc: 0.994 - ETA: 5s - loss: 0.0236 - acc: 0.994 - ETA: 5s - loss: 0.0228 - acc: 0.995 - ETA: 5s - loss: 0.0222 - acc: 0.995 - ETA: 5s - loss: 0.0213 - acc: 0.995 - ETA: 5s - loss: 0.0208 - acc: 0.995 - ETA: 4s - loss: 0.0213 - acc: 0.995 - ETA: 4s - loss: 0.0242 - acc: 0.994 - ETA: 4s - loss: 0.0238 - acc: 0.994 - ETA: 4s - loss: 0.0239 - acc: 0.994 - ETA: 4s - loss: 0.0231 - acc: 0.994 - ETA: 4s - loss: 0.0229 - acc: 0.994 - ETA: 4s - loss: 0.0237 - acc: 0.994 - ETA: 4s - loss: 0.0256 - acc: 0.994 - ETA: 4s - loss: 0.0263 - acc: 0.993 - ETA: 4s - loss: 0.0289 - acc: 0.992 - ETA: 4s - loss: 0.0312 - acc: 0.992 - ETA: 4s - loss: 0.0305 - acc: 0.992 - ETA: 4s - loss: 0.0342 - acc: 0.991 - ETA: 4s - loss: 0.0348 - acc: 0.991 - ETA: 4s - loss: 0.0354 - acc: 0.990 - ETA: 4s - loss: 0.0349 - acc: 0.991 - ETA: 4s - loss: 0.0342 - acc: 0.991 - ETA: 3s - loss: 0.0340 - acc: 0.991 - ETA: 3s - loss: 0.0336 - acc: 0.991 - ETA: 3s - loss: 0.0330 - acc: 0.991 - ETA: 3s - loss: 0.0327 - acc: 0.991 - ETA: 3s - loss: 0.0326 - acc: 0.991 - ETA: 3s - loss: 0.0325 - acc: 0.991 - ETA: 3s - loss: 0.0325 - acc: 0.991 - ETA: 3s - loss: 0.0324 - acc: 0.991 - ETA: 3s - loss: 0.0318 - acc: 0.991 - ETA: 3s - loss: 0.0334 - acc: 0.991 - ETA: 3s - loss: 0.0330 - acc: 0.991 - ETA: 3s - loss: 0.0335 - acc: 0.991 - ETA: 3s - loss: 0.0350 - acc: 0.990 - ETA: 3s - loss: 0.0359 - acc: 0.990 - ETA: 3s - loss: 0.0356 - acc: 0.990 - ETA: 3s - loss: 0.0351 - acc: 0.991 - ETA: 3s - loss: 0.0351 - acc: 0.990 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0343 - acc: 0.991 - ETA: 2s - loss: 0.0354 - acc: 0.990 - ETA: 2s - loss: 0.0359 - acc: 0.990 - ETA: 2s - loss: 0.0355 - acc: 0.990 - ETA: 2s - loss: 0.0369 - acc: 0.990 - ETA: 2s - loss: 0.0365 - acc: 0.990 - ETA: 2s - loss: 0.0360 - acc: 0.990 - ETA: 2s - loss: 0.0356 - acc: 0.990 - ETA: 2s - loss: 0.0351 - acc: 0.990 - ETA: 2s - loss: 0.0348 - acc: 0.990 - ETA: 2s - loss: 0.0345 - acc: 0.991 - ETA: 2s - loss: 0.0340 - acc: 0.991 - ETA: 2s - loss: 0.0345 - acc: 0.991 - ETA: 2s - loss: 0.0343 - acc: 0.991 - ETA: 2s - loss: 0.0368 - acc: 0.990 - ETA: 2s - loss: 0.0388 - acc: 0.990 - ETA: 2s - loss: 0.0392 - acc: 0.990 - ETA: 1s - loss: 0.0389 - acc: 0.990 - ETA: 1s - loss: 0.0391 - acc: 0.990 - ETA: 1s - loss: 0.0386 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0396 - acc: 0.990 - ETA: 1s - loss: 0.0391 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0395 - acc: 0.990 - ETA: 1s - loss: 0.0392 - acc: 0.990 - ETA: 1s - loss: 0.0398 - acc: 0.990 - ETA: 1s - loss: 0.0397 - acc: 0.989 - ETA: 1s - loss: 0.0395 - acc: 0.989 - ETA: 1s - loss: 0.0405 - acc: 0.989 - ETA: 1s - loss: 0.0412 - acc: 0.989 - ETA: 1s - loss: 0.0410 - acc: 0.989 - ETA: 1s - loss: 0.0413 - acc: 0.989 - ETA: 1s - loss: 0.0414 - acc: 0.989 - ETA: 1s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0419 - acc: 0.989 - ETA: 0s - loss: 0.0415 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0417 - acc: 0.989 - ETA: 0s - loss: 0.0413 - acc: 0.989 - ETA: 0s - loss: 0.0411 - acc: 0.989 - ETA: 0s - loss: 0.0418 - acc: 0.989 - ETA: 0s - loss: 0.0416 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0410 - acc: 0.989 - ETA: 0s - loss: 0.0408 - acc: 0.989 - ETA: 0s - loss: 0.0404 - acc: 0.989 - ETA: 0s - loss: 0.0402 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.989 - ETA: 0s - loss: 0.0424 - acc: 0.988 - ETA: 0s - loss: 0.0422 - acc: 0.988 - ETA: 0s - loss: 0.0420 - acc: 0.9890Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0419 - acc: 0.9891 - val_loss: 0.8921 - val_acc: 0.8551\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 6s - loss: 0.0011 - acc: 1.000 - ETA: 5s - loss: 0.0095 - acc: 1.000 - ETA: 5s - loss: 0.0198 - acc: 0.992 - ETA: 5s - loss: 0.0151 - acc: 0.995 - ETA: 5s - loss: 0.0221 - acc: 0.992 - ETA: 5s - loss: 0.0199 - acc: 0.993 - ETA: 5s - loss: 0.0185 - acc: 0.994 - ETA: 5s - loss: 0.0175 - acc: 0.995 - ETA: 5s - loss: 0.0201 - acc: 0.994 - ETA: 5s - loss: 0.0199 - acc: 0.994 - ETA: 5s - loss: 0.0245 - acc: 0.991 - ETA: 5s - loss: 0.0234 - acc: 0.992 - ETA: 5s - loss: 0.0228 - acc: 0.993 - ETA: 5s - loss: 0.0215 - acc: 0.993 - ETA: 5s - loss: 0.0203 - acc: 0.994 - ETA: 5s - loss: 0.0216 - acc: 0.993 - ETA: 5s - loss: 0.0213 - acc: 0.993 - ETA: 5s - loss: 0.0208 - acc: 0.994 - ETA: 5s - loss: 0.0210 - acc: 0.993 - ETA: 5s - loss: 0.0204 - acc: 0.994 - ETA: 5s - loss: 0.0214 - acc: 0.993 - ETA: 4s - loss: 0.0234 - acc: 0.993 - ETA: 4s - loss: 0.0291 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.990 - ETA: 4s - loss: 0.0307 - acc: 0.990 - ETA: 4s - loss: 0.0297 - acc: 0.990 - ETA: 4s - loss: 0.0290 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.990 - ETA: 4s - loss: 0.0307 - acc: 0.991 - ETA: 4s - loss: 0.0299 - acc: 0.991 - ETA: 4s - loss: 0.0300 - acc: 0.990 - ETA: 4s - loss: 0.0304 - acc: 0.990 - ETA: 4s - loss: 0.0317 - acc: 0.989 - ETA: 4s - loss: 0.0328 - acc: 0.988 - ETA: 4s - loss: 0.0325 - acc: 0.988 - ETA: 4s - loss: 0.0322 - acc: 0.988 - ETA: 4s - loss: 0.0327 - acc: 0.988 - ETA: 4s - loss: 0.0321 - acc: 0.988 - ETA: 4s - loss: 0.0318 - acc: 0.988 - ETA: 3s - loss: 0.0313 - acc: 0.989 - ETA: 3s - loss: 0.0307 - acc: 0.989 - ETA: 3s - loss: 0.0307 - acc: 0.989 - ETA: 3s - loss: 0.0301 - acc: 0.989 - ETA: 3s - loss: 0.0294 - acc: 0.989 - ETA: 3s - loss: 0.0288 - acc: 0.989 - ETA: 3s - loss: 0.0283 - acc: 0.990 - ETA: 3s - loss: 0.0288 - acc: 0.989 - ETA: 3s - loss: 0.0301 - acc: 0.989 - ETA: 3s - loss: 0.0295 - acc: 0.989 - ETA: 3s - loss: 0.0318 - acc: 0.989 - ETA: 3s - loss: 0.0320 - acc: 0.989 - ETA: 3s - loss: 0.0316 - acc: 0.989 - ETA: 3s - loss: 0.0312 - acc: 0.989 - ETA: 3s - loss: 0.0318 - acc: 0.989 - ETA: 3s - loss: 0.0328 - acc: 0.989 - ETA: 3s - loss: 0.0325 - acc: 0.989 - ETA: 3s - loss: 0.0319 - acc: 0.989 - ETA: 3s - loss: 0.0315 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0311 - acc: 0.989 - ETA: 2s - loss: 0.0322 - acc: 0.989 - ETA: 2s - loss: 0.0319 - acc: 0.989 - ETA: 2s - loss: 0.0314 - acc: 0.989 - ETA: 2s - loss: 0.0314 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0308 - acc: 0.989 - ETA: 2s - loss: 0.0312 - acc: 0.989 - ETA: 2s - loss: 0.0309 - acc: 0.989 - ETA: 2s - loss: 0.0306 - acc: 0.990 - ETA: 2s - loss: 0.0303 - acc: 0.990 - ETA: 2s - loss: 0.0300 - acc: 0.990 - ETA: 2s - loss: 0.0309 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.989 - ETA: 2s - loss: 0.0328 - acc: 0.989 - ETA: 2s - loss: 0.0352 - acc: 0.989 - ETA: 2s - loss: 0.0351 - acc: 0.989 - ETA: 1s - loss: 0.0350 - acc: 0.989 - ETA: 1s - loss: 0.0378 - acc: 0.989 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0389 - acc: 0.988 - ETA: 1s - loss: 0.0385 - acc: 0.988 - ETA: 1s - loss: 0.0382 - acc: 0.988 - ETA: 1s - loss: 0.0379 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.989 - ETA: 1s - loss: 0.0373 - acc: 0.988 - ETA: 1s - loss: 0.0370 - acc: 0.989 - ETA: 1s - loss: 0.0377 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 1s - loss: 0.0376 - acc: 0.988 - ETA: 1s - loss: 0.0377 - acc: 0.988 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 0s - loss: 0.0374 - acc: 0.988 - ETA: 0s - loss: 0.0373 - acc: 0.988 - ETA: 0s - loss: 0.0382 - acc: 0.987 - ETA: 0s - loss: 0.0382 - acc: 0.987 - ETA: 0s - loss: 0.0378 - acc: 0.987 - ETA: 0s - loss: 0.0377 - acc: 0.987 - ETA: 0s - loss: 0.0374 - acc: 0.987 - ETA: 0s - loss: 0.0372 - acc: 0.987 - ETA: 0s - loss: 0.0370 - acc: 0.987 - ETA: 0s - loss: 0.0370 - acc: 0.987 - ETA: 0s - loss: 0.0368 - acc: 0.988 - ETA: 0s - loss: 0.0368 - acc: 0.988 - ETA: 0s - loss: 0.0367 - acc: 0.988 - ETA: 0s - loss: 0.0365 - acc: 0.988 - ETA: 0s - loss: 0.0363 - acc: 0.988 - ETA: 0s - loss: 0.0365 - acc: 0.988 - ETA: 0s - loss: 0.0368 - acc: 0.9879Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0369 - acc: 0.9879 - val_loss: 0.9159 - val_acc: 0.8479\n",
      "Epoch 19/20\n",
      "6620/6680 [============================>.] - ETA: 5s - loss: 0.0123 - acc: 1.000 - ETA: 7s - loss: 0.0471 - acc: 0.975 - ETA: 6s - loss: 0.0320 - acc: 0.985 - ETA: 6s - loss: 0.0232 - acc: 0.990 - ETA: 6s - loss: 0.0192 - acc: 0.992 - ETA: 6s - loss: 0.0318 - acc: 0.990 - ETA: 6s - loss: 0.0280 - acc: 0.992 - ETA: 6s - loss: 0.0264 - acc: 0.993 - ETA: 5s - loss: 0.0440 - acc: 0.990 - ETA: 5s - loss: 0.0397 - acc: 0.991 - ETA: 5s - loss: 0.0364 - acc: 0.991 - ETA: 5s - loss: 0.0395 - acc: 0.989 - ETA: 5s - loss: 0.0367 - acc: 0.990 - ETA: 5s - loss: 0.0356 - acc: 0.990 - ETA: 5s - loss: 0.0336 - acc: 0.990 - ETA: 5s - loss: 0.0317 - acc: 0.991 - ETA: 5s - loss: 0.0315 - acc: 0.990 - ETA: 5s - loss: 0.0298 - acc: 0.991 - ETA: 5s - loss: 0.0315 - acc: 0.990 - ETA: 5s - loss: 0.0300 - acc: 0.990 - ETA: 5s - loss: 0.0288 - acc: 0.991 - ETA: 5s - loss: 0.0301 - acc: 0.990 - ETA: 5s - loss: 0.0294 - acc: 0.991 - ETA: 4s - loss: 0.0312 - acc: 0.990 - ETA: 4s - loss: 0.0302 - acc: 0.991 - ETA: 4s - loss: 0.0297 - acc: 0.991 - ETA: 4s - loss: 0.0292 - acc: 0.991 - ETA: 4s - loss: 0.0343 - acc: 0.990 - ETA: 4s - loss: 0.0337 - acc: 0.990 - ETA: 4s - loss: 0.0345 - acc: 0.990 - ETA: 4s - loss: 0.0335 - acc: 0.990 - ETA: 4s - loss: 0.0325 - acc: 0.991 - ETA: 4s - loss: 0.0317 - acc: 0.991 - ETA: 4s - loss: 0.0315 - acc: 0.991 - ETA: 4s - loss: 0.0307 - acc: 0.991 - ETA: 4s - loss: 0.0301 - acc: 0.991 - ETA: 4s - loss: 0.0299 - acc: 0.991 - ETA: 4s - loss: 0.0292 - acc: 0.991 - ETA: 4s - loss: 0.0287 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0281 - acc: 0.991 - ETA: 3s - loss: 0.0283 - acc: 0.991 - ETA: 3s - loss: 0.0280 - acc: 0.991 - ETA: 3s - loss: 0.0274 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0276 - acc: 0.991 - ETA: 3s - loss: 0.0276 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.992 - ETA: 3s - loss: 0.0269 - acc: 0.992 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 3s - loss: 0.0289 - acc: 0.991 - ETA: 3s - loss: 0.0284 - acc: 0.991 - ETA: 3s - loss: 0.0279 - acc: 0.991 - ETA: 3s - loss: 0.0293 - acc: 0.991 - ETA: 3s - loss: 0.0290 - acc: 0.991 - ETA: 3s - loss: 0.0298 - acc: 0.991 - ETA: 3s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 2s - loss: 0.0316 - acc: 0.990 - ETA: 2s - loss: 0.0316 - acc: 0.990 - ETA: 2s - loss: 0.0320 - acc: 0.990 - ETA: 2s - loss: 0.0321 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0313 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0312 - acc: 0.990 - ETA: 2s - loss: 0.0314 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0314 - acc: 0.990 - ETA: 2s - loss: 0.0317 - acc: 0.990 - ETA: 2s - loss: 0.0315 - acc: 0.990 - ETA: 2s - loss: 0.0325 - acc: 0.989 - ETA: 2s - loss: 0.0322 - acc: 0.989 - ETA: 2s - loss: 0.0339 - acc: 0.989 - ETA: 2s - loss: 0.0335 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.990 - ETA: 1s - loss: 0.0329 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0332 - acc: 0.989 - ETA: 1s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0330 - acc: 0.989 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0348 - acc: 0.988 - ETA: 1s - loss: 0.0345 - acc: 0.988 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.989 - ETA: 1s - loss: 0.0343 - acc: 0.989 - ETA: 1s - loss: 0.0342 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.989 - ETA: 1s - loss: 0.0334 - acc: 0.989 - ETA: 1s - loss: 0.0335 - acc: 0.989 - ETA: 0s - loss: 0.0333 - acc: 0.989 - ETA: 0s - loss: 0.0343 - acc: 0.989 - ETA: 0s - loss: 0.0340 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0334 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0332 - acc: 0.989 - ETA: 0s - loss: 0.0329 - acc: 0.989 - ETA: 0s - loss: 0.0328 - acc: 0.989 - ETA: 0s - loss: 0.0325 - acc: 0.989 - ETA: 0s - loss: 0.0323 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0330 - acc: 0.989 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0329 - acc: 0.9897Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0327 - acc: 0.9898 - val_loss: 0.9622 - val_acc: 0.8431\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 6s - loss: 0.0108 - acc: 1.000 - ETA: 7s - loss: 0.0286 - acc: 0.987 - ETA: 6s - loss: 0.0372 - acc: 0.985 - ETA: 7s - loss: 0.0302 - acc: 0.988 - ETA: 6s - loss: 0.0237 - acc: 0.991 - ETA: 6s - loss: 0.0235 - acc: 0.993 - ETA: 6s - loss: 0.0199 - acc: 0.994 - ETA: 6s - loss: 0.0181 - acc: 0.995 - ETA: 6s - loss: 0.0213 - acc: 0.993 - ETA: 6s - loss: 0.0203 - acc: 0.994 - ETA: 6s - loss: 0.0190 - acc: 0.995 - ETA: 5s - loss: 0.0174 - acc: 0.995 - ETA: 5s - loss: 0.0165 - acc: 0.995 - ETA: 5s - loss: 0.0183 - acc: 0.994 - ETA: 5s - loss: 0.0181 - acc: 0.995 - ETA: 5s - loss: 0.0195 - acc: 0.994 - ETA: 5s - loss: 0.0214 - acc: 0.993 - ETA: 5s - loss: 0.0202 - acc: 0.994 - ETA: 5s - loss: 0.0200 - acc: 0.994 - ETA: 5s - loss: 0.0195 - acc: 0.994 - ETA: 5s - loss: 0.0192 - acc: 0.995 - ETA: 5s - loss: 0.0186 - acc: 0.995 - ETA: 5s - loss: 0.0180 - acc: 0.995 - ETA: 5s - loss: 0.0215 - acc: 0.994 - ETA: 4s - loss: 0.0217 - acc: 0.993 - ETA: 4s - loss: 0.0211 - acc: 0.994 - ETA: 4s - loss: 0.0222 - acc: 0.993 - ETA: 4s - loss: 0.0219 - acc: 0.993 - ETA: 4s - loss: 0.0244 - acc: 0.992 - ETA: 4s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0251 - acc: 0.992 - ETA: 4s - loss: 0.0259 - acc: 0.991 - ETA: 4s - loss: 0.0253 - acc: 0.992 - ETA: 4s - loss: 0.0272 - acc: 0.991 - ETA: 4s - loss: 0.0289 - acc: 0.991 - ETA: 4s - loss: 0.0286 - acc: 0.991 - ETA: 4s - loss: 0.0280 - acc: 0.991 - ETA: 4s - loss: 0.0279 - acc: 0.991 - ETA: 4s - loss: 0.0294 - acc: 0.991 - ETA: 4s - loss: 0.0293 - acc: 0.991 - ETA: 4s - loss: 0.0298 - acc: 0.990 - ETA: 3s - loss: 0.0293 - acc: 0.991 - ETA: 3s - loss: 0.0297 - acc: 0.990 - ETA: 3s - loss: 0.0291 - acc: 0.991 - ETA: 3s - loss: 0.0286 - acc: 0.991 - ETA: 3s - loss: 0.0280 - acc: 0.991 - ETA: 3s - loss: 0.0277 - acc: 0.991 - ETA: 3s - loss: 0.0271 - acc: 0.991 - ETA: 3s - loss: 0.0266 - acc: 0.992 - ETA: 3s - loss: 0.0263 - acc: 0.992 - ETA: 3s - loss: 0.0258 - acc: 0.992 - ETA: 3s - loss: 0.0264 - acc: 0.992 - ETA: 3s - loss: 0.0259 - acc: 0.992 - ETA: 3s - loss: 0.0260 - acc: 0.992 - ETA: 3s - loss: 0.0258 - acc: 0.992 - ETA: 3s - loss: 0.0255 - acc: 0.992 - ETA: 3s - loss: 0.0253 - acc: 0.992 - ETA: 3s - loss: 0.0250 - acc: 0.992 - ETA: 3s - loss: 0.0273 - acc: 0.992 - ETA: 2s - loss: 0.0275 - acc: 0.991 - ETA: 2s - loss: 0.0271 - acc: 0.991 - ETA: 2s - loss: 0.0268 - acc: 0.992 - ETA: 2s - loss: 0.0265 - acc: 0.992 - ETA: 2s - loss: 0.0263 - acc: 0.992 - ETA: 2s - loss: 0.0261 - acc: 0.992 - ETA: 2s - loss: 0.0258 - acc: 0.992 - ETA: 2s - loss: 0.0255 - acc: 0.992 - ETA: 2s - loss: 0.0253 - acc: 0.992 - ETA: 2s - loss: 0.0249 - acc: 0.992 - ETA: 2s - loss: 0.0247 - acc: 0.993 - ETA: 2s - loss: 0.0244 - acc: 0.993 - ETA: 2s - loss: 0.0245 - acc: 0.993 - ETA: 2s - loss: 0.0242 - acc: 0.993 - ETA: 2s - loss: 0.0240 - acc: 0.993 - ETA: 2s - loss: 0.0238 - acc: 0.993 - ETA: 2s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0260 - acc: 0.992 - ETA: 1s - loss: 0.0258 - acc: 0.992 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0256 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 1s - loss: 0.0247 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.992 - ETA: 1s - loss: 0.0249 - acc: 0.992 - ETA: 1s - loss: 0.0248 - acc: 0.992 - ETA: 0s - loss: 0.0246 - acc: 0.992 - ETA: 0s - loss: 0.0256 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.992 - ETA: 0s - loss: 0.0262 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0295 - acc: 0.991 - ETA: 0s - loss: 0.0293 - acc: 0.992 - ETA: 0s - loss: 0.0301 - acc: 0.991 - ETA: 0s - loss: 0.0298 - acc: 0.992 - ETA: 0s - loss: 0.0310 - acc: 0.991 - ETA: 0s - loss: 0.0308 - acc: 0.991 - ETA: 0s - loss: 0.0305 - acc: 0.991 - ETA: 0s - loss: 0.0303 - acc: 0.991 - ETA: 0s - loss: 0.0300 - acc: 0.9919Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.0299 - acc: 0.9919 - val_loss: 0.9197 - val_acc: 0.8443\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_6 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 149s - loss: 4.9721 - acc: 0.050 - ETA: 57s - loss: 5.1120 - acc: 0.033 - ETA: 46s - loss: 4.9689 - acc: 0.07 - ETA: 34s - loss: 4.9221 - acc: 0.06 - ETA: 28s - loss: 4.9152 - acc: 0.07 - ETA: 25s - loss: 4.8410 - acc: 0.10 - ETA: 22s - loss: 4.7128 - acc: 0.12 - ETA: 20s - loss: 4.5848 - acc: 0.15 - ETA: 19s - loss: 4.4773 - acc: 0.18 - ETA: 18s - loss: 4.3857 - acc: 0.20 - ETA: 17s - loss: 4.2558 - acc: 0.23 - ETA: 16s - loss: 4.1138 - acc: 0.24 - ETA: 15s - loss: 4.0237 - acc: 0.25 - ETA: 15s - loss: 3.9527 - acc: 0.26 - ETA: 14s - loss: 3.8342 - acc: 0.28 - ETA: 14s - loss: 3.7222 - acc: 0.30 - ETA: 14s - loss: 3.6515 - acc: 0.32 - ETA: 13s - loss: 3.5642 - acc: 0.33 - ETA: 13s - loss: 3.4724 - acc: 0.34 - ETA: 12s - loss: 3.4026 - acc: 0.35 - ETA: 12s - loss: 3.3346 - acc: 0.36 - ETA: 12s - loss: 3.2670 - acc: 0.37 - ETA: 11s - loss: 3.2088 - acc: 0.38 - ETA: 11s - loss: 3.1358 - acc: 0.40 - ETA: 11s - loss: 3.0690 - acc: 0.41 - ETA: 11s - loss: 3.0045 - acc: 0.42 - ETA: 11s - loss: 2.9381 - acc: 0.43 - ETA: 10s - loss: 2.8786 - acc: 0.44 - ETA: 10s - loss: 2.8295 - acc: 0.45 - ETA: 10s - loss: 2.7872 - acc: 0.45 - ETA: 10s - loss: 2.7444 - acc: 0.46 - ETA: 10s - loss: 2.6940 - acc: 0.47 - ETA: 9s - loss: 2.6588 - acc: 0.4789 - ETA: 9s - loss: 2.6127 - acc: 0.486 - ETA: 9s - loss: 2.5756 - acc: 0.492 - ETA: 9s - loss: 2.5336 - acc: 0.500 - ETA: 9s - loss: 2.4910 - acc: 0.506 - ETA: 9s - loss: 2.4525 - acc: 0.512 - ETA: 9s - loss: 2.4175 - acc: 0.519 - ETA: 8s - loss: 2.3712 - acc: 0.528 - ETA: 8s - loss: 2.3385 - acc: 0.532 - ETA: 8s - loss: 2.3057 - acc: 0.537 - ETA: 8s - loss: 2.2739 - acc: 0.543 - ETA: 8s - loss: 2.2452 - acc: 0.547 - ETA: 8s - loss: 2.2178 - acc: 0.550 - ETA: 8s - loss: 2.1884 - acc: 0.555 - ETA: 8s - loss: 2.1645 - acc: 0.557 - ETA: 8s - loss: 2.1347 - acc: 0.562 - ETA: 7s - loss: 2.1158 - acc: 0.564 - ETA: 7s - loss: 2.0859 - acc: 0.570 - ETA: 7s - loss: 2.0575 - acc: 0.575 - ETA: 7s - loss: 2.0376 - acc: 0.576 - ETA: 7s - loss: 2.0161 - acc: 0.579 - ETA: 7s - loss: 1.9928 - acc: 0.584 - ETA: 7s - loss: 1.9791 - acc: 0.584 - ETA: 7s - loss: 1.9549 - acc: 0.589 - ETA: 7s - loss: 1.9347 - acc: 0.593 - ETA: 7s - loss: 1.9184 - acc: 0.595 - ETA: 7s - loss: 1.8974 - acc: 0.599 - ETA: 7s - loss: 1.8826 - acc: 0.600 - ETA: 6s - loss: 1.8685 - acc: 0.602 - ETA: 6s - loss: 1.8499 - acc: 0.604 - ETA: 6s - loss: 1.8348 - acc: 0.606 - ETA: 6s - loss: 1.8156 - acc: 0.610 - ETA: 6s - loss: 1.7963 - acc: 0.614 - ETA: 6s - loss: 1.7818 - acc: 0.615 - ETA: 6s - loss: 1.7644 - acc: 0.618 - ETA: 6s - loss: 1.7448 - acc: 0.623 - ETA: 6s - loss: 1.7292 - acc: 0.626 - ETA: 6s - loss: 1.7171 - acc: 0.627 - ETA: 6s - loss: 1.7027 - acc: 0.630 - ETA: 6s - loss: 1.6876 - acc: 0.633 - ETA: 5s - loss: 1.6722 - acc: 0.636 - ETA: 5s - loss: 1.6558 - acc: 0.639 - ETA: 5s - loss: 1.6428 - acc: 0.642 - ETA: 5s - loss: 1.6304 - acc: 0.644 - ETA: 5s - loss: 1.6160 - acc: 0.646 - ETA: 5s - loss: 1.6003 - acc: 0.649 - ETA: 5s - loss: 1.5942 - acc: 0.648 - ETA: 5s - loss: 1.5812 - acc: 0.650 - ETA: 5s - loss: 1.5683 - acc: 0.653 - ETA: 5s - loss: 1.5566 - acc: 0.656 - ETA: 5s - loss: 1.5465 - acc: 0.657 - ETA: 5s - loss: 1.5362 - acc: 0.658 - ETA: 5s - loss: 1.5241 - acc: 0.661 - ETA: 5s - loss: 1.5139 - acc: 0.663 - ETA: 4s - loss: 1.5026 - acc: 0.664 - ETA: 4s - loss: 1.4952 - acc: 0.665 - ETA: 4s - loss: 1.4863 - acc: 0.665 - ETA: 4s - loss: 1.4769 - acc: 0.666 - ETA: 4s - loss: 1.4676 - acc: 0.668 - ETA: 4s - loss: 1.4579 - acc: 0.670 - ETA: 4s - loss: 1.4475 - acc: 0.671 - ETA: 4s - loss: 1.4412 - acc: 0.672 - ETA: 4s - loss: 1.4306 - acc: 0.674 - ETA: 4s - loss: 1.4190 - acc: 0.677 - ETA: 4s - loss: 1.4082 - acc: 0.679 - ETA: 4s - loss: 1.3997 - acc: 0.681 - ETA: 4s - loss: 1.3906 - acc: 0.682 - ETA: 4s - loss: 1.3846 - acc: 0.682 - ETA: 4s - loss: 1.3747 - acc: 0.684 - ETA: 4s - loss: 1.3666 - acc: 0.685 - ETA: 3s - loss: 1.3588 - acc: 0.687 - ETA: 3s - loss: 1.3516 - acc: 0.688 - ETA: 3s - loss: 1.3454 - acc: 0.690 - ETA: 3s - loss: 1.3390 - acc: 0.691 - ETA: 3s - loss: 1.3323 - acc: 0.692 - ETA: 3s - loss: 1.3262 - acc: 0.693 - ETA: 3s - loss: 1.3201 - acc: 0.694 - ETA: 3s - loss: 1.3116 - acc: 0.696 - ETA: 3s - loss: 1.3040 - acc: 0.697 - ETA: 3s - loss: 1.2982 - acc: 0.697 - ETA: 3s - loss: 1.2942 - acc: 0.698 - ETA: 3s - loss: 1.2889 - acc: 0.699 - ETA: 3s - loss: 1.2808 - acc: 0.700 - ETA: 3s - loss: 1.2746 - acc: 0.701 - ETA: 3s - loss: 1.2676 - acc: 0.702 - ETA: 2s - loss: 1.2647 - acc: 0.703 - ETA: 2s - loss: 1.2578 - acc: 0.704 - ETA: 2s - loss: 1.2526 - acc: 0.705 - ETA: 2s - loss: 1.2475 - acc: 0.706 - ETA: 2s - loss: 1.2435 - acc: 0.706 - ETA: 2s - loss: 1.2371 - acc: 0.707 - ETA: 2s - loss: 1.2300 - acc: 0.709 - ETA: 2s - loss: 1.2272 - acc: 0.710 - ETA: 2s - loss: 1.2224 - acc: 0.710 - ETA: 2s - loss: 1.2179 - acc: 0.711 - ETA: 2s - loss: 1.2110 - acc: 0.712 - ETA: 2s - loss: 1.2064 - acc: 0.713 - ETA: 2s - loss: 1.2008 - acc: 0.714 - ETA: 2s - loss: 1.1962 - acc: 0.715 - ETA: 2s - loss: 1.1914 - acc: 0.716 - ETA: 2s - loss: 1.1864 - acc: 0.717 - ETA: 2s - loss: 1.1804 - acc: 0.718 - ETA: 1s - loss: 1.1779 - acc: 0.718 - ETA: 1s - loss: 1.1753 - acc: 0.719 - ETA: 1s - loss: 1.1704 - acc: 0.719 - ETA: 1s - loss: 1.1663 - acc: 0.720 - ETA: 1s - loss: 1.1604 - acc: 0.721 - ETA: 1s - loss: 1.1555 - acc: 0.721 - ETA: 1s - loss: 1.1499 - acc: 0.722 - ETA: 1s - loss: 1.1449 - acc: 0.723 - ETA: 1s - loss: 1.1411 - acc: 0.724 - ETA: 1s - loss: 1.1351 - acc: 0.725 - ETA: 1s - loss: 1.1302 - acc: 0.726 - ETA: 1s - loss: 1.1244 - acc: 0.727 - ETA: 1s - loss: 1.1210 - acc: 0.728 - ETA: 1s - loss: 1.1160 - acc: 0.729 - ETA: 1s - loss: 1.1137 - acc: 0.730 - ETA: 1s - loss: 1.1079 - acc: 0.731 - ETA: 0s - loss: 1.1043 - acc: 0.732 - ETA: 0s - loss: 1.0993 - acc: 0.733 - ETA: 0s - loss: 1.0951 - acc: 0.734 - ETA: 0s - loss: 1.0913 - acc: 0.735 - ETA: 0s - loss: 1.0884 - acc: 0.735 - ETA: 0s - loss: 1.0861 - acc: 0.736 - ETA: 0s - loss: 1.0815 - acc: 0.737 - ETA: 0s - loss: 1.0792 - acc: 0.737 - ETA: 0s - loss: 1.0762 - acc: 0.738 - ETA: 0s - loss: 1.0723 - acc: 0.738 - ETA: 0s - loss: 1.0707 - acc: 0.738 - ETA: 0s - loss: 1.0701 - acc: 0.738 - ETA: 0s - loss: 1.0680 - acc: 0.738 - ETA: 0s - loss: 1.0643 - acc: 0.739 - ETA: 0s - loss: 1.0610 - acc: 0.739 - ETA: 0s - loss: 1.0600 - acc: 0.739 - ETA: 0s - loss: 1.0567 - acc: 0.7401Epoch 00000: val_loss improved from inf to 0.52450, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 10s - loss: 1.0546 - acc: 0.7400 - val_loss: 0.5245 - val_acc: 0.8287\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.2522 - acc: 0.900 - ETA: 8s - loss: 0.3479 - acc: 0.866 - ETA: 8s - loss: 0.2895 - acc: 0.910 - ETA: 8s - loss: 0.2792 - acc: 0.907 - ETA: 8s - loss: 0.3140 - acc: 0.894 - ETA: 8s - loss: 0.3764 - acc: 0.872 - ETA: 8s - loss: 0.3825 - acc: 0.861 - ETA: 8s - loss: 0.3594 - acc: 0.873 - ETA: 8s - loss: 0.3612 - acc: 0.876 - ETA: 8s - loss: 0.3610 - acc: 0.881 - ETA: 8s - loss: 0.3594 - acc: 0.881 - ETA: 8s - loss: 0.3624 - acc: 0.880 - ETA: 8s - loss: 0.3843 - acc: 0.876 - ETA: 8s - loss: 0.3934 - acc: 0.874 - ETA: 8s - loss: 0.3874 - acc: 0.874 - ETA: 7s - loss: 0.3953 - acc: 0.872 - ETA: 7s - loss: 0.3921 - acc: 0.877 - ETA: 7s - loss: 0.4097 - acc: 0.868 - ETA: 7s - loss: 0.4089 - acc: 0.868 - ETA: 7s - loss: 0.4133 - acc: 0.866 - ETA: 7s - loss: 0.4059 - acc: 0.869 - ETA: 7s - loss: 0.4028 - acc: 0.872 - ETA: 7s - loss: 0.4043 - acc: 0.874 - ETA: 7s - loss: 0.4007 - acc: 0.873 - ETA: 7s - loss: 0.3951 - acc: 0.875 - ETA: 7s - loss: 0.3903 - acc: 0.876 - ETA: 7s - loss: 0.3905 - acc: 0.876 - ETA: 7s - loss: 0.3943 - acc: 0.873 - ETA: 7s - loss: 0.3936 - acc: 0.873 - ETA: 7s - loss: 0.3937 - acc: 0.873 - ETA: 7s - loss: 0.4014 - acc: 0.869 - ETA: 7s - loss: 0.4011 - acc: 0.869 - ETA: 7s - loss: 0.4044 - acc: 0.869 - ETA: 7s - loss: 0.4062 - acc: 0.867 - ETA: 7s - loss: 0.4072 - acc: 0.867 - ETA: 7s - loss: 0.4060 - acc: 0.866 - ETA: 7s - loss: 0.4036 - acc: 0.866 - ETA: 7s - loss: 0.3990 - acc: 0.868 - ETA: 7s - loss: 0.3979 - acc: 0.867 - ETA: 7s - loss: 0.3956 - acc: 0.868 - ETA: 7s - loss: 0.3901 - acc: 0.870 - ETA: 6s - loss: 0.3979 - acc: 0.868 - ETA: 6s - loss: 0.3969 - acc: 0.868 - ETA: 6s - loss: 0.3947 - acc: 0.870 - ETA: 6s - loss: 0.3917 - acc: 0.871 - ETA: 6s - loss: 0.3893 - acc: 0.871 - ETA: 6s - loss: 0.3898 - acc: 0.871 - ETA: 6s - loss: 0.3920 - acc: 0.870 - ETA: 6s - loss: 0.3892 - acc: 0.871 - ETA: 6s - loss: 0.3926 - acc: 0.870 - ETA: 6s - loss: 0.3884 - acc: 0.871 - ETA: 6s - loss: 0.3878 - acc: 0.871 - ETA: 6s - loss: 0.3871 - acc: 0.871 - ETA: 6s - loss: 0.3831 - acc: 0.872 - ETA: 6s - loss: 0.3843 - acc: 0.872 - ETA: 6s - loss: 0.3847 - acc: 0.872 - ETA: 6s - loss: 0.3816 - acc: 0.873 - ETA: 5s - loss: 0.3861 - acc: 0.871 - ETA: 5s - loss: 0.3837 - acc: 0.873 - ETA: 5s - loss: 0.3848 - acc: 0.872 - ETA: 5s - loss: 0.3847 - acc: 0.871 - ETA: 5s - loss: 0.3869 - acc: 0.871 - ETA: 5s - loss: 0.3879 - acc: 0.871 - ETA: 5s - loss: 0.3877 - acc: 0.871 - ETA: 5s - loss: 0.3873 - acc: 0.872 - ETA: 5s - loss: 0.3915 - acc: 0.871 - ETA: 5s - loss: 0.3905 - acc: 0.872 - ETA: 5s - loss: 0.3921 - acc: 0.871 - ETA: 5s - loss: 0.3941 - acc: 0.871 - ETA: 5s - loss: 0.3930 - acc: 0.870 - ETA: 5s - loss: 0.3979 - acc: 0.869 - ETA: 5s - loss: 0.4034 - acc: 0.868 - ETA: 5s - loss: 0.4039 - acc: 0.867 - ETA: 5s - loss: 0.4026 - acc: 0.868 - ETA: 5s - loss: 0.4021 - acc: 0.868 - ETA: 4s - loss: 0.4045 - acc: 0.867 - ETA: 4s - loss: 0.4011 - acc: 0.869 - ETA: 4s - loss: 0.3995 - acc: 0.869 - ETA: 4s - loss: 0.4006 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.868 - ETA: 4s - loss: 0.4018 - acc: 0.869 - ETA: 4s - loss: 0.4028 - acc: 0.869 - ETA: 4s - loss: 0.4035 - acc: 0.869 - ETA: 4s - loss: 0.4002 - acc: 0.870 - ETA: 4s - loss: 0.4037 - acc: 0.869 - ETA: 4s - loss: 0.4042 - acc: 0.869 - ETA: 4s - loss: 0.4032 - acc: 0.869 - ETA: 4s - loss: 0.4053 - acc: 0.868 - ETA: 4s - loss: 0.4037 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.869 - ETA: 4s - loss: 0.4025 - acc: 0.869 - ETA: 4s - loss: 0.4005 - acc: 0.870 - ETA: 4s - loss: 0.3995 - acc: 0.870 - ETA: 3s - loss: 0.4001 - acc: 0.869 - ETA: 3s - loss: 0.4016 - acc: 0.869 - ETA: 3s - loss: 0.3998 - acc: 0.870 - ETA: 3s - loss: 0.4008 - acc: 0.870 - ETA: 3s - loss: 0.3998 - acc: 0.870 - ETA: 3s - loss: 0.3976 - acc: 0.871 - ETA: 3s - loss: 0.3968 - acc: 0.871 - ETA: 3s - loss: 0.3957 - acc: 0.871 - ETA: 3s - loss: 0.3950 - acc: 0.871 - ETA: 3s - loss: 0.3948 - acc: 0.871 - ETA: 3s - loss: 0.3948 - acc: 0.871 - ETA: 3s - loss: 0.3938 - acc: 0.871 - ETA: 3s - loss: 0.3949 - acc: 0.870 - ETA: 3s - loss: 0.3952 - acc: 0.870 - ETA: 3s - loss: 0.3970 - acc: 0.870 - ETA: 3s - loss: 0.3959 - acc: 0.870 - ETA: 3s - loss: 0.3937 - acc: 0.871 - ETA: 3s - loss: 0.3922 - acc: 0.871 - ETA: 2s - loss: 0.3941 - acc: 0.871 - ETA: 2s - loss: 0.3940 - acc: 0.871 - ETA: 2s - loss: 0.3921 - acc: 0.872 - ETA: 2s - loss: 0.3938 - acc: 0.872 - ETA: 2s - loss: 0.3948 - acc: 0.871 - ETA: 2s - loss: 0.3941 - acc: 0.871 - ETA: 2s - loss: 0.3943 - acc: 0.871 - ETA: 2s - loss: 0.3939 - acc: 0.871 - ETA: 2s - loss: 0.3926 - acc: 0.872 - ETA: 2s - loss: 0.3935 - acc: 0.872 - ETA: 2s - loss: 0.3918 - acc: 0.872 - ETA: 2s - loss: 0.3935 - acc: 0.872 - ETA: 2s - loss: 0.3959 - acc: 0.871 - ETA: 2s - loss: 0.3976 - acc: 0.870 - ETA: 2s - loss: 0.3979 - acc: 0.870 - ETA: 2s - loss: 0.3991 - acc: 0.870 - ETA: 2s - loss: 0.4016 - acc: 0.869 - ETA: 2s - loss: 0.4020 - acc: 0.869 - ETA: 2s - loss: 0.4010 - acc: 0.870 - ETA: 1s - loss: 0.3999 - acc: 0.870 - ETA: 1s - loss: 0.4017 - acc: 0.869 - ETA: 1s - loss: 0.4022 - acc: 0.869 - ETA: 1s - loss: 0.4018 - acc: 0.868 - ETA: 1s - loss: 0.4016 - acc: 0.869 - ETA: 1s - loss: 0.4002 - acc: 0.869 - ETA: 1s - loss: 0.4000 - acc: 0.869 - ETA: 1s - loss: 0.3999 - acc: 0.869 - ETA: 1s - loss: 0.3989 - acc: 0.869 - ETA: 1s - loss: 0.3979 - acc: 0.869 - ETA: 1s - loss: 0.3972 - acc: 0.869 - ETA: 1s - loss: 0.3963 - acc: 0.870 - ETA: 1s - loss: 0.3965 - acc: 0.870 - ETA: 1s - loss: 0.3974 - acc: 0.870 - ETA: 1s - loss: 0.3970 - acc: 0.870 - ETA: 1s - loss: 0.3972 - acc: 0.870 - ETA: 1s - loss: 0.3962 - acc: 0.870 - ETA: 1s - loss: 0.3958 - acc: 0.870 - ETA: 0s - loss: 0.3958 - acc: 0.870 - ETA: 0s - loss: 0.3941 - acc: 0.871 - ETA: 0s - loss: 0.3948 - acc: 0.871 - ETA: 0s - loss: 0.3944 - acc: 0.871 - ETA: 0s - loss: 0.3948 - acc: 0.871 - ETA: 0s - loss: 0.3941 - acc: 0.871 - ETA: 0s - loss: 0.3935 - acc: 0.871 - ETA: 0s - loss: 0.3932 - acc: 0.871 - ETA: 0s - loss: 0.3925 - acc: 0.871 - ETA: 0s - loss: 0.3934 - acc: 0.871 - ETA: 0s - loss: 0.3920 - acc: 0.871 - ETA: 0s - loss: 0.3923 - acc: 0.871 - ETA: 0s - loss: 0.3932 - acc: 0.871 - ETA: 0s - loss: 0.3931 - acc: 0.871 - ETA: 0s - loss: 0.3924 - acc: 0.871 - ETA: 0s - loss: 0.3939 - acc: 0.871 - ETA: 0s - loss: 0.3957 - acc: 0.871 - ETA: 0s - loss: 0.3952 - acc: 0.8712Epoch 00001: val_loss improved from 0.52450 to 0.48827, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 0.3951 - acc: 0.8713 - val_loss: 0.4883 - val_acc: 0.8407\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.3868 - acc: 0.800 - ETA: 9s - loss: 0.2796 - acc: 0.900 - ETA: 9s - loss: 0.2060 - acc: 0.930 - ETA: 9s - loss: 0.1949 - acc: 0.935 - ETA: 9s - loss: 0.2653 - acc: 0.916 - ETA: 8s - loss: 0.2927 - acc: 0.913 - ETA: 8s - loss: 0.2928 - acc: 0.911 - ETA: 8s - loss: 0.2932 - acc: 0.913 - ETA: 8s - loss: 0.2808 - acc: 0.911 - ETA: 8s - loss: 0.2847 - acc: 0.913 - ETA: 8s - loss: 0.2669 - acc: 0.919 - ETA: 8s - loss: 0.2844 - acc: 0.913 - ETA: 8s - loss: 0.2814 - acc: 0.914 - ETA: 8s - loss: 0.2828 - acc: 0.913 - ETA: 8s - loss: 0.2745 - acc: 0.913 - ETA: 8s - loss: 0.2693 - acc: 0.914 - ETA: 8s - loss: 0.2714 - acc: 0.913 - ETA: 8s - loss: 0.2698 - acc: 0.914 - ETA: 8s - loss: 0.2744 - acc: 0.913 - ETA: 7s - loss: 0.2802 - acc: 0.912 - ETA: 7s - loss: 0.2823 - acc: 0.911 - ETA: 7s - loss: 0.2823 - acc: 0.912 - ETA: 7s - loss: 0.2744 - acc: 0.915 - ETA: 7s - loss: 0.2820 - acc: 0.913 - ETA: 7s - loss: 0.2831 - acc: 0.912 - ETA: 7s - loss: 0.2775 - acc: 0.914 - ETA: 7s - loss: 0.2743 - acc: 0.916 - ETA: 7s - loss: 0.2733 - acc: 0.915 - ETA: 7s - loss: 0.2706 - acc: 0.915 - ETA: 7s - loss: 0.2744 - acc: 0.914 - ETA: 7s - loss: 0.2714 - acc: 0.915 - ETA: 7s - loss: 0.2698 - acc: 0.915 - ETA: 7s - loss: 0.2680 - acc: 0.916 - ETA: 7s - loss: 0.2640 - acc: 0.918 - ETA: 7s - loss: 0.2616 - acc: 0.920 - ETA: 7s - loss: 0.2674 - acc: 0.917 - ETA: 7s - loss: 0.2640 - acc: 0.918 - ETA: 6s - loss: 0.2671 - acc: 0.917 - ETA: 6s - loss: 0.2715 - acc: 0.916 - ETA: 6s - loss: 0.2709 - acc: 0.917 - ETA: 6s - loss: 0.2769 - acc: 0.916 - ETA: 6s - loss: 0.2752 - acc: 0.916 - ETA: 6s - loss: 0.2871 - acc: 0.915 - ETA: 6s - loss: 0.2864 - acc: 0.915 - ETA: 6s - loss: 0.2880 - acc: 0.914 - ETA: 6s - loss: 0.2872 - acc: 0.914 - ETA: 6s - loss: 0.2888 - acc: 0.914 - ETA: 6s - loss: 0.2878 - acc: 0.914 - ETA: 6s - loss: 0.2955 - acc: 0.913 - ETA: 6s - loss: 0.2973 - acc: 0.913 - ETA: 6s - loss: 0.2966 - acc: 0.912 - ETA: 6s - loss: 0.2983 - acc: 0.911 - ETA: 6s - loss: 0.3012 - acc: 0.910 - ETA: 6s - loss: 0.2994 - acc: 0.910 - ETA: 6s - loss: 0.3017 - acc: 0.909 - ETA: 6s - loss: 0.3000 - acc: 0.910 - ETA: 6s - loss: 0.3010 - acc: 0.910 - ETA: 5s - loss: 0.3030 - acc: 0.909 - ETA: 5s - loss: 0.3068 - acc: 0.909 - ETA: 5s - loss: 0.3067 - acc: 0.908 - ETA: 5s - loss: 0.3063 - acc: 0.907 - ETA: 5s - loss: 0.3057 - acc: 0.907 - ETA: 5s - loss: 0.3062 - acc: 0.906 - ETA: 5s - loss: 0.3081 - acc: 0.906 - ETA: 5s - loss: 0.3093 - acc: 0.906 - ETA: 5s - loss: 0.3091 - acc: 0.905 - ETA: 5s - loss: 0.3083 - acc: 0.904 - ETA: 5s - loss: 0.3063 - acc: 0.905 - ETA: 5s - loss: 0.3081 - acc: 0.903 - ETA: 5s - loss: 0.3077 - acc: 0.903 - ETA: 5s - loss: 0.3104 - acc: 0.902 - ETA: 5s - loss: 0.3173 - acc: 0.901 - ETA: 5s - loss: 0.3178 - acc: 0.901 - ETA: 5s - loss: 0.3187 - acc: 0.900 - ETA: 5s - loss: 0.3199 - acc: 0.899 - ETA: 5s - loss: 0.3209 - acc: 0.899 - ETA: 4s - loss: 0.3213 - acc: 0.898 - ETA: 4s - loss: 0.3199 - acc: 0.899 - ETA: 4s - loss: 0.3250 - acc: 0.897 - ETA: 4s - loss: 0.3237 - acc: 0.898 - ETA: 4s - loss: 0.3216 - acc: 0.898 - ETA: 4s - loss: 0.3219 - acc: 0.897 - ETA: 4s - loss: 0.3231 - acc: 0.896 - ETA: 4s - loss: 0.3236 - acc: 0.896 - ETA: 4s - loss: 0.3235 - acc: 0.896 - ETA: 4s - loss: 0.3241 - acc: 0.896 - ETA: 4s - loss: 0.3216 - acc: 0.897 - ETA: 4s - loss: 0.3229 - acc: 0.897 - ETA: 4s - loss: 0.3223 - acc: 0.896 - ETA: 4s - loss: 0.3223 - acc: 0.896 - ETA: 4s - loss: 0.3220 - acc: 0.896 - ETA: 4s - loss: 0.3226 - acc: 0.897 - ETA: 4s - loss: 0.3220 - acc: 0.897 - ETA: 4s - loss: 0.3216 - acc: 0.897 - ETA: 3s - loss: 0.3216 - acc: 0.897 - ETA: 3s - loss: 0.3192 - acc: 0.898 - ETA: 3s - loss: 0.3198 - acc: 0.898 - ETA: 3s - loss: 0.3218 - acc: 0.897 - ETA: 3s - loss: 0.3256 - acc: 0.897 - ETA: 3s - loss: 0.3246 - acc: 0.897 - ETA: 3s - loss: 0.3280 - acc: 0.897 - ETA: 3s - loss: 0.3294 - acc: 0.896 - ETA: 3s - loss: 0.3288 - acc: 0.896 - ETA: 3s - loss: 0.3274 - acc: 0.896 - ETA: 3s - loss: 0.3279 - acc: 0.896 - ETA: 3s - loss: 0.3301 - acc: 0.895 - ETA: 3s - loss: 0.3295 - acc: 0.896 - ETA: 3s - loss: 0.3292 - acc: 0.896 - ETA: 3s - loss: 0.3302 - acc: 0.895 - ETA: 3s - loss: 0.3303 - acc: 0.895 - ETA: 3s - loss: 0.3281 - acc: 0.896 - ETA: 3s - loss: 0.3261 - acc: 0.897 - ETA: 2s - loss: 0.3244 - acc: 0.897 - ETA: 2s - loss: 0.3233 - acc: 0.898 - ETA: 2s - loss: 0.3231 - acc: 0.898 - ETA: 2s - loss: 0.3218 - acc: 0.898 - ETA: 2s - loss: 0.3209 - acc: 0.899 - ETA: 2s - loss: 0.3216 - acc: 0.899 - ETA: 2s - loss: 0.3207 - acc: 0.898 - ETA: 2s - loss: 0.3195 - acc: 0.899 - ETA: 2s - loss: 0.3196 - acc: 0.899 - ETA: 2s - loss: 0.3197 - acc: 0.899 - ETA: 2s - loss: 0.3208 - acc: 0.899 - ETA: 2s - loss: 0.3207 - acc: 0.899 - ETA: 2s - loss: 0.3187 - acc: 0.899 - ETA: 2s - loss: 0.3178 - acc: 0.899 - ETA: 2s - loss: 0.3163 - acc: 0.900 - ETA: 2s - loss: 0.3167 - acc: 0.899 - ETA: 2s - loss: 0.3174 - acc: 0.899 - ETA: 2s - loss: 0.3161 - acc: 0.899 - ETA: 2s - loss: 0.3157 - acc: 0.899 - ETA: 1s - loss: 0.3169 - acc: 0.899 - ETA: 1s - loss: 0.3167 - acc: 0.899 - ETA: 1s - loss: 0.3172 - acc: 0.899 - ETA: 1s - loss: 0.3189 - acc: 0.898 - ETA: 1s - loss: 0.3193 - acc: 0.898 - ETA: 1s - loss: 0.3187 - acc: 0.898 - ETA: 1s - loss: 0.3192 - acc: 0.897 - ETA: 1s - loss: 0.3194 - acc: 0.897 - ETA: 1s - loss: 0.3186 - acc: 0.897 - ETA: 1s - loss: 0.3203 - acc: 0.897 - ETA: 1s - loss: 0.3224 - acc: 0.897 - ETA: 1s - loss: 0.3225 - acc: 0.897 - ETA: 1s - loss: 0.3225 - acc: 0.896 - ETA: 1s - loss: 0.3220 - acc: 0.897 - ETA: 1s - loss: 0.3222 - acc: 0.897 - ETA: 1s - loss: 0.3218 - acc: 0.897 - ETA: 1s - loss: 0.3212 - acc: 0.897 - ETA: 1s - loss: 0.3211 - acc: 0.897 - ETA: 0s - loss: 0.3221 - acc: 0.897 - ETA: 0s - loss: 0.3233 - acc: 0.896 - ETA: 0s - loss: 0.3229 - acc: 0.896 - ETA: 0s - loss: 0.3228 - acc: 0.896 - ETA: 0s - loss: 0.3224 - acc: 0.896 - ETA: 0s - loss: 0.3232 - acc: 0.895 - ETA: 0s - loss: 0.3235 - acc: 0.896 - ETA: 0s - loss: 0.3222 - acc: 0.896 - ETA: 0s - loss: 0.3214 - acc: 0.896 - ETA: 0s - loss: 0.3244 - acc: 0.896 - ETA: 0s - loss: 0.3234 - acc: 0.896 - ETA: 0s - loss: 0.3229 - acc: 0.896 - ETA: 0s - loss: 0.3226 - acc: 0.896 - ETA: 0s - loss: 0.3221 - acc: 0.896 - ETA: 0s - loss: 0.3212 - acc: 0.896 - ETA: 0s - loss: 0.3217 - acc: 0.896 - ETA: 0s - loss: 0.3210 - acc: 0.897 - ETA: 0s - loss: 0.3217 - acc: 0.8965Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.3212 - acc: 0.8966 - val_loss: 0.5060 - val_acc: 0.8431\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.3634 - acc: 0.900 - ETA: 8s - loss: 0.4092 - acc: 0.850 - ETA: 8s - loss: 0.3306 - acc: 0.900 - ETA: 8s - loss: 0.3132 - acc: 0.900 - ETA: 8s - loss: 0.3101 - acc: 0.905 - ETA: 8s - loss: 0.2988 - acc: 0.904 - ETA: 8s - loss: 0.2897 - acc: 0.903 - ETA: 8s - loss: 0.2771 - acc: 0.910 - ETA: 8s - loss: 0.3025 - acc: 0.905 - ETA: 8s - loss: 0.3059 - acc: 0.897 - ETA: 8s - loss: 0.2902 - acc: 0.904 - ETA: 8s - loss: 0.2793 - acc: 0.910 - ETA: 8s - loss: 0.2757 - acc: 0.912 - ETA: 8s - loss: 0.2654 - acc: 0.913 - ETA: 8s - loss: 0.2668 - acc: 0.913 - ETA: 8s - loss: 0.2666 - acc: 0.917 - ETA: 8s - loss: 0.2526 - acc: 0.922 - ETA: 8s - loss: 0.2424 - acc: 0.925 - ETA: 8s - loss: 0.2381 - acc: 0.925 - ETA: 7s - loss: 0.2507 - acc: 0.924 - ETA: 7s - loss: 0.2481 - acc: 0.923 - ETA: 7s - loss: 0.2526 - acc: 0.923 - ETA: 7s - loss: 0.2487 - acc: 0.923 - ETA: 7s - loss: 0.2543 - acc: 0.920 - ETA: 7s - loss: 0.2493 - acc: 0.922 - ETA: 7s - loss: 0.2512 - acc: 0.922 - ETA: 7s - loss: 0.2501 - acc: 0.921 - ETA: 7s - loss: 0.2441 - acc: 0.923 - ETA: 7s - loss: 0.2437 - acc: 0.922 - ETA: 7s - loss: 0.2427 - acc: 0.922 - ETA: 7s - loss: 0.2384 - acc: 0.923 - ETA: 7s - loss: 0.2391 - acc: 0.922 - ETA: 7s - loss: 0.2432 - acc: 0.920 - ETA: 7s - loss: 0.2467 - acc: 0.917 - ETA: 7s - loss: 0.2436 - acc: 0.918 - ETA: 7s - loss: 0.2425 - acc: 0.919 - ETA: 7s - loss: 0.2439 - acc: 0.920 - ETA: 6s - loss: 0.2425 - acc: 0.922 - ETA: 6s - loss: 0.2419 - acc: 0.922 - ETA: 6s - loss: 0.2405 - acc: 0.922 - ETA: 6s - loss: 0.2436 - acc: 0.922 - ETA: 6s - loss: 0.2425 - acc: 0.922 - ETA: 6s - loss: 0.2385 - acc: 0.923 - ETA: 6s - loss: 0.2467 - acc: 0.922 - ETA: 6s - loss: 0.2480 - acc: 0.921 - ETA: 6s - loss: 0.2473 - acc: 0.920 - ETA: 6s - loss: 0.2481 - acc: 0.921 - ETA: 6s - loss: 0.2459 - acc: 0.921 - ETA: 6s - loss: 0.2479 - acc: 0.921 - ETA: 6s - loss: 0.2469 - acc: 0.922 - ETA: 6s - loss: 0.2453 - acc: 0.922 - ETA: 6s - loss: 0.2427 - acc: 0.923 - ETA: 6s - loss: 0.2425 - acc: 0.923 - ETA: 6s - loss: 0.2393 - acc: 0.924 - ETA: 6s - loss: 0.2380 - acc: 0.924 - ETA: 5s - loss: 0.2391 - acc: 0.923 - ETA: 5s - loss: 0.2404 - acc: 0.923 - ETA: 5s - loss: 0.2393 - acc: 0.923 - ETA: 5s - loss: 0.2411 - acc: 0.923 - ETA: 5s - loss: 0.2432 - acc: 0.922 - ETA: 5s - loss: 0.2445 - acc: 0.921 - ETA: 5s - loss: 0.2491 - acc: 0.919 - ETA: 5s - loss: 0.2525 - acc: 0.918 - ETA: 5s - loss: 0.2543 - acc: 0.917 - ETA: 5s - loss: 0.2527 - acc: 0.918 - ETA: 5s - loss: 0.2544 - acc: 0.918 - ETA: 5s - loss: 0.2534 - acc: 0.918 - ETA: 5s - loss: 0.2551 - acc: 0.918 - ETA: 5s - loss: 0.2522 - acc: 0.919 - ETA: 5s - loss: 0.2530 - acc: 0.917 - ETA: 5s - loss: 0.2550 - acc: 0.917 - ETA: 5s - loss: 0.2544 - acc: 0.917 - ETA: 5s - loss: 0.2567 - acc: 0.917 - ETA: 5s - loss: 0.2557 - acc: 0.917 - ETA: 4s - loss: 0.2555 - acc: 0.918 - ETA: 4s - loss: 0.2587 - acc: 0.917 - ETA: 4s - loss: 0.2573 - acc: 0.918 - ETA: 4s - loss: 0.2616 - acc: 0.916 - ETA: 4s - loss: 0.2594 - acc: 0.917 - ETA: 4s - loss: 0.2587 - acc: 0.917 - ETA: 4s - loss: 0.2580 - acc: 0.917 - ETA: 4s - loss: 0.2607 - acc: 0.917 - ETA: 4s - loss: 0.2624 - acc: 0.917 - ETA: 4s - loss: 0.2633 - acc: 0.916 - ETA: 4s - loss: 0.2636 - acc: 0.916 - ETA: 4s - loss: 0.2639 - acc: 0.916 - ETA: 4s - loss: 0.2625 - acc: 0.917 - ETA: 4s - loss: 0.2602 - acc: 0.918 - ETA: 4s - loss: 0.2599 - acc: 0.917 - ETA: 4s - loss: 0.2600 - acc: 0.917 - ETA: 4s - loss: 0.2593 - acc: 0.917 - ETA: 4s - loss: 0.2605 - acc: 0.916 - ETA: 3s - loss: 0.2588 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2599 - acc: 0.917 - ETA: 3s - loss: 0.2601 - acc: 0.917 - ETA: 3s - loss: 0.2585 - acc: 0.917 - ETA: 3s - loss: 0.2590 - acc: 0.917 - ETA: 3s - loss: 0.2610 - acc: 0.917 - ETA: 3s - loss: 0.2604 - acc: 0.916 - ETA: 3s - loss: 0.2611 - acc: 0.916 - ETA: 3s - loss: 0.2604 - acc: 0.916 - ETA: 3s - loss: 0.2598 - acc: 0.917 - ETA: 3s - loss: 0.2586 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2578 - acc: 0.917 - ETA: 3s - loss: 0.2591 - acc: 0.916 - ETA: 3s - loss: 0.2590 - acc: 0.916 - ETA: 3s - loss: 0.2602 - acc: 0.915 - ETA: 3s - loss: 0.2596 - acc: 0.915 - ETA: 3s - loss: 0.2606 - acc: 0.915 - ETA: 2s - loss: 0.2609 - acc: 0.915 - ETA: 2s - loss: 0.2636 - acc: 0.914 - ETA: 2s - loss: 0.2625 - acc: 0.914 - ETA: 2s - loss: 0.2621 - acc: 0.914 - ETA: 2s - loss: 0.2610 - acc: 0.914 - ETA: 2s - loss: 0.2624 - acc: 0.914 - ETA: 2s - loss: 0.2626 - acc: 0.914 - ETA: 2s - loss: 0.2634 - acc: 0.914 - ETA: 2s - loss: 0.2635 - acc: 0.913 - ETA: 2s - loss: 0.2651 - acc: 0.913 - ETA: 2s - loss: 0.2673 - acc: 0.913 - ETA: 2s - loss: 0.2701 - acc: 0.912 - ETA: 2s - loss: 0.2739 - acc: 0.911 - ETA: 2s - loss: 0.2727 - acc: 0.911 - ETA: 2s - loss: 0.2717 - acc: 0.912 - ETA: 2s - loss: 0.2738 - acc: 0.911 - ETA: 2s - loss: 0.2725 - acc: 0.912 - ETA: 2s - loss: 0.2751 - acc: 0.911 - ETA: 2s - loss: 0.2763 - acc: 0.911 - ETA: 1s - loss: 0.2759 - acc: 0.911 - ETA: 1s - loss: 0.2755 - acc: 0.911 - ETA: 1s - loss: 0.2742 - acc: 0.911 - ETA: 1s - loss: 0.2728 - acc: 0.912 - ETA: 1s - loss: 0.2718 - acc: 0.912 - ETA: 1s - loss: 0.2712 - acc: 0.912 - ETA: 1s - loss: 0.2704 - acc: 0.912 - ETA: 1s - loss: 0.2708 - acc: 0.912 - ETA: 1s - loss: 0.2720 - acc: 0.912 - ETA: 1s - loss: 0.2717 - acc: 0.912 - ETA: 1s - loss: 0.2714 - acc: 0.912 - ETA: 1s - loss: 0.2707 - acc: 0.912 - ETA: 1s - loss: 0.2700 - acc: 0.912 - ETA: 1s - loss: 0.2719 - acc: 0.912 - ETA: 1s - loss: 0.2735 - acc: 0.911 - ETA: 1s - loss: 0.2734 - acc: 0.912 - ETA: 1s - loss: 0.2738 - acc: 0.911 - ETA: 1s - loss: 0.2746 - acc: 0.911 - ETA: 0s - loss: 0.2736 - acc: 0.912 - ETA: 0s - loss: 0.2750 - acc: 0.911 - ETA: 0s - loss: 0.2745 - acc: 0.912 - ETA: 0s - loss: 0.2743 - acc: 0.912 - ETA: 0s - loss: 0.2742 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.911 - ETA: 0s - loss: 0.2743 - acc: 0.912 - ETA: 0s - loss: 0.2740 - acc: 0.912 - ETA: 0s - loss: 0.2744 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.911 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2729 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2737 - acc: 0.912 - ETA: 0s - loss: 0.2755 - acc: 0.912 - ETA: 0s - loss: 0.2747 - acc: 0.912 - ETA: 0s - loss: 0.2738 - acc: 0.912 - ETA: 0s - loss: 0.2735 - acc: 0.9129Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2737 - acc: 0.9130 - val_loss: 0.4910 - val_acc: 0.8503\n",
      "Epoch 5/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0845 - acc: 0.950 - ETA: 9s - loss: 0.3129 - acc: 0.883 - ETA: 9s - loss: 0.2535 - acc: 0.900 - ETA: 8s - loss: 0.2735 - acc: 0.907 - ETA: 8s - loss: 0.3418 - acc: 0.894 - ETA: 8s - loss: 0.3197 - acc: 0.900 - ETA: 8s - loss: 0.2845 - acc: 0.907 - ETA: 8s - loss: 0.2968 - acc: 0.906 - ETA: 8s - loss: 0.2768 - acc: 0.911 - ETA: 8s - loss: 0.2617 - acc: 0.913 - ETA: 8s - loss: 0.2460 - acc: 0.919 - ETA: 8s - loss: 0.2398 - acc: 0.919 - ETA: 8s - loss: 0.2295 - acc: 0.924 - ETA: 8s - loss: 0.2241 - acc: 0.924 - ETA: 8s - loss: 0.2198 - acc: 0.924 - ETA: 8s - loss: 0.2144 - acc: 0.927 - ETA: 8s - loss: 0.2244 - acc: 0.927 - ETA: 7s - loss: 0.2135 - acc: 0.930 - ETA: 7s - loss: 0.2124 - acc: 0.928 - ETA: 7s - loss: 0.2061 - acc: 0.929 - ETA: 7s - loss: 0.2124 - acc: 0.926 - ETA: 7s - loss: 0.2108 - acc: 0.929 - ETA: 7s - loss: 0.2105 - acc: 0.930 - ETA: 7s - loss: 0.2098 - acc: 0.929 - ETA: 7s - loss: 0.2101 - acc: 0.929 - ETA: 7s - loss: 0.2164 - acc: 0.926 - ETA: 7s - loss: 0.2138 - acc: 0.927 - ETA: 7s - loss: 0.2095 - acc: 0.929 - ETA: 7s - loss: 0.2159 - acc: 0.928 - ETA: 7s - loss: 0.2134 - acc: 0.928 - ETA: 7s - loss: 0.2098 - acc: 0.930 - ETA: 7s - loss: 0.2074 - acc: 0.931 - ETA: 7s - loss: 0.2064 - acc: 0.932 - ETA: 7s - loss: 0.2068 - acc: 0.932 - ETA: 7s - loss: 0.2083 - acc: 0.931 - ETA: 7s - loss: 0.2133 - acc: 0.932 - ETA: 6s - loss: 0.2110 - acc: 0.933 - ETA: 6s - loss: 0.2159 - acc: 0.932 - ETA: 6s - loss: 0.2166 - acc: 0.931 - ETA: 6s - loss: 0.2190 - acc: 0.930 - ETA: 6s - loss: 0.2162 - acc: 0.930 - ETA: 6s - loss: 0.2232 - acc: 0.930 - ETA: 6s - loss: 0.2279 - acc: 0.929 - ETA: 6s - loss: 0.2277 - acc: 0.929 - ETA: 6s - loss: 0.2256 - acc: 0.929 - ETA: 6s - loss: 0.2276 - acc: 0.928 - ETA: 6s - loss: 0.2315 - acc: 0.926 - ETA: 6s - loss: 0.2293 - acc: 0.927 - ETA: 6s - loss: 0.2272 - acc: 0.926 - ETA: 6s - loss: 0.2287 - acc: 0.925 - ETA: 6s - loss: 0.2339 - acc: 0.924 - ETA: 6s - loss: 0.2358 - acc: 0.923 - ETA: 6s - loss: 0.2358 - acc: 0.924 - ETA: 6s - loss: 0.2387 - acc: 0.923 - ETA: 5s - loss: 0.2381 - acc: 0.923 - ETA: 5s - loss: 0.2372 - acc: 0.923 - ETA: 5s - loss: 0.2370 - acc: 0.923 - ETA: 5s - loss: 0.2341 - acc: 0.924 - ETA: 5s - loss: 0.2339 - acc: 0.924 - ETA: 5s - loss: 0.2344 - acc: 0.924 - ETA: 5s - loss: 0.2339 - acc: 0.924 - ETA: 5s - loss: 0.2346 - acc: 0.923 - ETA: 5s - loss: 0.2345 - acc: 0.923 - ETA: 5s - loss: 0.2331 - acc: 0.924 - ETA: 5s - loss: 0.2323 - acc: 0.924 - ETA: 5s - loss: 0.2320 - acc: 0.923 - ETA: 5s - loss: 0.2300 - acc: 0.924 - ETA: 5s - loss: 0.2313 - acc: 0.923 - ETA: 5s - loss: 0.2325 - acc: 0.922 - ETA: 5s - loss: 0.2341 - acc: 0.922 - ETA: 5s - loss: 0.2347 - acc: 0.923 - ETA: 5s - loss: 0.2351 - acc: 0.923 - ETA: 5s - loss: 0.2358 - acc: 0.923 - ETA: 4s - loss: 0.2347 - acc: 0.923 - ETA: 4s - loss: 0.2336 - acc: 0.924 - ETA: 4s - loss: 0.2329 - acc: 0.924 - ETA: 4s - loss: 0.2344 - acc: 0.924 - ETA: 4s - loss: 0.2347 - acc: 0.923 - ETA: 4s - loss: 0.2339 - acc: 0.923 - ETA: 4s - loss: 0.2357 - acc: 0.923 - ETA: 4s - loss: 0.2339 - acc: 0.924 - ETA: 4s - loss: 0.2339 - acc: 0.924 - ETA: 4s - loss: 0.2324 - acc: 0.925 - ETA: 4s - loss: 0.2325 - acc: 0.925 - ETA: 4s - loss: 0.2335 - acc: 0.925 - ETA: 4s - loss: 0.2323 - acc: 0.925 - ETA: 4s - loss: 0.2325 - acc: 0.925 - ETA: 4s - loss: 0.2354 - acc: 0.925 - ETA: 4s - loss: 0.2349 - acc: 0.925 - ETA: 4s - loss: 0.2356 - acc: 0.925 - ETA: 4s - loss: 0.2347 - acc: 0.925 - ETA: 4s - loss: 0.2341 - acc: 0.925 - ETA: 3s - loss: 0.2328 - acc: 0.925 - ETA: 3s - loss: 0.2324 - acc: 0.925 - ETA: 3s - loss: 0.2358 - acc: 0.925 - ETA: 3s - loss: 0.2338 - acc: 0.925 - ETA: 3s - loss: 0.2340 - acc: 0.925 - ETA: 3s - loss: 0.2333 - acc: 0.925 - ETA: 3s - loss: 0.2340 - acc: 0.924 - ETA: 3s - loss: 0.2353 - acc: 0.924 - ETA: 3s - loss: 0.2358 - acc: 0.925 - ETA: 3s - loss: 0.2375 - acc: 0.925 - ETA: 3s - loss: 0.2370 - acc: 0.925 - ETA: 3s - loss: 0.2389 - acc: 0.925 - ETA: 3s - loss: 0.2398 - acc: 0.925 - ETA: 3s - loss: 0.2404 - acc: 0.925 - ETA: 3s - loss: 0.2391 - acc: 0.926 - ETA: 3s - loss: 0.2389 - acc: 0.926 - ETA: 3s - loss: 0.2371 - acc: 0.926 - ETA: 3s - loss: 0.2380 - acc: 0.926 - ETA: 2s - loss: 0.2387 - acc: 0.926 - ETA: 2s - loss: 0.2398 - acc: 0.926 - ETA: 2s - loss: 0.2392 - acc: 0.926 - ETA: 2s - loss: 0.2393 - acc: 0.926 - ETA: 2s - loss: 0.2398 - acc: 0.925 - ETA: 2s - loss: 0.2393 - acc: 0.926 - ETA: 2s - loss: 0.2385 - acc: 0.926 - ETA: 2s - loss: 0.2392 - acc: 0.926 - ETA: 2s - loss: 0.2385 - acc: 0.925 - ETA: 2s - loss: 0.2397 - acc: 0.925 - ETA: 2s - loss: 0.2391 - acc: 0.925 - ETA: 2s - loss: 0.2392 - acc: 0.925 - ETA: 2s - loss: 0.2409 - acc: 0.925 - ETA: 2s - loss: 0.2417 - acc: 0.925 - ETA: 2s - loss: 0.2420 - acc: 0.925 - ETA: 2s - loss: 0.2436 - acc: 0.924 - ETA: 2s - loss: 0.2456 - acc: 0.924 - ETA: 2s - loss: 0.2468 - acc: 0.923 - ETA: 1s - loss: 0.2461 - acc: 0.924 - ETA: 1s - loss: 0.2468 - acc: 0.924 - ETA: 1s - loss: 0.2454 - acc: 0.924 - ETA: 1s - loss: 0.2449 - acc: 0.924 - ETA: 1s - loss: 0.2446 - acc: 0.924 - ETA: 1s - loss: 0.2456 - acc: 0.924 - ETA: 1s - loss: 0.2453 - acc: 0.924 - ETA: 1s - loss: 0.2445 - acc: 0.924 - ETA: 1s - loss: 0.2469 - acc: 0.924 - ETA: 1s - loss: 0.2465 - acc: 0.924 - ETA: 1s - loss: 0.2475 - acc: 0.923 - ETA: 1s - loss: 0.2474 - acc: 0.923 - ETA: 1s - loss: 0.2476 - acc: 0.923 - ETA: 1s - loss: 0.2464 - acc: 0.924 - ETA: 1s - loss: 0.2460 - acc: 0.924 - ETA: 1s - loss: 0.2476 - acc: 0.923 - ETA: 1s - loss: 0.2474 - acc: 0.923 - ETA: 1s - loss: 0.2465 - acc: 0.924 - ETA: 1s - loss: 0.2478 - acc: 0.923 - ETA: 0s - loss: 0.2480 - acc: 0.923 - ETA: 0s - loss: 0.2501 - acc: 0.923 - ETA: 0s - loss: 0.2509 - acc: 0.923 - ETA: 0s - loss: 0.2503 - acc: 0.923 - ETA: 0s - loss: 0.2492 - acc: 0.923 - ETA: 0s - loss: 0.2484 - acc: 0.924 - ETA: 0s - loss: 0.2474 - acc: 0.924 - ETA: 0s - loss: 0.2464 - acc: 0.924 - ETA: 0s - loss: 0.2454 - acc: 0.925 - ETA: 0s - loss: 0.2459 - acc: 0.924 - ETA: 0s - loss: 0.2452 - acc: 0.924 - ETA: 0s - loss: 0.2444 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2446 - acc: 0.924 - ETA: 0s - loss: 0.2445 - acc: 0.924 - ETA: 0s - loss: 0.2437 - acc: 0.925 - ETA: 0s - loss: 0.2427 - acc: 0.925 - ETA: 0s - loss: 0.2423 - acc: 0.9251Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2423 - acc: 0.9250 - val_loss: 0.5193 - val_acc: 0.8551\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.1784 - acc: 0.950 - ETA: 9s - loss: 0.1546 - acc: 0.950 - ETA: 9s - loss: 0.1164 - acc: 0.970 - ETA: 9s - loss: 0.1314 - acc: 0.964 - ETA: 9s - loss: 0.1246 - acc: 0.966 - ETA: 8s - loss: 0.1258 - acc: 0.963 - ETA: 8s - loss: 0.1210 - acc: 0.965 - ETA: 8s - loss: 0.1135 - acc: 0.966 - ETA: 8s - loss: 0.1535 - acc: 0.961 - ETA: 8s - loss: 0.1592 - acc: 0.957 - ETA: 8s - loss: 0.1829 - acc: 0.950 - ETA: 8s - loss: 0.2078 - acc: 0.947 - ETA: 8s - loss: 0.2005 - acc: 0.948 - ETA: 8s - loss: 0.1952 - acc: 0.948 - ETA: 8s - loss: 0.1895 - acc: 0.950 - ETA: 8s - loss: 0.1895 - acc: 0.950 - ETA: 8s - loss: 0.1881 - acc: 0.947 - ETA: 8s - loss: 0.1847 - acc: 0.948 - ETA: 8s - loss: 0.1819 - acc: 0.947 - ETA: 7s - loss: 0.1775 - acc: 0.947 - ETA: 7s - loss: 0.1729 - acc: 0.948 - ETA: 7s - loss: 0.1823 - acc: 0.946 - ETA: 7s - loss: 0.1900 - acc: 0.944 - ETA: 7s - loss: 0.1887 - acc: 0.943 - ETA: 7s - loss: 0.1918 - acc: 0.940 - ETA: 7s - loss: 0.1894 - acc: 0.940 - ETA: 7s - loss: 0.1958 - acc: 0.937 - ETA: 7s - loss: 0.1932 - acc: 0.937 - ETA: 7s - loss: 0.1890 - acc: 0.938 - ETA: 7s - loss: 0.1886 - acc: 0.938 - ETA: 7s - loss: 0.1863 - acc: 0.939 - ETA: 7s - loss: 0.1892 - acc: 0.937 - ETA: 7s - loss: 0.1873 - acc: 0.936 - ETA: 7s - loss: 0.1840 - acc: 0.938 - ETA: 7s - loss: 0.1902 - acc: 0.936 - ETA: 7s - loss: 0.1877 - acc: 0.937 - ETA: 7s - loss: 0.1918 - acc: 0.935 - ETA: 6s - loss: 0.1886 - acc: 0.936 - ETA: 6s - loss: 0.1904 - acc: 0.937 - ETA: 6s - loss: 0.1918 - acc: 0.936 - ETA: 6s - loss: 0.1925 - acc: 0.935 - ETA: 6s - loss: 0.1890 - acc: 0.936 - ETA: 6s - loss: 0.1874 - acc: 0.937 - ETA: 6s - loss: 0.1879 - acc: 0.936 - ETA: 6s - loss: 0.1854 - acc: 0.937 - ETA: 6s - loss: 0.1847 - acc: 0.937 - ETA: 6s - loss: 0.1855 - acc: 0.937 - ETA: 6s - loss: 0.1835 - acc: 0.937 - ETA: 6s - loss: 0.1827 - acc: 0.937 - ETA: 6s - loss: 0.1826 - acc: 0.938 - ETA: 6s - loss: 0.1831 - acc: 0.938 - ETA: 6s - loss: 0.1822 - acc: 0.938 - ETA: 6s - loss: 0.1824 - acc: 0.939 - ETA: 6s - loss: 0.1841 - acc: 0.938 - ETA: 6s - loss: 0.1827 - acc: 0.939 - ETA: 6s - loss: 0.1807 - acc: 0.939 - ETA: 5s - loss: 0.1846 - acc: 0.937 - ETA: 5s - loss: 0.1836 - acc: 0.937 - ETA: 5s - loss: 0.1876 - acc: 0.936 - ETA: 5s - loss: 0.1879 - acc: 0.936 - ETA: 5s - loss: 0.1889 - acc: 0.935 - ETA: 5s - loss: 0.1879 - acc: 0.935 - ETA: 5s - loss: 0.1856 - acc: 0.936 - ETA: 5s - loss: 0.1848 - acc: 0.936 - ETA: 5s - loss: 0.1831 - acc: 0.936 - ETA: 5s - loss: 0.1828 - acc: 0.937 - ETA: 5s - loss: 0.1828 - acc: 0.936 - ETA: 5s - loss: 0.1829 - acc: 0.936 - ETA: 5s - loss: 0.1816 - acc: 0.937 - ETA: 5s - loss: 0.1827 - acc: 0.936 - ETA: 5s - loss: 0.1851 - acc: 0.935 - ETA: 5s - loss: 0.1880 - acc: 0.933 - ETA: 5s - loss: 0.1878 - acc: 0.934 - ETA: 5s - loss: 0.1884 - acc: 0.933 - ETA: 4s - loss: 0.1887 - acc: 0.932 - ETA: 4s - loss: 0.1912 - acc: 0.932 - ETA: 4s - loss: 0.1917 - acc: 0.932 - ETA: 4s - loss: 0.1910 - acc: 0.932 - ETA: 4s - loss: 0.1928 - acc: 0.932 - ETA: 4s - loss: 0.1909 - acc: 0.933 - ETA: 4s - loss: 0.1909 - acc: 0.933 - ETA: 4s - loss: 0.1891 - acc: 0.934 - ETA: 4s - loss: 0.1916 - acc: 0.933 - ETA: 4s - loss: 0.1939 - acc: 0.933 - ETA: 4s - loss: 0.1951 - acc: 0.933 - ETA: 4s - loss: 0.1941 - acc: 0.933 - ETA: 4s - loss: 0.1937 - acc: 0.933 - ETA: 4s - loss: 0.1950 - acc: 0.933 - ETA: 4s - loss: 0.1959 - acc: 0.932 - ETA: 4s - loss: 0.1983 - acc: 0.931 - ETA: 4s - loss: 0.1997 - acc: 0.932 - ETA: 4s - loss: 0.1992 - acc: 0.932 - ETA: 4s - loss: 0.1998 - acc: 0.931 - ETA: 3s - loss: 0.1981 - acc: 0.932 - ETA: 3s - loss: 0.1972 - acc: 0.933 - ETA: 3s - loss: 0.2006 - acc: 0.932 - ETA: 3s - loss: 0.1991 - acc: 0.933 - ETA: 3s - loss: 0.2013 - acc: 0.932 - ETA: 3s - loss: 0.2008 - acc: 0.932 - ETA: 3s - loss: 0.2047 - acc: 0.931 - ETA: 3s - loss: 0.2062 - acc: 0.931 - ETA: 3s - loss: 0.2048 - acc: 0.932 - ETA: 3s - loss: 0.2042 - acc: 0.932 - ETA: 3s - loss: 0.2060 - acc: 0.931 - ETA: 3s - loss: 0.2059 - acc: 0.931 - ETA: 3s - loss: 0.2049 - acc: 0.931 - ETA: 3s - loss: 0.2068 - acc: 0.931 - ETA: 3s - loss: 0.2061 - acc: 0.931 - ETA: 3s - loss: 0.2076 - acc: 0.931 - ETA: 3s - loss: 0.2070 - acc: 0.931 - ETA: 3s - loss: 0.2065 - acc: 0.931 - ETA: 2s - loss: 0.2079 - acc: 0.930 - ETA: 2s - loss: 0.2072 - acc: 0.931 - ETA: 2s - loss: 0.2062 - acc: 0.931 - ETA: 2s - loss: 0.2084 - acc: 0.931 - ETA: 2s - loss: 0.2086 - acc: 0.931 - ETA: 2s - loss: 0.2089 - acc: 0.931 - ETA: 2s - loss: 0.2110 - acc: 0.930 - ETA: 2s - loss: 0.2103 - acc: 0.930 - ETA: 2s - loss: 0.2123 - acc: 0.931 - ETA: 2s - loss: 0.2120 - acc: 0.931 - ETA: 2s - loss: 0.2111 - acc: 0.931 - ETA: 2s - loss: 0.2106 - acc: 0.931 - ETA: 2s - loss: 0.2109 - acc: 0.931 - ETA: 2s - loss: 0.2115 - acc: 0.931 - ETA: 2s - loss: 0.2111 - acc: 0.932 - ETA: 2s - loss: 0.2112 - acc: 0.931 - ETA: 2s - loss: 0.2115 - acc: 0.931 - ETA: 2s - loss: 0.2105 - acc: 0.932 - ETA: 1s - loss: 0.2106 - acc: 0.931 - ETA: 1s - loss: 0.2112 - acc: 0.931 - ETA: 1s - loss: 0.2105 - acc: 0.931 - ETA: 1s - loss: 0.2110 - acc: 0.931 - ETA: 1s - loss: 0.2115 - acc: 0.931 - ETA: 1s - loss: 0.2108 - acc: 0.931 - ETA: 1s - loss: 0.2108 - acc: 0.931 - ETA: 1s - loss: 0.2098 - acc: 0.931 - ETA: 1s - loss: 0.2086 - acc: 0.932 - ETA: 1s - loss: 0.2077 - acc: 0.932 - ETA: 1s - loss: 0.2067 - acc: 0.932 - ETA: 1s - loss: 0.2056 - acc: 0.933 - ETA: 1s - loss: 0.2067 - acc: 0.932 - ETA: 1s - loss: 0.2078 - acc: 0.932 - ETA: 1s - loss: 0.2087 - acc: 0.932 - ETA: 1s - loss: 0.2101 - acc: 0.931 - ETA: 1s - loss: 0.2114 - acc: 0.931 - ETA: 1s - loss: 0.2123 - acc: 0.931 - ETA: 1s - loss: 0.2127 - acc: 0.931 - ETA: 0s - loss: 0.2122 - acc: 0.931 - ETA: 0s - loss: 0.2126 - acc: 0.931 - ETA: 0s - loss: 0.2125 - acc: 0.931 - ETA: 0s - loss: 0.2128 - acc: 0.931 - ETA: 0s - loss: 0.2121 - acc: 0.931 - ETA: 0s - loss: 0.2113 - acc: 0.932 - ETA: 0s - loss: 0.2110 - acc: 0.932 - ETA: 0s - loss: 0.2125 - acc: 0.932 - ETA: 0s - loss: 0.2121 - acc: 0.932 - ETA: 0s - loss: 0.2123 - acc: 0.931 - ETA: 0s - loss: 0.2134 - acc: 0.930 - ETA: 0s - loss: 0.2129 - acc: 0.930 - ETA: 0s - loss: 0.2141 - acc: 0.930 - ETA: 0s - loss: 0.2131 - acc: 0.930 - ETA: 0s - loss: 0.2144 - acc: 0.930 - ETA: 0s - loss: 0.2152 - acc: 0.930 - ETA: 0s - loss: 0.2157 - acc: 0.930 - ETA: 0s - loss: 0.2147 - acc: 0.9304Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.2142 - acc: 0.9305 - val_loss: 0.5187 - val_acc: 0.8563\n",
      "Epoch 7/20\n",
      "6640/6680 [============================>.] - ETA: 9s - loss: 0.4738 - acc: 0.850 - ETA: 10s - loss: 0.1968 - acc: 0.93 - ETA: 10s - loss: 0.1823 - acc: 0.93 - ETA: 9s - loss: 0.2099 - acc: 0.9214 - ETA: 9s - loss: 0.1875 - acc: 0.933 - ETA: 9s - loss: 0.1944 - acc: 0.940 - ETA: 9s - loss: 0.1757 - acc: 0.946 - ETA: 9s - loss: 0.2056 - acc: 0.933 - ETA: 8s - loss: 0.2013 - acc: 0.935 - ETA: 8s - loss: 0.1948 - acc: 0.936 - ETA: 8s - loss: 0.1953 - acc: 0.935 - ETA: 8s - loss: 0.1936 - acc: 0.934 - ETA: 8s - loss: 0.2034 - acc: 0.934 - ETA: 8s - loss: 0.1939 - acc: 0.937 - ETA: 8s - loss: 0.1848 - acc: 0.941 - ETA: 8s - loss: 0.1808 - acc: 0.940 - ETA: 8s - loss: 0.1933 - acc: 0.936 - ETA: 8s - loss: 0.1857 - acc: 0.940 - ETA: 8s - loss: 0.1905 - acc: 0.939 - ETA: 8s - loss: 0.1944 - acc: 0.938 - ETA: 7s - loss: 0.1878 - acc: 0.941 - ETA: 7s - loss: 0.1868 - acc: 0.941 - ETA: 7s - loss: 0.1826 - acc: 0.943 - ETA: 7s - loss: 0.1935 - acc: 0.940 - ETA: 7s - loss: 0.1960 - acc: 0.941 - ETA: 7s - loss: 0.1910 - acc: 0.943 - ETA: 7s - loss: 0.1886 - acc: 0.943 - ETA: 7s - loss: 0.1841 - acc: 0.944 - ETA: 7s - loss: 0.1813 - acc: 0.945 - ETA: 7s - loss: 0.1810 - acc: 0.944 - ETA: 7s - loss: 0.1818 - acc: 0.943 - ETA: 7s - loss: 0.1848 - acc: 0.943 - ETA: 7s - loss: 0.1826 - acc: 0.943 - ETA: 7s - loss: 0.1800 - acc: 0.943 - ETA: 7s - loss: 0.1793 - acc: 0.942 - ETA: 7s - loss: 0.1755 - acc: 0.943 - ETA: 7s - loss: 0.1742 - acc: 0.944 - ETA: 6s - loss: 0.1770 - acc: 0.944 - ETA: 6s - loss: 0.1874 - acc: 0.942 - ETA: 6s - loss: 0.1835 - acc: 0.943 - ETA: 6s - loss: 0.1852 - acc: 0.944 - ETA: 6s - loss: 0.1827 - acc: 0.945 - ETA: 6s - loss: 0.1812 - acc: 0.944 - ETA: 6s - loss: 0.1782 - acc: 0.945 - ETA: 6s - loss: 0.1760 - acc: 0.946 - ETA: 6s - loss: 0.1806 - acc: 0.945 - ETA: 6s - loss: 0.1831 - acc: 0.944 - ETA: 6s - loss: 0.1912 - acc: 0.942 - ETA: 6s - loss: 0.1939 - acc: 0.941 - ETA: 6s - loss: 0.1919 - acc: 0.942 - ETA: 6s - loss: 0.1903 - acc: 0.943 - ETA: 6s - loss: 0.1899 - acc: 0.942 - ETA: 6s - loss: 0.1871 - acc: 0.943 - ETA: 6s - loss: 0.1869 - acc: 0.943 - ETA: 6s - loss: 0.1894 - acc: 0.942 - ETA: 6s - loss: 0.1887 - acc: 0.941 - ETA: 5s - loss: 0.1890 - acc: 0.941 - ETA: 5s - loss: 0.1864 - acc: 0.942 - ETA: 5s - loss: 0.1852 - acc: 0.942 - ETA: 5s - loss: 0.1872 - acc: 0.942 - ETA: 5s - loss: 0.1857 - acc: 0.943 - ETA: 5s - loss: 0.1838 - acc: 0.943 - ETA: 5s - loss: 0.1833 - acc: 0.944 - ETA: 5s - loss: 0.1836 - acc: 0.943 - ETA: 5s - loss: 0.1817 - acc: 0.944 - ETA: 5s - loss: 0.1857 - acc: 0.944 - ETA: 5s - loss: 0.1884 - acc: 0.943 - ETA: 5s - loss: 0.1862 - acc: 0.943 - ETA: 5s - loss: 0.1881 - acc: 0.943 - ETA: 5s - loss: 0.1892 - acc: 0.943 - ETA: 5s - loss: 0.1892 - acc: 0.942 - ETA: 5s - loss: 0.1891 - acc: 0.942 - ETA: 5s - loss: 0.1909 - acc: 0.942 - ETA: 5s - loss: 0.1916 - acc: 0.941 - ETA: 4s - loss: 0.1900 - acc: 0.941 - ETA: 4s - loss: 0.1898 - acc: 0.941 - ETA: 4s - loss: 0.1901 - acc: 0.941 - ETA: 4s - loss: 0.1952 - acc: 0.940 - ETA: 4s - loss: 0.1956 - acc: 0.940 - ETA: 4s - loss: 0.1941 - acc: 0.940 - ETA: 4s - loss: 0.1978 - acc: 0.940 - ETA: 4s - loss: 0.1987 - acc: 0.939 - ETA: 4s - loss: 0.2005 - acc: 0.939 - ETA: 4s - loss: 0.2016 - acc: 0.939 - ETA: 4s - loss: 0.2003 - acc: 0.940 - ETA: 4s - loss: 0.2016 - acc: 0.939 - ETA: 4s - loss: 0.2007 - acc: 0.939 - ETA: 4s - loss: 0.1998 - acc: 0.940 - ETA: 4s - loss: 0.1993 - acc: 0.939 - ETA: 4s - loss: 0.1990 - acc: 0.940 - ETA: 4s - loss: 0.1983 - acc: 0.940 - ETA: 4s - loss: 0.1999 - acc: 0.939 - ETA: 3s - loss: 0.2004 - acc: 0.939 - ETA: 3s - loss: 0.2015 - acc: 0.938 - ETA: 3s - loss: 0.2001 - acc: 0.939 - ETA: 3s - loss: 0.2000 - acc: 0.939 - ETA: 3s - loss: 0.2031 - acc: 0.938 - ETA: 3s - loss: 0.2041 - acc: 0.938 - ETA: 3s - loss: 0.2030 - acc: 0.938 - ETA: 3s - loss: 0.2028 - acc: 0.938 - ETA: 3s - loss: 0.2016 - acc: 0.938 - ETA: 3s - loss: 0.2012 - acc: 0.939 - ETA: 3s - loss: 0.2027 - acc: 0.939 - ETA: 3s - loss: 0.2046 - acc: 0.938 - ETA: 3s - loss: 0.2036 - acc: 0.938 - ETA: 3s - loss: 0.2022 - acc: 0.939 - ETA: 3s - loss: 0.2020 - acc: 0.939 - ETA: 3s - loss: 0.2017 - acc: 0.939 - ETA: 3s - loss: 0.2018 - acc: 0.939 - ETA: 3s - loss: 0.2045 - acc: 0.938 - ETA: 3s - loss: 0.2034 - acc: 0.938 - ETA: 2s - loss: 0.2059 - acc: 0.937 - ETA: 2s - loss: 0.2050 - acc: 0.938 - ETA: 2s - loss: 0.2053 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.937 - ETA: 2s - loss: 0.2042 - acc: 0.938 - ETA: 2s - loss: 0.2049 - acc: 0.938 - ETA: 2s - loss: 0.2041 - acc: 0.938 - ETA: 2s - loss: 0.2032 - acc: 0.938 - ETA: 2s - loss: 0.2032 - acc: 0.938 - ETA: 2s - loss: 0.2042 - acc: 0.938 - ETA: 2s - loss: 0.2044 - acc: 0.938 - ETA: 2s - loss: 0.2039 - acc: 0.938 - ETA: 2s - loss: 0.2046 - acc: 0.938 - ETA: 2s - loss: 0.2033 - acc: 0.938 - ETA: 2s - loss: 0.2020 - acc: 0.939 - ETA: 2s - loss: 0.2013 - acc: 0.939 - ETA: 2s - loss: 0.2026 - acc: 0.939 - ETA: 1s - loss: 0.2032 - acc: 0.938 - ETA: 1s - loss: 0.2024 - acc: 0.938 - ETA: 1s - loss: 0.2010 - acc: 0.939 - ETA: 1s - loss: 0.2014 - acc: 0.939 - ETA: 1s - loss: 0.2006 - acc: 0.939 - ETA: 1s - loss: 0.1999 - acc: 0.939 - ETA: 1s - loss: 0.1986 - acc: 0.940 - ETA: 1s - loss: 0.1987 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.940 - ETA: 1s - loss: 0.1972 - acc: 0.940 - ETA: 1s - loss: 0.1976 - acc: 0.939 - ETA: 1s - loss: 0.1969 - acc: 0.940 - ETA: 1s - loss: 0.1977 - acc: 0.940 - ETA: 1s - loss: 0.1976 - acc: 0.940 - ETA: 1s - loss: 0.1989 - acc: 0.939 - ETA: 1s - loss: 0.2009 - acc: 0.938 - ETA: 1s - loss: 0.2000 - acc: 0.938 - ETA: 1s - loss: 0.2013 - acc: 0.938 - ETA: 1s - loss: 0.2011 - acc: 0.938 - ETA: 0s - loss: 0.2002 - acc: 0.939 - ETA: 0s - loss: 0.1992 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1990 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1977 - acc: 0.939 - ETA: 0s - loss: 0.1981 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1988 - acc: 0.939 - ETA: 0s - loss: 0.2000 - acc: 0.939 - ETA: 0s - loss: 0.1996 - acc: 0.939 - ETA: 0s - loss: 0.1999 - acc: 0.939 - ETA: 0s - loss: 0.1994 - acc: 0.939 - ETA: 0s - loss: 0.1988 - acc: 0.939 - ETA: 0s - loss: 0.1987 - acc: 0.939 - ETA: 0s - loss: 0.1991 - acc: 0.939 - ETA: 0s - loss: 0.1989 - acc: 0.939 - ETA: 0s - loss: 0.1979 - acc: 0.9399Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1978 - acc: 0.9400 - val_loss: 0.5460 - val_acc: 0.8467\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.2594 - acc: 0.950 - ETA: 9s - loss: 0.3138 - acc: 0.950 - ETA: 9s - loss: 0.2051 - acc: 0.970 - ETA: 9s - loss: 0.1795 - acc: 0.971 - ETA: 9s - loss: 0.1771 - acc: 0.972 - ETA: 9s - loss: 0.1664 - acc: 0.968 - ETA: 9s - loss: 0.1544 - acc: 0.969 - ETA: 8s - loss: 0.1602 - acc: 0.963 - ETA: 8s - loss: 0.1690 - acc: 0.955 - ETA: 8s - loss: 0.1625 - acc: 0.957 - ETA: 8s - loss: 0.1718 - acc: 0.957 - ETA: 8s - loss: 0.1640 - acc: 0.958 - ETA: 8s - loss: 0.1583 - acc: 0.958 - ETA: 8s - loss: 0.1594 - acc: 0.957 - ETA: 8s - loss: 0.1687 - acc: 0.953 - ETA: 8s - loss: 0.1662 - acc: 0.953 - ETA: 8s - loss: 0.1639 - acc: 0.954 - ETA: 8s - loss: 0.1588 - acc: 0.955 - ETA: 8s - loss: 0.1613 - acc: 0.952 - ETA: 8s - loss: 0.1726 - acc: 0.952 - ETA: 7s - loss: 0.1762 - acc: 0.948 - ETA: 7s - loss: 0.1804 - acc: 0.948 - ETA: 7s - loss: 0.1761 - acc: 0.948 - ETA: 7s - loss: 0.1756 - acc: 0.948 - ETA: 7s - loss: 0.1752 - acc: 0.946 - ETA: 7s - loss: 0.1727 - acc: 0.947 - ETA: 7s - loss: 0.1748 - acc: 0.947 - ETA: 7s - loss: 0.1729 - acc: 0.947 - ETA: 7s - loss: 0.1682 - acc: 0.949 - ETA: 7s - loss: 0.1665 - acc: 0.949 - ETA: 7s - loss: 0.1726 - acc: 0.946 - ETA: 7s - loss: 0.1734 - acc: 0.946 - ETA: 7s - loss: 0.1749 - acc: 0.944 - ETA: 7s - loss: 0.1740 - acc: 0.943 - ETA: 7s - loss: 0.1754 - acc: 0.941 - ETA: 7s - loss: 0.1718 - acc: 0.942 - ETA: 7s - loss: 0.1706 - acc: 0.941 - ETA: 7s - loss: 0.1718 - acc: 0.942 - ETA: 6s - loss: 0.1755 - acc: 0.942 - ETA: 6s - loss: 0.1718 - acc: 0.944 - ETA: 6s - loss: 0.1689 - acc: 0.945 - ETA: 6s - loss: 0.1692 - acc: 0.944 - ETA: 6s - loss: 0.1703 - acc: 0.943 - ETA: 6s - loss: 0.1758 - acc: 0.942 - ETA: 6s - loss: 0.1759 - acc: 0.942 - ETA: 6s - loss: 0.1742 - acc: 0.942 - ETA: 6s - loss: 0.1764 - acc: 0.941 - ETA: 6s - loss: 0.1829 - acc: 0.940 - ETA: 6s - loss: 0.1802 - acc: 0.941 - ETA: 6s - loss: 0.1776 - acc: 0.942 - ETA: 6s - loss: 0.1790 - acc: 0.942 - ETA: 6s - loss: 0.1765 - acc: 0.943 - ETA: 6s - loss: 0.1736 - acc: 0.944 - ETA: 6s - loss: 0.1727 - acc: 0.943 - ETA: 6s - loss: 0.1722 - acc: 0.943 - ETA: 5s - loss: 0.1703 - acc: 0.944 - ETA: 5s - loss: 0.1694 - acc: 0.944 - ETA: 5s - loss: 0.1690 - acc: 0.944 - ETA: 5s - loss: 0.1687 - acc: 0.944 - ETA: 5s - loss: 0.1723 - acc: 0.943 - ETA: 5s - loss: 0.1711 - acc: 0.943 - ETA: 5s - loss: 0.1742 - acc: 0.942 - ETA: 5s - loss: 0.1732 - acc: 0.942 - ETA: 5s - loss: 0.1713 - acc: 0.942 - ETA: 5s - loss: 0.1732 - acc: 0.942 - ETA: 5s - loss: 0.1740 - acc: 0.941 - ETA: 5s - loss: 0.1742 - acc: 0.942 - ETA: 5s - loss: 0.1754 - acc: 0.941 - ETA: 5s - loss: 0.1803 - acc: 0.941 - ETA: 5s - loss: 0.1797 - acc: 0.941 - ETA: 5s - loss: 0.1806 - acc: 0.941 - ETA: 5s - loss: 0.1809 - acc: 0.941 - ETA: 5s - loss: 0.1796 - acc: 0.942 - ETA: 5s - loss: 0.1782 - acc: 0.942 - ETA: 4s - loss: 0.1781 - acc: 0.942 - ETA: 4s - loss: 0.1795 - acc: 0.941 - ETA: 4s - loss: 0.1810 - acc: 0.941 - ETA: 4s - loss: 0.1818 - acc: 0.940 - ETA: 4s - loss: 0.1819 - acc: 0.940 - ETA: 4s - loss: 0.1812 - acc: 0.940 - ETA: 4s - loss: 0.1797 - acc: 0.941 - ETA: 4s - loss: 0.1793 - acc: 0.941 - ETA: 4s - loss: 0.1788 - acc: 0.940 - ETA: 4s - loss: 0.1782 - acc: 0.941 - ETA: 4s - loss: 0.1777 - acc: 0.941 - ETA: 4s - loss: 0.1772 - acc: 0.941 - ETA: 4s - loss: 0.1763 - acc: 0.941 - ETA: 4s - loss: 0.1745 - acc: 0.942 - ETA: 4s - loss: 0.1735 - acc: 0.942 - ETA: 4s - loss: 0.1736 - acc: 0.942 - ETA: 4s - loss: 0.1731 - acc: 0.942 - ETA: 4s - loss: 0.1719 - acc: 0.943 - ETA: 4s - loss: 0.1719 - acc: 0.943 - ETA: 3s - loss: 0.1712 - acc: 0.942 - ETA: 3s - loss: 0.1719 - acc: 0.942 - ETA: 3s - loss: 0.1713 - acc: 0.942 - ETA: 3s - loss: 0.1703 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.943 - ETA: 3s - loss: 0.1681 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.943 - ETA: 3s - loss: 0.1719 - acc: 0.943 - ETA: 3s - loss: 0.1720 - acc: 0.943 - ETA: 3s - loss: 0.1708 - acc: 0.943 - ETA: 3s - loss: 0.1708 - acc: 0.943 - ETA: 3s - loss: 0.1702 - acc: 0.943 - ETA: 3s - loss: 0.1705 - acc: 0.943 - ETA: 3s - loss: 0.1701 - acc: 0.943 - ETA: 3s - loss: 0.1692 - acc: 0.944 - ETA: 3s - loss: 0.1685 - acc: 0.944 - ETA: 3s - loss: 0.1682 - acc: 0.943 - ETA: 3s - loss: 0.1671 - acc: 0.944 - ETA: 2s - loss: 0.1663 - acc: 0.944 - ETA: 2s - loss: 0.1651 - acc: 0.944 - ETA: 2s - loss: 0.1643 - acc: 0.944 - ETA: 2s - loss: 0.1647 - acc: 0.945 - ETA: 2s - loss: 0.1678 - acc: 0.944 - ETA: 2s - loss: 0.1671 - acc: 0.944 - ETA: 2s - loss: 0.1670 - acc: 0.945 - ETA: 2s - loss: 0.1663 - acc: 0.945 - ETA: 2s - loss: 0.1689 - acc: 0.944 - ETA: 2s - loss: 0.1706 - acc: 0.944 - ETA: 2s - loss: 0.1709 - acc: 0.943 - ETA: 2s - loss: 0.1697 - acc: 0.944 - ETA: 2s - loss: 0.1695 - acc: 0.943 - ETA: 2s - loss: 0.1706 - acc: 0.943 - ETA: 2s - loss: 0.1700 - acc: 0.943 - ETA: 2s - loss: 0.1695 - acc: 0.944 - ETA: 2s - loss: 0.1702 - acc: 0.943 - ETA: 2s - loss: 0.1696 - acc: 0.943 - ETA: 2s - loss: 0.1693 - acc: 0.944 - ETA: 1s - loss: 0.1697 - acc: 0.943 - ETA: 1s - loss: 0.1698 - acc: 0.943 - ETA: 1s - loss: 0.1691 - acc: 0.944 - ETA: 1s - loss: 0.1695 - acc: 0.944 - ETA: 1s - loss: 0.1695 - acc: 0.944 - ETA: 1s - loss: 0.1693 - acc: 0.944 - ETA: 1s - loss: 0.1683 - acc: 0.944 - ETA: 1s - loss: 0.1681 - acc: 0.944 - ETA: 1s - loss: 0.1683 - acc: 0.944 - ETA: 1s - loss: 0.1685 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.944 - ETA: 1s - loss: 0.1692 - acc: 0.944 - ETA: 1s - loss: 0.1704 - acc: 0.944 - ETA: 1s - loss: 0.1704 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.944 - ETA: 1s - loss: 0.1701 - acc: 0.944 - ETA: 1s - loss: 0.1698 - acc: 0.945 - ETA: 1s - loss: 0.1694 - acc: 0.945 - ETA: 0s - loss: 0.1686 - acc: 0.945 - ETA: 0s - loss: 0.1703 - acc: 0.945 - ETA: 0s - loss: 0.1715 - acc: 0.945 - ETA: 0s - loss: 0.1711 - acc: 0.945 - ETA: 0s - loss: 0.1708 - acc: 0.945 - ETA: 0s - loss: 0.1704 - acc: 0.945 - ETA: 0s - loss: 0.1708 - acc: 0.945 - ETA: 0s - loss: 0.1713 - acc: 0.945 - ETA: 0s - loss: 0.1723 - acc: 0.945 - ETA: 0s - loss: 0.1721 - acc: 0.945 - ETA: 0s - loss: 0.1737 - acc: 0.945 - ETA: 0s - loss: 0.1733 - acc: 0.945 - ETA: 0s - loss: 0.1730 - acc: 0.945 - ETA: 0s - loss: 0.1744 - acc: 0.944 - ETA: 0s - loss: 0.1746 - acc: 0.944 - ETA: 0s - loss: 0.1746 - acc: 0.945 - ETA: 0s - loss: 0.1739 - acc: 0.945 - ETA: 0s - loss: 0.1756 - acc: 0.944 - ETA: 0s - loss: 0.1756 - acc: 0.9444Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1753 - acc: 0.9445 - val_loss: 0.5412 - val_acc: 0.8551\n",
      "Epoch 9/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0574 - acc: 0.950 - ETA: 9s - loss: 0.0555 - acc: 0.983 - ETA: 9s - loss: 0.0542 - acc: 0.990 - ETA: 9s - loss: 0.0561 - acc: 0.985 - ETA: 9s - loss: 0.0568 - acc: 0.983 - ETA: 9s - loss: 0.0938 - acc: 0.968 - ETA: 9s - loss: 0.1167 - acc: 0.961 - ETA: 9s - loss: 0.1215 - acc: 0.956 - ETA: 8s - loss: 0.1126 - acc: 0.958 - ETA: 8s - loss: 0.1044 - acc: 0.963 - ETA: 8s - loss: 0.0983 - acc: 0.966 - ETA: 8s - loss: 0.1017 - acc: 0.963 - ETA: 8s - loss: 0.0977 - acc: 0.966 - ETA: 8s - loss: 0.0968 - acc: 0.966 - ETA: 8s - loss: 0.1113 - acc: 0.958 - ETA: 8s - loss: 0.1068 - acc: 0.959 - ETA: 8s - loss: 0.1104 - acc: 0.959 - ETA: 8s - loss: 0.1079 - acc: 0.960 - ETA: 8s - loss: 0.1043 - acc: 0.960 - ETA: 8s - loss: 0.1122 - acc: 0.960 - ETA: 8s - loss: 0.1091 - acc: 0.962 - ETA: 7s - loss: 0.1048 - acc: 0.964 - ETA: 7s - loss: 0.1107 - acc: 0.961 - ETA: 7s - loss: 0.1093 - acc: 0.961 - ETA: 7s - loss: 0.1183 - acc: 0.960 - ETA: 7s - loss: 0.1185 - acc: 0.958 - ETA: 7s - loss: 0.1217 - acc: 0.956 - ETA: 7s - loss: 0.1208 - acc: 0.957 - ETA: 7s - loss: 0.1241 - acc: 0.957 - ETA: 7s - loss: 0.1235 - acc: 0.956 - ETA: 7s - loss: 0.1228 - acc: 0.956 - ETA: 7s - loss: 0.1216 - acc: 0.957 - ETA: 7s - loss: 0.1188 - acc: 0.958 - ETA: 7s - loss: 0.1212 - acc: 0.958 - ETA: 7s - loss: 0.1247 - acc: 0.958 - ETA: 7s - loss: 0.1265 - acc: 0.957 - ETA: 7s - loss: 0.1275 - acc: 0.957 - ETA: 7s - loss: 0.1295 - acc: 0.957 - ETA: 6s - loss: 0.1268 - acc: 0.958 - ETA: 6s - loss: 0.1301 - acc: 0.956 - ETA: 6s - loss: 0.1298 - acc: 0.956 - ETA: 6s - loss: 0.1277 - acc: 0.957 - ETA: 6s - loss: 0.1261 - acc: 0.957 - ETA: 6s - loss: 0.1272 - acc: 0.956 - ETA: 6s - loss: 0.1253 - acc: 0.956 - ETA: 6s - loss: 0.1235 - acc: 0.957 - ETA: 6s - loss: 0.1267 - acc: 0.956 - ETA: 6s - loss: 0.1272 - acc: 0.955 - ETA: 6s - loss: 0.1266 - acc: 0.955 - ETA: 6s - loss: 0.1290 - acc: 0.955 - ETA: 6s - loss: 0.1272 - acc: 0.955 - ETA: 6s - loss: 0.1319 - acc: 0.953 - ETA: 6s - loss: 0.1343 - acc: 0.951 - ETA: 6s - loss: 0.1354 - acc: 0.951 - ETA: 6s - loss: 0.1436 - acc: 0.950 - ETA: 6s - loss: 0.1467 - acc: 0.950 - ETA: 5s - loss: 0.1447 - acc: 0.950 - ETA: 5s - loss: 0.1453 - acc: 0.950 - ETA: 5s - loss: 0.1434 - acc: 0.951 - ETA: 5s - loss: 0.1443 - acc: 0.950 - ETA: 5s - loss: 0.1422 - acc: 0.951 - ETA: 5s - loss: 0.1460 - acc: 0.950 - ETA: 5s - loss: 0.1450 - acc: 0.951 - ETA: 5s - loss: 0.1444 - acc: 0.951 - ETA: 5s - loss: 0.1424 - acc: 0.952 - ETA: 5s - loss: 0.1406 - acc: 0.953 - ETA: 5s - loss: 0.1425 - acc: 0.952 - ETA: 5s - loss: 0.1407 - acc: 0.953 - ETA: 5s - loss: 0.1390 - acc: 0.954 - ETA: 5s - loss: 0.1425 - acc: 0.952 - ETA: 5s - loss: 0.1421 - acc: 0.952 - ETA: 5s - loss: 0.1408 - acc: 0.953 - ETA: 5s - loss: 0.1402 - acc: 0.953 - ETA: 4s - loss: 0.1394 - acc: 0.954 - ETA: 4s - loss: 0.1379 - acc: 0.954 - ETA: 4s - loss: 0.1378 - acc: 0.954 - ETA: 4s - loss: 0.1369 - acc: 0.954 - ETA: 4s - loss: 0.1373 - acc: 0.954 - ETA: 4s - loss: 0.1388 - acc: 0.953 - ETA: 4s - loss: 0.1416 - acc: 0.953 - ETA: 4s - loss: 0.1418 - acc: 0.952 - ETA: 4s - loss: 0.1431 - acc: 0.952 - ETA: 4s - loss: 0.1451 - acc: 0.952 - ETA: 4s - loss: 0.1458 - acc: 0.952 - ETA: 4s - loss: 0.1465 - acc: 0.952 - ETA: 4s - loss: 0.1481 - acc: 0.951 - ETA: 4s - loss: 0.1495 - acc: 0.951 - ETA: 4s - loss: 0.1506 - acc: 0.951 - ETA: 4s - loss: 0.1502 - acc: 0.951 - ETA: 4s - loss: 0.1497 - acc: 0.951 - ETA: 4s - loss: 0.1497 - acc: 0.951 - ETA: 3s - loss: 0.1490 - acc: 0.951 - ETA: 3s - loss: 0.1510 - acc: 0.952 - ETA: 3s - loss: 0.1516 - acc: 0.951 - ETA: 3s - loss: 0.1511 - acc: 0.952 - ETA: 3s - loss: 0.1518 - acc: 0.951 - ETA: 3s - loss: 0.1510 - acc: 0.952 - ETA: 3s - loss: 0.1506 - acc: 0.952 - ETA: 3s - loss: 0.1494 - acc: 0.952 - ETA: 3s - loss: 0.1497 - acc: 0.952 - ETA: 3s - loss: 0.1487 - acc: 0.952 - ETA: 3s - loss: 0.1503 - acc: 0.952 - ETA: 3s - loss: 0.1509 - acc: 0.952 - ETA: 3s - loss: 0.1545 - acc: 0.951 - ETA: 3s - loss: 0.1535 - acc: 0.951 - ETA: 3s - loss: 0.1538 - acc: 0.951 - ETA: 3s - loss: 0.1554 - acc: 0.951 - ETA: 3s - loss: 0.1574 - acc: 0.950 - ETA: 3s - loss: 0.1564 - acc: 0.951 - ETA: 3s - loss: 0.1559 - acc: 0.951 - ETA: 2s - loss: 0.1560 - acc: 0.951 - ETA: 2s - loss: 0.1562 - acc: 0.951 - ETA: 2s - loss: 0.1583 - acc: 0.950 - ETA: 2s - loss: 0.1581 - acc: 0.949 - ETA: 2s - loss: 0.1587 - acc: 0.949 - ETA: 2s - loss: 0.1598 - acc: 0.948 - ETA: 2s - loss: 0.1599 - acc: 0.947 - ETA: 2s - loss: 0.1589 - acc: 0.948 - ETA: 2s - loss: 0.1581 - acc: 0.948 - ETA: 2s - loss: 0.1574 - acc: 0.948 - ETA: 2s - loss: 0.1570 - acc: 0.949 - ETA: 2s - loss: 0.1559 - acc: 0.949 - ETA: 2s - loss: 0.1553 - acc: 0.949 - ETA: 2s - loss: 0.1545 - acc: 0.949 - ETA: 2s - loss: 0.1547 - acc: 0.949 - ETA: 2s - loss: 0.1556 - acc: 0.949 - ETA: 2s - loss: 0.1554 - acc: 0.949 - ETA: 2s - loss: 0.1547 - acc: 0.949 - ETA: 2s - loss: 0.1540 - acc: 0.949 - ETA: 1s - loss: 0.1536 - acc: 0.949 - ETA: 1s - loss: 0.1540 - acc: 0.949 - ETA: 1s - loss: 0.1539 - acc: 0.949 - ETA: 1s - loss: 0.1547 - acc: 0.948 - ETA: 1s - loss: 0.1538 - acc: 0.949 - ETA: 1s - loss: 0.1535 - acc: 0.949 - ETA: 1s - loss: 0.1552 - acc: 0.949 - ETA: 1s - loss: 0.1545 - acc: 0.949 - ETA: 1s - loss: 0.1539 - acc: 0.949 - ETA: 1s - loss: 0.1537 - acc: 0.949 - ETA: 1s - loss: 0.1535 - acc: 0.949 - ETA: 1s - loss: 0.1557 - acc: 0.949 - ETA: 1s - loss: 0.1554 - acc: 0.949 - ETA: 1s - loss: 0.1556 - acc: 0.949 - ETA: 1s - loss: 0.1555 - acc: 0.949 - ETA: 1s - loss: 0.1572 - acc: 0.949 - ETA: 1s - loss: 0.1564 - acc: 0.949 - ETA: 1s - loss: 0.1554 - acc: 0.949 - ETA: 0s - loss: 0.1548 - acc: 0.949 - ETA: 0s - loss: 0.1542 - acc: 0.950 - ETA: 0s - loss: 0.1538 - acc: 0.950 - ETA: 0s - loss: 0.1545 - acc: 0.950 - ETA: 0s - loss: 0.1555 - acc: 0.950 - ETA: 0s - loss: 0.1558 - acc: 0.949 - ETA: 0s - loss: 0.1573 - acc: 0.949 - ETA: 0s - loss: 0.1572 - acc: 0.949 - ETA: 0s - loss: 0.1598 - acc: 0.949 - ETA: 0s - loss: 0.1595 - acc: 0.949 - ETA: 0s - loss: 0.1618 - acc: 0.948 - ETA: 0s - loss: 0.1619 - acc: 0.948 - ETA: 0s - loss: 0.1622 - acc: 0.948 - ETA: 0s - loss: 0.1622 - acc: 0.948 - ETA: 0s - loss: 0.1620 - acc: 0.948 - ETA: 0s - loss: 0.1630 - acc: 0.948 - ETA: 0s - loss: 0.1629 - acc: 0.948 - ETA: 0s - loss: 0.1648 - acc: 0.947 - ETA: 0s - loss: 0.1643 - acc: 0.9480Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1641 - acc: 0.9481 - val_loss: 0.5540 - val_acc: 0.8587\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1739 - acc: 0.900 - ETA: 9s - loss: 0.2295 - acc: 0.933 - ETA: 9s - loss: 0.2222 - acc: 0.940 - ETA: 9s - loss: 0.1620 - acc: 0.957 - ETA: 9s - loss: 0.1885 - acc: 0.950 - ETA: 9s - loss: 0.1715 - acc: 0.954 - ETA: 9s - loss: 0.1852 - acc: 0.957 - ETA: 8s - loss: 0.1658 - acc: 0.963 - ETA: 8s - loss: 0.1644 - acc: 0.958 - ETA: 8s - loss: 0.1551 - acc: 0.960 - ETA: 8s - loss: 0.1557 - acc: 0.959 - ETA: 8s - loss: 0.1472 - acc: 0.960 - ETA: 8s - loss: 0.1384 - acc: 0.964 - ETA: 8s - loss: 0.1297 - acc: 0.966 - ETA: 8s - loss: 0.1246 - acc: 0.967 - ETA: 8s - loss: 0.1278 - acc: 0.964 - ETA: 8s - loss: 0.1258 - acc: 0.963 - ETA: 8s - loss: 0.1196 - acc: 0.965 - ETA: 8s - loss: 0.1271 - acc: 0.963 - ETA: 8s - loss: 0.1219 - acc: 0.965 - ETA: 8s - loss: 0.1187 - acc: 0.965 - ETA: 8s - loss: 0.1157 - acc: 0.966 - ETA: 8s - loss: 0.1143 - acc: 0.965 - ETA: 7s - loss: 0.1112 - acc: 0.967 - ETA: 7s - loss: 0.1077 - acc: 0.968 - ETA: 7s - loss: 0.1170 - acc: 0.967 - ETA: 7s - loss: 0.1132 - acc: 0.968 - ETA: 7s - loss: 0.1110 - acc: 0.970 - ETA: 7s - loss: 0.1152 - acc: 0.969 - ETA: 7s - loss: 0.1119 - acc: 0.970 - ETA: 7s - loss: 0.1159 - acc: 0.969 - ETA: 7s - loss: 0.1189 - acc: 0.969 - ETA: 7s - loss: 0.1175 - acc: 0.969 - ETA: 7s - loss: 0.1206 - acc: 0.968 - ETA: 7s - loss: 0.1218 - acc: 0.968 - ETA: 7s - loss: 0.1237 - acc: 0.968 - ETA: 7s - loss: 0.1267 - acc: 0.967 - ETA: 7s - loss: 0.1254 - acc: 0.968 - ETA: 7s - loss: 0.1269 - acc: 0.966 - ETA: 6s - loss: 0.1255 - acc: 0.966 - ETA: 6s - loss: 0.1270 - acc: 0.966 - ETA: 6s - loss: 0.1295 - acc: 0.965 - ETA: 6s - loss: 0.1292 - acc: 0.964 - ETA: 6s - loss: 0.1314 - acc: 0.962 - ETA: 6s - loss: 0.1358 - acc: 0.961 - ETA: 6s - loss: 0.1394 - acc: 0.960 - ETA: 6s - loss: 0.1383 - acc: 0.960 - ETA: 6s - loss: 0.1362 - acc: 0.961 - ETA: 6s - loss: 0.1354 - acc: 0.961 - ETA: 6s - loss: 0.1338 - acc: 0.962 - ETA: 6s - loss: 0.1316 - acc: 0.962 - ETA: 6s - loss: 0.1325 - acc: 0.962 - ETA: 6s - loss: 0.1318 - acc: 0.962 - ETA: 6s - loss: 0.1298 - acc: 0.963 - ETA: 6s - loss: 0.1299 - acc: 0.962 - ETA: 6s - loss: 0.1287 - acc: 0.963 - ETA: 6s - loss: 0.1282 - acc: 0.963 - ETA: 5s - loss: 0.1289 - acc: 0.963 - ETA: 5s - loss: 0.1293 - acc: 0.962 - ETA: 5s - loss: 0.1307 - acc: 0.961 - ETA: 5s - loss: 0.1296 - acc: 0.962 - ETA: 5s - loss: 0.1350 - acc: 0.962 - ETA: 5s - loss: 0.1333 - acc: 0.962 - ETA: 5s - loss: 0.1331 - acc: 0.962 - ETA: 5s - loss: 0.1314 - acc: 0.963 - ETA: 5s - loss: 0.1300 - acc: 0.963 - ETA: 5s - loss: 0.1307 - acc: 0.962 - ETA: 5s - loss: 0.1351 - acc: 0.961 - ETA: 5s - loss: 0.1355 - acc: 0.960 - ETA: 5s - loss: 0.1373 - acc: 0.960 - ETA: 5s - loss: 0.1357 - acc: 0.961 - ETA: 5s - loss: 0.1396 - acc: 0.960 - ETA: 5s - loss: 0.1394 - acc: 0.959 - ETA: 5s - loss: 0.1394 - acc: 0.958 - ETA: 5s - loss: 0.1469 - acc: 0.957 - ETA: 4s - loss: 0.1456 - acc: 0.957 - ETA: 4s - loss: 0.1476 - acc: 0.956 - ETA: 4s - loss: 0.1490 - acc: 0.956 - ETA: 4s - loss: 0.1474 - acc: 0.956 - ETA: 4s - loss: 0.1471 - acc: 0.956 - ETA: 4s - loss: 0.1461 - acc: 0.956 - ETA: 4s - loss: 0.1470 - acc: 0.955 - ETA: 4s - loss: 0.1468 - acc: 0.955 - ETA: 4s - loss: 0.1456 - acc: 0.956 - ETA: 4s - loss: 0.1465 - acc: 0.955 - ETA: 4s - loss: 0.1459 - acc: 0.955 - ETA: 4s - loss: 0.1457 - acc: 0.955 - ETA: 4s - loss: 0.1471 - acc: 0.955 - ETA: 4s - loss: 0.1493 - acc: 0.954 - ETA: 4s - loss: 0.1483 - acc: 0.955 - ETA: 4s - loss: 0.1469 - acc: 0.955 - ETA: 4s - loss: 0.1513 - acc: 0.955 - ETA: 4s - loss: 0.1503 - acc: 0.955 - ETA: 4s - loss: 0.1539 - acc: 0.955 - ETA: 3s - loss: 0.1530 - acc: 0.956 - ETA: 3s - loss: 0.1533 - acc: 0.956 - ETA: 3s - loss: 0.1528 - acc: 0.956 - ETA: 3s - loss: 0.1527 - acc: 0.956 - ETA: 3s - loss: 0.1513 - acc: 0.956 - ETA: 3s - loss: 0.1520 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1512 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1504 - acc: 0.957 - ETA: 3s - loss: 0.1502 - acc: 0.956 - ETA: 3s - loss: 0.1507 - acc: 0.956 - ETA: 3s - loss: 0.1506 - acc: 0.956 - ETA: 3s - loss: 0.1495 - acc: 0.957 - ETA: 3s - loss: 0.1499 - acc: 0.956 - ETA: 3s - loss: 0.1514 - acc: 0.956 - ETA: 3s - loss: 0.1517 - acc: 0.956 - ETA: 3s - loss: 0.1511 - acc: 0.956 - ETA: 2s - loss: 0.1505 - acc: 0.956 - ETA: 2s - loss: 0.1507 - acc: 0.956 - ETA: 2s - loss: 0.1503 - acc: 0.956 - ETA: 2s - loss: 0.1509 - acc: 0.956 - ETA: 2s - loss: 0.1508 - acc: 0.956 - ETA: 2s - loss: 0.1511 - acc: 0.956 - ETA: 2s - loss: 0.1517 - acc: 0.955 - ETA: 2s - loss: 0.1509 - acc: 0.956 - ETA: 2s - loss: 0.1504 - acc: 0.956 - ETA: 2s - loss: 0.1501 - acc: 0.956 - ETA: 2s - loss: 0.1494 - acc: 0.956 - ETA: 2s - loss: 0.1485 - acc: 0.956 - ETA: 2s - loss: 0.1488 - acc: 0.956 - ETA: 2s - loss: 0.1496 - acc: 0.956 - ETA: 2s - loss: 0.1491 - acc: 0.956 - ETA: 2s - loss: 0.1487 - acc: 0.956 - ETA: 2s - loss: 0.1483 - acc: 0.956 - ETA: 2s - loss: 0.1480 - acc: 0.956 - ETA: 1s - loss: 0.1470 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1453 - acc: 0.957 - ETA: 1s - loss: 0.1452 - acc: 0.957 - ETA: 1s - loss: 0.1450 - acc: 0.957 - ETA: 1s - loss: 0.1453 - acc: 0.957 - ETA: 1s - loss: 0.1452 - acc: 0.957 - ETA: 1s - loss: 0.1446 - acc: 0.957 - ETA: 1s - loss: 0.1444 - acc: 0.957 - ETA: 1s - loss: 0.1438 - acc: 0.957 - ETA: 1s - loss: 0.1461 - acc: 0.956 - ETA: 1s - loss: 0.1461 - acc: 0.956 - ETA: 1s - loss: 0.1452 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1459 - acc: 0.956 - ETA: 1s - loss: 0.1460 - acc: 0.956 - ETA: 1s - loss: 0.1474 - acc: 0.955 - ETA: 1s - loss: 0.1467 - acc: 0.955 - ETA: 1s - loss: 0.1470 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1479 - acc: 0.955 - ETA: 0s - loss: 0.1473 - acc: 0.955 - ETA: 0s - loss: 0.1478 - acc: 0.955 - ETA: 0s - loss: 0.1486 - acc: 0.954 - ETA: 0s - loss: 0.1484 - acc: 0.954 - ETA: 0s - loss: 0.1484 - acc: 0.954 - ETA: 0s - loss: 0.1479 - acc: 0.954 - ETA: 0s - loss: 0.1474 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1462 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1466 - acc: 0.955 - ETA: 0s - loss: 0.1465 - acc: 0.955 - ETA: 0s - loss: 0.1471 - acc: 0.955 - ETA: 0s - loss: 0.1470 - acc: 0.9554Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1467 - acc: 0.9555 - val_loss: 0.5868 - val_acc: 0.8563\n",
      "Epoch 11/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1681 - acc: 0.900 - ETA: 8s - loss: 0.1590 - acc: 0.950 - ETA: 8s - loss: 0.0993 - acc: 0.970 - ETA: 8s - loss: 0.0927 - acc: 0.964 - ETA: 8s - loss: 0.1060 - acc: 0.955 - ETA: 8s - loss: 0.1220 - acc: 0.954 - ETA: 8s - loss: 0.1073 - acc: 0.961 - ETA: 8s - loss: 0.0991 - acc: 0.966 - ETA: 8s - loss: 0.1053 - acc: 0.958 - ETA: 8s - loss: 0.1268 - acc: 0.955 - ETA: 8s - loss: 0.1202 - acc: 0.957 - ETA: 8s - loss: 0.1146 - acc: 0.960 - ETA: 8s - loss: 0.1118 - acc: 0.962 - ETA: 8s - loss: 0.1053 - acc: 0.964 - ETA: 8s - loss: 0.1033 - acc: 0.965 - ETA: 8s - loss: 0.1121 - acc: 0.962 - ETA: 8s - loss: 0.1149 - acc: 0.962 - ETA: 7s - loss: 0.1261 - acc: 0.958 - ETA: 7s - loss: 0.1317 - acc: 0.959 - ETA: 7s - loss: 0.1268 - acc: 0.961 - ETA: 7s - loss: 0.1253 - acc: 0.961 - ETA: 7s - loss: 0.1298 - acc: 0.958 - ETA: 7s - loss: 0.1294 - acc: 0.957 - ETA: 7s - loss: 0.1251 - acc: 0.959 - ETA: 7s - loss: 0.1235 - acc: 0.959 - ETA: 7s - loss: 0.1275 - acc: 0.958 - ETA: 7s - loss: 0.1237 - acc: 0.960 - ETA: 7s - loss: 0.1334 - acc: 0.959 - ETA: 7s - loss: 0.1327 - acc: 0.958 - ETA: 7s - loss: 0.1321 - acc: 0.959 - ETA: 7s - loss: 0.1315 - acc: 0.959 - ETA: 7s - loss: 0.1443 - acc: 0.958 - ETA: 7s - loss: 0.1421 - acc: 0.959 - ETA: 7s - loss: 0.1403 - acc: 0.959 - ETA: 7s - loss: 0.1378 - acc: 0.959 - ETA: 7s - loss: 0.1356 - acc: 0.959 - ETA: 6s - loss: 0.1361 - acc: 0.958 - ETA: 6s - loss: 0.1368 - acc: 0.958 - ETA: 6s - loss: 0.1347 - acc: 0.959 - ETA: 6s - loss: 0.1321 - acc: 0.960 - ETA: 6s - loss: 0.1291 - acc: 0.961 - ETA: 6s - loss: 0.1319 - acc: 0.959 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1282 - acc: 0.960 - ETA: 6s - loss: 0.1290 - acc: 0.959 - ETA: 6s - loss: 0.1297 - acc: 0.959 - ETA: 6s - loss: 0.1313 - acc: 0.958 - ETA: 6s - loss: 0.1315 - acc: 0.958 - ETA: 6s - loss: 0.1323 - acc: 0.958 - ETA: 6s - loss: 0.1298 - acc: 0.958 - ETA: 6s - loss: 0.1315 - acc: 0.959 - ETA: 6s - loss: 0.1296 - acc: 0.960 - ETA: 6s - loss: 0.1283 - acc: 0.960 - ETA: 6s - loss: 0.1263 - acc: 0.961 - ETA: 6s - loss: 0.1271 - acc: 0.961 - ETA: 5s - loss: 0.1274 - acc: 0.961 - ETA: 5s - loss: 0.1256 - acc: 0.961 - ETA: 5s - loss: 0.1261 - acc: 0.961 - ETA: 5s - loss: 0.1261 - acc: 0.961 - ETA: 5s - loss: 0.1291 - acc: 0.960 - ETA: 5s - loss: 0.1292 - acc: 0.960 - ETA: 5s - loss: 0.1279 - acc: 0.960 - ETA: 5s - loss: 0.1291 - acc: 0.960 - ETA: 5s - loss: 0.1287 - acc: 0.960 - ETA: 5s - loss: 0.1275 - acc: 0.961 - ETA: 5s - loss: 0.1277 - acc: 0.960 - ETA: 5s - loss: 0.1313 - acc: 0.960 - ETA: 5s - loss: 0.1304 - acc: 0.960 - ETA: 5s - loss: 0.1307 - acc: 0.960 - ETA: 5s - loss: 0.1300 - acc: 0.960 - ETA: 5s - loss: 0.1309 - acc: 0.960 - ETA: 5s - loss: 0.1295 - acc: 0.960 - ETA: 5s - loss: 0.1312 - acc: 0.960 - ETA: 4s - loss: 0.1299 - acc: 0.960 - ETA: 4s - loss: 0.1294 - acc: 0.960 - ETA: 4s - loss: 0.1291 - acc: 0.960 - ETA: 4s - loss: 0.1309 - acc: 0.960 - ETA: 4s - loss: 0.1297 - acc: 0.960 - ETA: 4s - loss: 0.1305 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1328 - acc: 0.960 - ETA: 4s - loss: 0.1315 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1316 - acc: 0.960 - ETA: 4s - loss: 0.1317 - acc: 0.960 - ETA: 4s - loss: 0.1340 - acc: 0.960 - ETA: 4s - loss: 0.1329 - acc: 0.960 - ETA: 4s - loss: 0.1328 - acc: 0.960 - ETA: 4s - loss: 0.1339 - acc: 0.960 - ETA: 4s - loss: 0.1330 - acc: 0.960 - ETA: 4s - loss: 0.1334 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.960 - ETA: 3s - loss: 0.1321 - acc: 0.960 - ETA: 3s - loss: 0.1313 - acc: 0.960 - ETA: 3s - loss: 0.1302 - acc: 0.961 - ETA: 3s - loss: 0.1298 - acc: 0.960 - ETA: 3s - loss: 0.1299 - acc: 0.960 - ETA: 3s - loss: 0.1311 - acc: 0.960 - ETA: 3s - loss: 0.1306 - acc: 0.960 - ETA: 3s - loss: 0.1313 - acc: 0.960 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1326 - acc: 0.959 - ETA: 3s - loss: 0.1317 - acc: 0.959 - ETA: 3s - loss: 0.1323 - acc: 0.959 - ETA: 3s - loss: 0.1315 - acc: 0.959 - ETA: 3s - loss: 0.1323 - acc: 0.959 - ETA: 3s - loss: 0.1316 - acc: 0.959 - ETA: 3s - loss: 0.1316 - acc: 0.959 - ETA: 3s - loss: 0.1329 - acc: 0.959 - ETA: 3s - loss: 0.1329 - acc: 0.959 - ETA: 2s - loss: 0.1330 - acc: 0.959 - ETA: 2s - loss: 0.1345 - acc: 0.959 - ETA: 2s - loss: 0.1341 - acc: 0.959 - ETA: 2s - loss: 0.1340 - acc: 0.959 - ETA: 2s - loss: 0.1350 - acc: 0.959 - ETA: 2s - loss: 0.1366 - acc: 0.958 - ETA: 2s - loss: 0.1362 - acc: 0.958 - ETA: 2s - loss: 0.1369 - acc: 0.958 - ETA: 2s - loss: 0.1367 - acc: 0.957 - ETA: 2s - loss: 0.1362 - acc: 0.958 - ETA: 2s - loss: 0.1371 - acc: 0.957 - ETA: 2s - loss: 0.1377 - acc: 0.957 - ETA: 2s - loss: 0.1377 - acc: 0.957 - ETA: 2s - loss: 0.1379 - acc: 0.957 - ETA: 2s - loss: 0.1372 - acc: 0.957 - ETA: 2s - loss: 0.1364 - acc: 0.957 - ETA: 2s - loss: 0.1375 - acc: 0.957 - ETA: 2s - loss: 0.1372 - acc: 0.958 - ETA: 2s - loss: 0.1366 - acc: 0.958 - ETA: 1s - loss: 0.1358 - acc: 0.958 - ETA: 1s - loss: 0.1350 - acc: 0.958 - ETA: 1s - loss: 0.1342 - acc: 0.958 - ETA: 1s - loss: 0.1335 - acc: 0.959 - ETA: 1s - loss: 0.1327 - acc: 0.959 - ETA: 1s - loss: 0.1321 - acc: 0.959 - ETA: 1s - loss: 0.1315 - acc: 0.959 - ETA: 1s - loss: 0.1318 - acc: 0.959 - ETA: 1s - loss: 0.1321 - acc: 0.959 - ETA: 1s - loss: 0.1336 - acc: 0.959 - ETA: 1s - loss: 0.1343 - acc: 0.958 - ETA: 1s - loss: 0.1343 - acc: 0.958 - ETA: 1s - loss: 0.1338 - acc: 0.958 - ETA: 1s - loss: 0.1333 - acc: 0.959 - ETA: 1s - loss: 0.1328 - acc: 0.959 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1329 - acc: 0.958 - ETA: 1s - loss: 0.1325 - acc: 0.958 - ETA: 1s - loss: 0.1318 - acc: 0.959 - ETA: 0s - loss: 0.1310 - acc: 0.959 - ETA: 0s - loss: 0.1323 - acc: 0.959 - ETA: 0s - loss: 0.1342 - acc: 0.959 - ETA: 0s - loss: 0.1342 - acc: 0.959 - ETA: 0s - loss: 0.1341 - acc: 0.959 - ETA: 0s - loss: 0.1346 - acc: 0.959 - ETA: 0s - loss: 0.1341 - acc: 0.959 - ETA: 0s - loss: 0.1346 - acc: 0.959 - ETA: 0s - loss: 0.1339 - acc: 0.959 - ETA: 0s - loss: 0.1350 - acc: 0.959 - ETA: 0s - loss: 0.1345 - acc: 0.959 - ETA: 0s - loss: 0.1362 - acc: 0.958 - ETA: 0s - loss: 0.1366 - acc: 0.958 - ETA: 0s - loss: 0.1371 - acc: 0.958 - ETA: 0s - loss: 0.1374 - acc: 0.958 - ETA: 0s - loss: 0.1373 - acc: 0.958 - ETA: 0s - loss: 0.1368 - acc: 0.958 - ETA: 0s - loss: 0.1390 - acc: 0.9587Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1391 - acc: 0.9585 - val_loss: 0.5643 - val_acc: 0.8599\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.3356 - acc: 0.950 - ETA: 9s - loss: 0.1709 - acc: 0.966 - ETA: 9s - loss: 0.1071 - acc: 0.980 - ETA: 9s - loss: 0.0961 - acc: 0.978 - ETA: 8s - loss: 0.0794 - acc: 0.983 - ETA: 8s - loss: 0.0931 - acc: 0.981 - ETA: 8s - loss: 0.0909 - acc: 0.980 - ETA: 8s - loss: 0.0969 - acc: 0.976 - ETA: 8s - loss: 0.0993 - acc: 0.973 - ETA: 8s - loss: 0.1060 - acc: 0.971 - ETA: 8s - loss: 0.0975 - acc: 0.973 - ETA: 8s - loss: 0.1072 - acc: 0.973 - ETA: 8s - loss: 0.1095 - acc: 0.974 - ETA: 8s - loss: 0.1156 - acc: 0.972 - ETA: 8s - loss: 0.1101 - acc: 0.972 - ETA: 8s - loss: 0.1095 - acc: 0.971 - ETA: 8s - loss: 0.1042 - acc: 0.973 - ETA: 7s - loss: 0.1011 - acc: 0.973 - ETA: 7s - loss: 0.0984 - acc: 0.973 - ETA: 7s - loss: 0.0960 - acc: 0.973 - ETA: 7s - loss: 0.1033 - acc: 0.972 - ETA: 7s - loss: 0.0989 - acc: 0.973 - ETA: 7s - loss: 0.0980 - acc: 0.973 - ETA: 7s - loss: 0.0975 - acc: 0.974 - ETA: 7s - loss: 0.0954 - acc: 0.974 - ETA: 7s - loss: 0.0990 - acc: 0.971 - ETA: 7s - loss: 0.0976 - acc: 0.971 - ETA: 7s - loss: 0.0958 - acc: 0.971 - ETA: 7s - loss: 0.0967 - acc: 0.969 - ETA: 7s - loss: 0.0972 - acc: 0.969 - ETA: 7s - loss: 0.0997 - acc: 0.968 - ETA: 7s - loss: 0.1007 - acc: 0.968 - ETA: 7s - loss: 0.1021 - acc: 0.968 - ETA: 7s - loss: 0.1009 - acc: 0.968 - ETA: 7s - loss: 0.1120 - acc: 0.966 - ETA: 7s - loss: 0.1097 - acc: 0.967 - ETA: 7s - loss: 0.1088 - acc: 0.966 - ETA: 6s - loss: 0.1137 - acc: 0.965 - ETA: 6s - loss: 0.1137 - acc: 0.965 - ETA: 6s - loss: 0.1160 - acc: 0.965 - ETA: 6s - loss: 0.1181 - acc: 0.964 - ETA: 6s - loss: 0.1199 - acc: 0.964 - ETA: 6s - loss: 0.1190 - acc: 0.964 - ETA: 6s - loss: 0.1166 - acc: 0.965 - ETA: 6s - loss: 0.1147 - acc: 0.966 - ETA: 6s - loss: 0.1143 - acc: 0.965 - ETA: 6s - loss: 0.1170 - acc: 0.964 - ETA: 6s - loss: 0.1162 - acc: 0.964 - ETA: 6s - loss: 0.1142 - acc: 0.964 - ETA: 6s - loss: 0.1151 - acc: 0.964 - ETA: 6s - loss: 0.1178 - acc: 0.963 - ETA: 6s - loss: 0.1164 - acc: 0.963 - ETA: 6s - loss: 0.1184 - acc: 0.962 - ETA: 6s - loss: 0.1182 - acc: 0.963 - ETA: 6s - loss: 0.1185 - acc: 0.962 - ETA: 6s - loss: 0.1176 - acc: 0.962 - ETA: 5s - loss: 0.1173 - acc: 0.962 - ETA: 5s - loss: 0.1175 - acc: 0.962 - ETA: 5s - loss: 0.1180 - acc: 0.962 - ETA: 5s - loss: 0.1166 - acc: 0.963 - ETA: 5s - loss: 0.1195 - acc: 0.962 - ETA: 5s - loss: 0.1198 - acc: 0.962 - ETA: 5s - loss: 0.1252 - acc: 0.962 - ETA: 5s - loss: 0.1238 - acc: 0.962 - ETA: 5s - loss: 0.1224 - acc: 0.963 - ETA: 5s - loss: 0.1208 - acc: 0.963 - ETA: 5s - loss: 0.1208 - acc: 0.963 - ETA: 5s - loss: 0.1194 - acc: 0.964 - ETA: 5s - loss: 0.1188 - acc: 0.963 - ETA: 5s - loss: 0.1184 - acc: 0.963 - ETA: 5s - loss: 0.1172 - acc: 0.964 - ETA: 5s - loss: 0.1189 - acc: 0.964 - ETA: 5s - loss: 0.1192 - acc: 0.964 - ETA: 5s - loss: 0.1192 - acc: 0.964 - ETA: 4s - loss: 0.1184 - acc: 0.964 - ETA: 4s - loss: 0.1173 - acc: 0.964 - ETA: 4s - loss: 0.1192 - acc: 0.964 - ETA: 4s - loss: 0.1181 - acc: 0.965 - ETA: 4s - loss: 0.1170 - acc: 0.965 - ETA: 4s - loss: 0.1173 - acc: 0.965 - ETA: 4s - loss: 0.1164 - acc: 0.965 - ETA: 4s - loss: 0.1152 - acc: 0.965 - ETA: 4s - loss: 0.1161 - acc: 0.965 - ETA: 4s - loss: 0.1205 - acc: 0.965 - ETA: 4s - loss: 0.1194 - acc: 0.965 - ETA: 4s - loss: 0.1182 - acc: 0.966 - ETA: 4s - loss: 0.1180 - acc: 0.966 - ETA: 4s - loss: 0.1174 - acc: 0.966 - ETA: 4s - loss: 0.1183 - acc: 0.966 - ETA: 4s - loss: 0.1185 - acc: 0.966 - ETA: 4s - loss: 0.1176 - acc: 0.966 - ETA: 4s - loss: 0.1182 - acc: 0.966 - ETA: 4s - loss: 0.1187 - acc: 0.965 - ETA: 3s - loss: 0.1178 - acc: 0.965 - ETA: 3s - loss: 0.1177 - acc: 0.965 - ETA: 3s - loss: 0.1191 - acc: 0.965 - ETA: 3s - loss: 0.1189 - acc: 0.965 - ETA: 3s - loss: 0.1184 - acc: 0.965 - ETA: 3s - loss: 0.1197 - acc: 0.965 - ETA: 3s - loss: 0.1191 - acc: 0.965 - ETA: 3s - loss: 0.1190 - acc: 0.965 - ETA: 3s - loss: 0.1207 - acc: 0.965 - ETA: 3s - loss: 0.1197 - acc: 0.966 - ETA: 3s - loss: 0.1188 - acc: 0.966 - ETA: 3s - loss: 0.1183 - acc: 0.966 - ETA: 3s - loss: 0.1190 - acc: 0.966 - ETA: 3s - loss: 0.1188 - acc: 0.966 - ETA: 3s - loss: 0.1186 - acc: 0.966 - ETA: 3s - loss: 0.1179 - acc: 0.966 - ETA: 3s - loss: 0.1178 - acc: 0.966 - ETA: 3s - loss: 0.1182 - acc: 0.966 - ETA: 2s - loss: 0.1178 - acc: 0.965 - ETA: 2s - loss: 0.1173 - acc: 0.966 - ETA: 2s - loss: 0.1169 - acc: 0.966 - ETA: 2s - loss: 0.1177 - acc: 0.965 - ETA: 2s - loss: 0.1168 - acc: 0.966 - ETA: 2s - loss: 0.1177 - acc: 0.966 - ETA: 2s - loss: 0.1183 - acc: 0.965 - ETA: 2s - loss: 0.1186 - acc: 0.965 - ETA: 2s - loss: 0.1207 - acc: 0.965 - ETA: 2s - loss: 0.1217 - acc: 0.965 - ETA: 2s - loss: 0.1214 - acc: 0.965 - ETA: 2s - loss: 0.1218 - acc: 0.965 - ETA: 2s - loss: 0.1226 - acc: 0.964 - ETA: 2s - loss: 0.1254 - acc: 0.964 - ETA: 2s - loss: 0.1257 - acc: 0.964 - ETA: 2s - loss: 0.1248 - acc: 0.965 - ETA: 2s - loss: 0.1245 - acc: 0.964 - ETA: 2s - loss: 0.1236 - acc: 0.965 - ETA: 2s - loss: 0.1264 - acc: 0.964 - ETA: 1s - loss: 0.1255 - acc: 0.965 - ETA: 1s - loss: 0.1248 - acc: 0.965 - ETA: 1s - loss: 0.1244 - acc: 0.965 - ETA: 1s - loss: 0.1262 - acc: 0.964 - ETA: 1s - loss: 0.1256 - acc: 0.964 - ETA: 1s - loss: 0.1249 - acc: 0.964 - ETA: 1s - loss: 0.1252 - acc: 0.965 - ETA: 1s - loss: 0.1249 - acc: 0.965 - ETA: 1s - loss: 0.1252 - acc: 0.965 - ETA: 1s - loss: 0.1263 - acc: 0.965 - ETA: 1s - loss: 0.1266 - acc: 0.964 - ETA: 1s - loss: 0.1270 - acc: 0.964 - ETA: 1s - loss: 0.1276 - acc: 0.964 - ETA: 1s - loss: 0.1270 - acc: 0.964 - ETA: 1s - loss: 0.1265 - acc: 0.964 - ETA: 1s - loss: 0.1263 - acc: 0.964 - ETA: 1s - loss: 0.1256 - acc: 0.964 - ETA: 1s - loss: 0.1264 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1280 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1271 - acc: 0.964 - ETA: 0s - loss: 0.1273 - acc: 0.964 - ETA: 0s - loss: 0.1275 - acc: 0.964 - ETA: 0s - loss: 0.1273 - acc: 0.964 - ETA: 0s - loss: 0.1269 - acc: 0.964 - ETA: 0s - loss: 0.1268 - acc: 0.964 - ETA: 0s - loss: 0.1277 - acc: 0.964 - ETA: 0s - loss: 0.1278 - acc: 0.963 - ETA: 0s - loss: 0.1273 - acc: 0.963 - ETA: 0s - loss: 0.1278 - acc: 0.963 - ETA: 0s - loss: 0.1277 - acc: 0.963 - ETA: 0s - loss: 0.1277 - acc: 0.963 - ETA: 0s - loss: 0.1275 - acc: 0.963 - ETA: 0s - loss: 0.1283 - acc: 0.963 - ETA: 0s - loss: 0.1282 - acc: 0.9637Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1280 - acc: 0.9638 - val_loss: 0.6099 - val_acc: 0.8539\n",
      "Epoch 13/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0916 - acc: 0.950 - ETA: 8s - loss: 0.1493 - acc: 0.950 - ETA: 8s - loss: 0.1048 - acc: 0.970 - ETA: 8s - loss: 0.0780 - acc: 0.978 - ETA: 8s - loss: 0.1037 - acc: 0.977 - ETA: 8s - loss: 0.1021 - acc: 0.977 - ETA: 8s - loss: 0.1030 - acc: 0.969 - ETA: 8s - loss: 0.1076 - acc: 0.963 - ETA: 8s - loss: 0.1017 - acc: 0.964 - ETA: 8s - loss: 0.1327 - acc: 0.963 - ETA: 8s - loss: 0.1258 - acc: 0.964 - ETA: 8s - loss: 0.1312 - acc: 0.965 - ETA: 8s - loss: 0.1353 - acc: 0.962 - ETA: 8s - loss: 0.1284 - acc: 0.964 - ETA: 8s - loss: 0.1212 - acc: 0.967 - ETA: 8s - loss: 0.1147 - acc: 0.969 - ETA: 8s - loss: 0.1433 - acc: 0.965 - ETA: 8s - loss: 0.1441 - acc: 0.964 - ETA: 7s - loss: 0.1371 - acc: 0.966 - ETA: 7s - loss: 0.1360 - acc: 0.965 - ETA: 7s - loss: 0.1326 - acc: 0.964 - ETA: 7s - loss: 0.1325 - acc: 0.964 - ETA: 7s - loss: 0.1301 - acc: 0.964 - ETA: 7s - loss: 0.1280 - acc: 0.964 - ETA: 7s - loss: 0.1240 - acc: 0.965 - ETA: 7s - loss: 0.1217 - acc: 0.965 - ETA: 7s - loss: 0.1254 - acc: 0.964 - ETA: 7s - loss: 0.1221 - acc: 0.965 - ETA: 7s - loss: 0.1357 - acc: 0.964 - ETA: 7s - loss: 0.1349 - acc: 0.963 - ETA: 7s - loss: 0.1330 - acc: 0.963 - ETA: 7s - loss: 0.1303 - acc: 0.965 - ETA: 7s - loss: 0.1286 - acc: 0.965 - ETA: 7s - loss: 0.1273 - acc: 0.965 - ETA: 7s - loss: 0.1270 - acc: 0.964 - ETA: 7s - loss: 0.1241 - acc: 0.965 - ETA: 7s - loss: 0.1231 - acc: 0.965 - ETA: 6s - loss: 0.1223 - acc: 0.964 - ETA: 6s - loss: 0.1218 - acc: 0.964 - ETA: 6s - loss: 0.1202 - acc: 0.965 - ETA: 6s - loss: 0.1179 - acc: 0.966 - ETA: 6s - loss: 0.1231 - acc: 0.965 - ETA: 6s - loss: 0.1211 - acc: 0.966 - ETA: 6s - loss: 0.1204 - acc: 0.966 - ETA: 6s - loss: 0.1190 - acc: 0.966 - ETA: 6s - loss: 0.1235 - acc: 0.965 - ETA: 6s - loss: 0.1216 - acc: 0.965 - ETA: 6s - loss: 0.1197 - acc: 0.965 - ETA: 6s - loss: 0.1177 - acc: 0.966 - ETA: 6s - loss: 0.1182 - acc: 0.965 - ETA: 6s - loss: 0.1179 - acc: 0.964 - ETA: 6s - loss: 0.1170 - acc: 0.964 - ETA: 6s - loss: 0.1190 - acc: 0.963 - ETA: 6s - loss: 0.1185 - acc: 0.963 - ETA: 6s - loss: 0.1234 - acc: 0.962 - ETA: 6s - loss: 0.1227 - acc: 0.963 - ETA: 5s - loss: 0.1261 - acc: 0.962 - ETA: 5s - loss: 0.1255 - acc: 0.962 - ETA: 5s - loss: 0.1267 - acc: 0.962 - ETA: 5s - loss: 0.1339 - acc: 0.960 - ETA: 5s - loss: 0.1325 - acc: 0.961 - ETA: 5s - loss: 0.1344 - acc: 0.961 - ETA: 5s - loss: 0.1346 - acc: 0.960 - ETA: 5s - loss: 0.1341 - acc: 0.961 - ETA: 5s - loss: 0.1334 - acc: 0.960 - ETA: 5s - loss: 0.1316 - acc: 0.961 - ETA: 5s - loss: 0.1323 - acc: 0.961 - ETA: 5s - loss: 0.1311 - acc: 0.961 - ETA: 5s - loss: 0.1296 - acc: 0.962 - ETA: 5s - loss: 0.1297 - acc: 0.961 - ETA: 5s - loss: 0.1289 - acc: 0.961 - ETA: 5s - loss: 0.1322 - acc: 0.961 - ETA: 5s - loss: 0.1345 - acc: 0.960 - ETA: 5s - loss: 0.1331 - acc: 0.960 - ETA: 4s - loss: 0.1334 - acc: 0.961 - ETA: 4s - loss: 0.1332 - acc: 0.960 - ETA: 4s - loss: 0.1326 - acc: 0.960 - ETA: 4s - loss: 0.1309 - acc: 0.960 - ETA: 4s - loss: 0.1314 - acc: 0.960 - ETA: 4s - loss: 0.1304 - acc: 0.960 - ETA: 4s - loss: 0.1297 - acc: 0.960 - ETA: 4s - loss: 0.1305 - acc: 0.960 - ETA: 4s - loss: 0.1317 - acc: 0.960 - ETA: 4s - loss: 0.1307 - acc: 0.960 - ETA: 4s - loss: 0.1302 - acc: 0.960 - ETA: 4s - loss: 0.1291 - acc: 0.960 - ETA: 4s - loss: 0.1282 - acc: 0.961 - ETA: 4s - loss: 0.1283 - acc: 0.960 - ETA: 4s - loss: 0.1292 - acc: 0.960 - ETA: 4s - loss: 0.1280 - acc: 0.961 - ETA: 4s - loss: 0.1278 - acc: 0.961 - ETA: 4s - loss: 0.1293 - acc: 0.960 - ETA: 4s - loss: 0.1290 - acc: 0.960 - ETA: 3s - loss: 0.1298 - acc: 0.960 - ETA: 3s - loss: 0.1285 - acc: 0.960 - ETA: 3s - loss: 0.1280 - acc: 0.961 - ETA: 3s - loss: 0.1271 - acc: 0.961 - ETA: 3s - loss: 0.1266 - acc: 0.961 - ETA: 3s - loss: 0.1257 - acc: 0.961 - ETA: 3s - loss: 0.1254 - acc: 0.961 - ETA: 3s - loss: 0.1249 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1235 - acc: 0.961 - ETA: 3s - loss: 0.1240 - acc: 0.961 - ETA: 3s - loss: 0.1236 - acc: 0.960 - ETA: 3s - loss: 0.1240 - acc: 0.960 - ETA: 3s - loss: 0.1234 - acc: 0.960 - ETA: 3s - loss: 0.1223 - acc: 0.961 - ETA: 3s - loss: 0.1216 - acc: 0.961 - ETA: 3s - loss: 0.1220 - acc: 0.961 - ETA: 2s - loss: 0.1235 - acc: 0.961 - ETA: 2s - loss: 0.1227 - acc: 0.962 - ETA: 2s - loss: 0.1227 - acc: 0.961 - ETA: 2s - loss: 0.1233 - acc: 0.961 - ETA: 2s - loss: 0.1230 - acc: 0.961 - ETA: 2s - loss: 0.1221 - acc: 0.962 - ETA: 2s - loss: 0.1214 - acc: 0.962 - ETA: 2s - loss: 0.1205 - acc: 0.962 - ETA: 2s - loss: 0.1203 - acc: 0.962 - ETA: 2s - loss: 0.1207 - acc: 0.962 - ETA: 2s - loss: 0.1208 - acc: 0.962 - ETA: 2s - loss: 0.1201 - acc: 0.962 - ETA: 2s - loss: 0.1194 - acc: 0.962 - ETA: 2s - loss: 0.1204 - acc: 0.962 - ETA: 2s - loss: 0.1198 - acc: 0.962 - ETA: 2s - loss: 0.1198 - acc: 0.962 - ETA: 2s - loss: 0.1213 - acc: 0.962 - ETA: 2s - loss: 0.1219 - acc: 0.962 - ETA: 2s - loss: 0.1227 - acc: 0.962 - ETA: 1s - loss: 0.1225 - acc: 0.962 - ETA: 1s - loss: 0.1235 - acc: 0.962 - ETA: 1s - loss: 0.1255 - acc: 0.961 - ETA: 1s - loss: 0.1267 - acc: 0.961 - ETA: 1s - loss: 0.1264 - acc: 0.961 - ETA: 1s - loss: 0.1274 - acc: 0.960 - ETA: 1s - loss: 0.1266 - acc: 0.961 - ETA: 1s - loss: 0.1258 - acc: 0.961 - ETA: 1s - loss: 0.1257 - acc: 0.961 - ETA: 1s - loss: 0.1249 - acc: 0.961 - ETA: 1s - loss: 0.1245 - acc: 0.961 - ETA: 1s - loss: 0.1244 - acc: 0.961 - ETA: 1s - loss: 0.1247 - acc: 0.961 - ETA: 1s - loss: 0.1242 - acc: 0.961 - ETA: 1s - loss: 0.1236 - acc: 0.961 - ETA: 1s - loss: 0.1231 - acc: 0.961 - ETA: 1s - loss: 0.1228 - acc: 0.961 - ETA: 1s - loss: 0.1221 - acc: 0.961 - ETA: 0s - loss: 0.1220 - acc: 0.962 - ETA: 0s - loss: 0.1217 - acc: 0.962 - ETA: 0s - loss: 0.1213 - acc: 0.962 - ETA: 0s - loss: 0.1220 - acc: 0.962 - ETA: 0s - loss: 0.1218 - acc: 0.962 - ETA: 0s - loss: 0.1212 - acc: 0.962 - ETA: 0s - loss: 0.1207 - acc: 0.962 - ETA: 0s - loss: 0.1203 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1194 - acc: 0.962 - ETA: 0s - loss: 0.1198 - acc: 0.962 - ETA: 0s - loss: 0.1198 - acc: 0.962 - ETA: 0s - loss: 0.1193 - acc: 0.962 - ETA: 0s - loss: 0.1195 - acc: 0.962 - ETA: 0s - loss: 0.1189 - acc: 0.962 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1192 - acc: 0.962 - ETA: 0s - loss: 0.1187 - acc: 0.9628Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1185 - acc: 0.9629 - val_loss: 0.6032 - val_acc: 0.8467\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0441 - acc: 1.000 - ETA: 9s - loss: 0.0349 - acc: 1.000 - ETA: 9s - loss: 0.0720 - acc: 0.970 - ETA: 9s - loss: 0.0633 - acc: 0.971 - ETA: 9s - loss: 0.1019 - acc: 0.961 - ETA: 9s - loss: 0.1228 - acc: 0.950 - ETA: 9s - loss: 0.1226 - acc: 0.953 - ETA: 8s - loss: 0.1694 - acc: 0.950 - ETA: 8s - loss: 0.1504 - acc: 0.955 - ETA: 8s - loss: 0.1389 - acc: 0.960 - ETA: 8s - loss: 0.1395 - acc: 0.959 - ETA: 8s - loss: 0.1490 - acc: 0.960 - ETA: 8s - loss: 0.1471 - acc: 0.960 - ETA: 8s - loss: 0.1384 - acc: 0.963 - ETA: 8s - loss: 0.1406 - acc: 0.962 - ETA: 8s - loss: 0.1324 - acc: 0.964 - ETA: 8s - loss: 0.1267 - acc: 0.965 - ETA: 8s - loss: 0.1289 - acc: 0.964 - ETA: 8s - loss: 0.1257 - acc: 0.964 - ETA: 8s - loss: 0.1306 - acc: 0.965 - ETA: 8s - loss: 0.1253 - acc: 0.967 - ETA: 8s - loss: 0.1215 - acc: 0.967 - ETA: 7s - loss: 0.1175 - acc: 0.968 - ETA: 7s - loss: 0.1151 - acc: 0.969 - ETA: 7s - loss: 0.1128 - acc: 0.969 - ETA: 7s - loss: 0.1093 - acc: 0.970 - ETA: 7s - loss: 0.1106 - acc: 0.969 - ETA: 7s - loss: 0.1072 - acc: 0.970 - ETA: 7s - loss: 0.1091 - acc: 0.971 - ETA: 7s - loss: 0.1061 - acc: 0.972 - ETA: 7s - loss: 0.1047 - acc: 0.972 - ETA: 7s - loss: 0.1030 - acc: 0.972 - ETA: 7s - loss: 0.1012 - acc: 0.973 - ETA: 7s - loss: 0.1040 - acc: 0.972 - ETA: 7s - loss: 0.1011 - acc: 0.973 - ETA: 7s - loss: 0.0993 - acc: 0.973 - ETA: 7s - loss: 0.0985 - acc: 0.973 - ETA: 7s - loss: 0.0973 - acc: 0.973 - ETA: 7s - loss: 0.0952 - acc: 0.974 - ETA: 6s - loss: 0.0957 - acc: 0.972 - ETA: 6s - loss: 0.0943 - acc: 0.973 - ETA: 6s - loss: 0.0929 - acc: 0.974 - ETA: 6s - loss: 0.0922 - acc: 0.974 - ETA: 6s - loss: 0.0902 - acc: 0.974 - ETA: 6s - loss: 0.0901 - acc: 0.973 - ETA: 6s - loss: 0.0893 - acc: 0.973 - ETA: 6s - loss: 0.0902 - acc: 0.973 - ETA: 6s - loss: 0.0887 - acc: 0.973 - ETA: 6s - loss: 0.0878 - acc: 0.973 - ETA: 6s - loss: 0.0885 - acc: 0.973 - ETA: 6s - loss: 0.0873 - acc: 0.973 - ETA: 6s - loss: 0.0860 - acc: 0.973 - ETA: 6s - loss: 0.0851 - acc: 0.974 - ETA: 6s - loss: 0.0907 - acc: 0.973 - ETA: 6s - loss: 0.0904 - acc: 0.972 - ETA: 6s - loss: 0.0966 - acc: 0.971 - ETA: 5s - loss: 0.0961 - acc: 0.971 - ETA: 5s - loss: 0.0996 - acc: 0.971 - ETA: 5s - loss: 0.0987 - acc: 0.971 - ETA: 5s - loss: 0.0989 - acc: 0.971 - ETA: 5s - loss: 0.0990 - acc: 0.970 - ETA: 5s - loss: 0.0986 - acc: 0.970 - ETA: 5s - loss: 0.0975 - acc: 0.970 - ETA: 5s - loss: 0.0980 - acc: 0.970 - ETA: 5s - loss: 0.0968 - acc: 0.971 - ETA: 5s - loss: 0.0971 - acc: 0.970 - ETA: 5s - loss: 0.0961 - acc: 0.970 - ETA: 5s - loss: 0.0965 - acc: 0.970 - ETA: 5s - loss: 0.0954 - acc: 0.971 - ETA: 5s - loss: 0.0941 - acc: 0.971 - ETA: 5s - loss: 0.0953 - acc: 0.971 - ETA: 5s - loss: 0.0958 - acc: 0.971 - ETA: 5s - loss: 0.0952 - acc: 0.971 - ETA: 5s - loss: 0.0947 - acc: 0.971 - ETA: 5s - loss: 0.0949 - acc: 0.970 - ETA: 4s - loss: 0.0943 - acc: 0.970 - ETA: 4s - loss: 0.0937 - acc: 0.970 - ETA: 4s - loss: 0.0954 - acc: 0.970 - ETA: 4s - loss: 0.0943 - acc: 0.970 - ETA: 4s - loss: 0.0956 - acc: 0.970 - ETA: 4s - loss: 0.0975 - acc: 0.970 - ETA: 4s - loss: 0.0970 - acc: 0.970 - ETA: 4s - loss: 0.0964 - acc: 0.970 - ETA: 4s - loss: 0.0953 - acc: 0.970 - ETA: 4s - loss: 0.0949 - acc: 0.970 - ETA: 4s - loss: 0.0956 - acc: 0.970 - ETA: 4s - loss: 0.0946 - acc: 0.970 - ETA: 4s - loss: 0.0938 - acc: 0.970 - ETA: 4s - loss: 0.0938 - acc: 0.970 - ETA: 4s - loss: 0.0932 - acc: 0.970 - ETA: 4s - loss: 0.0933 - acc: 0.970 - ETA: 4s - loss: 0.0948 - acc: 0.970 - ETA: 4s - loss: 0.0967 - acc: 0.970 - ETA: 3s - loss: 0.0960 - acc: 0.970 - ETA: 3s - loss: 0.0953 - acc: 0.970 - ETA: 3s - loss: 0.0972 - acc: 0.970 - ETA: 3s - loss: 0.0969 - acc: 0.970 - ETA: 3s - loss: 0.0963 - acc: 0.970 - ETA: 3s - loss: 0.0959 - acc: 0.970 - ETA: 3s - loss: 0.0978 - acc: 0.969 - ETA: 3s - loss: 0.0982 - acc: 0.969 - ETA: 3s - loss: 0.0980 - acc: 0.969 - ETA: 3s - loss: 0.1007 - acc: 0.969 - ETA: 3s - loss: 0.1011 - acc: 0.969 - ETA: 3s - loss: 0.1011 - acc: 0.969 - ETA: 3s - loss: 0.1031 - acc: 0.969 - ETA: 3s - loss: 0.1023 - acc: 0.969 - ETA: 3s - loss: 0.1030 - acc: 0.969 - ETA: 3s - loss: 0.1034 - acc: 0.969 - ETA: 3s - loss: 0.1025 - acc: 0.969 - ETA: 3s - loss: 0.1036 - acc: 0.969 - ETA: 2s - loss: 0.1032 - acc: 0.969 - ETA: 2s - loss: 0.1037 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.968 - ETA: 2s - loss: 0.1038 - acc: 0.968 - ETA: 2s - loss: 0.1047 - acc: 0.968 - ETA: 2s - loss: 0.1046 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.968 - ETA: 2s - loss: 0.1043 - acc: 0.967 - ETA: 2s - loss: 0.1047 - acc: 0.967 - ETA: 2s - loss: 0.1044 - acc: 0.968 - ETA: 2s - loss: 0.1045 - acc: 0.968 - ETA: 2s - loss: 0.1040 - acc: 0.968 - ETA: 2s - loss: 0.1063 - acc: 0.967 - ETA: 2s - loss: 0.1080 - acc: 0.967 - ETA: 2s - loss: 0.1085 - acc: 0.966 - ETA: 2s - loss: 0.1094 - acc: 0.966 - ETA: 2s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1105 - acc: 0.966 - ETA: 1s - loss: 0.1101 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1098 - acc: 0.966 - ETA: 1s - loss: 0.1095 - acc: 0.966 - ETA: 1s - loss: 0.1096 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1106 - acc: 0.966 - ETA: 1s - loss: 0.1104 - acc: 0.966 - ETA: 1s - loss: 0.1097 - acc: 0.966 - ETA: 1s - loss: 0.1091 - acc: 0.967 - ETA: 1s - loss: 0.1085 - acc: 0.967 - ETA: 1s - loss: 0.1079 - acc: 0.967 - ETA: 0s - loss: 0.1078 - acc: 0.967 - ETA: 0s - loss: 0.1085 - acc: 0.967 - ETA: 0s - loss: 0.1082 - acc: 0.967 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1079 - acc: 0.966 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1081 - acc: 0.966 - ETA: 0s - loss: 0.1082 - acc: 0.966 - ETA: 0s - loss: 0.1077 - acc: 0.966 - ETA: 0s - loss: 0.1071 - acc: 0.967 - ETA: 0s - loss: 0.1083 - acc: 0.966 - ETA: 0s - loss: 0.1080 - acc: 0.966 - ETA: 0s - loss: 0.1076 - acc: 0.966 - ETA: 0s - loss: 0.1070 - acc: 0.967 - ETA: 0s - loss: 0.1066 - acc: 0.967 - ETA: 0s - loss: 0.1061 - acc: 0.967 - ETA: 0s - loss: 0.1085 - acc: 0.967 - ETA: 0s - loss: 0.1079 - acc: 0.967 - ETA: 0s - loss: 0.1076 - acc: 0.9674Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1088 - acc: 0.9674 - val_loss: 0.6280 - val_acc: 0.8611\n",
      "Epoch 15/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.2747 - acc: 0.950 - ETA: 9s - loss: 0.2058 - acc: 0.966 - ETA: 9s - loss: 0.1702 - acc: 0.970 - ETA: 9s - loss: 0.1245 - acc: 0.978 - ETA: 9s - loss: 0.1100 - acc: 0.977 - ETA: 9s - loss: 0.0962 - acc: 0.977 - ETA: 9s - loss: 0.0863 - acc: 0.976 - ETA: 9s - loss: 0.0816 - acc: 0.976 - ETA: 8s - loss: 0.0739 - acc: 0.979 - ETA: 8s - loss: 0.0823 - acc: 0.976 - ETA: 8s - loss: 0.0939 - acc: 0.976 - ETA: 8s - loss: 0.0882 - acc: 0.978 - ETA: 8s - loss: 0.0819 - acc: 0.980 - ETA: 8s - loss: 0.0767 - acc: 0.981 - ETA: 8s - loss: 0.0740 - acc: 0.981 - ETA: 8s - loss: 0.0741 - acc: 0.980 - ETA: 8s - loss: 0.0730 - acc: 0.980 - ETA: 8s - loss: 0.0718 - acc: 0.980 - ETA: 8s - loss: 0.0715 - acc: 0.979 - ETA: 8s - loss: 0.0721 - acc: 0.978 - ETA: 7s - loss: 0.0743 - acc: 0.976 - ETA: 7s - loss: 0.0848 - acc: 0.974 - ETA: 7s - loss: 0.0817 - acc: 0.975 - ETA: 7s - loss: 0.0841 - acc: 0.975 - ETA: 7s - loss: 0.0918 - acc: 0.973 - ETA: 7s - loss: 0.0897 - acc: 0.973 - ETA: 7s - loss: 0.0915 - acc: 0.973 - ETA: 7s - loss: 0.0923 - acc: 0.972 - ETA: 7s - loss: 0.0911 - acc: 0.972 - ETA: 7s - loss: 0.0901 - acc: 0.972 - ETA: 7s - loss: 0.0919 - acc: 0.971 - ETA: 7s - loss: 0.1017 - acc: 0.969 - ETA: 7s - loss: 0.0998 - acc: 0.969 - ETA: 7s - loss: 0.0989 - acc: 0.969 - ETA: 7s - loss: 0.0988 - acc: 0.968 - ETA: 7s - loss: 0.0985 - acc: 0.968 - ETA: 7s - loss: 0.0979 - acc: 0.968 - ETA: 6s - loss: 0.0964 - acc: 0.968 - ETA: 6s - loss: 0.0999 - acc: 0.968 - ETA: 6s - loss: 0.0980 - acc: 0.969 - ETA: 6s - loss: 0.0965 - acc: 0.969 - ETA: 6s - loss: 0.1002 - acc: 0.968 - ETA: 6s - loss: 0.0980 - acc: 0.968 - ETA: 6s - loss: 0.0959 - acc: 0.969 - ETA: 6s - loss: 0.0959 - acc: 0.969 - ETA: 6s - loss: 0.1049 - acc: 0.968 - ETA: 6s - loss: 0.1046 - acc: 0.968 - ETA: 6s - loss: 0.1048 - acc: 0.967 - ETA: 6s - loss: 0.1049 - acc: 0.968 - ETA: 6s - loss: 0.1064 - acc: 0.967 - ETA: 6s - loss: 0.1045 - acc: 0.968 - ETA: 6s - loss: 0.1047 - acc: 0.968 - ETA: 6s - loss: 0.1030 - acc: 0.968 - ETA: 6s - loss: 0.1043 - acc: 0.968 - ETA: 6s - loss: 0.1041 - acc: 0.967 - ETA: 6s - loss: 0.1029 - acc: 0.968 - ETA: 5s - loss: 0.1017 - acc: 0.968 - ETA: 5s - loss: 0.1002 - acc: 0.968 - ETA: 5s - loss: 0.1010 - acc: 0.968 - ETA: 5s - loss: 0.1000 - acc: 0.968 - ETA: 5s - loss: 0.0986 - acc: 0.969 - ETA: 5s - loss: 0.0972 - acc: 0.969 - ETA: 5s - loss: 0.0995 - acc: 0.969 - ETA: 5s - loss: 0.0995 - acc: 0.968 - ETA: 5s - loss: 0.0986 - acc: 0.969 - ETA: 5s - loss: 0.0982 - acc: 0.969 - ETA: 5s - loss: 0.0971 - acc: 0.969 - ETA: 5s - loss: 0.0958 - acc: 0.970 - ETA: 5s - loss: 0.0982 - acc: 0.969 - ETA: 5s - loss: 0.0980 - acc: 0.969 - ETA: 5s - loss: 0.0997 - acc: 0.968 - ETA: 5s - loss: 0.0994 - acc: 0.968 - ETA: 5s - loss: 0.1005 - acc: 0.967 - ETA: 5s - loss: 0.1000 - acc: 0.967 - ETA: 4s - loss: 0.0991 - acc: 0.967 - ETA: 4s - loss: 0.0987 - acc: 0.967 - ETA: 4s - loss: 0.1005 - acc: 0.966 - ETA: 4s - loss: 0.0993 - acc: 0.967 - ETA: 4s - loss: 0.0987 - acc: 0.967 - ETA: 4s - loss: 0.0999 - acc: 0.967 - ETA: 4s - loss: 0.0994 - acc: 0.967 - ETA: 4s - loss: 0.1008 - acc: 0.966 - ETA: 4s - loss: 0.0997 - acc: 0.966 - ETA: 4s - loss: 0.0990 - acc: 0.966 - ETA: 4s - loss: 0.0980 - acc: 0.967 - ETA: 4s - loss: 0.0980 - acc: 0.967 - ETA: 4s - loss: 0.0971 - acc: 0.967 - ETA: 4s - loss: 0.0964 - acc: 0.967 - ETA: 4s - loss: 0.0956 - acc: 0.967 - ETA: 4s - loss: 0.0953 - acc: 0.967 - ETA: 4s - loss: 0.0944 - acc: 0.968 - ETA: 4s - loss: 0.0968 - acc: 0.968 - ETA: 4s - loss: 0.0979 - acc: 0.967 - ETA: 3s - loss: 0.0976 - acc: 0.967 - ETA: 3s - loss: 0.0968 - acc: 0.968 - ETA: 3s - loss: 0.0959 - acc: 0.968 - ETA: 3s - loss: 0.0952 - acc: 0.968 - ETA: 3s - loss: 0.0944 - acc: 0.969 - ETA: 3s - loss: 0.0949 - acc: 0.968 - ETA: 3s - loss: 0.0941 - acc: 0.969 - ETA: 3s - loss: 0.0941 - acc: 0.968 - ETA: 3s - loss: 0.0934 - acc: 0.969 - ETA: 3s - loss: 0.0928 - acc: 0.969 - ETA: 3s - loss: 0.0928 - acc: 0.969 - ETA: 3s - loss: 0.0924 - acc: 0.969 - ETA: 3s - loss: 0.0921 - acc: 0.969 - ETA: 3s - loss: 0.0922 - acc: 0.969 - ETA: 3s - loss: 0.0933 - acc: 0.969 - ETA: 3s - loss: 0.0929 - acc: 0.969 - ETA: 3s - loss: 0.0948 - acc: 0.968 - ETA: 3s - loss: 0.0946 - acc: 0.968 - ETA: 2s - loss: 0.0938 - acc: 0.968 - ETA: 2s - loss: 0.0937 - acc: 0.968 - ETA: 2s - loss: 0.0935 - acc: 0.968 - ETA: 2s - loss: 0.0962 - acc: 0.968 - ETA: 2s - loss: 0.0964 - acc: 0.968 - ETA: 2s - loss: 0.0969 - acc: 0.968 - ETA: 2s - loss: 0.0984 - acc: 0.967 - ETA: 2s - loss: 0.0982 - acc: 0.967 - ETA: 2s - loss: 0.0978 - acc: 0.967 - ETA: 2s - loss: 0.0971 - acc: 0.968 - ETA: 2s - loss: 0.0973 - acc: 0.968 - ETA: 2s - loss: 0.0981 - acc: 0.967 - ETA: 2s - loss: 0.0975 - acc: 0.968 - ETA: 2s - loss: 0.0977 - acc: 0.967 - ETA: 2s - loss: 0.0970 - acc: 0.967 - ETA: 2s - loss: 0.0973 - acc: 0.968 - ETA: 2s - loss: 0.0967 - acc: 0.968 - ETA: 2s - loss: 0.0980 - acc: 0.968 - ETA: 2s - loss: 0.0978 - acc: 0.968 - ETA: 1s - loss: 0.0976 - acc: 0.968 - ETA: 1s - loss: 0.0975 - acc: 0.968 - ETA: 1s - loss: 0.0968 - acc: 0.968 - ETA: 1s - loss: 0.0963 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0961 - acc: 0.968 - ETA: 1s - loss: 0.0964 - acc: 0.968 - ETA: 1s - loss: 0.0970 - acc: 0.968 - ETA: 1s - loss: 0.0977 - acc: 0.968 - ETA: 1s - loss: 0.0973 - acc: 0.968 - ETA: 1s - loss: 0.0969 - acc: 0.968 - ETA: 1s - loss: 0.0965 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 1s - loss: 0.0956 - acc: 0.968 - ETA: 1s - loss: 0.0966 - acc: 0.968 - ETA: 1s - loss: 0.0960 - acc: 0.968 - ETA: 0s - loss: 0.0971 - acc: 0.968 - ETA: 0s - loss: 0.0982 - acc: 0.968 - ETA: 0s - loss: 0.0991 - acc: 0.968 - ETA: 0s - loss: 0.0995 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1011 - acc: 0.967 - ETA: 0s - loss: 0.1009 - acc: 0.967 - ETA: 0s - loss: 0.1004 - acc: 0.967 - ETA: 0s - loss: 0.1021 - acc: 0.967 - ETA: 0s - loss: 0.1016 - acc: 0.967 - ETA: 0s - loss: 0.1012 - acc: 0.967 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1006 - acc: 0.968 - ETA: 0s - loss: 0.1004 - acc: 0.968 - ETA: 0s - loss: 0.1000 - acc: 0.968 - ETA: 0s - loss: 0.1006 - acc: 0.968 - ETA: 0s - loss: 0.1002 - acc: 0.9685Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.1001 - acc: 0.9684 - val_loss: 0.6567 - val_acc: 0.8467\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.0088 - acc: 1.000 - ETA: 10s - loss: 0.2076 - acc: 0.95 - ETA: 10s - loss: 0.2090 - acc: 0.96 - ETA: 10s - loss: 0.1576 - acc: 0.96 - ETA: 10s - loss: 0.1258 - acc: 0.97 - ETA: 9s - loss: 0.1247 - acc: 0.9727 - ETA: 9s - loss: 0.1289 - acc: 0.969 - ETA: 9s - loss: 0.1225 - acc: 0.966 - ETA: 9s - loss: 0.1084 - acc: 0.970 - ETA: 9s - loss: 0.0996 - acc: 0.973 - ETA: 9s - loss: 0.0915 - acc: 0.976 - ETA: 9s - loss: 0.0881 - acc: 0.978 - ETA: 8s - loss: 0.0825 - acc: 0.980 - ETA: 8s - loss: 0.0785 - acc: 0.981 - ETA: 8s - loss: 0.0834 - acc: 0.981 - ETA: 8s - loss: 0.0881 - acc: 0.980 - ETA: 8s - loss: 0.0833 - acc: 0.981 - ETA: 8s - loss: 0.0921 - acc: 0.981 - ETA: 8s - loss: 0.0885 - acc: 0.982 - ETA: 8s - loss: 0.0866 - acc: 0.982 - ETA: 8s - loss: 0.0931 - acc: 0.980 - ETA: 8s - loss: 0.0897 - acc: 0.981 - ETA: 8s - loss: 0.0946 - acc: 0.980 - ETA: 8s - loss: 0.0909 - acc: 0.980 - ETA: 8s - loss: 0.0892 - acc: 0.980 - ETA: 8s - loss: 0.0890 - acc: 0.980 - ETA: 8s - loss: 0.0869 - acc: 0.981 - ETA: 8s - loss: 0.0996 - acc: 0.980 - ETA: 7s - loss: 0.0966 - acc: 0.980 - ETA: 7s - loss: 0.0969 - acc: 0.980 - ETA: 7s - loss: 0.1020 - acc: 0.978 - ETA: 7s - loss: 0.1021 - acc: 0.977 - ETA: 7s - loss: 0.1062 - acc: 0.976 - ETA: 7s - loss: 0.1035 - acc: 0.976 - ETA: 7s - loss: 0.1066 - acc: 0.976 - ETA: 7s - loss: 0.1044 - acc: 0.976 - ETA: 7s - loss: 0.1036 - acc: 0.976 - ETA: 7s - loss: 0.1041 - acc: 0.976 - ETA: 7s - loss: 0.1015 - acc: 0.977 - ETA: 7s - loss: 0.1000 - acc: 0.977 - ETA: 7s - loss: 0.0979 - acc: 0.977 - ETA: 7s - loss: 0.0959 - acc: 0.978 - ETA: 7s - loss: 0.0961 - acc: 0.978 - ETA: 7s - loss: 0.0953 - acc: 0.977 - ETA: 6s - loss: 0.0955 - acc: 0.977 - ETA: 6s - loss: 0.0961 - acc: 0.976 - ETA: 6s - loss: 0.0943 - acc: 0.976 - ETA: 6s - loss: 0.0949 - acc: 0.976 - ETA: 6s - loss: 0.0958 - acc: 0.975 - ETA: 6s - loss: 0.0945 - acc: 0.976 - ETA: 6s - loss: 0.0930 - acc: 0.976 - ETA: 6s - loss: 0.0921 - acc: 0.977 - ETA: 6s - loss: 0.0928 - acc: 0.977 - ETA: 6s - loss: 0.0959 - acc: 0.976 - ETA: 6s - loss: 0.0943 - acc: 0.976 - ETA: 6s - loss: 0.0945 - acc: 0.976 - ETA: 6s - loss: 0.0955 - acc: 0.975 - ETA: 6s - loss: 0.0942 - acc: 0.975 - ETA: 6s - loss: 0.0936 - acc: 0.975 - ETA: 6s - loss: 0.0936 - acc: 0.975 - ETA: 5s - loss: 0.0924 - acc: 0.975 - ETA: 5s - loss: 0.0912 - acc: 0.976 - ETA: 5s - loss: 0.0906 - acc: 0.975 - ETA: 5s - loss: 0.0897 - acc: 0.975 - ETA: 5s - loss: 0.0910 - acc: 0.975 - ETA: 5s - loss: 0.0902 - acc: 0.975 - ETA: 5s - loss: 0.0903 - acc: 0.974 - ETA: 5s - loss: 0.0911 - acc: 0.974 - ETA: 5s - loss: 0.0915 - acc: 0.974 - ETA: 5s - loss: 0.0910 - acc: 0.974 - ETA: 5s - loss: 0.0898 - acc: 0.974 - ETA: 5s - loss: 0.0891 - acc: 0.974 - ETA: 5s - loss: 0.0887 - acc: 0.974 - ETA: 5s - loss: 0.0875 - acc: 0.975 - ETA: 5s - loss: 0.0865 - acc: 0.975 - ETA: 5s - loss: 0.0866 - acc: 0.975 - ETA: 5s - loss: 0.0860 - acc: 0.975 - ETA: 4s - loss: 0.0859 - acc: 0.975 - ETA: 4s - loss: 0.0854 - acc: 0.975 - ETA: 4s - loss: 0.0855 - acc: 0.975 - ETA: 4s - loss: 0.0864 - acc: 0.975 - ETA: 4s - loss: 0.0857 - acc: 0.975 - ETA: 4s - loss: 0.0871 - acc: 0.975 - ETA: 4s - loss: 0.0865 - acc: 0.975 - ETA: 4s - loss: 0.0861 - acc: 0.975 - ETA: 4s - loss: 0.0856 - acc: 0.975 - ETA: 4s - loss: 0.0850 - acc: 0.975 - ETA: 4s - loss: 0.0843 - acc: 0.975 - ETA: 4s - loss: 0.0834 - acc: 0.975 - ETA: 4s - loss: 0.0827 - acc: 0.976 - ETA: 4s - loss: 0.0853 - acc: 0.975 - ETA: 4s - loss: 0.0845 - acc: 0.975 - ETA: 4s - loss: 0.0837 - acc: 0.975 - ETA: 4s - loss: 0.0852 - acc: 0.975 - ETA: 4s - loss: 0.0864 - acc: 0.974 - ETA: 3s - loss: 0.0870 - acc: 0.974 - ETA: 3s - loss: 0.0874 - acc: 0.974 - ETA: 3s - loss: 0.0887 - acc: 0.974 - ETA: 3s - loss: 0.0887 - acc: 0.974 - ETA: 3s - loss: 0.0883 - acc: 0.974 - ETA: 3s - loss: 0.0898 - acc: 0.974 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0910 - acc: 0.973 - ETA: 3s - loss: 0.0906 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0905 - acc: 0.973 - ETA: 3s - loss: 0.0898 - acc: 0.973 - ETA: 3s - loss: 0.0892 - acc: 0.974 - ETA: 3s - loss: 0.0890 - acc: 0.974 - ETA: 3s - loss: 0.0885 - acc: 0.974 - ETA: 3s - loss: 0.0884 - acc: 0.973 - ETA: 3s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0904 - acc: 0.973 - ETA: 2s - loss: 0.0916 - acc: 0.973 - ETA: 2s - loss: 0.0910 - acc: 0.973 - ETA: 2s - loss: 0.0902 - acc: 0.973 - ETA: 2s - loss: 0.0920 - acc: 0.973 - ETA: 2s - loss: 0.0915 - acc: 0.973 - ETA: 2s - loss: 0.0916 - acc: 0.973 - ETA: 2s - loss: 0.0912 - acc: 0.973 - ETA: 2s - loss: 0.0914 - acc: 0.972 - ETA: 2s - loss: 0.0915 - acc: 0.972 - ETA: 2s - loss: 0.0919 - acc: 0.972 - ETA: 2s - loss: 0.0912 - acc: 0.972 - ETA: 2s - loss: 0.0909 - acc: 0.972 - ETA: 2s - loss: 0.0909 - acc: 0.972 - ETA: 2s - loss: 0.0908 - acc: 0.972 - ETA: 2s - loss: 0.0919 - acc: 0.972 - ETA: 2s - loss: 0.0913 - acc: 0.972 - ETA: 2s - loss: 0.0906 - acc: 0.973 - ETA: 2s - loss: 0.0912 - acc: 0.972 - ETA: 1s - loss: 0.0911 - acc: 0.972 - ETA: 1s - loss: 0.0908 - acc: 0.972 - ETA: 1s - loss: 0.0905 - acc: 0.972 - ETA: 1s - loss: 0.0909 - acc: 0.972 - ETA: 1s - loss: 0.0923 - acc: 0.972 - ETA: 1s - loss: 0.0917 - acc: 0.972 - ETA: 1s - loss: 0.0913 - acc: 0.972 - ETA: 1s - loss: 0.0909 - acc: 0.972 - ETA: 1s - loss: 0.0938 - acc: 0.972 - ETA: 1s - loss: 0.0946 - acc: 0.972 - ETA: 1s - loss: 0.0940 - acc: 0.972 - ETA: 1s - loss: 0.0941 - acc: 0.972 - ETA: 1s - loss: 0.0943 - acc: 0.972 - ETA: 1s - loss: 0.0937 - acc: 0.972 - ETA: 1s - loss: 0.0932 - acc: 0.972 - ETA: 1s - loss: 0.0930 - acc: 0.972 - ETA: 1s - loss: 0.0934 - acc: 0.972 - ETA: 1s - loss: 0.0944 - acc: 0.971 - ETA: 0s - loss: 0.0952 - acc: 0.971 - ETA: 0s - loss: 0.0947 - acc: 0.971 - ETA: 0s - loss: 0.0945 - acc: 0.971 - ETA: 0s - loss: 0.0944 - acc: 0.971 - ETA: 0s - loss: 0.0943 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.971 - ETA: 0s - loss: 0.0940 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.971 - ETA: 0s - loss: 0.0937 - acc: 0.971 - ETA: 0s - loss: 0.0938 - acc: 0.971 - ETA: 0s - loss: 0.0934 - acc: 0.971 - ETA: 0s - loss: 0.0930 - acc: 0.972 - ETA: 0s - loss: 0.0934 - acc: 0.972 - ETA: 0s - loss: 0.0940 - acc: 0.971 - ETA: 0s - loss: 0.0938 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.971 - ETA: 0s - loss: 0.0946 - acc: 0.9719Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0944 - acc: 0.9720 - val_loss: 0.6636 - val_acc: 0.8479\n",
      "Epoch 17/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1768 - acc: 0.900 - ETA: 11s - loss: 0.1312 - acc: 0.93 - ETA: 10s - loss: 0.0970 - acc: 0.95 - ETA: 10s - loss: 0.0804 - acc: 0.95 - ETA: 9s - loss: 0.1457 - acc: 0.9556 - ETA: 9s - loss: 0.2187 - acc: 0.945 - ETA: 9s - loss: 0.1951 - acc: 0.950 - ETA: 9s - loss: 0.1724 - acc: 0.956 - ETA: 9s - loss: 0.1878 - acc: 0.952 - ETA: 9s - loss: 0.1693 - acc: 0.957 - ETA: 8s - loss: 0.1562 - acc: 0.961 - ETA: 8s - loss: 0.1444 - acc: 0.965 - ETA: 8s - loss: 0.1337 - acc: 0.968 - ETA: 8s - loss: 0.1288 - acc: 0.968 - ETA: 8s - loss: 0.1322 - acc: 0.969 - ETA: 8s - loss: 0.1249 - acc: 0.971 - ETA: 8s - loss: 0.1212 - acc: 0.971 - ETA: 8s - loss: 0.1148 - acc: 0.972 - ETA: 8s - loss: 0.1131 - acc: 0.973 - ETA: 8s - loss: 0.1095 - acc: 0.973 - ETA: 8s - loss: 0.1076 - acc: 0.972 - ETA: 8s - loss: 0.1105 - acc: 0.970 - ETA: 7s - loss: 0.1232 - acc: 0.970 - ETA: 7s - loss: 0.1182 - acc: 0.971 - ETA: 7s - loss: 0.1183 - acc: 0.970 - ETA: 7s - loss: 0.1203 - acc: 0.969 - ETA: 7s - loss: 0.1237 - acc: 0.969 - ETA: 7s - loss: 0.1269 - acc: 0.969 - ETA: 7s - loss: 0.1226 - acc: 0.970 - ETA: 7s - loss: 0.1234 - acc: 0.969 - ETA: 7s - loss: 0.1239 - acc: 0.968 - ETA: 7s - loss: 0.1204 - acc: 0.969 - ETA: 7s - loss: 0.1193 - acc: 0.969 - ETA: 7s - loss: 0.1185 - acc: 0.969 - ETA: 7s - loss: 0.1167 - acc: 0.969 - ETA: 7s - loss: 0.1146 - acc: 0.969 - ETA: 7s - loss: 0.1120 - acc: 0.970 - ETA: 7s - loss: 0.1097 - acc: 0.971 - ETA: 7s - loss: 0.1085 - acc: 0.971 - ETA: 7s - loss: 0.1082 - acc: 0.971 - ETA: 6s - loss: 0.1109 - acc: 0.971 - ETA: 6s - loss: 0.1106 - acc: 0.971 - ETA: 6s - loss: 0.1123 - acc: 0.971 - ETA: 6s - loss: 0.1099 - acc: 0.971 - ETA: 6s - loss: 0.1079 - acc: 0.972 - ETA: 6s - loss: 0.1058 - acc: 0.973 - ETA: 6s - loss: 0.1050 - acc: 0.973 - ETA: 6s - loss: 0.1058 - acc: 0.972 - ETA: 6s - loss: 0.1043 - acc: 0.972 - ETA: 6s - loss: 0.1049 - acc: 0.972 - ETA: 6s - loss: 0.1032 - acc: 0.973 - ETA: 6s - loss: 0.1019 - acc: 0.973 - ETA: 6s - loss: 0.1006 - acc: 0.973 - ETA: 6s - loss: 0.0993 - acc: 0.973 - ETA: 6s - loss: 0.0985 - acc: 0.973 - ETA: 6s - loss: 0.1017 - acc: 0.973 - ETA: 6s - loss: 0.1000 - acc: 0.973 - ETA: 5s - loss: 0.0984 - acc: 0.974 - ETA: 5s - loss: 0.0988 - acc: 0.974 - ETA: 5s - loss: 0.0986 - acc: 0.974 - ETA: 5s - loss: 0.1060 - acc: 0.973 - ETA: 5s - loss: 0.1046 - acc: 0.973 - ETA: 5s - loss: 0.1037 - acc: 0.973 - ETA: 5s - loss: 0.1047 - acc: 0.973 - ETA: 5s - loss: 0.1043 - acc: 0.973 - ETA: 5s - loss: 0.1048 - acc: 0.973 - ETA: 5s - loss: 0.1038 - acc: 0.974 - ETA: 5s - loss: 0.1036 - acc: 0.974 - ETA: 5s - loss: 0.1025 - acc: 0.974 - ETA: 5s - loss: 0.1025 - acc: 0.974 - ETA: 5s - loss: 0.1034 - acc: 0.974 - ETA: 5s - loss: 0.1055 - acc: 0.973 - ETA: 5s - loss: 0.1043 - acc: 0.974 - ETA: 5s - loss: 0.1033 - acc: 0.974 - ETA: 5s - loss: 0.1027 - acc: 0.974 - ETA: 4s - loss: 0.1019 - acc: 0.974 - ETA: 4s - loss: 0.1008 - acc: 0.974 - ETA: 4s - loss: 0.1011 - acc: 0.974 - ETA: 4s - loss: 0.1050 - acc: 0.973 - ETA: 4s - loss: 0.1054 - acc: 0.973 - ETA: 4s - loss: 0.1046 - acc: 0.973 - ETA: 4s - loss: 0.1054 - acc: 0.973 - ETA: 4s - loss: 0.1051 - acc: 0.973 - ETA: 4s - loss: 0.1058 - acc: 0.973 - ETA: 4s - loss: 0.1065 - acc: 0.973 - ETA: 4s - loss: 0.1059 - acc: 0.973 - ETA: 4s - loss: 0.1050 - acc: 0.973 - ETA: 4s - loss: 0.1039 - acc: 0.973 - ETA: 4s - loss: 0.1030 - acc: 0.974 - ETA: 4s - loss: 0.1023 - acc: 0.974 - ETA: 4s - loss: 0.1014 - acc: 0.974 - ETA: 4s - loss: 0.1003 - acc: 0.974 - ETA: 4s - loss: 0.1006 - acc: 0.974 - ETA: 3s - loss: 0.1008 - acc: 0.973 - ETA: 3s - loss: 0.0999 - acc: 0.974 - ETA: 3s - loss: 0.0990 - acc: 0.974 - ETA: 3s - loss: 0.0982 - acc: 0.974 - ETA: 3s - loss: 0.0973 - acc: 0.974 - ETA: 3s - loss: 0.0964 - acc: 0.975 - ETA: 3s - loss: 0.0959 - acc: 0.975 - ETA: 3s - loss: 0.0953 - acc: 0.975 - ETA: 3s - loss: 0.0950 - acc: 0.975 - ETA: 3s - loss: 0.0944 - acc: 0.975 - ETA: 3s - loss: 0.0946 - acc: 0.975 - ETA: 3s - loss: 0.0944 - acc: 0.975 - ETA: 3s - loss: 0.0953 - acc: 0.974 - ETA: 3s - loss: 0.0946 - acc: 0.974 - ETA: 3s - loss: 0.0938 - acc: 0.974 - ETA: 3s - loss: 0.0941 - acc: 0.974 - ETA: 3s - loss: 0.0947 - acc: 0.974 - ETA: 3s - loss: 0.0941 - acc: 0.974 - ETA: 3s - loss: 0.0934 - acc: 0.974 - ETA: 2s - loss: 0.0930 - acc: 0.974 - ETA: 2s - loss: 0.0929 - acc: 0.974 - ETA: 2s - loss: 0.0927 - acc: 0.974 - ETA: 2s - loss: 0.0923 - acc: 0.975 - ETA: 2s - loss: 0.0921 - acc: 0.975 - ETA: 2s - loss: 0.0918 - acc: 0.974 - ETA: 2s - loss: 0.0922 - acc: 0.974 - ETA: 2s - loss: 0.0928 - acc: 0.974 - ETA: 2s - loss: 0.0932 - acc: 0.974 - ETA: 2s - loss: 0.0925 - acc: 0.974 - ETA: 2s - loss: 0.0922 - acc: 0.974 - ETA: 2s - loss: 0.0926 - acc: 0.973 - ETA: 2s - loss: 0.0922 - acc: 0.973 - ETA: 2s - loss: 0.0928 - acc: 0.973 - ETA: 2s - loss: 0.0923 - acc: 0.973 - ETA: 2s - loss: 0.0917 - acc: 0.973 - ETA: 2s - loss: 0.0918 - acc: 0.973 - ETA: 2s - loss: 0.0917 - acc: 0.973 - ETA: 1s - loss: 0.0915 - acc: 0.974 - ETA: 1s - loss: 0.0909 - acc: 0.974 - ETA: 1s - loss: 0.0910 - acc: 0.974 - ETA: 1s - loss: 0.0906 - acc: 0.974 - ETA: 1s - loss: 0.0900 - acc: 0.974 - ETA: 1s - loss: 0.0902 - acc: 0.974 - ETA: 1s - loss: 0.0897 - acc: 0.974 - ETA: 1s - loss: 0.0900 - acc: 0.974 - ETA: 1s - loss: 0.0892 - acc: 0.974 - ETA: 1s - loss: 0.0914 - acc: 0.974 - ETA: 1s - loss: 0.0910 - acc: 0.974 - ETA: 1s - loss: 0.0914 - acc: 0.974 - ETA: 1s - loss: 0.0913 - acc: 0.974 - ETA: 1s - loss: 0.0907 - acc: 0.974 - ETA: 1s - loss: 0.0901 - acc: 0.974 - ETA: 1s - loss: 0.0902 - acc: 0.974 - ETA: 1s - loss: 0.0901 - acc: 0.974 - ETA: 0s - loss: 0.0896 - acc: 0.974 - ETA: 0s - loss: 0.0890 - acc: 0.974 - ETA: 0s - loss: 0.0888 - acc: 0.974 - ETA: 0s - loss: 0.0906 - acc: 0.974 - ETA: 0s - loss: 0.0909 - acc: 0.974 - ETA: 0s - loss: 0.0911 - acc: 0.974 - ETA: 0s - loss: 0.0907 - acc: 0.974 - ETA: 0s - loss: 0.0905 - acc: 0.974 - ETA: 0s - loss: 0.0900 - acc: 0.974 - ETA: 0s - loss: 0.0900 - acc: 0.974 - ETA: 0s - loss: 0.0895 - acc: 0.974 - ETA: 0s - loss: 0.0891 - acc: 0.974 - ETA: 0s - loss: 0.0889 - acc: 0.974 - ETA: 0s - loss: 0.0884 - acc: 0.975 - ETA: 0s - loss: 0.0894 - acc: 0.974 - ETA: 0s - loss: 0.0894 - acc: 0.974 - ETA: 0s - loss: 0.0889 - acc: 0.974 - ETA: 0s - loss: 0.0887 - acc: 0.9749Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0889 - acc: 0.9749 - val_loss: 0.6564 - val_acc: 0.8479\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.0029 - acc: 1.000 - ETA: 8s - loss: 0.0327 - acc: 0.983 - ETA: 8s - loss: 0.1211 - acc: 0.950 - ETA: 8s - loss: 0.1113 - acc: 0.957 - ETA: 8s - loss: 0.0878 - acc: 0.966 - ETA: 8s - loss: 0.0781 - acc: 0.968 - ETA: 8s - loss: 0.0673 - acc: 0.973 - ETA: 8s - loss: 0.0615 - acc: 0.976 - ETA: 8s - loss: 0.0613 - acc: 0.976 - ETA: 8s - loss: 0.0579 - acc: 0.978 - ETA: 8s - loss: 0.0544 - acc: 0.981 - ETA: 8s - loss: 0.0660 - acc: 0.980 - ETA: 8s - loss: 0.0633 - acc: 0.982 - ETA: 8s - loss: 0.0750 - acc: 0.977 - ETA: 8s - loss: 0.0774 - acc: 0.977 - ETA: 8s - loss: 0.0743 - acc: 0.977 - ETA: 8s - loss: 0.0731 - acc: 0.977 - ETA: 7s - loss: 0.0696 - acc: 0.978 - ETA: 7s - loss: 0.0708 - acc: 0.977 - ETA: 7s - loss: 0.0676 - acc: 0.978 - ETA: 7s - loss: 0.0698 - acc: 0.978 - ETA: 7s - loss: 0.0670 - acc: 0.979 - ETA: 7s - loss: 0.0713 - acc: 0.978 - ETA: 7s - loss: 0.0726 - acc: 0.978 - ETA: 7s - loss: 0.0756 - acc: 0.978 - ETA: 7s - loss: 0.0749 - acc: 0.978 - ETA: 7s - loss: 0.0757 - acc: 0.977 - ETA: 7s - loss: 0.0732 - acc: 0.978 - ETA: 7s - loss: 0.0752 - acc: 0.977 - ETA: 7s - loss: 0.0740 - acc: 0.977 - ETA: 7s - loss: 0.0719 - acc: 0.977 - ETA: 7s - loss: 0.0703 - acc: 0.978 - ETA: 7s - loss: 0.0689 - acc: 0.979 - ETA: 7s - loss: 0.0673 - acc: 0.979 - ETA: 7s - loss: 0.0662 - acc: 0.980 - ETA: 7s - loss: 0.0654 - acc: 0.980 - ETA: 6s - loss: 0.0657 - acc: 0.980 - ETA: 6s - loss: 0.0674 - acc: 0.980 - ETA: 6s - loss: 0.0758 - acc: 0.979 - ETA: 6s - loss: 0.0746 - acc: 0.979 - ETA: 6s - loss: 0.0748 - acc: 0.979 - ETA: 6s - loss: 0.0734 - acc: 0.980 - ETA: 6s - loss: 0.0718 - acc: 0.980 - ETA: 6s - loss: 0.0706 - acc: 0.981 - ETA: 6s - loss: 0.0721 - acc: 0.980 - ETA: 6s - loss: 0.0708 - acc: 0.980 - ETA: 6s - loss: 0.0697 - acc: 0.981 - ETA: 6s - loss: 0.0690 - acc: 0.981 - ETA: 6s - loss: 0.0691 - acc: 0.981 - ETA: 6s - loss: 0.0698 - acc: 0.980 - ETA: 6s - loss: 0.0725 - acc: 0.980 - ETA: 6s - loss: 0.0714 - acc: 0.980 - ETA: 6s - loss: 0.0708 - acc: 0.980 - ETA: 6s - loss: 0.0713 - acc: 0.980 - ETA: 6s - loss: 0.0703 - acc: 0.980 - ETA: 5s - loss: 0.0709 - acc: 0.980 - ETA: 5s - loss: 0.0711 - acc: 0.979 - ETA: 5s - loss: 0.0702 - acc: 0.980 - ETA: 5s - loss: 0.0763 - acc: 0.979 - ETA: 5s - loss: 0.0753 - acc: 0.980 - ETA: 5s - loss: 0.0745 - acc: 0.980 - ETA: 5s - loss: 0.0745 - acc: 0.980 - ETA: 5s - loss: 0.0743 - acc: 0.980 - ETA: 5s - loss: 0.0734 - acc: 0.980 - ETA: 5s - loss: 0.0731 - acc: 0.981 - ETA: 5s - loss: 0.0761 - acc: 0.980 - ETA: 5s - loss: 0.0756 - acc: 0.980 - ETA: 5s - loss: 0.0747 - acc: 0.980 - ETA: 5s - loss: 0.0748 - acc: 0.980 - ETA: 5s - loss: 0.0743 - acc: 0.980 - ETA: 5s - loss: 0.0734 - acc: 0.980 - ETA: 5s - loss: 0.0727 - acc: 0.981 - ETA: 5s - loss: 0.0717 - acc: 0.981 - ETA: 5s - loss: 0.0714 - acc: 0.981 - ETA: 4s - loss: 0.0714 - acc: 0.981 - ETA: 4s - loss: 0.0710 - acc: 0.981 - ETA: 4s - loss: 0.0716 - acc: 0.980 - ETA: 4s - loss: 0.0746 - acc: 0.980 - ETA: 4s - loss: 0.0739 - acc: 0.980 - ETA: 4s - loss: 0.0738 - acc: 0.980 - ETA: 4s - loss: 0.0762 - acc: 0.979 - ETA: 4s - loss: 0.0762 - acc: 0.979 - ETA: 4s - loss: 0.0754 - acc: 0.980 - ETA: 4s - loss: 0.0761 - acc: 0.979 - ETA: 4s - loss: 0.0772 - acc: 0.979 - ETA: 4s - loss: 0.0764 - acc: 0.979 - ETA: 4s - loss: 0.0785 - acc: 0.978 - ETA: 4s - loss: 0.0786 - acc: 0.978 - ETA: 4s - loss: 0.0778 - acc: 0.978 - ETA: 4s - loss: 0.0774 - acc: 0.978 - ETA: 4s - loss: 0.0784 - acc: 0.978 - ETA: 4s - loss: 0.0781 - acc: 0.978 - ETA: 4s - loss: 0.0774 - acc: 0.978 - ETA: 3s - loss: 0.0782 - acc: 0.978 - ETA: 3s - loss: 0.0775 - acc: 0.978 - ETA: 3s - loss: 0.0768 - acc: 0.979 - ETA: 3s - loss: 0.0762 - acc: 0.979 - ETA: 3s - loss: 0.0760 - acc: 0.979 - ETA: 3s - loss: 0.0769 - acc: 0.979 - ETA: 3s - loss: 0.0762 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0751 - acc: 0.979 - ETA: 3s - loss: 0.0760 - acc: 0.978 - ETA: 3s - loss: 0.0765 - acc: 0.978 - ETA: 3s - loss: 0.0761 - acc: 0.978 - ETA: 3s - loss: 0.0767 - acc: 0.978 - ETA: 3s - loss: 0.0762 - acc: 0.978 - ETA: 3s - loss: 0.0765 - acc: 0.978 - ETA: 3s - loss: 0.0782 - acc: 0.977 - ETA: 3s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0777 - acc: 0.977 - ETA: 2s - loss: 0.0773 - acc: 0.977 - ETA: 2s - loss: 0.0772 - acc: 0.977 - ETA: 2s - loss: 0.0779 - acc: 0.977 - ETA: 2s - loss: 0.0775 - acc: 0.977 - ETA: 2s - loss: 0.0774 - acc: 0.977 - ETA: 2s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0779 - acc: 0.977 - ETA: 2s - loss: 0.0787 - acc: 0.976 - ETA: 2s - loss: 0.0781 - acc: 0.977 - ETA: 2s - loss: 0.0781 - acc: 0.976 - ETA: 2s - loss: 0.0806 - acc: 0.976 - ETA: 2s - loss: 0.0801 - acc: 0.976 - ETA: 2s - loss: 0.0805 - acc: 0.976 - ETA: 2s - loss: 0.0800 - acc: 0.977 - ETA: 2s - loss: 0.0808 - acc: 0.976 - ETA: 2s - loss: 0.0803 - acc: 0.977 - ETA: 2s - loss: 0.0811 - acc: 0.977 - ETA: 2s - loss: 0.0807 - acc: 0.977 - ETA: 1s - loss: 0.0805 - acc: 0.977 - ETA: 1s - loss: 0.0824 - acc: 0.976 - ETA: 1s - loss: 0.0830 - acc: 0.976 - ETA: 1s - loss: 0.0828 - acc: 0.976 - ETA: 1s - loss: 0.0825 - acc: 0.976 - ETA: 1s - loss: 0.0841 - acc: 0.976 - ETA: 1s - loss: 0.0835 - acc: 0.976 - ETA: 1s - loss: 0.0854 - acc: 0.976 - ETA: 1s - loss: 0.0850 - acc: 0.976 - ETA: 1s - loss: 0.0852 - acc: 0.976 - ETA: 1s - loss: 0.0848 - acc: 0.976 - ETA: 1s - loss: 0.0843 - acc: 0.976 - ETA: 1s - loss: 0.0838 - acc: 0.976 - ETA: 1s - loss: 0.0834 - acc: 0.976 - ETA: 1s - loss: 0.0830 - acc: 0.976 - ETA: 1s - loss: 0.0825 - acc: 0.976 - ETA: 1s - loss: 0.0822 - acc: 0.976 - ETA: 1s - loss: 0.0820 - acc: 0.976 - ETA: 0s - loss: 0.0819 - acc: 0.976 - ETA: 0s - loss: 0.0830 - acc: 0.976 - ETA: 0s - loss: 0.0826 - acc: 0.976 - ETA: 0s - loss: 0.0832 - acc: 0.976 - ETA: 0s - loss: 0.0828 - acc: 0.976 - ETA: 0s - loss: 0.0828 - acc: 0.976 - ETA: 0s - loss: 0.0844 - acc: 0.976 - ETA: 0s - loss: 0.0841 - acc: 0.976 - ETA: 0s - loss: 0.0836 - acc: 0.976 - ETA: 0s - loss: 0.0843 - acc: 0.976 - ETA: 0s - loss: 0.0839 - acc: 0.976 - ETA: 0s - loss: 0.0834 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.976 - ETA: 0s - loss: 0.0853 - acc: 0.976 - ETA: 0s - loss: 0.0849 - acc: 0.976 - ETA: 0s - loss: 0.0845 - acc: 0.976 - ETA: 0s - loss: 0.0846 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.9770Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0841 - acc: 0.9769 - val_loss: 0.7012 - val_acc: 0.8575\n",
      "Epoch 19/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.1307 - acc: 0.950 - ETA: 10s - loss: 0.0482 - acc: 0.98 - ETA: 11s - loss: 0.0479 - acc: 0.98 - ETA: 10s - loss: 0.0354 - acc: 0.98 - ETA: 10s - loss: 0.0293 - acc: 0.98 - ETA: 9s - loss: 0.0472 - acc: 0.9818 - ETA: 9s - loss: 0.0924 - acc: 0.980 - ETA: 9s - loss: 0.0901 - acc: 0.980 - ETA: 9s - loss: 0.1124 - acc: 0.976 - ETA: 9s - loss: 0.1164 - acc: 0.976 - ETA: 9s - loss: 0.1165 - acc: 0.976 - ETA: 8s - loss: 0.1135 - acc: 0.976 - ETA: 8s - loss: 0.1386 - acc: 0.976 - ETA: 8s - loss: 0.1287 - acc: 0.977 - ETA: 8s - loss: 0.1214 - acc: 0.979 - ETA: 8s - loss: 0.1268 - acc: 0.979 - ETA: 8s - loss: 0.1194 - acc: 0.980 - ETA: 8s - loss: 0.1228 - acc: 0.980 - ETA: 8s - loss: 0.1164 - acc: 0.981 - ETA: 8s - loss: 0.1163 - acc: 0.979 - ETA: 8s - loss: 0.1152 - acc: 0.979 - ETA: 8s - loss: 0.1113 - acc: 0.980 - ETA: 8s - loss: 0.1076 - acc: 0.981 - ETA: 7s - loss: 0.1064 - acc: 0.979 - ETA: 7s - loss: 0.1021 - acc: 0.980 - ETA: 7s - loss: 0.0994 - acc: 0.980 - ETA: 7s - loss: 0.1042 - acc: 0.979 - ETA: 7s - loss: 0.1165 - acc: 0.979 - ETA: 7s - loss: 0.1160 - acc: 0.978 - ETA: 7s - loss: 0.1163 - acc: 0.977 - ETA: 7s - loss: 0.1144 - acc: 0.977 - ETA: 7s - loss: 0.1109 - acc: 0.977 - ETA: 7s - loss: 0.1089 - acc: 0.978 - ETA: 7s - loss: 0.1059 - acc: 0.979 - ETA: 7s - loss: 0.1036 - acc: 0.979 - ETA: 7s - loss: 0.1024 - acc: 0.978 - ETA: 7s - loss: 0.1016 - acc: 0.978 - ETA: 7s - loss: 0.0991 - acc: 0.979 - ETA: 7s - loss: 0.0997 - acc: 0.977 - ETA: 6s - loss: 0.0999 - acc: 0.977 - ETA: 6s - loss: 0.0998 - acc: 0.977 - ETA: 6s - loss: 0.0976 - acc: 0.977 - ETA: 6s - loss: 0.0955 - acc: 0.978 - ETA: 6s - loss: 0.0935 - acc: 0.978 - ETA: 6s - loss: 0.0933 - acc: 0.978 - ETA: 6s - loss: 0.0922 - acc: 0.978 - ETA: 6s - loss: 0.0922 - acc: 0.978 - ETA: 6s - loss: 0.0904 - acc: 0.978 - ETA: 6s - loss: 0.0898 - acc: 0.977 - ETA: 6s - loss: 0.0889 - acc: 0.977 - ETA: 6s - loss: 0.0889 - acc: 0.977 - ETA: 6s - loss: 0.0887 - acc: 0.977 - ETA: 6s - loss: 0.0873 - acc: 0.977 - ETA: 6s - loss: 0.0872 - acc: 0.977 - ETA: 6s - loss: 0.0859 - acc: 0.978 - ETA: 6s - loss: 0.0846 - acc: 0.978 - ETA: 5s - loss: 0.0838 - acc: 0.978 - ETA: 5s - loss: 0.0856 - acc: 0.977 - ETA: 5s - loss: 0.0847 - acc: 0.977 - ETA: 5s - loss: 0.0851 - acc: 0.977 - ETA: 5s - loss: 0.0839 - acc: 0.978 - ETA: 5s - loss: 0.0836 - acc: 0.978 - ETA: 5s - loss: 0.0830 - acc: 0.977 - ETA: 5s - loss: 0.0830 - acc: 0.977 - ETA: 5s - loss: 0.0866 - acc: 0.976 - ETA: 5s - loss: 0.0857 - acc: 0.977 - ETA: 5s - loss: 0.0876 - acc: 0.976 - ETA: 5s - loss: 0.0880 - acc: 0.976 - ETA: 5s - loss: 0.0868 - acc: 0.976 - ETA: 5s - loss: 0.0875 - acc: 0.975 - ETA: 5s - loss: 0.0867 - acc: 0.976 - ETA: 5s - loss: 0.0858 - acc: 0.976 - ETA: 5s - loss: 0.0857 - acc: 0.976 - ETA: 5s - loss: 0.0845 - acc: 0.976 - ETA: 4s - loss: 0.0837 - acc: 0.976 - ETA: 4s - loss: 0.0882 - acc: 0.976 - ETA: 4s - loss: 0.0873 - acc: 0.976 - ETA: 4s - loss: 0.0864 - acc: 0.977 - ETA: 4s - loss: 0.0867 - acc: 0.976 - ETA: 4s - loss: 0.0857 - acc: 0.977 - ETA: 4s - loss: 0.0849 - acc: 0.977 - ETA: 4s - loss: 0.0841 - acc: 0.977 - ETA: 4s - loss: 0.0834 - acc: 0.977 - ETA: 4s - loss: 0.0827 - acc: 0.977 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0819 - acc: 0.978 - ETA: 4s - loss: 0.0814 - acc: 0.978 - ETA: 4s - loss: 0.0820 - acc: 0.978 - ETA: 4s - loss: 0.0838 - acc: 0.977 - ETA: 4s - loss: 0.0846 - acc: 0.977 - ETA: 4s - loss: 0.0858 - acc: 0.977 - ETA: 4s - loss: 0.0852 - acc: 0.977 - ETA: 3s - loss: 0.0848 - acc: 0.977 - ETA: 3s - loss: 0.0847 - acc: 0.977 - ETA: 3s - loss: 0.0861 - acc: 0.977 - ETA: 3s - loss: 0.0857 - acc: 0.977 - ETA: 3s - loss: 0.0859 - acc: 0.977 - ETA: 3s - loss: 0.0852 - acc: 0.977 - ETA: 3s - loss: 0.0845 - acc: 0.977 - ETA: 3s - loss: 0.0843 - acc: 0.977 - ETA: 3s - loss: 0.0839 - acc: 0.977 - ETA: 3s - loss: 0.0836 - acc: 0.977 - ETA: 3s - loss: 0.0831 - acc: 0.977 - ETA: 3s - loss: 0.0824 - acc: 0.978 - ETA: 3s - loss: 0.0825 - acc: 0.977 - ETA: 3s - loss: 0.0832 - acc: 0.977 - ETA: 3s - loss: 0.0829 - acc: 0.977 - ETA: 3s - loss: 0.0822 - acc: 0.977 - ETA: 3s - loss: 0.0822 - acc: 0.977 - ETA: 3s - loss: 0.0815 - acc: 0.977 - ETA: 2s - loss: 0.0814 - acc: 0.977 - ETA: 2s - loss: 0.0815 - acc: 0.977 - ETA: 2s - loss: 0.0819 - acc: 0.977 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 2s - loss: 0.0810 - acc: 0.977 - ETA: 2s - loss: 0.0805 - acc: 0.977 - ETA: 2s - loss: 0.0801 - acc: 0.977 - ETA: 2s - loss: 0.0799 - acc: 0.977 - ETA: 2s - loss: 0.0794 - acc: 0.978 - ETA: 2s - loss: 0.0791 - acc: 0.978 - ETA: 2s - loss: 0.0787 - acc: 0.978 - ETA: 2s - loss: 0.0786 - acc: 0.977 - ETA: 2s - loss: 0.0783 - acc: 0.977 - ETA: 2s - loss: 0.0786 - acc: 0.977 - ETA: 2s - loss: 0.0784 - acc: 0.977 - ETA: 2s - loss: 0.0791 - acc: 0.977 - ETA: 2s - loss: 0.0787 - acc: 0.977 - ETA: 2s - loss: 0.0792 - acc: 0.977 - ETA: 1s - loss: 0.0787 - acc: 0.977 - ETA: 1s - loss: 0.0781 - acc: 0.977 - ETA: 1s - loss: 0.0778 - acc: 0.977 - ETA: 1s - loss: 0.0777 - acc: 0.977 - ETA: 1s - loss: 0.0772 - acc: 0.977 - ETA: 1s - loss: 0.0774 - acc: 0.977 - ETA: 1s - loss: 0.0769 - acc: 0.977 - ETA: 1s - loss: 0.0772 - acc: 0.977 - ETA: 1s - loss: 0.0770 - acc: 0.977 - ETA: 1s - loss: 0.0771 - acc: 0.977 - ETA: 1s - loss: 0.0768 - acc: 0.977 - ETA: 1s - loss: 0.0763 - acc: 0.977 - ETA: 1s - loss: 0.0764 - acc: 0.977 - ETA: 1s - loss: 0.0760 - acc: 0.977 - ETA: 1s - loss: 0.0756 - acc: 0.977 - ETA: 1s - loss: 0.0761 - acc: 0.977 - ETA: 1s - loss: 0.0757 - acc: 0.977 - ETA: 1s - loss: 0.0753 - acc: 0.978 - ETA: 0s - loss: 0.0757 - acc: 0.977 - ETA: 0s - loss: 0.0777 - acc: 0.977 - ETA: 0s - loss: 0.0790 - acc: 0.977 - ETA: 0s - loss: 0.0788 - acc: 0.977 - ETA: 0s - loss: 0.0784 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.977 - ETA: 0s - loss: 0.0783 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.978 - ETA: 0s - loss: 0.0782 - acc: 0.977 - ETA: 0s - loss: 0.0791 - acc: 0.977 - ETA: 0s - loss: 0.0791 - acc: 0.977 - ETA: 0s - loss: 0.0788 - acc: 0.977 - ETA: 0s - loss: 0.0784 - acc: 0.977 - ETA: 0s - loss: 0.0781 - acc: 0.977 - ETA: 0s - loss: 0.0777 - acc: 0.977 - ETA: 0s - loss: 0.0773 - acc: 0.978 - ETA: 0s - loss: 0.0770 - acc: 0.978 - ETA: 0s - loss: 0.0775 - acc: 0.978 - ETA: 0s - loss: 0.0772 - acc: 0.9781Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0770 - acc: 0.9781 - val_loss: 0.6545 - val_acc: 0.8575\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.0357 - acc: 1.000 - ETA: 9s - loss: 0.0740 - acc: 0.983 - ETA: 9s - loss: 0.0459 - acc: 0.990 - ETA: 9s - loss: 0.0761 - acc: 0.971 - ETA: 9s - loss: 0.0886 - acc: 0.972 - ETA: 9s - loss: 0.1477 - acc: 0.972 - ETA: 9s - loss: 0.1435 - acc: 0.969 - ETA: 9s - loss: 0.1367 - acc: 0.966 - ETA: 8s - loss: 0.1242 - acc: 0.970 - ETA: 8s - loss: 0.1121 - acc: 0.973 - ETA: 8s - loss: 0.1017 - acc: 0.976 - ETA: 8s - loss: 0.0948 - acc: 0.978 - ETA: 8s - loss: 0.0874 - acc: 0.980 - ETA: 8s - loss: 0.0850 - acc: 0.977 - ETA: 8s - loss: 0.0824 - acc: 0.977 - ETA: 8s - loss: 0.0783 - acc: 0.979 - ETA: 8s - loss: 0.0834 - acc: 0.975 - ETA: 8s - loss: 0.0796 - acc: 0.977 - ETA: 8s - loss: 0.0761 - acc: 0.978 - ETA: 8s - loss: 0.0724 - acc: 0.979 - ETA: 8s - loss: 0.0701 - acc: 0.980 - ETA: 7s - loss: 0.0700 - acc: 0.980 - ETA: 7s - loss: 0.0674 - acc: 0.981 - ETA: 7s - loss: 0.0659 - acc: 0.981 - ETA: 7s - loss: 0.0641 - acc: 0.982 - ETA: 7s - loss: 0.0701 - acc: 0.982 - ETA: 7s - loss: 0.0717 - acc: 0.982 - ETA: 7s - loss: 0.0693 - acc: 0.982 - ETA: 7s - loss: 0.0671 - acc: 0.983 - ETA: 7s - loss: 0.0734 - acc: 0.982 - ETA: 7s - loss: 0.0716 - acc: 0.982 - ETA: 7s - loss: 0.0701 - acc: 0.983 - ETA: 7s - loss: 0.0766 - acc: 0.981 - ETA: 7s - loss: 0.0767 - acc: 0.980 - ETA: 7s - loss: 0.0767 - acc: 0.979 - ETA: 7s - loss: 0.0755 - acc: 0.979 - ETA: 7s - loss: 0.0739 - acc: 0.980 - ETA: 7s - loss: 0.0721 - acc: 0.980 - ETA: 7s - loss: 0.0715 - acc: 0.980 - ETA: 6s - loss: 0.0712 - acc: 0.980 - ETA: 6s - loss: 0.0717 - acc: 0.980 - ETA: 6s - loss: 0.0710 - acc: 0.980 - ETA: 6s - loss: 0.0700 - acc: 0.980 - ETA: 6s - loss: 0.0686 - acc: 0.980 - ETA: 6s - loss: 0.0704 - acc: 0.980 - ETA: 6s - loss: 0.0692 - acc: 0.980 - ETA: 6s - loss: 0.0688 - acc: 0.980 - ETA: 6s - loss: 0.0691 - acc: 0.980 - ETA: 6s - loss: 0.0679 - acc: 0.980 - ETA: 6s - loss: 0.0666 - acc: 0.980 - ETA: 6s - loss: 0.0689 - acc: 0.980 - ETA: 6s - loss: 0.0677 - acc: 0.980 - ETA: 6s - loss: 0.0685 - acc: 0.980 - ETA: 6s - loss: 0.0703 - acc: 0.979 - ETA: 6s - loss: 0.0695 - acc: 0.980 - ETA: 6s - loss: 0.0724 - acc: 0.979 - ETA: 6s - loss: 0.0748 - acc: 0.978 - ETA: 6s - loss: 0.0738 - acc: 0.979 - ETA: 5s - loss: 0.0726 - acc: 0.979 - ETA: 5s - loss: 0.0719 - acc: 0.979 - ETA: 5s - loss: 0.0709 - acc: 0.979 - ETA: 5s - loss: 0.0700 - acc: 0.980 - ETA: 5s - loss: 0.0690 - acc: 0.980 - ETA: 5s - loss: 0.0689 - acc: 0.980 - ETA: 5s - loss: 0.0686 - acc: 0.980 - ETA: 5s - loss: 0.0679 - acc: 0.980 - ETA: 5s - loss: 0.0674 - acc: 0.980 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0660 - acc: 0.981 - ETA: 5s - loss: 0.0665 - acc: 0.980 - ETA: 5s - loss: 0.0683 - acc: 0.980 - ETA: 5s - loss: 0.0676 - acc: 0.980 - ETA: 5s - loss: 0.0671 - acc: 0.981 - ETA: 5s - loss: 0.0664 - acc: 0.981 - ETA: 5s - loss: 0.0659 - acc: 0.981 - ETA: 4s - loss: 0.0667 - acc: 0.980 - ETA: 4s - loss: 0.0662 - acc: 0.981 - ETA: 4s - loss: 0.0654 - acc: 0.981 - ETA: 4s - loss: 0.0651 - acc: 0.981 - ETA: 4s - loss: 0.0654 - acc: 0.981 - ETA: 4s - loss: 0.0648 - acc: 0.981 - ETA: 4s - loss: 0.0641 - acc: 0.981 - ETA: 4s - loss: 0.0668 - acc: 0.980 - ETA: 4s - loss: 0.0679 - acc: 0.980 - ETA: 4s - loss: 0.0688 - acc: 0.980 - ETA: 4s - loss: 0.0734 - acc: 0.980 - ETA: 4s - loss: 0.0728 - acc: 0.980 - ETA: 4s - loss: 0.0722 - acc: 0.980 - ETA: 4s - loss: 0.0724 - acc: 0.980 - ETA: 4s - loss: 0.0732 - acc: 0.980 - ETA: 4s - loss: 0.0727 - acc: 0.980 - ETA: 4s - loss: 0.0719 - acc: 0.981 - ETA: 4s - loss: 0.0730 - acc: 0.980 - ETA: 4s - loss: 0.0723 - acc: 0.981 - ETA: 3s - loss: 0.0734 - acc: 0.980 - ETA: 3s - loss: 0.0741 - acc: 0.980 - ETA: 3s - loss: 0.0736 - acc: 0.980 - ETA: 3s - loss: 0.0739 - acc: 0.980 - ETA: 3s - loss: 0.0733 - acc: 0.981 - ETA: 3s - loss: 0.0730 - acc: 0.980 - ETA: 3s - loss: 0.0766 - acc: 0.980 - ETA: 3s - loss: 0.0765 - acc: 0.980 - ETA: 3s - loss: 0.0758 - acc: 0.980 - ETA: 3s - loss: 0.0752 - acc: 0.980 - ETA: 3s - loss: 0.0751 - acc: 0.980 - ETA: 3s - loss: 0.0746 - acc: 0.980 - ETA: 3s - loss: 0.0744 - acc: 0.980 - ETA: 3s - loss: 0.0744 - acc: 0.980 - ETA: 3s - loss: 0.0747 - acc: 0.980 - ETA: 3s - loss: 0.0748 - acc: 0.979 - ETA: 3s - loss: 0.0755 - acc: 0.979 - ETA: 3s - loss: 0.0757 - acc: 0.979 - ETA: 2s - loss: 0.0764 - acc: 0.979 - ETA: 2s - loss: 0.0778 - acc: 0.978 - ETA: 2s - loss: 0.0772 - acc: 0.978 - ETA: 2s - loss: 0.0767 - acc: 0.979 - ETA: 2s - loss: 0.0770 - acc: 0.979 - ETA: 2s - loss: 0.0785 - acc: 0.978 - ETA: 2s - loss: 0.0780 - acc: 0.979 - ETA: 2s - loss: 0.0785 - acc: 0.979 - ETA: 2s - loss: 0.0781 - acc: 0.979 - ETA: 2s - loss: 0.0776 - acc: 0.979 - ETA: 2s - loss: 0.0773 - acc: 0.979 - ETA: 2s - loss: 0.0768 - acc: 0.979 - ETA: 2s - loss: 0.0777 - acc: 0.979 - ETA: 2s - loss: 0.0775 - acc: 0.979 - ETA: 2s - loss: 0.0772 - acc: 0.979 - ETA: 2s - loss: 0.0778 - acc: 0.978 - ETA: 2s - loss: 0.0776 - acc: 0.978 - ETA: 2s - loss: 0.0771 - acc: 0.979 - ETA: 1s - loss: 0.0770 - acc: 0.978 - ETA: 1s - loss: 0.0783 - acc: 0.978 - ETA: 1s - loss: 0.0784 - acc: 0.978 - ETA: 1s - loss: 0.0785 - acc: 0.978 - ETA: 1s - loss: 0.0779 - acc: 0.978 - ETA: 1s - loss: 0.0775 - acc: 0.978 - ETA: 1s - loss: 0.0770 - acc: 0.978 - ETA: 1s - loss: 0.0767 - acc: 0.978 - ETA: 1s - loss: 0.0762 - acc: 0.978 - ETA: 1s - loss: 0.0760 - acc: 0.978 - ETA: 1s - loss: 0.0755 - acc: 0.979 - ETA: 1s - loss: 0.0751 - acc: 0.979 - ETA: 1s - loss: 0.0747 - acc: 0.979 - ETA: 1s - loss: 0.0757 - acc: 0.979 - ETA: 1s - loss: 0.0755 - acc: 0.979 - ETA: 1s - loss: 0.0762 - acc: 0.978 - ETA: 1s - loss: 0.0761 - acc: 0.978 - ETA: 1s - loss: 0.0763 - acc: 0.978 - ETA: 1s - loss: 0.0759 - acc: 0.978 - ETA: 0s - loss: 0.0754 - acc: 0.978 - ETA: 0s - loss: 0.0751 - acc: 0.978 - ETA: 0s - loss: 0.0747 - acc: 0.979 - ETA: 0s - loss: 0.0743 - acc: 0.979 - ETA: 0s - loss: 0.0739 - acc: 0.979 - ETA: 0s - loss: 0.0736 - acc: 0.979 - ETA: 0s - loss: 0.0740 - acc: 0.979 - ETA: 0s - loss: 0.0741 - acc: 0.978 - ETA: 0s - loss: 0.0746 - acc: 0.978 - ETA: 0s - loss: 0.0742 - acc: 0.978 - ETA: 0s - loss: 0.0743 - acc: 0.978 - ETA: 0s - loss: 0.0742 - acc: 0.978 - ETA: 0s - loss: 0.0738 - acc: 0.978 - ETA: 0s - loss: 0.0734 - acc: 0.979 - ETA: 0s - loss: 0.0730 - acc: 0.979 - ETA: 0s - loss: 0.0727 - acc: 0.979 - ETA: 0s - loss: 0.0723 - acc: 0.979 - ETA: 0s - loss: 0.0723 - acc: 0.9793Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.0723 - acc: 0.9792 - val_loss: 0.7066 - val_acc: 0.8515\n"
     ]
    }
   ],
   "source": [
    "results = [with_bottleneck(network) for network in networks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40.550239234449762, 'saved_models/weights.best.VGG19.hdf5'), (79.904306220095691, 'saved_models/weights.best.Resnet50.hdf5'), (80.502392344497608, 'saved_models/weights.best.InceptionV3.hdf5'), (85.167464114832541, 'saved_models/weights.best.Xception.hdf5')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "def model1(network1, network2):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network1))\n",
    "    train_VGG16 = bottleneck_features['train']\n",
    "    valid_VGG16 = bottleneck_features['valid']\n",
    "    test_VGG16 = bottleneck_features['test']\n",
    "\n",
    "    VGG16_model = Sequential()\n",
    "    VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "    \n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network2))\n",
    "    train_network2 = bottleneck_features['train']\n",
    "    valid_network2 = bottleneck_features['valid']\n",
    "    test_network2 = bottleneck_features['test']\n",
    "\n",
    "    model_2 = Sequential()\n",
    "    model_2.add(GlobalAveragePooling2D(input_shape=train_network2.shape[1:]))\n",
    "    \n",
    "    merged = Merge([VGG16_model, model_2], mode='sum')\n",
    "    \n",
    "    merged_model = Sequential()\n",
    "    merged_model.add(merged)\n",
    "    merged_model.add(Dropout(0.7))\n",
    "    merged_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "    merged_model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    # merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    merged_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}_{}.hdf5'.format(network1, network2)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    merged_model.fit([train_VGG16, train_network2], train_targets, \n",
    "              validation_data=([valid_VGG16, valid_network2], valid_targets),\n",
    "              epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    merged_model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    merged_predictions = [np.argmax(merged_model.predict([np.expand_dims(feature[0], axis=0), np.expand_dims(feature[1], axis=0)])) for feature in zip(test_VGG16, test_network2)]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "    return (test_accuracy, VGG16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dep-nogi\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\ipykernel\\__main__.py:22: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 155s - loss: 7.1193 - acc: 0.0000e+0 - ETA: 58s - loss: 7.5688 - acc: 0.0000e+0 - ETA: 48s - loss: 7.3856 - acc: 0.0125   - ETA: 35s - loss: 7.3339 - acc: 0.00 - ETA: 29s - loss: 7.4092 - acc: 0.00 - ETA: 25s - loss: 7.3560 - acc: 0.00 - ETA: 22s - loss: 7.2440 - acc: 0.00 - ETA: 20s - loss: 7.1260 - acc: 0.00 - ETA: 19s - loss: 7.0663 - acc: 0.00 - ETA: 17s - loss: 6.9736 - acc: 0.01 - ETA: 16s - loss: 7.0317 - acc: 0.01 - ETA: 16s - loss: 6.9600 - acc: 0.01 - ETA: 15s - loss: 6.9379 - acc: 0.01 - ETA: 14s - loss: 6.9083 - acc: 0.01 - ETA: 14s - loss: 6.8635 - acc: 0.01 - ETA: 14s - loss: 6.8622 - acc: 0.01 - ETA: 13s - loss: 6.8166 - acc: 0.01 - ETA: 13s - loss: 6.8151 - acc: 0.01 - ETA: 13s - loss: 6.7392 - acc: 0.01 - ETA: 12s - loss: 6.7071 - acc: 0.01 - ETA: 12s - loss: 6.6617 - acc: 0.01 - ETA: 12s - loss: 6.6435 - acc: 0.01 - ETA: 11s - loss: 6.6290 - acc: 0.01 - ETA: 11s - loss: 6.6015 - acc: 0.01 - ETA: 11s - loss: 6.5881 - acc: 0.01 - ETA: 11s - loss: 6.5958 - acc: 0.01 - ETA: 11s - loss: 6.5535 - acc: 0.01 - ETA: 11s - loss: 6.5195 - acc: 0.01 - ETA: 10s - loss: 6.4961 - acc: 0.01 - ETA: 10s - loss: 6.4796 - acc: 0.01 - ETA: 10s - loss: 6.4534 - acc: 0.01 - ETA: 10s - loss: 6.4166 - acc: 0.01 - ETA: 10s - loss: 6.3922 - acc: 0.02 - ETA: 10s - loss: 6.3665 - acc: 0.02 - ETA: 9s - loss: 6.3288 - acc: 0.0250 - ETA: 9s - loss: 6.3158 - acc: 0.025 - ETA: 9s - loss: 6.2921 - acc: 0.027 - ETA: 9s - loss: 6.2635 - acc: 0.030 - ETA: 9s - loss: 6.2367 - acc: 0.031 - ETA: 9s - loss: 6.2140 - acc: 0.033 - ETA: 9s - loss: 6.1860 - acc: 0.035 - ETA: 9s - loss: 6.1457 - acc: 0.037 - ETA: 8s - loss: 6.1260 - acc: 0.037 - ETA: 8s - loss: 6.1086 - acc: 0.037 - ETA: 8s - loss: 6.0827 - acc: 0.036 - ETA: 8s - loss: 6.0685 - acc: 0.037 - ETA: 8s - loss: 6.0491 - acc: 0.038 - ETA: 8s - loss: 6.0231 - acc: 0.040 - ETA: 8s - loss: 6.0018 - acc: 0.041 - ETA: 8s - loss: 5.9691 - acc: 0.044 - ETA: 8s - loss: 5.9518 - acc: 0.044 - ETA: 7s - loss: 5.9301 - acc: 0.048 - ETA: 7s - loss: 5.9043 - acc: 0.050 - ETA: 7s - loss: 5.8807 - acc: 0.052 - ETA: 7s - loss: 5.8393 - acc: 0.055 - ETA: 7s - loss: 5.8140 - acc: 0.057 - ETA: 7s - loss: 5.7993 - acc: 0.058 - ETA: 7s - loss: 5.7758 - acc: 0.060 - ETA: 7s - loss: 5.7560 - acc: 0.062 - ETA: 7s - loss: 5.7340 - acc: 0.064 - ETA: 7s - loss: 5.7167 - acc: 0.065 - ETA: 6s - loss: 5.6746 - acc: 0.069 - ETA: 6s - loss: 5.6465 - acc: 0.070 - ETA: 6s - loss: 5.6221 - acc: 0.073 - ETA: 6s - loss: 5.5967 - acc: 0.074 - ETA: 6s - loss: 5.5767 - acc: 0.076 - ETA: 6s - loss: 5.5688 - acc: 0.076 - ETA: 6s - loss: 5.5373 - acc: 0.079 - ETA: 6s - loss: 5.5236 - acc: 0.079 - ETA: 6s - loss: 5.5042 - acc: 0.080 - ETA: 6s - loss: 5.4781 - acc: 0.081 - ETA: 6s - loss: 5.4474 - acc: 0.083 - ETA: 5s - loss: 5.4288 - acc: 0.083 - ETA: 5s - loss: 5.4054 - acc: 0.086 - ETA: 5s - loss: 5.3863 - acc: 0.087 - ETA: 5s - loss: 5.3704 - acc: 0.088 - ETA: 5s - loss: 5.3556 - acc: 0.089 - ETA: 5s - loss: 5.3413 - acc: 0.089 - ETA: 5s - loss: 5.3204 - acc: 0.092 - ETA: 5s - loss: 5.3067 - acc: 0.094 - ETA: 5s - loss: 5.2873 - acc: 0.095 - ETA: 5s - loss: 5.2658 - acc: 0.096 - ETA: 5s - loss: 5.2503 - acc: 0.097 - ETA: 5s - loss: 5.2326 - acc: 0.098 - ETA: 5s - loss: 5.2238 - acc: 0.099 - ETA: 5s - loss: 5.1989 - acc: 0.102 - ETA: 4s - loss: 5.1823 - acc: 0.102 - ETA: 4s - loss: 5.1564 - acc: 0.104 - ETA: 4s - loss: 5.1444 - acc: 0.106 - ETA: 4s - loss: 5.1286 - acc: 0.107 - ETA: 4s - loss: 5.1092 - acc: 0.110 - ETA: 4s - loss: 5.0955 - acc: 0.111 - ETA: 4s - loss: 5.0772 - acc: 0.113 - ETA: 4s - loss: 5.0622 - acc: 0.114 - ETA: 4s - loss: 5.0486 - acc: 0.115 - ETA: 4s - loss: 5.0294 - acc: 0.116 - ETA: 4s - loss: 5.0201 - acc: 0.117 - ETA: 4s - loss: 5.0049 - acc: 0.119 - ETA: 4s - loss: 4.9849 - acc: 0.121 - ETA: 3s - loss: 4.9725 - acc: 0.122 - ETA: 3s - loss: 4.9601 - acc: 0.123 - ETA: 3s - loss: 4.9363 - acc: 0.125 - ETA: 3s - loss: 4.9227 - acc: 0.126 - ETA: 3s - loss: 4.9122 - acc: 0.127 - ETA: 3s - loss: 4.9004 - acc: 0.128 - ETA: 3s - loss: 4.8864 - acc: 0.128 - ETA: 3s - loss: 4.8710 - acc: 0.130 - ETA: 3s - loss: 4.8502 - acc: 0.133 - ETA: 3s - loss: 4.8373 - acc: 0.133 - ETA: 3s - loss: 4.8221 - acc: 0.135 - ETA: 3s - loss: 4.8108 - acc: 0.136 - ETA: 3s - loss: 4.7963 - acc: 0.138 - ETA: 3s - loss: 4.7832 - acc: 0.139 - ETA: 3s - loss: 4.7713 - acc: 0.140 - ETA: 2s - loss: 4.7554 - acc: 0.142 - ETA: 2s - loss: 4.7440 - acc: 0.143 - ETA: 2s - loss: 4.7365 - acc: 0.144 - ETA: 2s - loss: 4.7232 - acc: 0.145 - ETA: 2s - loss: 4.7094 - acc: 0.145 - ETA: 2s - loss: 4.6963 - acc: 0.146 - ETA: 2s - loss: 4.6858 - acc: 0.147 - ETA: 2s - loss: 4.6763 - acc: 0.148 - ETA: 2s - loss: 4.6655 - acc: 0.149 - ETA: 2s - loss: 4.6418 - acc: 0.151 - ETA: 2s - loss: 4.6222 - acc: 0.153 - ETA: 2s - loss: 4.6089 - acc: 0.155 - ETA: 2s - loss: 4.5988 - acc: 0.156 - ETA: 2s - loss: 4.5826 - acc: 0.159 - ETA: 2s - loss: 4.5689 - acc: 0.160 - ETA: 1s - loss: 4.5583 - acc: 0.162 - ETA: 1s - loss: 4.5465 - acc: 0.163 - ETA: 1s - loss: 4.5377 - acc: 0.163 - ETA: 1s - loss: 4.5234 - acc: 0.165 - ETA: 1s - loss: 4.5089 - acc: 0.167 - ETA: 1s - loss: 4.4961 - acc: 0.169 - ETA: 1s - loss: 4.4814 - acc: 0.171 - ETA: 1s - loss: 4.4643 - acc: 0.173 - ETA: 1s - loss: 4.4556 - acc: 0.174 - ETA: 1s - loss: 4.4434 - acc: 0.175 - ETA: 1s - loss: 4.4392 - acc: 0.176 - ETA: 1s - loss: 4.4298 - acc: 0.177 - ETA: 1s - loss: 4.4177 - acc: 0.179 - ETA: 1s - loss: 4.4083 - acc: 0.180 - ETA: 1s - loss: 4.3980 - acc: 0.181 - ETA: 1s - loss: 4.3881 - acc: 0.181 - ETA: 1s - loss: 4.3806 - acc: 0.182 - ETA: 0s - loss: 4.3691 - acc: 0.184 - ETA: 0s - loss: 4.3608 - acc: 0.185 - ETA: 0s - loss: 4.3479 - acc: 0.186 - ETA: 0s - loss: 4.3367 - acc: 0.188 - ETA: 0s - loss: 4.3256 - acc: 0.189 - ETA: 0s - loss: 4.3167 - acc: 0.190 - ETA: 0s - loss: 4.3059 - acc: 0.191 - ETA: 0s - loss: 4.2979 - acc: 0.192 - ETA: 0s - loss: 4.2886 - acc: 0.193 - ETA: 0s - loss: 4.2783 - acc: 0.194 - ETA: 0s - loss: 4.2622 - acc: 0.196 - ETA: 0s - loss: 4.2461 - acc: 0.198 - ETA: 0s - loss: 4.2369 - acc: 0.199 - ETA: 0s - loss: 4.2329 - acc: 0.199 - ETA: 0s - loss: 4.2238 - acc: 0.200 - ETA: 0s - loss: 4.2162 - acc: 0.2015Epoch 00000: val_loss improved from inf to 1.23737, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 10s - loss: 4.2072 - acc: 0.2024 - val_loss: 1.2374 - val_acc: 0.6934\n",
      "Epoch 2/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 2.0206 - acc: 0.550 - ETA: 8s - loss: 2.0685 - acc: 0.516 - ETA: 8s - loss: 2.0916 - acc: 0.490 - ETA: 8s - loss: 2.2560 - acc: 0.457 - ETA: 8s - loss: 2.2312 - acc: 0.461 - ETA: 8s - loss: 2.1647 - acc: 0.450 - ETA: 8s - loss: 2.2325 - acc: 0.426 - ETA: 8s - loss: 2.2642 - acc: 0.420 - ETA: 8s - loss: 2.3164 - acc: 0.414 - ETA: 7s - loss: 2.3101 - acc: 0.427 - ETA: 7s - loss: 2.3501 - acc: 0.432 - ETA: 7s - loss: 2.3346 - acc: 0.438 - ETA: 7s - loss: 2.3243 - acc: 0.444 - ETA: 7s - loss: 2.3196 - acc: 0.445 - ETA: 7s - loss: 2.3246 - acc: 0.434 - ETA: 7s - loss: 2.3033 - acc: 0.440 - ETA: 7s - loss: 2.3036 - acc: 0.440 - ETA: 7s - loss: 2.2941 - acc: 0.443 - ETA: 7s - loss: 2.2976 - acc: 0.441 - ETA: 7s - loss: 2.3049 - acc: 0.438 - ETA: 7s - loss: 2.3147 - acc: 0.432 - ETA: 7s - loss: 2.3141 - acc: 0.431 - ETA: 7s - loss: 2.3079 - acc: 0.429 - ETA: 7s - loss: 2.3024 - acc: 0.430 - ETA: 6s - loss: 2.2983 - acc: 0.431 - ETA: 6s - loss: 2.2753 - acc: 0.434 - ETA: 6s - loss: 2.2702 - acc: 0.435 - ETA: 6s - loss: 2.2501 - acc: 0.442 - ETA: 6s - loss: 2.2468 - acc: 0.443 - ETA: 6s - loss: 2.2416 - acc: 0.444 - ETA: 6s - loss: 2.2430 - acc: 0.442 - ETA: 6s - loss: 2.2297 - acc: 0.443 - ETA: 6s - loss: 2.2277 - acc: 0.443 - ETA: 6s - loss: 2.2438 - acc: 0.442 - ETA: 6s - loss: 2.2339 - acc: 0.444 - ETA: 6s - loss: 2.2415 - acc: 0.447 - ETA: 6s - loss: 2.2304 - acc: 0.449 - ETA: 6s - loss: 2.2350 - acc: 0.448 - ETA: 6s - loss: 2.2228 - acc: 0.451 - ETA: 6s - loss: 2.2174 - acc: 0.453 - ETA: 5s - loss: 2.2174 - acc: 0.454 - ETA: 5s - loss: 2.2246 - acc: 0.454 - ETA: 5s - loss: 2.2132 - acc: 0.454 - ETA: 5s - loss: 2.2068 - acc: 0.455 - ETA: 5s - loss: 2.1947 - acc: 0.457 - ETA: 5s - loss: 2.1826 - acc: 0.458 - ETA: 5s - loss: 2.1765 - acc: 0.459 - ETA: 5s - loss: 2.1800 - acc: 0.458 - ETA: 5s - loss: 2.1730 - acc: 0.460 - ETA: 5s - loss: 2.1698 - acc: 0.459 - ETA: 5s - loss: 2.1666 - acc: 0.459 - ETA: 5s - loss: 2.1589 - acc: 0.460 - ETA: 5s - loss: 2.1593 - acc: 0.461 - ETA: 5s - loss: 2.1481 - acc: 0.462 - ETA: 5s - loss: 2.1541 - acc: 0.463 - ETA: 5s - loss: 2.1463 - acc: 0.465 - ETA: 4s - loss: 2.1432 - acc: 0.465 - ETA: 4s - loss: 2.1361 - acc: 0.466 - ETA: 4s - loss: 2.1330 - acc: 0.467 - ETA: 4s - loss: 2.1363 - acc: 0.466 - ETA: 4s - loss: 2.1348 - acc: 0.464 - ETA: 4s - loss: 2.1270 - acc: 0.465 - ETA: 4s - loss: 2.1256 - acc: 0.466 - ETA: 4s - loss: 2.1244 - acc: 0.465 - ETA: 4s - loss: 2.1206 - acc: 0.467 - ETA: 4s - loss: 2.1198 - acc: 0.466 - ETA: 4s - loss: 2.1149 - acc: 0.467 - ETA: 4s - loss: 2.1065 - acc: 0.469 - ETA: 4s - loss: 2.1035 - acc: 0.469 - ETA: 4s - loss: 2.0980 - acc: 0.471 - ETA: 4s - loss: 2.1007 - acc: 0.471 - ETA: 4s - loss: 2.0987 - acc: 0.472 - ETA: 4s - loss: 2.0965 - acc: 0.472 - ETA: 3s - loss: 2.0884 - acc: 0.474 - ETA: 3s - loss: 2.0854 - acc: 0.476 - ETA: 3s - loss: 2.0948 - acc: 0.474 - ETA: 3s - loss: 2.0912 - acc: 0.475 - ETA: 3s - loss: 2.0819 - acc: 0.477 - ETA: 3s - loss: 2.0820 - acc: 0.478 - ETA: 3s - loss: 2.0736 - acc: 0.480 - ETA: 3s - loss: 2.0787 - acc: 0.478 - ETA: 3s - loss: 2.0802 - acc: 0.478 - ETA: 3s - loss: 2.0871 - acc: 0.478 - ETA: 3s - loss: 2.0810 - acc: 0.480 - ETA: 3s - loss: 2.0802 - acc: 0.480 - ETA: 3s - loss: 2.0733 - acc: 0.480 - ETA: 3s - loss: 2.0684 - acc: 0.481 - ETA: 3s - loss: 2.0684 - acc: 0.481 - ETA: 3s - loss: 2.0647 - acc: 0.482 - ETA: 3s - loss: 2.0621 - acc: 0.481 - ETA: 3s - loss: 2.0551 - acc: 0.482 - ETA: 2s - loss: 2.0573 - acc: 0.482 - ETA: 2s - loss: 2.0625 - acc: 0.480 - ETA: 2s - loss: 2.0558 - acc: 0.482 - ETA: 2s - loss: 2.0520 - acc: 0.482 - ETA: 2s - loss: 2.0471 - acc: 0.483 - ETA: 2s - loss: 2.0451 - acc: 0.483 - ETA: 2s - loss: 2.0477 - acc: 0.482 - ETA: 2s - loss: 2.0430 - acc: 0.483 - ETA: 2s - loss: 2.0399 - acc: 0.483 - ETA: 2s - loss: 2.0375 - acc: 0.484 - ETA: 2s - loss: 2.0396 - acc: 0.484 - ETA: 2s - loss: 2.0377 - acc: 0.485 - ETA: 2s - loss: 2.0358 - acc: 0.486 - ETA: 2s - loss: 2.0372 - acc: 0.486 - ETA: 2s - loss: 2.0347 - acc: 0.485 - ETA: 2s - loss: 2.0349 - acc: 0.486 - ETA: 2s - loss: 2.0338 - acc: 0.486 - ETA: 2s - loss: 2.0300 - acc: 0.487 - ETA: 2s - loss: 2.0274 - acc: 0.487 - ETA: 1s - loss: 2.0280 - acc: 0.487 - ETA: 1s - loss: 2.0267 - acc: 0.487 - ETA: 1s - loss: 2.0263 - acc: 0.488 - ETA: 1s - loss: 2.0250 - acc: 0.489 - ETA: 1s - loss: 2.0196 - acc: 0.490 - ETA: 1s - loss: 2.0210 - acc: 0.490 - ETA: 1s - loss: 2.0196 - acc: 0.490 - ETA: 1s - loss: 2.0178 - acc: 0.490 - ETA: 1s - loss: 2.0165 - acc: 0.491 - ETA: 1s - loss: 2.0146 - acc: 0.492 - ETA: 1s - loss: 2.0128 - acc: 0.492 - ETA: 1s - loss: 2.0126 - acc: 0.491 - ETA: 1s - loss: 2.0116 - acc: 0.491 - ETA: 1s - loss: 2.0132 - acc: 0.490 - ETA: 1s - loss: 2.0088 - acc: 0.491 - ETA: 1s - loss: 2.0079 - acc: 0.491 - ETA: 1s - loss: 2.0064 - acc: 0.491 - ETA: 1s - loss: 2.0040 - acc: 0.491 - ETA: 0s - loss: 2.0017 - acc: 0.491 - ETA: 0s - loss: 2.0006 - acc: 0.492 - ETA: 0s - loss: 1.9965 - acc: 0.493 - ETA: 0s - loss: 1.9967 - acc: 0.493 - ETA: 0s - loss: 1.9914 - acc: 0.494 - ETA: 0s - loss: 1.9924 - acc: 0.494 - ETA: 0s - loss: 1.9907 - acc: 0.494 - ETA: 0s - loss: 1.9915 - acc: 0.495 - ETA: 0s - loss: 1.9877 - acc: 0.495 - ETA: 0s - loss: 1.9824 - acc: 0.497 - ETA: 0s - loss: 1.9816 - acc: 0.497 - ETA: 0s - loss: 1.9790 - acc: 0.497 - ETA: 0s - loss: 1.9746 - acc: 0.497 - ETA: 0s - loss: 1.9668 - acc: 0.499 - ETA: 0s - loss: 1.9666 - acc: 0.499 - ETA: 0s - loss: 1.9658 - acc: 0.500 - ETA: 0s - loss: 1.9602 - acc: 0.501 - ETA: 0s - loss: 1.9570 - acc: 0.5020Epoch 00001: val_loss improved from 1.23737 to 0.77463, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.9554 - acc: 0.5019 - val_loss: 0.7746 - val_acc: 0.7784\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 2.1534 - acc: 0.550 - ETA: 8s - loss: 1.6640 - acc: 0.566 - ETA: 8s - loss: 1.8193 - acc: 0.580 - ETA: 8s - loss: 1.6029 - acc: 0.631 - ETA: 8s - loss: 1.5934 - acc: 0.610 - ETA: 8s - loss: 1.5764 - acc: 0.612 - ETA: 7s - loss: 1.5723 - acc: 0.610 - ETA: 7s - loss: 1.5196 - acc: 0.620 - ETA: 7s - loss: 1.5162 - acc: 0.613 - ETA: 7s - loss: 1.4621 - acc: 0.631 - ETA: 7s - loss: 1.4686 - acc: 0.635 - ETA: 7s - loss: 1.4871 - acc: 0.622 - ETA: 7s - loss: 1.5170 - acc: 0.615 - ETA: 7s - loss: 1.5264 - acc: 0.610 - ETA: 7s - loss: 1.5087 - acc: 0.604 - ETA: 7s - loss: 1.5257 - acc: 0.600 - ETA: 7s - loss: 1.5022 - acc: 0.606 - ETA: 6s - loss: 1.4893 - acc: 0.604 - ETA: 6s - loss: 1.4876 - acc: 0.603 - ETA: 6s - loss: 1.4946 - acc: 0.602 - ETA: 6s - loss: 1.4870 - acc: 0.603 - ETA: 6s - loss: 1.4868 - acc: 0.601 - ETA: 6s - loss: 1.4839 - acc: 0.600 - ETA: 6s - loss: 1.4797 - acc: 0.600 - ETA: 6s - loss: 1.4838 - acc: 0.602 - ETA: 6s - loss: 1.4790 - acc: 0.604 - ETA: 6s - loss: 1.4901 - acc: 0.601 - ETA: 6s - loss: 1.4822 - acc: 0.604 - ETA: 6s - loss: 1.4716 - acc: 0.608 - ETA: 6s - loss: 1.4677 - acc: 0.611 - ETA: 6s - loss: 1.4604 - acc: 0.613 - ETA: 6s - loss: 1.4534 - acc: 0.613 - ETA: 6s - loss: 1.4586 - acc: 0.612 - ETA: 6s - loss: 1.4593 - acc: 0.610 - ETA: 6s - loss: 1.4515 - acc: 0.612 - ETA: 5s - loss: 1.4486 - acc: 0.612 - ETA: 5s - loss: 1.4538 - acc: 0.612 - ETA: 5s - loss: 1.4405 - acc: 0.616 - ETA: 5s - loss: 1.4448 - acc: 0.613 - ETA: 5s - loss: 1.4432 - acc: 0.615 - ETA: 5s - loss: 1.4455 - acc: 0.612 - ETA: 5s - loss: 1.4352 - acc: 0.614 - ETA: 5s - loss: 1.4351 - acc: 0.616 - ETA: 5s - loss: 1.4402 - acc: 0.616 - ETA: 5s - loss: 1.4343 - acc: 0.618 - ETA: 5s - loss: 1.4266 - acc: 0.619 - ETA: 5s - loss: 1.4246 - acc: 0.619 - ETA: 5s - loss: 1.4232 - acc: 0.621 - ETA: 5s - loss: 1.4323 - acc: 0.617 - ETA: 5s - loss: 1.4437 - acc: 0.615 - ETA: 5s - loss: 1.4494 - acc: 0.613 - ETA: 5s - loss: 1.4455 - acc: 0.614 - ETA: 4s - loss: 1.4391 - acc: 0.615 - ETA: 4s - loss: 1.4311 - acc: 0.617 - ETA: 4s - loss: 1.4239 - acc: 0.618 - ETA: 4s - loss: 1.4268 - acc: 0.616 - ETA: 4s - loss: 1.4215 - acc: 0.618 - ETA: 4s - loss: 1.4229 - acc: 0.618 - ETA: 4s - loss: 1.4192 - acc: 0.618 - ETA: 4s - loss: 1.4183 - acc: 0.618 - ETA: 4s - loss: 1.4211 - acc: 0.618 - ETA: 4s - loss: 1.4217 - acc: 0.618 - ETA: 4s - loss: 1.4255 - acc: 0.617 - ETA: 4s - loss: 1.4274 - acc: 0.615 - ETA: 4s - loss: 1.4240 - acc: 0.616 - ETA: 4s - loss: 1.4195 - acc: 0.617 - ETA: 4s - loss: 1.4209 - acc: 0.615 - ETA: 4s - loss: 1.4280 - acc: 0.614 - ETA: 4s - loss: 1.4221 - acc: 0.614 - ETA: 3s - loss: 1.4177 - acc: 0.614 - ETA: 3s - loss: 1.4139 - acc: 0.614 - ETA: 3s - loss: 1.4134 - acc: 0.615 - ETA: 3s - loss: 1.4107 - acc: 0.614 - ETA: 3s - loss: 1.4080 - acc: 0.615 - ETA: 3s - loss: 1.4123 - acc: 0.613 - ETA: 3s - loss: 1.4112 - acc: 0.613 - ETA: 3s - loss: 1.4138 - acc: 0.612 - ETA: 3s - loss: 1.4105 - acc: 0.613 - ETA: 3s - loss: 1.4114 - acc: 0.610 - ETA: 3s - loss: 1.4110 - acc: 0.610 - ETA: 3s - loss: 1.4107 - acc: 0.611 - ETA: 3s - loss: 1.4061 - acc: 0.612 - ETA: 3s - loss: 1.4061 - acc: 0.611 - ETA: 3s - loss: 1.4081 - acc: 0.612 - ETA: 3s - loss: 1.4072 - acc: 0.612 - ETA: 3s - loss: 1.4032 - acc: 0.613 - ETA: 3s - loss: 1.4022 - acc: 0.612 - ETA: 2s - loss: 1.3997 - acc: 0.613 - ETA: 2s - loss: 1.4004 - acc: 0.612 - ETA: 2s - loss: 1.4032 - acc: 0.612 - ETA: 2s - loss: 1.4003 - acc: 0.612 - ETA: 2s - loss: 1.3949 - acc: 0.613 - ETA: 2s - loss: 1.3965 - acc: 0.613 - ETA: 2s - loss: 1.3867 - acc: 0.616 - ETA: 2s - loss: 1.3862 - acc: 0.616 - ETA: 2s - loss: 1.3843 - acc: 0.617 - ETA: 2s - loss: 1.3824 - acc: 0.618 - ETA: 2s - loss: 1.3831 - acc: 0.617 - ETA: 2s - loss: 1.3827 - acc: 0.617 - ETA: 2s - loss: 1.3808 - acc: 0.617 - ETA: 2s - loss: 1.3803 - acc: 0.617 - ETA: 2s - loss: 1.3778 - acc: 0.618 - ETA: 2s - loss: 1.3825 - acc: 0.617 - ETA: 2s - loss: 1.3799 - acc: 0.618 - ETA: 1s - loss: 1.3790 - acc: 0.617 - ETA: 1s - loss: 1.3783 - acc: 0.617 - ETA: 1s - loss: 1.3797 - acc: 0.617 - ETA: 1s - loss: 1.3801 - acc: 0.617 - ETA: 1s - loss: 1.3800 - acc: 0.617 - ETA: 1s - loss: 1.3771 - acc: 0.618 - ETA: 1s - loss: 1.3769 - acc: 0.619 - ETA: 1s - loss: 1.3739 - acc: 0.620 - ETA: 1s - loss: 1.3785 - acc: 0.619 - ETA: 1s - loss: 1.3788 - acc: 0.619 - ETA: 1s - loss: 1.3815 - acc: 0.620 - ETA: 1s - loss: 1.3795 - acc: 0.619 - ETA: 1s - loss: 1.3778 - acc: 0.620 - ETA: 1s - loss: 1.3787 - acc: 0.619 - ETA: 1s - loss: 1.3783 - acc: 0.619 - ETA: 1s - loss: 1.3781 - acc: 0.620 - ETA: 1s - loss: 1.3780 - acc: 0.619 - ETA: 1s - loss: 1.3762 - acc: 0.620 - ETA: 1s - loss: 1.3746 - acc: 0.620 - ETA: 0s - loss: 1.3794 - acc: 0.620 - ETA: 0s - loss: 1.3779 - acc: 0.621 - ETA: 0s - loss: 1.3743 - acc: 0.621 - ETA: 0s - loss: 1.3771 - acc: 0.621 - ETA: 0s - loss: 1.3780 - acc: 0.620 - ETA: 0s - loss: 1.3761 - acc: 0.621 - ETA: 0s - loss: 1.3757 - acc: 0.620 - ETA: 0s - loss: 1.3731 - acc: 0.620 - ETA: 0s - loss: 1.3711 - acc: 0.621 - ETA: 0s - loss: 1.3698 - acc: 0.621 - ETA: 0s - loss: 1.3703 - acc: 0.622 - ETA: 0s - loss: 1.3698 - acc: 0.621 - ETA: 0s - loss: 1.3673 - acc: 0.621 - ETA: 0s - loss: 1.3675 - acc: 0.621 - ETA: 0s - loss: 1.3693 - acc: 0.621 - ETA: 0s - loss: 1.3690 - acc: 0.622 - ETA: 0s - loss: 1.3680 - acc: 0.621 - ETA: 0s - loss: 1.3674 - acc: 0.622 - ETA: 0s - loss: 1.3671 - acc: 0.622 - ETA: 0s - loss: 1.3721 - acc: 0.621 - ETA: 0s - loss: 1.3686 - acc: 0.6222Epoch 00002: val_loss improved from 0.77463 to 0.63802, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.3678 - acc: 0.6225 - val_loss: 0.6380 - val_acc: 0.8060\n",
      "Epoch 4/20\n",
      "6620/6680 [============================>.] - ETA: 9s - loss: 0.7069 - acc: 0.750 - ETA: 8s - loss: 1.0513 - acc: 0.683 - ETA: 8s - loss: 1.1871 - acc: 0.680 - ETA: 8s - loss: 1.1503 - acc: 0.692 - ETA: 8s - loss: 1.1356 - acc: 0.705 - ETA: 8s - loss: 1.1233 - acc: 0.700 - ETA: 8s - loss: 1.1376 - acc: 0.696 - ETA: 8s - loss: 1.1431 - acc: 0.686 - ETA: 8s - loss: 1.1128 - acc: 0.691 - ETA: 8s - loss: 1.0962 - acc: 0.694 - ETA: 8s - loss: 1.0671 - acc: 0.702 - ETA: 7s - loss: 1.0347 - acc: 0.708 - ETA: 7s - loss: 1.0237 - acc: 0.713 - ETA: 7s - loss: 1.0380 - acc: 0.706 - ETA: 7s - loss: 1.0782 - acc: 0.700 - ETA: 7s - loss: 1.0887 - acc: 0.700 - ETA: 7s - loss: 1.1047 - acc: 0.691 - ETA: 7s - loss: 1.1228 - acc: 0.685 - ETA: 7s - loss: 1.1208 - acc: 0.686 - ETA: 7s - loss: 1.1299 - acc: 0.686 - ETA: 7s - loss: 1.1340 - acc: 0.683 - ETA: 6s - loss: 1.1207 - acc: 0.684 - ETA: 6s - loss: 1.1276 - acc: 0.682 - ETA: 6s - loss: 1.1170 - acc: 0.681 - ETA: 6s - loss: 1.1012 - acc: 0.685 - ETA: 6s - loss: 1.1062 - acc: 0.680 - ETA: 6s - loss: 1.0981 - acc: 0.682 - ETA: 6s - loss: 1.0942 - acc: 0.679 - ETA: 6s - loss: 1.1082 - acc: 0.677 - ETA: 6s - loss: 1.0908 - acc: 0.683 - ETA: 6s - loss: 1.0886 - acc: 0.681 - ETA: 6s - loss: 1.0829 - acc: 0.680 - ETA: 6s - loss: 1.0863 - acc: 0.682 - ETA: 6s - loss: 1.0862 - acc: 0.682 - ETA: 6s - loss: 1.0878 - acc: 0.682 - ETA: 5s - loss: 1.0831 - acc: 0.684 - ETA: 5s - loss: 1.0831 - acc: 0.683 - ETA: 5s - loss: 1.0811 - acc: 0.683 - ETA: 5s - loss: 1.0877 - acc: 0.681 - ETA: 5s - loss: 1.0962 - acc: 0.679 - ETA: 5s - loss: 1.0956 - acc: 0.679 - ETA: 5s - loss: 1.0963 - acc: 0.679 - ETA: 5s - loss: 1.0955 - acc: 0.678 - ETA: 5s - loss: 1.0863 - acc: 0.680 - ETA: 5s - loss: 1.0882 - acc: 0.679 - ETA: 5s - loss: 1.0893 - acc: 0.680 - ETA: 5s - loss: 1.0845 - acc: 0.681 - ETA: 5s - loss: 1.0828 - acc: 0.682 - ETA: 5s - loss: 1.0949 - acc: 0.678 - ETA: 5s - loss: 1.0943 - acc: 0.679 - ETA: 5s - loss: 1.0957 - acc: 0.680 - ETA: 5s - loss: 1.0912 - acc: 0.682 - ETA: 4s - loss: 1.1002 - acc: 0.679 - ETA: 4s - loss: 1.0988 - acc: 0.679 - ETA: 4s - loss: 1.0956 - acc: 0.680 - ETA: 4s - loss: 1.1087 - acc: 0.677 - ETA: 4s - loss: 1.1131 - acc: 0.677 - ETA: 4s - loss: 1.1150 - acc: 0.677 - ETA: 4s - loss: 1.1160 - acc: 0.677 - ETA: 4s - loss: 1.1220 - acc: 0.677 - ETA: 4s - loss: 1.1242 - acc: 0.676 - ETA: 4s - loss: 1.1227 - acc: 0.676 - ETA: 4s - loss: 1.1279 - acc: 0.674 - ETA: 4s - loss: 1.1276 - acc: 0.674 - ETA: 4s - loss: 1.1326 - acc: 0.673 - ETA: 4s - loss: 1.1337 - acc: 0.673 - ETA: 4s - loss: 1.1365 - acc: 0.674 - ETA: 4s - loss: 1.1315 - acc: 0.675 - ETA: 4s - loss: 1.1313 - acc: 0.675 - ETA: 3s - loss: 1.1266 - acc: 0.677 - ETA: 3s - loss: 1.1276 - acc: 0.676 - ETA: 3s - loss: 1.1232 - acc: 0.677 - ETA: 3s - loss: 1.1226 - acc: 0.677 - ETA: 3s - loss: 1.1157 - acc: 0.679 - ETA: 3s - loss: 1.1145 - acc: 0.679 - ETA: 3s - loss: 1.1123 - acc: 0.680 - ETA: 3s - loss: 1.1140 - acc: 0.679 - ETA: 3s - loss: 1.1094 - acc: 0.681 - ETA: 3s - loss: 1.1069 - acc: 0.681 - ETA: 3s - loss: 1.1068 - acc: 0.682 - ETA: 3s - loss: 1.1079 - acc: 0.681 - ETA: 3s - loss: 1.1116 - acc: 0.680 - ETA: 3s - loss: 1.1109 - acc: 0.680 - ETA: 3s - loss: 1.1102 - acc: 0.681 - ETA: 3s - loss: 1.1113 - acc: 0.680 - ETA: 3s - loss: 1.1067 - acc: 0.682 - ETA: 3s - loss: 1.1130 - acc: 0.681 - ETA: 2s - loss: 1.1110 - acc: 0.681 - ETA: 2s - loss: 1.1087 - acc: 0.681 - ETA: 2s - loss: 1.1083 - acc: 0.682 - ETA: 2s - loss: 1.1103 - acc: 0.682 - ETA: 2s - loss: 1.1082 - acc: 0.683 - ETA: 2s - loss: 1.1072 - acc: 0.683 - ETA: 2s - loss: 1.1059 - acc: 0.683 - ETA: 2s - loss: 1.1049 - acc: 0.683 - ETA: 2s - loss: 1.1109 - acc: 0.683 - ETA: 2s - loss: 1.1112 - acc: 0.683 - ETA: 2s - loss: 1.1111 - acc: 0.683 - ETA: 2s - loss: 1.1108 - acc: 0.683 - ETA: 2s - loss: 1.1091 - acc: 0.684 - ETA: 2s - loss: 1.1089 - acc: 0.682 - ETA: 2s - loss: 1.1060 - acc: 0.682 - ETA: 2s - loss: 1.1041 - acc: 0.683 - ETA: 2s - loss: 1.1022 - acc: 0.683 - ETA: 2s - loss: 1.1037 - acc: 0.683 - ETA: 1s - loss: 1.1029 - acc: 0.683 - ETA: 1s - loss: 1.1013 - acc: 0.684 - ETA: 1s - loss: 1.1000 - acc: 0.684 - ETA: 1s - loss: 1.1027 - acc: 0.683 - ETA: 1s - loss: 1.0988 - acc: 0.684 - ETA: 1s - loss: 1.1023 - acc: 0.684 - ETA: 1s - loss: 1.1028 - acc: 0.684 - ETA: 1s - loss: 1.1016 - acc: 0.684 - ETA: 1s - loss: 1.1045 - acc: 0.683 - ETA: 1s - loss: 1.1034 - acc: 0.684 - ETA: 1s - loss: 1.1070 - acc: 0.683 - ETA: 1s - loss: 1.1044 - acc: 0.683 - ETA: 1s - loss: 1.1027 - acc: 0.684 - ETA: 1s - loss: 1.1047 - acc: 0.683 - ETA: 1s - loss: 1.1033 - acc: 0.683 - ETA: 1s - loss: 1.1015 - acc: 0.684 - ETA: 1s - loss: 1.1029 - acc: 0.684 - ETA: 0s - loss: 1.0986 - acc: 0.684 - ETA: 0s - loss: 1.0971 - acc: 0.685 - ETA: 0s - loss: 1.0949 - acc: 0.686 - ETA: 0s - loss: 1.0951 - acc: 0.686 - ETA: 0s - loss: 1.0927 - acc: 0.687 - ETA: 0s - loss: 1.0930 - acc: 0.687 - ETA: 0s - loss: 1.0938 - acc: 0.687 - ETA: 0s - loss: 1.0931 - acc: 0.687 - ETA: 0s - loss: 1.0919 - acc: 0.687 - ETA: 0s - loss: 1.0909 - acc: 0.687 - ETA: 0s - loss: 1.0904 - acc: 0.688 - ETA: 0s - loss: 1.0892 - acc: 0.688 - ETA: 0s - loss: 1.0892 - acc: 0.688 - ETA: 0s - loss: 1.0913 - acc: 0.687 - ETA: 0s - loss: 1.0910 - acc: 0.687 - ETA: 0s - loss: 1.0919 - acc: 0.688 - ETA: 0s - loss: 1.0910 - acc: 0.6878Epoch 00003: val_loss improved from 0.63802 to 0.56725, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.0888 - acc: 0.6885 - val_loss: 0.5672 - val_acc: 0.8275\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 1.5036 - acc: 0.550 - ETA: 8s - loss: 1.1315 - acc: 0.683 - ETA: 8s - loss: 1.0921 - acc: 0.680 - ETA: 8s - loss: 1.1041 - acc: 0.671 - ETA: 8s - loss: 1.0980 - acc: 0.672 - ETA: 8s - loss: 1.0615 - acc: 0.681 - ETA: 8s - loss: 1.0502 - acc: 0.692 - ETA: 8s - loss: 1.0092 - acc: 0.700 - ETA: 8s - loss: 1.0046 - acc: 0.702 - ETA: 8s - loss: 1.0134 - acc: 0.700 - ETA: 7s - loss: 1.0101 - acc: 0.700 - ETA: 7s - loss: 0.9903 - acc: 0.706 - ETA: 7s - loss: 0.9913 - acc: 0.703 - ETA: 7s - loss: 0.9838 - acc: 0.705 - ETA: 7s - loss: 0.9819 - acc: 0.709 - ETA: 7s - loss: 0.9724 - acc: 0.713 - ETA: 7s - loss: 0.9682 - acc: 0.713 - ETA: 7s - loss: 0.9922 - acc: 0.707 - ETA: 7s - loss: 0.9996 - acc: 0.711 - ETA: 7s - loss: 1.0337 - acc: 0.703 - ETA: 7s - loss: 1.0355 - acc: 0.703 - ETA: 7s - loss: 1.0264 - acc: 0.707 - ETA: 7s - loss: 1.0397 - acc: 0.707 - ETA: 7s - loss: 1.0241 - acc: 0.710 - ETA: 7s - loss: 1.0236 - acc: 0.708 - ETA: 6s - loss: 1.0213 - acc: 0.710 - ETA: 6s - loss: 1.0165 - acc: 0.712 - ETA: 6s - loss: 1.0024 - acc: 0.715 - ETA: 6s - loss: 0.9916 - acc: 0.717 - ETA: 6s - loss: 0.9881 - acc: 0.716 - ETA: 6s - loss: 0.9827 - acc: 0.716 - ETA: 6s - loss: 0.9820 - acc: 0.718 - ETA: 6s - loss: 0.9834 - acc: 0.718 - ETA: 6s - loss: 0.9809 - acc: 0.718 - ETA: 6s - loss: 0.9764 - acc: 0.718 - ETA: 6s - loss: 0.9763 - acc: 0.720 - ETA: 6s - loss: 0.9694 - acc: 0.722 - ETA: 6s - loss: 0.9744 - acc: 0.720 - ETA: 6s - loss: 0.9734 - acc: 0.720 - ETA: 6s - loss: 0.9835 - acc: 0.719 - ETA: 6s - loss: 0.9822 - acc: 0.717 - ETA: 5s - loss: 0.9766 - acc: 0.719 - ETA: 5s - loss: 0.9743 - acc: 0.721 - ETA: 5s - loss: 0.9780 - acc: 0.720 - ETA: 5s - loss: 0.9739 - acc: 0.720 - ETA: 5s - loss: 0.9773 - acc: 0.720 - ETA: 5s - loss: 0.9709 - acc: 0.722 - ETA: 5s - loss: 0.9669 - acc: 0.723 - ETA: 5s - loss: 0.9616 - acc: 0.724 - ETA: 5s - loss: 0.9613 - acc: 0.723 - ETA: 5s - loss: 0.9598 - acc: 0.722 - ETA: 5s - loss: 0.9563 - acc: 0.722 - ETA: 5s - loss: 0.9518 - acc: 0.723 - ETA: 5s - loss: 0.9513 - acc: 0.723 - ETA: 5s - loss: 0.9535 - acc: 0.722 - ETA: 5s - loss: 0.9577 - acc: 0.721 - ETA: 5s - loss: 0.9616 - acc: 0.720 - ETA: 5s - loss: 0.9603 - acc: 0.720 - ETA: 5s - loss: 0.9566 - acc: 0.721 - ETA: 5s - loss: 0.9509 - acc: 0.723 - ETA: 5s - loss: 0.9497 - acc: 0.724 - ETA: 5s - loss: 0.9508 - acc: 0.724 - ETA: 5s - loss: 0.9497 - acc: 0.724 - ETA: 4s - loss: 0.9449 - acc: 0.725 - ETA: 4s - loss: 0.9460 - acc: 0.725 - ETA: 4s - loss: 0.9468 - acc: 0.724 - ETA: 4s - loss: 0.9465 - acc: 0.724 - ETA: 4s - loss: 0.9459 - acc: 0.723 - ETA: 4s - loss: 0.9453 - acc: 0.723 - ETA: 4s - loss: 0.9482 - acc: 0.722 - ETA: 4s - loss: 0.9437 - acc: 0.724 - ETA: 4s - loss: 0.9478 - acc: 0.724 - ETA: 4s - loss: 0.9470 - acc: 0.725 - ETA: 4s - loss: 0.9460 - acc: 0.724 - ETA: 4s - loss: 0.9442 - acc: 0.724 - ETA: 4s - loss: 0.9464 - acc: 0.723 - ETA: 4s - loss: 0.9502 - acc: 0.722 - ETA: 4s - loss: 0.9529 - acc: 0.721 - ETA: 4s - loss: 0.9539 - acc: 0.721 - ETA: 4s - loss: 0.9543 - acc: 0.721 - ETA: 4s - loss: 0.9546 - acc: 0.721 - ETA: 4s - loss: 0.9530 - acc: 0.721 - ETA: 4s - loss: 0.9542 - acc: 0.721 - ETA: 4s - loss: 0.9565 - acc: 0.721 - ETA: 3s - loss: 0.9533 - acc: 0.722 - ETA: 3s - loss: 0.9505 - acc: 0.723 - ETA: 3s - loss: 0.9517 - acc: 0.721 - ETA: 3s - loss: 0.9486 - acc: 0.722 - ETA: 3s - loss: 0.9461 - acc: 0.723 - ETA: 3s - loss: 0.9468 - acc: 0.723 - ETA: 3s - loss: 0.9492 - acc: 0.723 - ETA: 3s - loss: 0.9481 - acc: 0.722 - ETA: 3s - loss: 0.9480 - acc: 0.722 - ETA: 3s - loss: 0.9457 - acc: 0.722 - ETA: 3s - loss: 0.9446 - acc: 0.723 - ETA: 3s - loss: 0.9429 - acc: 0.723 - ETA: 3s - loss: 0.9391 - acc: 0.724 - ETA: 3s - loss: 0.9393 - acc: 0.723 - ETA: 3s - loss: 0.9373 - acc: 0.724 - ETA: 3s - loss: 0.9359 - acc: 0.724 - ETA: 3s - loss: 0.9317 - acc: 0.725 - ETA: 3s - loss: 0.9395 - acc: 0.722 - ETA: 3s - loss: 0.9391 - acc: 0.722 - ETA: 3s - loss: 0.9349 - acc: 0.723 - ETA: 2s - loss: 0.9362 - acc: 0.723 - ETA: 2s - loss: 0.9402 - acc: 0.722 - ETA: 2s - loss: 0.9459 - acc: 0.721 - ETA: 2s - loss: 0.9474 - acc: 0.720 - ETA: 2s - loss: 0.9459 - acc: 0.721 - ETA: 2s - loss: 0.9446 - acc: 0.721 - ETA: 2s - loss: 0.9434 - acc: 0.721 - ETA: 2s - loss: 0.9411 - acc: 0.722 - ETA: 2s - loss: 0.9398 - acc: 0.722 - ETA: 2s - loss: 0.9391 - acc: 0.723 - ETA: 2s - loss: 0.9411 - acc: 0.722 - ETA: 2s - loss: 0.9402 - acc: 0.722 - ETA: 2s - loss: 0.9381 - acc: 0.723 - ETA: 2s - loss: 0.9379 - acc: 0.723 - ETA: 2s - loss: 0.9393 - acc: 0.722 - ETA: 2s - loss: 0.9429 - acc: 0.721 - ETA: 2s - loss: 0.9410 - acc: 0.722 - ETA: 2s - loss: 0.9425 - acc: 0.721 - ETA: 1s - loss: 0.9421 - acc: 0.722 - ETA: 1s - loss: 0.9411 - acc: 0.722 - ETA: 1s - loss: 0.9392 - acc: 0.722 - ETA: 1s - loss: 0.9387 - acc: 0.722 - ETA: 1s - loss: 0.9354 - acc: 0.724 - ETA: 1s - loss: 0.9368 - acc: 0.723 - ETA: 1s - loss: 0.9369 - acc: 0.723 - ETA: 1s - loss: 0.9360 - acc: 0.723 - ETA: 1s - loss: 0.9355 - acc: 0.724 - ETA: 1s - loss: 0.9327 - acc: 0.725 - ETA: 1s - loss: 0.9345 - acc: 0.724 - ETA: 1s - loss: 0.9324 - acc: 0.725 - ETA: 1s - loss: 0.9332 - acc: 0.726 - ETA: 1s - loss: 0.9321 - acc: 0.726 - ETA: 1s - loss: 0.9335 - acc: 0.725 - ETA: 1s - loss: 0.9307 - acc: 0.726 - ETA: 0s - loss: 0.9303 - acc: 0.726 - ETA: 0s - loss: 0.9307 - acc: 0.726 - ETA: 0s - loss: 0.9334 - acc: 0.725 - ETA: 0s - loss: 0.9333 - acc: 0.725 - ETA: 0s - loss: 0.9309 - acc: 0.725 - ETA: 0s - loss: 0.9297 - acc: 0.725 - ETA: 0s - loss: 0.9276 - acc: 0.726 - ETA: 0s - loss: 0.9293 - acc: 0.725 - ETA: 0s - loss: 0.9294 - acc: 0.725 - ETA: 0s - loss: 0.9277 - acc: 0.726 - ETA: 0s - loss: 0.9263 - acc: 0.726 - ETA: 0s - loss: 0.9254 - acc: 0.726 - ETA: 0s - loss: 0.9236 - acc: 0.727 - ETA: 0s - loss: 0.9231 - acc: 0.727 - ETA: 0s - loss: 0.9222 - acc: 0.7276Epoch 00004: val_loss improved from 0.56725 to 0.53763, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 0.9221 - acc: 0.7278 - val_loss: 0.5376 - val_acc: 0.8287\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 9s - loss: 1.3252 - acc: 0.600 - ETA: 10s - loss: 1.0092 - acc: 0.70 - ETA: 10s - loss: 0.9460 - acc: 0.72 - ETA: 9s - loss: 0.9296 - acc: 0.7250 - ETA: 8s - loss: 0.9408 - acc: 0.720 - ETA: 8s - loss: 0.9278 - acc: 0.716 - ETA: 8s - loss: 0.9284 - acc: 0.716 - ETA: 8s - loss: 0.8996 - acc: 0.720 - ETA: 8s - loss: 0.8586 - acc: 0.735 - ETA: 8s - loss: 0.8738 - acc: 0.736 - ETA: 7s - loss: 0.8963 - acc: 0.735 - ETA: 7s - loss: 0.9232 - acc: 0.731 - ETA: 7s - loss: 0.9594 - acc: 0.718 - ETA: 7s - loss: 0.9547 - acc: 0.719 - ETA: 7s - loss: 0.9585 - acc: 0.711 - ETA: 7s - loss: 0.9547 - acc: 0.714 - ETA: 7s - loss: 0.9468 - acc: 0.715 - ETA: 7s - loss: 0.9566 - acc: 0.713 - ETA: 6s - loss: 0.9756 - acc: 0.709 - ETA: 6s - loss: 0.9435 - acc: 0.719 - ETA: 6s - loss: 0.9155 - acc: 0.727 - ETA: 6s - loss: 0.9066 - acc: 0.728 - ETA: 6s - loss: 0.9260 - acc: 0.722 - ETA: 6s - loss: 0.9341 - acc: 0.722 - ETA: 6s - loss: 0.9333 - acc: 0.721 - ETA: 6s - loss: 0.9248 - acc: 0.720 - ETA: 6s - loss: 0.9148 - acc: 0.723 - ETA: 6s - loss: 0.9168 - acc: 0.722 - ETA: 6s - loss: 0.9266 - acc: 0.721 - ETA: 6s - loss: 0.9336 - acc: 0.719 - ETA: 6s - loss: 0.9374 - acc: 0.719 - ETA: 5s - loss: 0.9284 - acc: 0.721 - ETA: 5s - loss: 0.9311 - acc: 0.722 - ETA: 5s - loss: 0.9256 - acc: 0.723 - ETA: 5s - loss: 0.9187 - acc: 0.726 - ETA: 5s - loss: 0.9106 - acc: 0.726 - ETA: 5s - loss: 0.9122 - acc: 0.726 - ETA: 5s - loss: 0.9068 - acc: 0.727 - ETA: 5s - loss: 0.9115 - acc: 0.727 - ETA: 5s - loss: 0.9118 - acc: 0.727 - ETA: 5s - loss: 0.9093 - acc: 0.728 - ETA: 5s - loss: 0.9043 - acc: 0.730 - ETA: 5s - loss: 0.9026 - acc: 0.730 - ETA: 5s - loss: 0.8946 - acc: 0.733 - ETA: 5s - loss: 0.8882 - acc: 0.735 - ETA: 5s - loss: 0.8942 - acc: 0.733 - ETA: 5s - loss: 0.8863 - acc: 0.735 - ETA: 5s - loss: 0.8868 - acc: 0.736 - ETA: 5s - loss: 0.8943 - acc: 0.735 - ETA: 4s - loss: 0.8935 - acc: 0.734 - ETA: 4s - loss: 0.8897 - acc: 0.735 - ETA: 4s - loss: 0.8897 - acc: 0.735 - ETA: 4s - loss: 0.8927 - acc: 0.734 - ETA: 4s - loss: 0.8904 - acc: 0.736 - ETA: 4s - loss: 0.8868 - acc: 0.737 - ETA: 4s - loss: 0.8825 - acc: 0.738 - ETA: 4s - loss: 0.8789 - acc: 0.739 - ETA: 4s - loss: 0.8738 - acc: 0.741 - ETA: 4s - loss: 0.8712 - acc: 0.741 - ETA: 4s - loss: 0.8720 - acc: 0.741 - ETA: 4s - loss: 0.8727 - acc: 0.741 - ETA: 4s - loss: 0.8791 - acc: 0.740 - ETA: 4s - loss: 0.8806 - acc: 0.739 - ETA: 4s - loss: 0.8756 - acc: 0.740 - ETA: 4s - loss: 0.8809 - acc: 0.739 - ETA: 4s - loss: 0.8814 - acc: 0.739 - ETA: 4s - loss: 0.8772 - acc: 0.740 - ETA: 4s - loss: 0.8828 - acc: 0.739 - ETA: 3s - loss: 0.8839 - acc: 0.740 - ETA: 3s - loss: 0.8881 - acc: 0.739 - ETA: 3s - loss: 0.8869 - acc: 0.738 - ETA: 3s - loss: 0.8846 - acc: 0.739 - ETA: 3s - loss: 0.8821 - acc: 0.741 - ETA: 3s - loss: 0.8832 - acc: 0.740 - ETA: 3s - loss: 0.8853 - acc: 0.740 - ETA: 3s - loss: 0.8837 - acc: 0.740 - ETA: 3s - loss: 0.8809 - acc: 0.740 - ETA: 3s - loss: 0.8761 - acc: 0.742 - ETA: 3s - loss: 0.8741 - acc: 0.742 - ETA: 3s - loss: 0.8701 - acc: 0.744 - ETA: 3s - loss: 0.8691 - acc: 0.744 - ETA: 3s - loss: 0.8682 - acc: 0.743 - ETA: 3s - loss: 0.8663 - acc: 0.744 - ETA: 3s - loss: 0.8659 - acc: 0.745 - ETA: 2s - loss: 0.8635 - acc: 0.745 - ETA: 2s - loss: 0.8577 - acc: 0.747 - ETA: 2s - loss: 0.8578 - acc: 0.747 - ETA: 2s - loss: 0.8592 - acc: 0.747 - ETA: 2s - loss: 0.8564 - acc: 0.747 - ETA: 2s - loss: 0.8543 - acc: 0.747 - ETA: 2s - loss: 0.8550 - acc: 0.747 - ETA: 2s - loss: 0.8519 - acc: 0.748 - ETA: 2s - loss: 0.8513 - acc: 0.748 - ETA: 2s - loss: 0.8522 - acc: 0.748 - ETA: 2s - loss: 0.8516 - acc: 0.748 - ETA: 2s - loss: 0.8472 - acc: 0.749 - ETA: 2s - loss: 0.8520 - acc: 0.748 - ETA: 2s - loss: 0.8527 - acc: 0.749 - ETA: 2s - loss: 0.8503 - acc: 0.750 - ETA: 2s - loss: 0.8542 - acc: 0.749 - ETA: 2s - loss: 0.8524 - acc: 0.749 - ETA: 1s - loss: 0.8485 - acc: 0.750 - ETA: 1s - loss: 0.8469 - acc: 0.751 - ETA: 1s - loss: 0.8477 - acc: 0.751 - ETA: 1s - loss: 0.8445 - acc: 0.751 - ETA: 1s - loss: 0.8419 - acc: 0.751 - ETA: 1s - loss: 0.8398 - acc: 0.752 - ETA: 1s - loss: 0.8408 - acc: 0.751 - ETA: 1s - loss: 0.8401 - acc: 0.751 - ETA: 1s - loss: 0.8422 - acc: 0.751 - ETA: 1s - loss: 0.8399 - acc: 0.751 - ETA: 1s - loss: 0.8390 - acc: 0.751 - ETA: 1s - loss: 0.8396 - acc: 0.750 - ETA: 1s - loss: 0.8395 - acc: 0.750 - ETA: 1s - loss: 0.8404 - acc: 0.750 - ETA: 1s - loss: 0.8390 - acc: 0.750 - ETA: 1s - loss: 0.8388 - acc: 0.750 - ETA: 0s - loss: 0.8413 - acc: 0.750 - ETA: 0s - loss: 0.8409 - acc: 0.750 - ETA: 0s - loss: 0.8406 - acc: 0.750 - ETA: 0s - loss: 0.8424 - acc: 0.749 - ETA: 0s - loss: 0.8420 - acc: 0.749 - ETA: 0s - loss: 0.8437 - acc: 0.749 - ETA: 0s - loss: 0.8427 - acc: 0.749 - ETA: 0s - loss: 0.8419 - acc: 0.750 - ETA: 0s - loss: 0.8421 - acc: 0.750 - ETA: 0s - loss: 0.8429 - acc: 0.750 - ETA: 0s - loss: 0.8417 - acc: 0.750 - ETA: 0s - loss: 0.8393 - acc: 0.750 - ETA: 0s - loss: 0.8385 - acc: 0.751 - ETA: 0s - loss: 0.8362 - acc: 0.752 - ETA: 0s - loss: 0.8342 - acc: 0.752 - ETA: 0s - loss: 0.8322 - acc: 0.753 - ETA: 0s - loss: 0.8303 - acc: 0.7536Epoch 00005: val_loss improved from 0.53763 to 0.48289, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.8299 - acc: 0.7537 - val_loss: 0.4829 - val_acc: 0.8467\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 8s - loss: 0.3655 - acc: 0.900 - ETA: 8s - loss: 0.5034 - acc: 0.816 - ETA: 7s - loss: 0.6415 - acc: 0.800 - ETA: 7s - loss: 0.6700 - acc: 0.800 - ETA: 7s - loss: 0.6540 - acc: 0.809 - ETA: 7s - loss: 0.6772 - acc: 0.815 - ETA: 7s - loss: 0.6859 - acc: 0.800 - ETA: 7s - loss: 0.7090 - acc: 0.791 - ETA: 7s - loss: 0.7193 - acc: 0.792 - ETA: 7s - loss: 0.7170 - acc: 0.785 - ETA: 7s - loss: 0.7393 - acc: 0.778 - ETA: 7s - loss: 0.7207 - acc: 0.788 - ETA: 7s - loss: 0.7276 - acc: 0.783 - ETA: 7s - loss: 0.7256 - acc: 0.782 - ETA: 7s - loss: 0.7215 - acc: 0.783 - ETA: 7s - loss: 0.7303 - acc: 0.780 - ETA: 7s - loss: 0.7183 - acc: 0.781 - ETA: 7s - loss: 0.7266 - acc: 0.779 - ETA: 7s - loss: 0.7197 - acc: 0.779 - ETA: 7s - loss: 0.7111 - acc: 0.778 - ETA: 6s - loss: 0.7045 - acc: 0.779 - ETA: 6s - loss: 0.7132 - acc: 0.778 - ETA: 6s - loss: 0.7308 - acc: 0.774 - ETA: 6s - loss: 0.7242 - acc: 0.777 - ETA: 6s - loss: 0.7260 - acc: 0.775 - ETA: 6s - loss: 0.7249 - acc: 0.775 - ETA: 6s - loss: 0.7262 - acc: 0.775 - ETA: 6s - loss: 0.7316 - acc: 0.773 - ETA: 6s - loss: 0.7303 - acc: 0.773 - ETA: 6s - loss: 0.7280 - acc: 0.774 - ETA: 6s - loss: 0.7262 - acc: 0.775 - ETA: 6s - loss: 0.7249 - acc: 0.773 - ETA: 6s - loss: 0.7340 - acc: 0.771 - ETA: 6s - loss: 0.7453 - acc: 0.769 - ETA: 6s - loss: 0.7475 - acc: 0.769 - ETA: 6s - loss: 0.7400 - acc: 0.772 - ETA: 6s - loss: 0.7457 - acc: 0.769 - ETA: 5s - loss: 0.7574 - acc: 0.767 - ETA: 5s - loss: 0.7646 - acc: 0.766 - ETA: 5s - loss: 0.7698 - acc: 0.765 - ETA: 5s - loss: 0.7730 - acc: 0.764 - ETA: 5s - loss: 0.7757 - acc: 0.763 - ETA: 5s - loss: 0.7759 - acc: 0.763 - ETA: 5s - loss: 0.7863 - acc: 0.761 - ETA: 5s - loss: 0.7827 - acc: 0.761 - ETA: 5s - loss: 0.7779 - acc: 0.761 - ETA: 5s - loss: 0.7730 - acc: 0.763 - ETA: 5s - loss: 0.7733 - acc: 0.763 - ETA: 5s - loss: 0.7717 - acc: 0.764 - ETA: 5s - loss: 0.7661 - acc: 0.765 - ETA: 5s - loss: 0.7666 - acc: 0.762 - ETA: 5s - loss: 0.7668 - acc: 0.762 - ETA: 5s - loss: 0.7604 - acc: 0.765 - ETA: 5s - loss: 0.7596 - acc: 0.765 - ETA: 4s - loss: 0.7581 - acc: 0.766 - ETA: 4s - loss: 0.7605 - acc: 0.766 - ETA: 4s - loss: 0.7546 - acc: 0.768 - ETA: 4s - loss: 0.7555 - acc: 0.768 - ETA: 4s - loss: 0.7540 - acc: 0.769 - ETA: 4s - loss: 0.7577 - acc: 0.768 - ETA: 4s - loss: 0.7559 - acc: 0.769 - ETA: 4s - loss: 0.7559 - acc: 0.768 - ETA: 4s - loss: 0.7534 - acc: 0.769 - ETA: 4s - loss: 0.7551 - acc: 0.769 - ETA: 4s - loss: 0.7600 - acc: 0.768 - ETA: 4s - loss: 0.7597 - acc: 0.768 - ETA: 4s - loss: 0.7553 - acc: 0.769 - ETA: 4s - loss: 0.7506 - acc: 0.771 - ETA: 4s - loss: 0.7501 - acc: 0.771 - ETA: 4s - loss: 0.7471 - acc: 0.772 - ETA: 4s - loss: 0.7473 - acc: 0.772 - ETA: 3s - loss: 0.7448 - acc: 0.773 - ETA: 3s - loss: 0.7485 - acc: 0.773 - ETA: 3s - loss: 0.7464 - acc: 0.774 - ETA: 3s - loss: 0.7469 - acc: 0.773 - ETA: 3s - loss: 0.7460 - acc: 0.774 - ETA: 3s - loss: 0.7437 - acc: 0.775 - ETA: 3s - loss: 0.7435 - acc: 0.774 - ETA: 3s - loss: 0.7430 - acc: 0.774 - ETA: 3s - loss: 0.7433 - acc: 0.774 - ETA: 3s - loss: 0.7452 - acc: 0.774 - ETA: 3s - loss: 0.7444 - acc: 0.773 - ETA: 3s - loss: 0.7446 - acc: 0.773 - ETA: 3s - loss: 0.7428 - acc: 0.773 - ETA: 3s - loss: 0.7488 - acc: 0.771 - ETA: 3s - loss: 0.7466 - acc: 0.772 - ETA: 3s - loss: 0.7474 - acc: 0.772 - ETA: 3s - loss: 0.7518 - acc: 0.771 - ETA: 3s - loss: 0.7542 - acc: 0.770 - ETA: 2s - loss: 0.7547 - acc: 0.770 - ETA: 2s - loss: 0.7539 - acc: 0.770 - ETA: 2s - loss: 0.7517 - acc: 0.770 - ETA: 2s - loss: 0.7514 - acc: 0.770 - ETA: 2s - loss: 0.7541 - acc: 0.769 - ETA: 2s - loss: 0.7524 - acc: 0.769 - ETA: 2s - loss: 0.7530 - acc: 0.770 - ETA: 2s - loss: 0.7509 - acc: 0.770 - ETA: 2s - loss: 0.7511 - acc: 0.770 - ETA: 2s - loss: 0.7498 - acc: 0.771 - ETA: 2s - loss: 0.7484 - acc: 0.771 - ETA: 2s - loss: 0.7479 - acc: 0.771 - ETA: 2s - loss: 0.7477 - acc: 0.771 - ETA: 2s - loss: 0.7440 - acc: 0.772 - ETA: 2s - loss: 0.7466 - acc: 0.771 - ETA: 2s - loss: 0.7504 - acc: 0.770 - ETA: 2s - loss: 0.7518 - acc: 0.770 - ETA: 2s - loss: 0.7524 - acc: 0.770 - ETA: 1s - loss: 0.7516 - acc: 0.771 - ETA: 1s - loss: 0.7521 - acc: 0.771 - ETA: 1s - loss: 0.7550 - acc: 0.771 - ETA: 1s - loss: 0.7566 - acc: 0.771 - ETA: 1s - loss: 0.7534 - acc: 0.772 - ETA: 1s - loss: 0.7539 - acc: 0.771 - ETA: 1s - loss: 0.7574 - acc: 0.771 - ETA: 1s - loss: 0.7529 - acc: 0.773 - ETA: 1s - loss: 0.7520 - acc: 0.773 - ETA: 1s - loss: 0.7534 - acc: 0.772 - ETA: 1s - loss: 0.7549 - acc: 0.772 - ETA: 1s - loss: 0.7582 - acc: 0.772 - ETA: 1s - loss: 0.7602 - acc: 0.772 - ETA: 1s - loss: 0.7583 - acc: 0.772 - ETA: 1s - loss: 0.7582 - acc: 0.772 - ETA: 1s - loss: 0.7589 - acc: 0.772 - ETA: 1s - loss: 0.7607 - acc: 0.771 - ETA: 0s - loss: 0.7603 - acc: 0.772 - ETA: 0s - loss: 0.7612 - acc: 0.771 - ETA: 0s - loss: 0.7606 - acc: 0.772 - ETA: 0s - loss: 0.7589 - acc: 0.772 - ETA: 0s - loss: 0.7578 - acc: 0.772 - ETA: 0s - loss: 0.7574 - acc: 0.773 - ETA: 0s - loss: 0.7559 - acc: 0.773 - ETA: 0s - loss: 0.7569 - acc: 0.773 - ETA: 0s - loss: 0.7549 - acc: 0.774 - ETA: 0s - loss: 0.7551 - acc: 0.774 - ETA: 0s - loss: 0.7574 - acc: 0.773 - ETA: 0s - loss: 0.7574 - acc: 0.773 - ETA: 0s - loss: 0.7570 - acc: 0.773 - ETA: 0s - loss: 0.7574 - acc: 0.773 - ETA: 0s - loss: 0.7559 - acc: 0.7739Epoch 00006: val_loss improved from 0.48289 to 0.46947, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.7562 - acc: 0.7737 - val_loss: 0.4695 - val_acc: 0.8539\n",
      "Epoch 8/20\n",
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.6932 - acc: 0.750 - ETA: 8s - loss: 0.6610 - acc: 0.816 - ETA: 8s - loss: 0.7097 - acc: 0.780 - ETA: 7s - loss: 0.7676 - acc: 0.762 - ETA: 7s - loss: 0.7252 - acc: 0.777 - ETA: 7s - loss: 0.7303 - acc: 0.776 - ETA: 7s - loss: 0.7305 - acc: 0.786 - ETA: 7s - loss: 0.7654 - acc: 0.779 - ETA: 7s - loss: 0.7796 - acc: 0.775 - ETA: 7s - loss: 0.7627 - acc: 0.787 - ETA: 7s - loss: 0.7436 - acc: 0.788 - ETA: 7s - loss: 0.7430 - acc: 0.791 - ETA: 7s - loss: 0.7430 - acc: 0.790 - ETA: 7s - loss: 0.7332 - acc: 0.790 - ETA: 7s - loss: 0.7190 - acc: 0.795 - ETA: 6s - loss: 0.7054 - acc: 0.797 - ETA: 6s - loss: 0.7051 - acc: 0.797 - ETA: 6s - loss: 0.6984 - acc: 0.800 - ETA: 6s - loss: 0.6950 - acc: 0.802 - ETA: 6s - loss: 0.6898 - acc: 0.803 - ETA: 6s - loss: 0.7010 - acc: 0.800 - ETA: 6s - loss: 0.6971 - acc: 0.800 - ETA: 6s - loss: 0.7056 - acc: 0.800 - ETA: 6s - loss: 0.7183 - acc: 0.793 - ETA: 6s - loss: 0.7084 - acc: 0.796 - ETA: 6s - loss: 0.7027 - acc: 0.798 - ETA: 6s - loss: 0.6972 - acc: 0.800 - ETA: 6s - loss: 0.6971 - acc: 0.796 - ETA: 6s - loss: 0.6974 - acc: 0.797 - ETA: 6s - loss: 0.6958 - acc: 0.797 - ETA: 6s - loss: 0.6898 - acc: 0.798 - ETA: 6s - loss: 0.6918 - acc: 0.796 - ETA: 6s - loss: 0.6902 - acc: 0.798 - ETA: 6s - loss: 0.6925 - acc: 0.796 - ETA: 5s - loss: 0.6933 - acc: 0.795 - ETA: 5s - loss: 0.6946 - acc: 0.794 - ETA: 5s - loss: 0.6873 - acc: 0.797 - ETA: 5s - loss: 0.6912 - acc: 0.794 - ETA: 5s - loss: 0.6921 - acc: 0.794 - ETA: 5s - loss: 0.6871 - acc: 0.794 - ETA: 5s - loss: 0.6878 - acc: 0.794 - ETA: 5s - loss: 0.6965 - acc: 0.793 - ETA: 5s - loss: 0.6955 - acc: 0.793 - ETA: 5s - loss: 0.6945 - acc: 0.793 - ETA: 5s - loss: 0.6952 - acc: 0.792 - ETA: 5s - loss: 0.6973 - acc: 0.792 - ETA: 5s - loss: 0.7036 - acc: 0.790 - ETA: 5s - loss: 0.7011 - acc: 0.791 - ETA: 4s - loss: 0.6990 - acc: 0.791 - ETA: 4s - loss: 0.7037 - acc: 0.789 - ETA: 4s - loss: 0.7067 - acc: 0.788 - ETA: 4s - loss: 0.7057 - acc: 0.788 - ETA: 4s - loss: 0.7099 - acc: 0.787 - ETA: 4s - loss: 0.7074 - acc: 0.789 - ETA: 4s - loss: 0.7075 - acc: 0.789 - ETA: 4s - loss: 0.7154 - acc: 0.785 - ETA: 4s - loss: 0.7166 - acc: 0.785 - ETA: 4s - loss: 0.7162 - acc: 0.786 - ETA: 4s - loss: 0.7138 - acc: 0.787 - ETA: 4s - loss: 0.7162 - acc: 0.785 - ETA: 4s - loss: 0.7224 - acc: 0.784 - ETA: 4s - loss: 0.7259 - acc: 0.784 - ETA: 4s - loss: 0.7274 - acc: 0.784 - ETA: 4s - loss: 0.7229 - acc: 0.786 - ETA: 4s - loss: 0.7209 - acc: 0.786 - ETA: 4s - loss: 0.7250 - acc: 0.785 - ETA: 3s - loss: 0.7265 - acc: 0.785 - ETA: 3s - loss: 0.7231 - acc: 0.786 - ETA: 3s - loss: 0.7226 - acc: 0.785 - ETA: 3s - loss: 0.7236 - acc: 0.784 - ETA: 3s - loss: 0.7253 - acc: 0.784 - ETA: 3s - loss: 0.7248 - acc: 0.784 - ETA: 3s - loss: 0.7239 - acc: 0.784 - ETA: 3s - loss: 0.7225 - acc: 0.784 - ETA: 3s - loss: 0.7199 - acc: 0.784 - ETA: 3s - loss: 0.7200 - acc: 0.784 - ETA: 3s - loss: 0.7195 - acc: 0.784 - ETA: 3s - loss: 0.7200 - acc: 0.784 - ETA: 3s - loss: 0.7175 - acc: 0.784 - ETA: 3s - loss: 0.7194 - acc: 0.784 - ETA: 3s - loss: 0.7166 - acc: 0.785 - ETA: 3s - loss: 0.7183 - acc: 0.784 - ETA: 3s - loss: 0.7181 - acc: 0.784 - ETA: 2s - loss: 0.7196 - acc: 0.784 - ETA: 2s - loss: 0.7148 - acc: 0.786 - ETA: 2s - loss: 0.7133 - acc: 0.787 - ETA: 2s - loss: 0.7170 - acc: 0.785 - ETA: 2s - loss: 0.7175 - acc: 0.785 - ETA: 2s - loss: 0.7149 - acc: 0.785 - ETA: 2s - loss: 0.7110 - acc: 0.786 - ETA: 2s - loss: 0.7102 - acc: 0.787 - ETA: 2s - loss: 0.7085 - acc: 0.788 - ETA: 2s - loss: 0.7053 - acc: 0.789 - ETA: 2s - loss: 0.7069 - acc: 0.788 - ETA: 2s - loss: 0.7093 - acc: 0.788 - ETA: 2s - loss: 0.7086 - acc: 0.788 - ETA: 2s - loss: 0.7099 - acc: 0.787 - ETA: 2s - loss: 0.7099 - acc: 0.787 - ETA: 2s - loss: 0.7105 - acc: 0.786 - ETA: 1s - loss: 0.7072 - acc: 0.787 - ETA: 1s - loss: 0.7077 - acc: 0.787 - ETA: 1s - loss: 0.7064 - acc: 0.787 - ETA: 1s - loss: 0.7057 - acc: 0.787 - ETA: 1s - loss: 0.7078 - acc: 0.787 - ETA: 1s - loss: 0.7054 - acc: 0.788 - ETA: 1s - loss: 0.7029 - acc: 0.788 - ETA: 1s - loss: 0.7037 - acc: 0.787 - ETA: 1s - loss: 0.7028 - acc: 0.787 - ETA: 1s - loss: 0.7024 - acc: 0.787 - ETA: 1s - loss: 0.7023 - acc: 0.787 - ETA: 1s - loss: 0.7022 - acc: 0.787 - ETA: 1s - loss: 0.7031 - acc: 0.786 - ETA: 1s - loss: 0.7025 - acc: 0.786 - ETA: 1s - loss: 0.7015 - acc: 0.786 - ETA: 1s - loss: 0.6996 - acc: 0.787 - ETA: 1s - loss: 0.6988 - acc: 0.787 - ETA: 0s - loss: 0.7003 - acc: 0.787 - ETA: 0s - loss: 0.7006 - acc: 0.787 - ETA: 0s - loss: 0.7006 - acc: 0.787 - ETA: 0s - loss: 0.7013 - acc: 0.786 - ETA: 0s - loss: 0.7013 - acc: 0.786 - ETA: 0s - loss: 0.6996 - acc: 0.787 - ETA: 0s - loss: 0.6995 - acc: 0.787 - ETA: 0s - loss: 0.6980 - acc: 0.787 - ETA: 0s - loss: 0.6978 - acc: 0.787 - ETA: 0s - loss: 0.6952 - acc: 0.788 - ETA: 0s - loss: 0.6934 - acc: 0.789 - ETA: 0s - loss: 0.6908 - acc: 0.789 - ETA: 0s - loss: 0.6913 - acc: 0.789 - ETA: 0s - loss: 0.6909 - acc: 0.789 - ETA: 0s - loss: 0.6906 - acc: 0.789 - ETA: 0s - loss: 0.6932 - acc: 0.789 - ETA: 0s - loss: 0.6939 - acc: 0.789 - ETA: 0s - loss: 0.6923 - acc: 0.7898Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 0.6919 - acc: 0.7898 - val_loss: 0.4816 - val_acc: 0.8527\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.4271 - acc: 0.800 - ETA: 7s - loss: 0.7847 - acc: 0.737 - ETA: 7s - loss: 0.6601 - acc: 0.785 - ETA: 7s - loss: 0.6397 - acc: 0.788 - ETA: 7s - loss: 0.6174 - acc: 0.816 - ETA: 7s - loss: 0.6362 - acc: 0.813 - ETA: 7s - loss: 0.6234 - acc: 0.811 - ETA: 7s - loss: 0.5895 - acc: 0.823 - ETA: 7s - loss: 0.6130 - acc: 0.815 - ETA: 7s - loss: 0.6502 - acc: 0.806 - ETA: 7s - loss: 0.6351 - acc: 0.811 - ETA: 7s - loss: 0.6317 - acc: 0.814 - ETA: 7s - loss: 0.6376 - acc: 0.813 - ETA: 7s - loss: 0.6514 - acc: 0.807 - ETA: 7s - loss: 0.6483 - acc: 0.808 - ETA: 7s - loss: 0.6452 - acc: 0.808 - ETA: 7s - loss: 0.6404 - acc: 0.807 - ETA: 7s - loss: 0.6364 - acc: 0.807 - ETA: 7s - loss: 0.6299 - acc: 0.809 - ETA: 7s - loss: 0.6255 - acc: 0.812 - ETA: 7s - loss: 0.6190 - acc: 0.815 - ETA: 7s - loss: 0.6149 - acc: 0.816 - ETA: 6s - loss: 0.5992 - acc: 0.819 - ETA: 6s - loss: 0.5980 - acc: 0.817 - ETA: 6s - loss: 0.5951 - acc: 0.820 - ETA: 6s - loss: 0.6046 - acc: 0.816 - ETA: 6s - loss: 0.5987 - acc: 0.817 - ETA: 6s - loss: 0.5980 - acc: 0.818 - ETA: 6s - loss: 0.5953 - acc: 0.820 - ETA: 6s - loss: 0.6041 - acc: 0.819 - ETA: 6s - loss: 0.6023 - acc: 0.819 - ETA: 6s - loss: 0.6051 - acc: 0.818 - ETA: 6s - loss: 0.6062 - acc: 0.818 - ETA: 6s - loss: 0.6071 - acc: 0.818 - ETA: 6s - loss: 0.6098 - acc: 0.816 - ETA: 6s - loss: 0.6177 - acc: 0.813 - ETA: 6s - loss: 0.6154 - acc: 0.813 - ETA: 6s - loss: 0.6194 - acc: 0.813 - ETA: 5s - loss: 0.6214 - acc: 0.813 - ETA: 5s - loss: 0.6198 - acc: 0.813 - ETA: 5s - loss: 0.6218 - acc: 0.811 - ETA: 5s - loss: 0.6245 - acc: 0.809 - ETA: 5s - loss: 0.6290 - acc: 0.808 - ETA: 5s - loss: 0.6271 - acc: 0.809 - ETA: 5s - loss: 0.6309 - acc: 0.807 - ETA: 5s - loss: 0.6354 - acc: 0.806 - ETA: 5s - loss: 0.6419 - acc: 0.806 - ETA: 5s - loss: 0.6364 - acc: 0.807 - ETA: 5s - loss: 0.6358 - acc: 0.808 - ETA: 5s - loss: 0.6334 - acc: 0.808 - ETA: 5s - loss: 0.6266 - acc: 0.810 - ETA: 5s - loss: 0.6268 - acc: 0.810 - ETA: 5s - loss: 0.6251 - acc: 0.810 - ETA: 5s - loss: 0.6258 - acc: 0.810 - ETA: 5s - loss: 0.6256 - acc: 0.809 - ETA: 4s - loss: 0.6226 - acc: 0.810 - ETA: 4s - loss: 0.6210 - acc: 0.811 - ETA: 4s - loss: 0.6165 - acc: 0.812 - ETA: 4s - loss: 0.6169 - acc: 0.813 - ETA: 4s - loss: 0.6212 - acc: 0.812 - ETA: 4s - loss: 0.6261 - acc: 0.811 - ETA: 4s - loss: 0.6271 - acc: 0.812 - ETA: 4s - loss: 0.6280 - acc: 0.811 - ETA: 4s - loss: 0.6314 - acc: 0.810 - ETA: 4s - loss: 0.6322 - acc: 0.810 - ETA: 4s - loss: 0.6292 - acc: 0.811 - ETA: 4s - loss: 0.6295 - acc: 0.811 - ETA: 4s - loss: 0.6298 - acc: 0.811 - ETA: 4s - loss: 0.6305 - acc: 0.811 - ETA: 4s - loss: 0.6311 - acc: 0.810 - ETA: 4s - loss: 0.6300 - acc: 0.809 - ETA: 4s - loss: 0.6338 - acc: 0.809 - ETA: 4s - loss: 0.6300 - acc: 0.810 - ETA: 4s - loss: 0.6334 - acc: 0.810 - ETA: 3s - loss: 0.6347 - acc: 0.810 - ETA: 3s - loss: 0.6320 - acc: 0.810 - ETA: 3s - loss: 0.6297 - acc: 0.810 - ETA: 3s - loss: 0.6301 - acc: 0.810 - ETA: 3s - loss: 0.6309 - acc: 0.809 - ETA: 3s - loss: 0.6324 - acc: 0.810 - ETA: 3s - loss: 0.6315 - acc: 0.810 - ETA: 3s - loss: 0.6322 - acc: 0.809 - ETA: 3s - loss: 0.6325 - acc: 0.809 - ETA: 3s - loss: 0.6331 - acc: 0.809 - ETA: 3s - loss: 0.6352 - acc: 0.808 - ETA: 3s - loss: 0.6345 - acc: 0.809 - ETA: 3s - loss: 0.6355 - acc: 0.809 - ETA: 3s - loss: 0.6339 - acc: 0.810 - ETA: 3s - loss: 0.6363 - acc: 0.810 - ETA: 3s - loss: 0.6354 - acc: 0.810 - ETA: 3s - loss: 0.6353 - acc: 0.810 - ETA: 3s - loss: 0.6360 - acc: 0.809 - ETA: 3s - loss: 0.6378 - acc: 0.809 - ETA: 2s - loss: 0.6363 - acc: 0.809 - ETA: 2s - loss: 0.6364 - acc: 0.809 - ETA: 2s - loss: 0.6381 - acc: 0.808 - ETA: 2s - loss: 0.6390 - acc: 0.807 - ETA: 2s - loss: 0.6398 - acc: 0.807 - ETA: 2s - loss: 0.6377 - acc: 0.807 - ETA: 2s - loss: 0.6352 - acc: 0.808 - ETA: 2s - loss: 0.6369 - acc: 0.808 - ETA: 2s - loss: 0.6361 - acc: 0.808 - ETA: 2s - loss: 0.6375 - acc: 0.808 - ETA: 2s - loss: 0.6353 - acc: 0.808 - ETA: 2s - loss: 0.6353 - acc: 0.808 - ETA: 2s - loss: 0.6358 - acc: 0.808 - ETA: 2s - loss: 0.6328 - acc: 0.808 - ETA: 2s - loss: 0.6284 - acc: 0.810 - ETA: 2s - loss: 0.6284 - acc: 0.810 - ETA: 2s - loss: 0.6295 - acc: 0.810 - ETA: 2s - loss: 0.6278 - acc: 0.811 - ETA: 1s - loss: 0.6264 - acc: 0.812 - ETA: 1s - loss: 0.6255 - acc: 0.812 - ETA: 1s - loss: 0.6290 - acc: 0.811 - ETA: 1s - loss: 0.6307 - acc: 0.811 - ETA: 1s - loss: 0.6301 - acc: 0.811 - ETA: 1s - loss: 0.6272 - acc: 0.812 - ETA: 1s - loss: 0.6267 - acc: 0.812 - ETA: 1s - loss: 0.6268 - acc: 0.812 - ETA: 1s - loss: 0.6269 - acc: 0.812 - ETA: 1s - loss: 0.6255 - acc: 0.812 - ETA: 1s - loss: 0.6253 - acc: 0.812 - ETA: 1s - loss: 0.6244 - acc: 0.813 - ETA: 1s - loss: 0.6237 - acc: 0.813 - ETA: 1s - loss: 0.6242 - acc: 0.812 - ETA: 1s - loss: 0.6239 - acc: 0.812 - ETA: 1s - loss: 0.6222 - acc: 0.813 - ETA: 1s - loss: 0.6252 - acc: 0.811 - ETA: 0s - loss: 0.6235 - acc: 0.812 - ETA: 0s - loss: 0.6228 - acc: 0.812 - ETA: 0s - loss: 0.6211 - acc: 0.812 - ETA: 0s - loss: 0.6224 - acc: 0.812 - ETA: 0s - loss: 0.6217 - acc: 0.812 - ETA: 0s - loss: 0.6234 - acc: 0.811 - ETA: 0s - loss: 0.6235 - acc: 0.811 - ETA: 0s - loss: 0.6254 - acc: 0.811 - ETA: 0s - loss: 0.6257 - acc: 0.811 - ETA: 0s - loss: 0.6258 - acc: 0.811 - ETA: 0s - loss: 0.6236 - acc: 0.811 - ETA: 0s - loss: 0.6238 - acc: 0.810 - ETA: 0s - loss: 0.6231 - acc: 0.811 - ETA: 0s - loss: 0.6226 - acc: 0.810 - ETA: 0s - loss: 0.6241 - acc: 0.810 - ETA: 0s - loss: 0.6238 - acc: 0.8110Epoch 00008: val_loss improved from 0.46947 to 0.45310, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.6225 - acc: 0.8117 - val_loss: 0.4531 - val_acc: 0.8611\n",
      "Epoch 10/20\n",
      "6660/6680 [============================>.] - ETA: 10s - loss: 0.5333 - acc: 0.80 - ETA: 9s - loss: 0.5466 - acc: 0.8333 - ETA: 9s - loss: 0.6796 - acc: 0.800 - ETA: 8s - loss: 0.5967 - acc: 0.818 - ETA: 8s - loss: 0.5632 - acc: 0.835 - ETA: 8s - loss: 0.5784 - acc: 0.833 - ETA: 8s - loss: 0.6161 - acc: 0.823 - ETA: 7s - loss: 0.5905 - acc: 0.830 - ETA: 7s - loss: 0.5862 - acc: 0.827 - ETA: 7s - loss: 0.5896 - acc: 0.821 - ETA: 7s - loss: 0.5811 - acc: 0.817 - ETA: 7s - loss: 0.5757 - acc: 0.821 - ETA: 7s - loss: 0.6115 - acc: 0.816 - ETA: 7s - loss: 0.6016 - acc: 0.820 - ETA: 7s - loss: 0.5954 - acc: 0.825 - ETA: 7s - loss: 0.5791 - acc: 0.824 - ETA: 7s - loss: 0.5667 - acc: 0.826 - ETA: 7s - loss: 0.5663 - acc: 0.826 - ETA: 7s - loss: 0.5670 - acc: 0.829 - ETA: 7s - loss: 0.5654 - acc: 0.829 - ETA: 6s - loss: 0.5570 - acc: 0.830 - ETA: 6s - loss: 0.5608 - acc: 0.828 - ETA: 6s - loss: 0.5564 - acc: 0.829 - ETA: 6s - loss: 0.5468 - acc: 0.831 - ETA: 6s - loss: 0.5462 - acc: 0.832 - ETA: 6s - loss: 0.5438 - acc: 0.832 - ETA: 6s - loss: 0.5481 - acc: 0.831 - ETA: 6s - loss: 0.5499 - acc: 0.830 - ETA: 6s - loss: 0.5492 - acc: 0.830 - ETA: 6s - loss: 0.5534 - acc: 0.828 - ETA: 6s - loss: 0.5528 - acc: 0.828 - ETA: 6s - loss: 0.5546 - acc: 0.828 - ETA: 6s - loss: 0.5505 - acc: 0.830 - ETA: 6s - loss: 0.5530 - acc: 0.828 - ETA: 6s - loss: 0.5537 - acc: 0.830 - ETA: 5s - loss: 0.5510 - acc: 0.832 - ETA: 5s - loss: 0.5510 - acc: 0.830 - ETA: 5s - loss: 0.5573 - acc: 0.828 - ETA: 5s - loss: 0.5634 - acc: 0.826 - ETA: 5s - loss: 0.5628 - acc: 0.827 - ETA: 5s - loss: 0.5584 - acc: 0.829 - ETA: 5s - loss: 0.5628 - acc: 0.828 - ETA: 5s - loss: 0.5694 - acc: 0.826 - ETA: 5s - loss: 0.5694 - acc: 0.826 - ETA: 5s - loss: 0.5761 - acc: 0.825 - ETA: 5s - loss: 0.5770 - acc: 0.825 - ETA: 5s - loss: 0.5762 - acc: 0.826 - ETA: 5s - loss: 0.5772 - acc: 0.825 - ETA: 5s - loss: 0.5774 - acc: 0.825 - ETA: 5s - loss: 0.5768 - acc: 0.825 - ETA: 5s - loss: 0.5772 - acc: 0.825 - ETA: 5s - loss: 0.5766 - acc: 0.824 - ETA: 4s - loss: 0.5806 - acc: 0.824 - ETA: 4s - loss: 0.5764 - acc: 0.825 - ETA: 4s - loss: 0.5768 - acc: 0.826 - ETA: 4s - loss: 0.5749 - acc: 0.827 - ETA: 4s - loss: 0.5748 - acc: 0.827 - ETA: 4s - loss: 0.5786 - acc: 0.825 - ETA: 4s - loss: 0.5776 - acc: 0.827 - ETA: 4s - loss: 0.5812 - acc: 0.825 - ETA: 4s - loss: 0.5791 - acc: 0.826 - ETA: 4s - loss: 0.5755 - acc: 0.827 - ETA: 4s - loss: 0.5767 - acc: 0.826 - ETA: 4s - loss: 0.5797 - acc: 0.826 - ETA: 4s - loss: 0.5815 - acc: 0.825 - ETA: 4s - loss: 0.5840 - acc: 0.825 - ETA: 4s - loss: 0.5837 - acc: 0.824 - ETA: 4s - loss: 0.5833 - acc: 0.824 - ETA: 4s - loss: 0.5790 - acc: 0.825 - ETA: 4s - loss: 0.5774 - acc: 0.825 - ETA: 4s - loss: 0.5760 - acc: 0.826 - ETA: 4s - loss: 0.5723 - acc: 0.828 - ETA: 3s - loss: 0.5774 - acc: 0.827 - ETA: 3s - loss: 0.5735 - acc: 0.827 - ETA: 3s - loss: 0.5726 - acc: 0.827 - ETA: 3s - loss: 0.5723 - acc: 0.827 - ETA: 3s - loss: 0.5766 - acc: 0.826 - ETA: 3s - loss: 0.5747 - acc: 0.827 - ETA: 3s - loss: 0.5715 - acc: 0.827 - ETA: 3s - loss: 0.5740 - acc: 0.826 - ETA: 3s - loss: 0.5722 - acc: 0.827 - ETA: 3s - loss: 0.5729 - acc: 0.826 - ETA: 3s - loss: 0.5737 - acc: 0.825 - ETA: 3s - loss: 0.5715 - acc: 0.826 - ETA: 3s - loss: 0.5705 - acc: 0.827 - ETA: 3s - loss: 0.5762 - acc: 0.825 - ETA: 3s - loss: 0.5746 - acc: 0.826 - ETA: 3s - loss: 0.5751 - acc: 0.826 - ETA: 3s - loss: 0.5764 - acc: 0.825 - ETA: 2s - loss: 0.5802 - acc: 0.824 - ETA: 2s - loss: 0.5811 - acc: 0.823 - ETA: 2s - loss: 0.5807 - acc: 0.823 - ETA: 2s - loss: 0.5807 - acc: 0.823 - ETA: 2s - loss: 0.5817 - acc: 0.823 - ETA: 2s - loss: 0.5844 - acc: 0.822 - ETA: 2s - loss: 0.5848 - acc: 0.821 - ETA: 2s - loss: 0.5851 - acc: 0.822 - ETA: 2s - loss: 0.5837 - acc: 0.822 - ETA: 2s - loss: 0.5829 - acc: 0.823 - ETA: 2s - loss: 0.5834 - acc: 0.822 - ETA: 2s - loss: 0.5849 - acc: 0.821 - ETA: 2s - loss: 0.5866 - acc: 0.821 - ETA: 2s - loss: 0.5883 - acc: 0.821 - ETA: 2s - loss: 0.5887 - acc: 0.821 - ETA: 2s - loss: 0.5887 - acc: 0.821 - ETA: 2s - loss: 0.5875 - acc: 0.822 - ETA: 2s - loss: 0.5874 - acc: 0.821 - ETA: 1s - loss: 0.5859 - acc: 0.822 - ETA: 1s - loss: 0.5870 - acc: 0.822 - ETA: 1s - loss: 0.5853 - acc: 0.822 - ETA: 1s - loss: 0.5874 - acc: 0.821 - ETA: 1s - loss: 0.5899 - acc: 0.821 - ETA: 1s - loss: 0.5875 - acc: 0.822 - ETA: 1s - loss: 0.5871 - acc: 0.822 - ETA: 1s - loss: 0.5861 - acc: 0.822 - ETA: 1s - loss: 0.5850 - acc: 0.823 - ETA: 1s - loss: 0.5851 - acc: 0.823 - ETA: 1s - loss: 0.5841 - acc: 0.823 - ETA: 1s - loss: 0.5836 - acc: 0.823 - ETA: 1s - loss: 0.5847 - acc: 0.823 - ETA: 1s - loss: 0.5832 - acc: 0.824 - ETA: 1s - loss: 0.5830 - acc: 0.823 - ETA: 1s - loss: 0.5816 - acc: 0.823 - ETA: 1s - loss: 0.5803 - acc: 0.823 - ETA: 1s - loss: 0.5795 - acc: 0.824 - ETA: 1s - loss: 0.5805 - acc: 0.823 - ETA: 1s - loss: 0.5839 - acc: 0.822 - ETA: 0s - loss: 0.5842 - acc: 0.822 - ETA: 0s - loss: 0.5844 - acc: 0.822 - ETA: 0s - loss: 0.5851 - acc: 0.822 - ETA: 0s - loss: 0.5839 - acc: 0.822 - ETA: 0s - loss: 0.5837 - acc: 0.822 - ETA: 0s - loss: 0.5830 - acc: 0.823 - ETA: 0s - loss: 0.5859 - acc: 0.822 - ETA: 0s - loss: 0.5845 - acc: 0.822 - ETA: 0s - loss: 0.5833 - acc: 0.823 - ETA: 0s - loss: 0.5811 - acc: 0.823 - ETA: 0s - loss: 0.5811 - acc: 0.823 - ETA: 0s - loss: 0.5825 - acc: 0.823 - ETA: 0s - loss: 0.5811 - acc: 0.824 - ETA: 0s - loss: 0.5791 - acc: 0.824 - ETA: 0s - loss: 0.5798 - acc: 0.824 - ETA: 0s - loss: 0.5778 - acc: 0.8249Epoch 00009: val_loss improved from 0.45310 to 0.44911, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.5783 - acc: 0.8246 - val_loss: 0.4491 - val_acc: 0.8491\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.5302 - acc: 0.850 - ETA: 8s - loss: 0.5386 - acc: 0.833 - ETA: 8s - loss: 0.5755 - acc: 0.830 - ETA: 8s - loss: 0.5688 - acc: 0.828 - ETA: 8s - loss: 0.5550 - acc: 0.844 - ETA: 7s - loss: 0.5589 - acc: 0.837 - ETA: 7s - loss: 0.5725 - acc: 0.825 - ETA: 7s - loss: 0.5524 - acc: 0.829 - ETA: 7s - loss: 0.5762 - acc: 0.830 - ETA: 7s - loss: 0.6052 - acc: 0.827 - ETA: 7s - loss: 0.5907 - acc: 0.834 - ETA: 7s - loss: 0.5941 - acc: 0.833 - ETA: 7s - loss: 0.5951 - acc: 0.831 - ETA: 7s - loss: 0.5877 - acc: 0.831 - ETA: 7s - loss: 0.5846 - acc: 0.829 - ETA: 7s - loss: 0.5890 - acc: 0.824 - ETA: 7s - loss: 0.5841 - acc: 0.824 - ETA: 7s - loss: 0.5803 - acc: 0.825 - ETA: 7s - loss: 0.5800 - acc: 0.826 - ETA: 6s - loss: 0.5821 - acc: 0.825 - ETA: 6s - loss: 0.5749 - acc: 0.826 - ETA: 6s - loss: 0.5625 - acc: 0.829 - ETA: 6s - loss: 0.5595 - acc: 0.829 - ETA: 6s - loss: 0.5666 - acc: 0.826 - ETA: 6s - loss: 0.5631 - acc: 0.826 - ETA: 6s - loss: 0.5639 - acc: 0.824 - ETA: 6s - loss: 0.5616 - acc: 0.825 - ETA: 6s - loss: 0.5598 - acc: 0.825 - ETA: 6s - loss: 0.5625 - acc: 0.825 - ETA: 6s - loss: 0.5664 - acc: 0.823 - ETA: 6s - loss: 0.5620 - acc: 0.823 - ETA: 6s - loss: 0.5556 - acc: 0.825 - ETA: 6s - loss: 0.5509 - acc: 0.826 - ETA: 6s - loss: 0.5539 - acc: 0.826 - ETA: 6s - loss: 0.5524 - acc: 0.826 - ETA: 6s - loss: 0.5482 - acc: 0.826 - ETA: 5s - loss: 0.5528 - acc: 0.824 - ETA: 5s - loss: 0.5543 - acc: 0.824 - ETA: 5s - loss: 0.5557 - acc: 0.824 - ETA: 5s - loss: 0.5561 - acc: 0.823 - ETA: 5s - loss: 0.5603 - acc: 0.822 - ETA: 5s - loss: 0.5695 - acc: 0.820 - ETA: 5s - loss: 0.5688 - acc: 0.821 - ETA: 5s - loss: 0.5671 - acc: 0.822 - ETA: 5s - loss: 0.5671 - acc: 0.821 - ETA: 5s - loss: 0.5681 - acc: 0.821 - ETA: 5s - loss: 0.5709 - acc: 0.821 - ETA: 5s - loss: 0.5669 - acc: 0.822 - ETA: 5s - loss: 0.5714 - acc: 0.821 - ETA: 5s - loss: 0.5660 - acc: 0.822 - ETA: 5s - loss: 0.5661 - acc: 0.822 - ETA: 5s - loss: 0.5633 - acc: 0.823 - ETA: 5s - loss: 0.5577 - acc: 0.825 - ETA: 5s - loss: 0.5621 - acc: 0.824 - ETA: 5s - loss: 0.5637 - acc: 0.823 - ETA: 5s - loss: 0.5633 - acc: 0.822 - ETA: 5s - loss: 0.5638 - acc: 0.822 - ETA: 5s - loss: 0.5601 - acc: 0.823 - ETA: 5s - loss: 0.5650 - acc: 0.823 - ETA: 4s - loss: 0.5647 - acc: 0.823 - ETA: 4s - loss: 0.5629 - acc: 0.822 - ETA: 4s - loss: 0.5661 - acc: 0.822 - ETA: 4s - loss: 0.5652 - acc: 0.822 - ETA: 4s - loss: 0.5623 - acc: 0.823 - ETA: 4s - loss: 0.5633 - acc: 0.822 - ETA: 4s - loss: 0.5623 - acc: 0.823 - ETA: 4s - loss: 0.5653 - acc: 0.822 - ETA: 4s - loss: 0.5613 - acc: 0.823 - ETA: 4s - loss: 0.5576 - acc: 0.824 - ETA: 4s - loss: 0.5629 - acc: 0.822 - ETA: 4s - loss: 0.5618 - acc: 0.823 - ETA: 4s - loss: 0.5619 - acc: 0.822 - ETA: 4s - loss: 0.5603 - acc: 0.823 - ETA: 4s - loss: 0.5578 - acc: 0.824 - ETA: 4s - loss: 0.5618 - acc: 0.823 - ETA: 3s - loss: 0.5653 - acc: 0.822 - ETA: 3s - loss: 0.5626 - acc: 0.824 - ETA: 3s - loss: 0.5618 - acc: 0.824 - ETA: 3s - loss: 0.5571 - acc: 0.825 - ETA: 3s - loss: 0.5578 - acc: 0.825 - ETA: 3s - loss: 0.5592 - acc: 0.824 - ETA: 3s - loss: 0.5598 - acc: 0.824 - ETA: 3s - loss: 0.5580 - acc: 0.825 - ETA: 3s - loss: 0.5546 - acc: 0.826 - ETA: 3s - loss: 0.5529 - acc: 0.826 - ETA: 3s - loss: 0.5534 - acc: 0.827 - ETA: 3s - loss: 0.5521 - acc: 0.827 - ETA: 3s - loss: 0.5512 - acc: 0.828 - ETA: 3s - loss: 0.5521 - acc: 0.827 - ETA: 3s - loss: 0.5503 - acc: 0.827 - ETA: 3s - loss: 0.5520 - acc: 0.827 - ETA: 2s - loss: 0.5513 - acc: 0.828 - ETA: 2s - loss: 0.5529 - acc: 0.828 - ETA: 2s - loss: 0.5507 - acc: 0.829 - ETA: 2s - loss: 0.5516 - acc: 0.829 - ETA: 2s - loss: 0.5510 - acc: 0.829 - ETA: 2s - loss: 0.5484 - acc: 0.829 - ETA: 2s - loss: 0.5508 - acc: 0.829 - ETA: 2s - loss: 0.5522 - acc: 0.829 - ETA: 2s - loss: 0.5537 - acc: 0.829 - ETA: 2s - loss: 0.5525 - acc: 0.829 - ETA: 2s - loss: 0.5554 - acc: 0.829 - ETA: 2s - loss: 0.5573 - acc: 0.828 - ETA: 2s - loss: 0.5576 - acc: 0.828 - ETA: 2s - loss: 0.5556 - acc: 0.829 - ETA: 2s - loss: 0.5535 - acc: 0.830 - ETA: 2s - loss: 0.5523 - acc: 0.830 - ETA: 2s - loss: 0.5532 - acc: 0.830 - ETA: 1s - loss: 0.5544 - acc: 0.829 - ETA: 1s - loss: 0.5544 - acc: 0.829 - ETA: 1s - loss: 0.5535 - acc: 0.830 - ETA: 1s - loss: 0.5539 - acc: 0.830 - ETA: 1s - loss: 0.5528 - acc: 0.831 - ETA: 1s - loss: 0.5506 - acc: 0.831 - ETA: 1s - loss: 0.5557 - acc: 0.830 - ETA: 1s - loss: 0.5557 - acc: 0.830 - ETA: 1s - loss: 0.5550 - acc: 0.830 - ETA: 1s - loss: 0.5548 - acc: 0.830 - ETA: 1s - loss: 0.5561 - acc: 0.830 - ETA: 1s - loss: 0.5578 - acc: 0.830 - ETA: 1s - loss: 0.5576 - acc: 0.829 - ETA: 1s - loss: 0.5580 - acc: 0.829 - ETA: 1s - loss: 0.5573 - acc: 0.829 - ETA: 1s - loss: 0.5571 - acc: 0.829 - ETA: 1s - loss: 0.5558 - acc: 0.829 - ETA: 0s - loss: 0.5553 - acc: 0.829 - ETA: 0s - loss: 0.5550 - acc: 0.830 - ETA: 0s - loss: 0.5562 - acc: 0.830 - ETA: 0s - loss: 0.5565 - acc: 0.829 - ETA: 0s - loss: 0.5567 - acc: 0.829 - ETA: 0s - loss: 0.5547 - acc: 0.830 - ETA: 0s - loss: 0.5580 - acc: 0.829 - ETA: 0s - loss: 0.5594 - acc: 0.829 - ETA: 0s - loss: 0.5594 - acc: 0.829 - ETA: 0s - loss: 0.5597 - acc: 0.829 - ETA: 0s - loss: 0.5594 - acc: 0.829 - ETA: 0s - loss: 0.5625 - acc: 0.828 - ETA: 0s - loss: 0.5634 - acc: 0.828 - ETA: 0s - loss: 0.5627 - acc: 0.828 - ETA: 0s - loss: 0.5635 - acc: 0.827 - ETA: 0s - loss: 0.5639 - acc: 0.827 - ETA: 0s - loss: 0.5642 - acc: 0.8273Epoch 00010: val_loss improved from 0.44911 to 0.43291, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.5638 - acc: 0.8274 - val_loss: 0.4329 - val_acc: 0.8611\n",
      "Epoch 12/20\n",
      "6640/6680 [============================>.] - ETA: 9s - loss: 0.7215 - acc: 0.850 - ETA: 9s - loss: 0.6523 - acc: 0.850 - ETA: 9s - loss: 0.6055 - acc: 0.840 - ETA: 9s - loss: 0.5951 - acc: 0.835 - ETA: 9s - loss: 0.6320 - acc: 0.833 - ETA: 9s - loss: 0.5502 - acc: 0.859 - ETA: 8s - loss: 0.5825 - acc: 0.828 - ETA: 8s - loss: 0.5414 - acc: 0.844 - ETA: 8s - loss: 0.5284 - acc: 0.842 - ETA: 7s - loss: 0.5346 - acc: 0.840 - ETA: 7s - loss: 0.5384 - acc: 0.837 - ETA: 7s - loss: 0.5314 - acc: 0.835 - ETA: 7s - loss: 0.5265 - acc: 0.838 - ETA: 7s - loss: 0.5241 - acc: 0.836 - ETA: 7s - loss: 0.5125 - acc: 0.838 - ETA: 7s - loss: 0.5039 - acc: 0.842 - ETA: 7s - loss: 0.5263 - acc: 0.833 - ETA: 7s - loss: 0.5266 - acc: 0.834 - ETA: 7s - loss: 0.5415 - acc: 0.828 - ETA: 7s - loss: 0.5299 - acc: 0.832 - ETA: 7s - loss: 0.5249 - acc: 0.833 - ETA: 7s - loss: 0.5214 - acc: 0.832 - ETA: 7s - loss: 0.5169 - acc: 0.835 - ETA: 6s - loss: 0.5125 - acc: 0.838 - ETA: 6s - loss: 0.5151 - acc: 0.837 - ETA: 6s - loss: 0.5150 - acc: 0.837 - ETA: 6s - loss: 0.5121 - acc: 0.838 - ETA: 6s - loss: 0.5081 - acc: 0.839 - ETA: 6s - loss: 0.5009 - acc: 0.839 - ETA: 6s - loss: 0.5033 - acc: 0.838 - ETA: 6s - loss: 0.5065 - acc: 0.839 - ETA: 6s - loss: 0.5090 - acc: 0.838 - ETA: 6s - loss: 0.5134 - acc: 0.837 - ETA: 6s - loss: 0.5173 - acc: 0.837 - ETA: 6s - loss: 0.5250 - acc: 0.835 - ETA: 6s - loss: 0.5235 - acc: 0.836 - ETA: 6s - loss: 0.5229 - acc: 0.836 - ETA: 6s - loss: 0.5199 - acc: 0.837 - ETA: 5s - loss: 0.5184 - acc: 0.839 - ETA: 5s - loss: 0.5142 - acc: 0.840 - ETA: 5s - loss: 0.5101 - acc: 0.840 - ETA: 5s - loss: 0.5082 - acc: 0.842 - ETA: 5s - loss: 0.5143 - acc: 0.840 - ETA: 5s - loss: 0.5160 - acc: 0.839 - ETA: 5s - loss: 0.5222 - acc: 0.838 - ETA: 5s - loss: 0.5230 - acc: 0.837 - ETA: 5s - loss: 0.5190 - acc: 0.839 - ETA: 5s - loss: 0.5182 - acc: 0.838 - ETA: 5s - loss: 0.5216 - acc: 0.837 - ETA: 5s - loss: 0.5174 - acc: 0.838 - ETA: 5s - loss: 0.5172 - acc: 0.838 - ETA: 5s - loss: 0.5182 - acc: 0.838 - ETA: 5s - loss: 0.5148 - acc: 0.840 - ETA: 5s - loss: 0.5123 - acc: 0.840 - ETA: 5s - loss: 0.5156 - acc: 0.839 - ETA: 5s - loss: 0.5136 - acc: 0.840 - ETA: 4s - loss: 0.5108 - acc: 0.841 - ETA: 4s - loss: 0.5142 - acc: 0.840 - ETA: 4s - loss: 0.5113 - acc: 0.841 - ETA: 4s - loss: 0.5118 - acc: 0.841 - ETA: 4s - loss: 0.5108 - acc: 0.841 - ETA: 4s - loss: 0.5133 - acc: 0.840 - ETA: 4s - loss: 0.5135 - acc: 0.840 - ETA: 4s - loss: 0.5123 - acc: 0.841 - ETA: 4s - loss: 0.5144 - acc: 0.839 - ETA: 4s - loss: 0.5177 - acc: 0.838 - ETA: 4s - loss: 0.5169 - acc: 0.837 - ETA: 4s - loss: 0.5151 - acc: 0.839 - ETA: 4s - loss: 0.5160 - acc: 0.838 - ETA: 4s - loss: 0.5175 - acc: 0.838 - ETA: 4s - loss: 0.5191 - acc: 0.838 - ETA: 4s - loss: 0.5201 - acc: 0.838 - ETA: 4s - loss: 0.5217 - acc: 0.838 - ETA: 4s - loss: 0.5207 - acc: 0.838 - ETA: 3s - loss: 0.5178 - acc: 0.839 - ETA: 3s - loss: 0.5171 - acc: 0.839 - ETA: 3s - loss: 0.5164 - acc: 0.839 - ETA: 3s - loss: 0.5180 - acc: 0.839 - ETA: 3s - loss: 0.5170 - acc: 0.839 - ETA: 3s - loss: 0.5196 - acc: 0.838 - ETA: 3s - loss: 0.5237 - acc: 0.837 - ETA: 3s - loss: 0.5249 - acc: 0.836 - ETA: 3s - loss: 0.5270 - acc: 0.835 - ETA: 3s - loss: 0.5266 - acc: 0.835 - ETA: 3s - loss: 0.5265 - acc: 0.834 - ETA: 3s - loss: 0.5282 - acc: 0.834 - ETA: 3s - loss: 0.5276 - acc: 0.834 - ETA: 3s - loss: 0.5295 - acc: 0.833 - ETA: 3s - loss: 0.5284 - acc: 0.834 - ETA: 3s - loss: 0.5302 - acc: 0.833 - ETA: 3s - loss: 0.5276 - acc: 0.834 - ETA: 3s - loss: 0.5256 - acc: 0.834 - ETA: 3s - loss: 0.5242 - acc: 0.834 - ETA: 3s - loss: 0.5237 - acc: 0.835 - ETA: 2s - loss: 0.5229 - acc: 0.835 - ETA: 2s - loss: 0.5239 - acc: 0.834 - ETA: 2s - loss: 0.5295 - acc: 0.834 - ETA: 2s - loss: 0.5267 - acc: 0.835 - ETA: 2s - loss: 0.5243 - acc: 0.836 - ETA: 2s - loss: 0.5250 - acc: 0.836 - ETA: 2s - loss: 0.5257 - acc: 0.835 - ETA: 2s - loss: 0.5216 - acc: 0.837 - ETA: 2s - loss: 0.5187 - acc: 0.838 - ETA: 2s - loss: 0.5187 - acc: 0.838 - ETA: 2s - loss: 0.5171 - acc: 0.839 - ETA: 2s - loss: 0.5165 - acc: 0.839 - ETA: 2s - loss: 0.5153 - acc: 0.839 - ETA: 2s - loss: 0.5148 - acc: 0.840 - ETA: 2s - loss: 0.5138 - acc: 0.840 - ETA: 2s - loss: 0.5116 - acc: 0.840 - ETA: 1s - loss: 0.5121 - acc: 0.839 - ETA: 1s - loss: 0.5107 - acc: 0.840 - ETA: 1s - loss: 0.5117 - acc: 0.840 - ETA: 1s - loss: 0.5105 - acc: 0.840 - ETA: 1s - loss: 0.5104 - acc: 0.841 - ETA: 1s - loss: 0.5102 - acc: 0.841 - ETA: 1s - loss: 0.5106 - acc: 0.841 - ETA: 1s - loss: 0.5102 - acc: 0.841 - ETA: 1s - loss: 0.5118 - acc: 0.841 - ETA: 1s - loss: 0.5101 - acc: 0.841 - ETA: 1s - loss: 0.5085 - acc: 0.841 - ETA: 1s - loss: 0.5106 - acc: 0.841 - ETA: 1s - loss: 0.5117 - acc: 0.840 - ETA: 1s - loss: 0.5128 - acc: 0.840 - ETA: 1s - loss: 0.5127 - acc: 0.840 - ETA: 1s - loss: 0.5154 - acc: 0.839 - ETA: 1s - loss: 0.5155 - acc: 0.839 - ETA: 1s - loss: 0.5159 - acc: 0.838 - ETA: 0s - loss: 0.5161 - acc: 0.839 - ETA: 0s - loss: 0.5151 - acc: 0.839 - ETA: 0s - loss: 0.5136 - acc: 0.839 - ETA: 0s - loss: 0.5151 - acc: 0.839 - ETA: 0s - loss: 0.5184 - acc: 0.838 - ETA: 0s - loss: 0.5182 - acc: 0.838 - ETA: 0s - loss: 0.5189 - acc: 0.838 - ETA: 0s - loss: 0.5192 - acc: 0.838 - ETA: 0s - loss: 0.5183 - acc: 0.838 - ETA: 0s - loss: 0.5176 - acc: 0.838 - ETA: 0s - loss: 0.5160 - acc: 0.839 - ETA: 0s - loss: 0.5152 - acc: 0.839 - ETA: 0s - loss: 0.5146 - acc: 0.839 - ETA: 0s - loss: 0.5154 - acc: 0.838 - ETA: 0s - loss: 0.5136 - acc: 0.839 - ETA: 0s - loss: 0.5128 - acc: 0.839 - ETA: 0s - loss: 0.5122 - acc: 0.839 - ETA: 0s - loss: 0.5122 - acc: 0.839 - ETA: 0s - loss: 0.5123 - acc: 0.8399Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 9s - loss: 0.5140 - acc: 0.8395 - val_loss: 0.4406 - val_acc: 0.8563\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.4638 - acc: 0.850 - ETA: 9s - loss: 0.5144 - acc: 0.833 - ETA: 9s - loss: 0.5137 - acc: 0.810 - ETA: 9s - loss: 0.4940 - acc: 0.814 - ETA: 9s - loss: 0.4360 - acc: 0.844 - ETA: 9s - loss: 0.4358 - acc: 0.845 - ETA: 9s - loss: 0.4835 - acc: 0.834 - ETA: 9s - loss: 0.4845 - acc: 0.833 - ETA: 9s - loss: 0.5264 - acc: 0.826 - ETA: 8s - loss: 0.5260 - acc: 0.821 - ETA: 8s - loss: 0.5062 - acc: 0.826 - ETA: 8s - loss: 0.4964 - acc: 0.826 - ETA: 8s - loss: 0.4852 - acc: 0.836 - ETA: 8s - loss: 0.4806 - acc: 0.837 - ETA: 8s - loss: 0.4898 - acc: 0.831 - ETA: 8s - loss: 0.4801 - acc: 0.835 - ETA: 8s - loss: 0.4719 - acc: 0.836 - ETA: 8s - loss: 0.4627 - acc: 0.841 - ETA: 8s - loss: 0.4732 - acc: 0.839 - ETA: 8s - loss: 0.4711 - acc: 0.842 - ETA: 8s - loss: 0.4881 - acc: 0.839 - ETA: 8s - loss: 0.4975 - acc: 0.839 - ETA: 8s - loss: 0.4938 - acc: 0.840 - ETA: 8s - loss: 0.4853 - acc: 0.842 - ETA: 7s - loss: 0.5011 - acc: 0.836 - ETA: 7s - loss: 0.5033 - acc: 0.837 - ETA: 7s - loss: 0.4979 - acc: 0.839 - ETA: 7s - loss: 0.4897 - acc: 0.843 - ETA: 7s - loss: 0.4912 - acc: 0.844 - ETA: 7s - loss: 0.4864 - acc: 0.844 - ETA: 7s - loss: 0.4849 - acc: 0.845 - ETA: 7s - loss: 0.4834 - acc: 0.846 - ETA: 7s - loss: 0.4839 - acc: 0.846 - ETA: 7s - loss: 0.4844 - acc: 0.845 - ETA: 7s - loss: 0.4883 - acc: 0.843 - ETA: 7s - loss: 0.4922 - acc: 0.843 - ETA: 7s - loss: 0.4929 - acc: 0.843 - ETA: 7s - loss: 0.4949 - acc: 0.842 - ETA: 7s - loss: 0.4973 - acc: 0.842 - ETA: 7s - loss: 0.5006 - acc: 0.841 - ETA: 7s - loss: 0.5016 - acc: 0.842 - ETA: 7s - loss: 0.4985 - acc: 0.841 - ETA: 7s - loss: 0.5027 - acc: 0.838 - ETA: 7s - loss: 0.4979 - acc: 0.839 - ETA: 6s - loss: 0.4970 - acc: 0.839 - ETA: 6s - loss: 0.4938 - acc: 0.840 - ETA: 6s - loss: 0.5009 - acc: 0.838 - ETA: 6s - loss: 0.5019 - acc: 0.838 - ETA: 6s - loss: 0.5017 - acc: 0.839 - ETA: 6s - loss: 0.5039 - acc: 0.839 - ETA: 6s - loss: 0.5078 - acc: 0.838 - ETA: 6s - loss: 0.5125 - acc: 0.836 - ETA: 6s - loss: 0.5086 - acc: 0.837 - ETA: 6s - loss: 0.5029 - acc: 0.838 - ETA: 6s - loss: 0.5032 - acc: 0.839 - ETA: 6s - loss: 0.5006 - acc: 0.839 - ETA: 6s - loss: 0.4998 - acc: 0.839 - ETA: 6s - loss: 0.4997 - acc: 0.839 - ETA: 6s - loss: 0.5039 - acc: 0.837 - ETA: 6s - loss: 0.5054 - acc: 0.838 - ETA: 5s - loss: 0.5039 - acc: 0.838 - ETA: 5s - loss: 0.5063 - acc: 0.838 - ETA: 5s - loss: 0.5051 - acc: 0.839 - ETA: 5s - loss: 0.5105 - acc: 0.838 - ETA: 5s - loss: 0.5111 - acc: 0.838 - ETA: 5s - loss: 0.5114 - acc: 0.838 - ETA: 5s - loss: 0.5117 - acc: 0.838 - ETA: 5s - loss: 0.5072 - acc: 0.839 - ETA: 5s - loss: 0.5041 - acc: 0.840 - ETA: 5s - loss: 0.5061 - acc: 0.838 - ETA: 5s - loss: 0.5051 - acc: 0.838 - ETA: 5s - loss: 0.5047 - acc: 0.838 - ETA: 5s - loss: 0.5041 - acc: 0.838 - ETA: 5s - loss: 0.5055 - acc: 0.837 - ETA: 5s - loss: 0.5061 - acc: 0.837 - ETA: 5s - loss: 0.5052 - acc: 0.837 - ETA: 5s - loss: 0.5056 - acc: 0.837 - ETA: 5s - loss: 0.5026 - acc: 0.838 - ETA: 4s - loss: 0.5011 - acc: 0.837 - ETA: 4s - loss: 0.5022 - acc: 0.837 - ETA: 4s - loss: 0.5007 - acc: 0.838 - ETA: 4s - loss: 0.5020 - acc: 0.837 - ETA: 4s - loss: 0.5000 - acc: 0.838 - ETA: 4s - loss: 0.5015 - acc: 0.838 - ETA: 4s - loss: 0.5016 - acc: 0.840 - ETA: 4s - loss: 0.5015 - acc: 0.840 - ETA: 4s - loss: 0.5020 - acc: 0.840 - ETA: 4s - loss: 0.5017 - acc: 0.840 - ETA: 4s - loss: 0.5030 - acc: 0.840 - ETA: 4s - loss: 0.5023 - acc: 0.840 - ETA: 4s - loss: 0.5011 - acc: 0.840 - ETA: 4s - loss: 0.5026 - acc: 0.840 - ETA: 4s - loss: 0.5019 - acc: 0.840 - ETA: 4s - loss: 0.5054 - acc: 0.839 - ETA: 4s - loss: 0.5057 - acc: 0.839 - ETA: 3s - loss: 0.5080 - acc: 0.838 - ETA: 3s - loss: 0.5119 - acc: 0.838 - ETA: 3s - loss: 0.5098 - acc: 0.838 - ETA: 3s - loss: 0.5069 - acc: 0.839 - ETA: 3s - loss: 0.5069 - acc: 0.839 - ETA: 3s - loss: 0.5100 - acc: 0.838 - ETA: 3s - loss: 0.5098 - acc: 0.838 - ETA: 3s - loss: 0.5102 - acc: 0.839 - ETA: 3s - loss: 0.5096 - acc: 0.839 - ETA: 3s - loss: 0.5079 - acc: 0.840 - ETA: 3s - loss: 0.5102 - acc: 0.839 - ETA: 3s - loss: 0.5128 - acc: 0.839 - ETA: 3s - loss: 0.5115 - acc: 0.839 - ETA: 3s - loss: 0.5083 - acc: 0.840 - ETA: 3s - loss: 0.5105 - acc: 0.840 - ETA: 3s - loss: 0.5117 - acc: 0.839 - ETA: 3s - loss: 0.5139 - acc: 0.839 - ETA: 3s - loss: 0.5141 - acc: 0.839 - ETA: 2s - loss: 0.5147 - acc: 0.839 - ETA: 2s - loss: 0.5156 - acc: 0.839 - ETA: 2s - loss: 0.5184 - acc: 0.838 - ETA: 2s - loss: 0.5156 - acc: 0.839 - ETA: 2s - loss: 0.5150 - acc: 0.839 - ETA: 2s - loss: 0.5158 - acc: 0.839 - ETA: 2s - loss: 0.5145 - acc: 0.840 - ETA: 2s - loss: 0.5134 - acc: 0.840 - ETA: 2s - loss: 0.5133 - acc: 0.840 - ETA: 2s - loss: 0.5130 - acc: 0.840 - ETA: 2s - loss: 0.5137 - acc: 0.840 - ETA: 2s - loss: 0.5120 - acc: 0.840 - ETA: 2s - loss: 0.5119 - acc: 0.840 - ETA: 2s - loss: 0.5110 - acc: 0.840 - ETA: 2s - loss: 0.5100 - acc: 0.840 - ETA: 2s - loss: 0.5117 - acc: 0.840 - ETA: 2s - loss: 0.5104 - acc: 0.840 - ETA: 2s - loss: 0.5110 - acc: 0.840 - ETA: 1s - loss: 0.5100 - acc: 0.841 - ETA: 1s - loss: 0.5073 - acc: 0.842 - ETA: 1s - loss: 0.5052 - acc: 0.842 - ETA: 1s - loss: 0.5050 - acc: 0.842 - ETA: 1s - loss: 0.5048 - acc: 0.842 - ETA: 1s - loss: 0.5030 - acc: 0.843 - ETA: 1s - loss: 0.5022 - acc: 0.843 - ETA: 1s - loss: 0.5023 - acc: 0.843 - ETA: 1s - loss: 0.5015 - acc: 0.843 - ETA: 1s - loss: 0.5027 - acc: 0.843 - ETA: 1s - loss: 0.5031 - acc: 0.842 - ETA: 1s - loss: 0.5041 - acc: 0.843 - ETA: 1s - loss: 0.5053 - acc: 0.842 - ETA: 1s - loss: 0.5063 - acc: 0.842 - ETA: 1s - loss: 0.5053 - acc: 0.843 - ETA: 1s - loss: 0.5059 - acc: 0.842 - ETA: 1s - loss: 0.5062 - acc: 0.842 - ETA: 1s - loss: 0.5052 - acc: 0.843 - ETA: 0s - loss: 0.5074 - acc: 0.843 - ETA: 0s - loss: 0.5064 - acc: 0.843 - ETA: 0s - loss: 0.5062 - acc: 0.843 - ETA: 0s - loss: 0.5060 - acc: 0.843 - ETA: 0s - loss: 0.5043 - acc: 0.843 - ETA: 0s - loss: 0.5060 - acc: 0.842 - ETA: 0s - loss: 0.5053 - acc: 0.842 - ETA: 0s - loss: 0.5052 - acc: 0.842 - ETA: 0s - loss: 0.5037 - acc: 0.843 - ETA: 0s - loss: 0.5053 - acc: 0.843 - ETA: 0s - loss: 0.5063 - acc: 0.843 - ETA: 0s - loss: 0.5049 - acc: 0.843 - ETA: 0s - loss: 0.5043 - acc: 0.843 - ETA: 0s - loss: 0.5037 - acc: 0.844 - ETA: 0s - loss: 0.5021 - acc: 0.844 - ETA: 0s - loss: 0.5017 - acc: 0.844 - ETA: 0s - loss: 0.5027 - acc: 0.845 - ETA: 0s - loss: 0.5018 - acc: 0.8453Epoch 00012: val_loss improved from 0.43291 to 0.41440, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 0.5019 - acc: 0.8454 - val_loss: 0.4144 - val_acc: 0.8635\n",
      "Epoch 14/20\n",
      "6660/6680 [============================>.] - ETA: 8s - loss: 0.4145 - acc: 0.900 - ETA: 7s - loss: 0.3647 - acc: 0.875 - ETA: 7s - loss: 0.3182 - acc: 0.883 - ETA: 7s - loss: 0.3516 - acc: 0.861 - ETA: 7s - loss: 0.4108 - acc: 0.840 - ETA: 7s - loss: 0.3790 - acc: 0.857 - ETA: 7s - loss: 0.4117 - acc: 0.853 - ETA: 7s - loss: 0.4389 - acc: 0.844 - ETA: 7s - loss: 0.4169 - acc: 0.852 - ETA: 7s - loss: 0.4089 - acc: 0.854 - ETA: 7s - loss: 0.4073 - acc: 0.855 - ETA: 7s - loss: 0.4020 - acc: 0.858 - ETA: 7s - loss: 0.3944 - acc: 0.863 - ETA: 7s - loss: 0.3843 - acc: 0.865 - ETA: 7s - loss: 0.3882 - acc: 0.863 - ETA: 7s - loss: 0.3955 - acc: 0.862 - ETA: 7s - loss: 0.3948 - acc: 0.865 - ETA: 7s - loss: 0.4108 - acc: 0.859 - ETA: 7s - loss: 0.4116 - acc: 0.858 - ETA: 7s - loss: 0.4108 - acc: 0.858 - ETA: 7s - loss: 0.4116 - acc: 0.857 - ETA: 7s - loss: 0.4151 - acc: 0.859 - ETA: 6s - loss: 0.4214 - acc: 0.857 - ETA: 6s - loss: 0.4324 - acc: 0.851 - ETA: 6s - loss: 0.4314 - acc: 0.852 - ETA: 6s - loss: 0.4399 - acc: 0.850 - ETA: 6s - loss: 0.4344 - acc: 0.852 - ETA: 6s - loss: 0.4449 - acc: 0.849 - ETA: 6s - loss: 0.4420 - acc: 0.851 - ETA: 6s - loss: 0.4456 - acc: 0.849 - ETA: 6s - loss: 0.4460 - acc: 0.849 - ETA: 6s - loss: 0.4393 - acc: 0.851 - ETA: 6s - loss: 0.4494 - acc: 0.850 - ETA: 6s - loss: 0.4488 - acc: 0.851 - ETA: 6s - loss: 0.4483 - acc: 0.851 - ETA: 5s - loss: 0.4543 - acc: 0.850 - ETA: 5s - loss: 0.4478 - acc: 0.852 - ETA: 5s - loss: 0.4556 - acc: 0.850 - ETA: 5s - loss: 0.4523 - acc: 0.850 - ETA: 5s - loss: 0.4541 - acc: 0.850 - ETA: 5s - loss: 0.4537 - acc: 0.850 - ETA: 5s - loss: 0.4532 - acc: 0.849 - ETA: 5s - loss: 0.4562 - acc: 0.848 - ETA: 5s - loss: 0.4577 - acc: 0.848 - ETA: 5s - loss: 0.4578 - acc: 0.849 - ETA: 5s - loss: 0.4671 - acc: 0.846 - ETA: 5s - loss: 0.4721 - acc: 0.846 - ETA: 5s - loss: 0.4722 - acc: 0.845 - ETA: 5s - loss: 0.4694 - acc: 0.845 - ETA: 5s - loss: 0.4684 - acc: 0.846 - ETA: 5s - loss: 0.4678 - acc: 0.847 - ETA: 4s - loss: 0.4679 - acc: 0.846 - ETA: 4s - loss: 0.4685 - acc: 0.846 - ETA: 4s - loss: 0.4647 - acc: 0.848 - ETA: 4s - loss: 0.4615 - acc: 0.849 - ETA: 4s - loss: 0.4628 - acc: 0.849 - ETA: 4s - loss: 0.4624 - acc: 0.850 - ETA: 4s - loss: 0.4651 - acc: 0.849 - ETA: 4s - loss: 0.4640 - acc: 0.848 - ETA: 4s - loss: 0.4671 - acc: 0.848 - ETA: 4s - loss: 0.4653 - acc: 0.849 - ETA: 4s - loss: 0.4645 - acc: 0.849 - ETA: 4s - loss: 0.4663 - acc: 0.848 - ETA: 4s - loss: 0.4628 - acc: 0.850 - ETA: 4s - loss: 0.4649 - acc: 0.849 - ETA: 4s - loss: 0.4608 - acc: 0.851 - ETA: 4s - loss: 0.4595 - acc: 0.852 - ETA: 4s - loss: 0.4642 - acc: 0.851 - ETA: 4s - loss: 0.4684 - acc: 0.851 - ETA: 4s - loss: 0.4679 - acc: 0.852 - ETA: 3s - loss: 0.4647 - acc: 0.853 - ETA: 3s - loss: 0.4642 - acc: 0.852 - ETA: 3s - loss: 0.4634 - acc: 0.852 - ETA: 3s - loss: 0.4614 - acc: 0.853 - ETA: 3s - loss: 0.4608 - acc: 0.853 - ETA: 3s - loss: 0.4627 - acc: 0.853 - ETA: 3s - loss: 0.4643 - acc: 0.853 - ETA: 3s - loss: 0.4625 - acc: 0.853 - ETA: 3s - loss: 0.4632 - acc: 0.853 - ETA: 3s - loss: 0.4645 - acc: 0.853 - ETA: 3s - loss: 0.4633 - acc: 0.853 - ETA: 3s - loss: 0.4619 - acc: 0.853 - ETA: 3s - loss: 0.4606 - acc: 0.854 - ETA: 3s - loss: 0.4596 - acc: 0.854 - ETA: 3s - loss: 0.4628 - acc: 0.853 - ETA: 3s - loss: 0.4634 - acc: 0.853 - ETA: 3s - loss: 0.4623 - acc: 0.854 - ETA: 3s - loss: 0.4610 - acc: 0.854 - ETA: 3s - loss: 0.4600 - acc: 0.854 - ETA: 2s - loss: 0.4587 - acc: 0.855 - ETA: 2s - loss: 0.4582 - acc: 0.854 - ETA: 2s - loss: 0.4591 - acc: 0.854 - ETA: 2s - loss: 0.4586 - acc: 0.854 - ETA: 2s - loss: 0.4596 - acc: 0.854 - ETA: 2s - loss: 0.4595 - acc: 0.854 - ETA: 2s - loss: 0.4579 - acc: 0.854 - ETA: 2s - loss: 0.4596 - acc: 0.854 - ETA: 2s - loss: 0.4574 - acc: 0.855 - ETA: 2s - loss: 0.4563 - acc: 0.855 - ETA: 2s - loss: 0.4547 - acc: 0.855 - ETA: 2s - loss: 0.4544 - acc: 0.855 - ETA: 2s - loss: 0.4577 - acc: 0.854 - ETA: 2s - loss: 0.4566 - acc: 0.855 - ETA: 2s - loss: 0.4563 - acc: 0.855 - ETA: 2s - loss: 0.4588 - acc: 0.855 - ETA: 2s - loss: 0.4567 - acc: 0.855 - ETA: 1s - loss: 0.4604 - acc: 0.854 - ETA: 1s - loss: 0.4618 - acc: 0.854 - ETA: 1s - loss: 0.4628 - acc: 0.853 - ETA: 1s - loss: 0.4614 - acc: 0.854 - ETA: 1s - loss: 0.4604 - acc: 0.854 - ETA: 1s - loss: 0.4595 - acc: 0.854 - ETA: 1s - loss: 0.4604 - acc: 0.854 - ETA: 1s - loss: 0.4597 - acc: 0.854 - ETA: 1s - loss: 0.4620 - acc: 0.854 - ETA: 1s - loss: 0.4618 - acc: 0.854 - ETA: 1s - loss: 0.4637 - acc: 0.854 - ETA: 1s - loss: 0.4637 - acc: 0.853 - ETA: 1s - loss: 0.4622 - acc: 0.854 - ETA: 1s - loss: 0.4618 - acc: 0.854 - ETA: 1s - loss: 0.4613 - acc: 0.854 - ETA: 1s - loss: 0.4607 - acc: 0.854 - ETA: 1s - loss: 0.4599 - acc: 0.854 - ETA: 0s - loss: 0.4619 - acc: 0.853 - ETA: 0s - loss: 0.4626 - acc: 0.852 - ETA: 0s - loss: 0.4644 - acc: 0.852 - ETA: 0s - loss: 0.4655 - acc: 0.852 - ETA: 0s - loss: 0.4647 - acc: 0.852 - ETA: 0s - loss: 0.4643 - acc: 0.852 - ETA: 0s - loss: 0.4657 - acc: 0.851 - ETA: 0s - loss: 0.4649 - acc: 0.851 - ETA: 0s - loss: 0.4641 - acc: 0.852 - ETA: 0s - loss: 0.4639 - acc: 0.851 - ETA: 0s - loss: 0.4656 - acc: 0.851 - ETA: 0s - loss: 0.4633 - acc: 0.852 - ETA: 0s - loss: 0.4620 - acc: 0.853 - ETA: 0s - loss: 0.4628 - acc: 0.853 - ETA: 0s - loss: 0.4639 - acc: 0.852 - ETA: 0s - loss: 0.4663 - acc: 0.852 - ETA: 0s - loss: 0.4666 - acc: 0.8518Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 0.4667 - acc: 0.8516 - val_loss: 0.4173 - val_acc: 0.8635\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 7s - loss: 0.3612 - acc: 0.800 - ETA: 9s - loss: 0.2948 - acc: 0.883 - ETA: 9s - loss: 0.2482 - acc: 0.920 - ETA: 8s - loss: 0.2607 - acc: 0.912 - ETA: 8s - loss: 0.3220 - acc: 0.905 - ETA: 8s - loss: 0.3529 - acc: 0.900 - ETA: 8s - loss: 0.3680 - acc: 0.889 - ETA: 7s - loss: 0.3391 - acc: 0.891 - ETA: 7s - loss: 0.3552 - acc: 0.892 - ETA: 7s - loss: 0.3957 - acc: 0.878 - ETA: 7s - loss: 0.3715 - acc: 0.881 - ETA: 7s - loss: 0.3892 - acc: 0.876 - ETA: 7s - loss: 0.3894 - acc: 0.879 - ETA: 7s - loss: 0.4041 - acc: 0.876 - ETA: 7s - loss: 0.4072 - acc: 0.874 - ETA: 7s - loss: 0.3978 - acc: 0.876 - ETA: 7s - loss: 0.4019 - acc: 0.875 - ETA: 6s - loss: 0.4092 - acc: 0.872 - ETA: 6s - loss: 0.4183 - acc: 0.870 - ETA: 6s - loss: 0.4232 - acc: 0.868 - ETA: 6s - loss: 0.4144 - acc: 0.871 - ETA: 6s - loss: 0.4101 - acc: 0.872 - ETA: 6s - loss: 0.4094 - acc: 0.872 - ETA: 6s - loss: 0.4145 - acc: 0.869 - ETA: 6s - loss: 0.4046 - acc: 0.870 - ETA: 6s - loss: 0.4062 - acc: 0.870 - ETA: 6s - loss: 0.4086 - acc: 0.869 - ETA: 6s - loss: 0.4037 - acc: 0.870 - ETA: 6s - loss: 0.4008 - acc: 0.872 - ETA: 6s - loss: 0.4036 - acc: 0.870 - ETA: 6s - loss: 0.4033 - acc: 0.869 - ETA: 6s - loss: 0.4076 - acc: 0.869 - ETA: 6s - loss: 0.4071 - acc: 0.869 - ETA: 6s - loss: 0.4035 - acc: 0.871 - ETA: 6s - loss: 0.4047 - acc: 0.871 - ETA: 5s - loss: 0.4072 - acc: 0.870 - ETA: 5s - loss: 0.4041 - acc: 0.870 - ETA: 5s - loss: 0.4038 - acc: 0.869 - ETA: 5s - loss: 0.4033 - acc: 0.869 - ETA: 5s - loss: 0.4044 - acc: 0.868 - ETA: 5s - loss: 0.4091 - acc: 0.867 - ETA: 5s - loss: 0.4094 - acc: 0.867 - ETA: 5s - loss: 0.4087 - acc: 0.867 - ETA: 5s - loss: 0.4107 - acc: 0.866 - ETA: 5s - loss: 0.4186 - acc: 0.865 - ETA: 5s - loss: 0.4170 - acc: 0.865 - ETA: 5s - loss: 0.4147 - acc: 0.866 - ETA: 5s - loss: 0.4171 - acc: 0.866 - ETA: 5s - loss: 0.4173 - acc: 0.866 - ETA: 5s - loss: 0.4125 - acc: 0.867 - ETA: 5s - loss: 0.4149 - acc: 0.868 - ETA: 5s - loss: 0.4156 - acc: 0.866 - ETA: 4s - loss: 0.4117 - acc: 0.867 - ETA: 4s - loss: 0.4114 - acc: 0.868 - ETA: 4s - loss: 0.4077 - acc: 0.869 - ETA: 4s - loss: 0.4103 - acc: 0.868 - ETA: 4s - loss: 0.4142 - acc: 0.867 - ETA: 4s - loss: 0.4127 - acc: 0.867 - ETA: 4s - loss: 0.4122 - acc: 0.867 - ETA: 4s - loss: 0.4125 - acc: 0.866 - ETA: 4s - loss: 0.4124 - acc: 0.867 - ETA: 4s - loss: 0.4124 - acc: 0.866 - ETA: 4s - loss: 0.4112 - acc: 0.867 - ETA: 4s - loss: 0.4116 - acc: 0.866 - ETA: 4s - loss: 0.4104 - acc: 0.867 - ETA: 4s - loss: 0.4093 - acc: 0.867 - ETA: 4s - loss: 0.4075 - acc: 0.868 - ETA: 3s - loss: 0.4081 - acc: 0.868 - ETA: 3s - loss: 0.4104 - acc: 0.867 - ETA: 3s - loss: 0.4093 - acc: 0.867 - ETA: 3s - loss: 0.4117 - acc: 0.867 - ETA: 3s - loss: 0.4130 - acc: 0.867 - ETA: 3s - loss: 0.4113 - acc: 0.868 - ETA: 3s - loss: 0.4112 - acc: 0.867 - ETA: 3s - loss: 0.4115 - acc: 0.866 - ETA: 3s - loss: 0.4114 - acc: 0.866 - ETA: 3s - loss: 0.4104 - acc: 0.867 - ETA: 3s - loss: 0.4099 - acc: 0.867 - ETA: 3s - loss: 0.4054 - acc: 0.868 - ETA: 3s - loss: 0.4057 - acc: 0.868 - ETA: 3s - loss: 0.4046 - acc: 0.869 - ETA: 3s - loss: 0.4050 - acc: 0.869 - ETA: 3s - loss: 0.4052 - acc: 0.868 - ETA: 2s - loss: 0.4049 - acc: 0.868 - ETA: 2s - loss: 0.4070 - acc: 0.868 - ETA: 2s - loss: 0.4083 - acc: 0.868 - ETA: 2s - loss: 0.4085 - acc: 0.868 - ETA: 2s - loss: 0.4101 - acc: 0.868 - ETA: 2s - loss: 0.4091 - acc: 0.869 - ETA: 2s - loss: 0.4123 - acc: 0.869 - ETA: 2s - loss: 0.4148 - acc: 0.869 - ETA: 2s - loss: 0.4164 - acc: 0.868 - ETA: 2s - loss: 0.4172 - acc: 0.867 - ETA: 2s - loss: 0.4161 - acc: 0.867 - ETA: 2s - loss: 0.4170 - acc: 0.867 - ETA: 2s - loss: 0.4192 - acc: 0.866 - ETA: 2s - loss: 0.4196 - acc: 0.866 - ETA: 2s - loss: 0.4194 - acc: 0.866 - ETA: 2s - loss: 0.4217 - acc: 0.866 - ETA: 2s - loss: 0.4239 - acc: 0.865 - ETA: 2s - loss: 0.4220 - acc: 0.866 - ETA: 1s - loss: 0.4214 - acc: 0.867 - ETA: 1s - loss: 0.4206 - acc: 0.866 - ETA: 1s - loss: 0.4182 - acc: 0.867 - ETA: 1s - loss: 0.4210 - acc: 0.867 - ETA: 1s - loss: 0.4212 - acc: 0.867 - ETA: 1s - loss: 0.4198 - acc: 0.868 - ETA: 1s - loss: 0.4199 - acc: 0.868 - ETA: 1s - loss: 0.4234 - acc: 0.867 - ETA: 1s - loss: 0.4232 - acc: 0.867 - ETA: 1s - loss: 0.4234 - acc: 0.867 - ETA: 1s - loss: 0.4234 - acc: 0.867 - ETA: 1s - loss: 0.4236 - acc: 0.866 - ETA: 1s - loss: 0.4253 - acc: 0.865 - ETA: 1s - loss: 0.4242 - acc: 0.866 - ETA: 1s - loss: 0.4246 - acc: 0.866 - ETA: 1s - loss: 0.4234 - acc: 0.866 - ETA: 1s - loss: 0.4213 - acc: 0.867 - ETA: 0s - loss: 0.4220 - acc: 0.866 - ETA: 0s - loss: 0.4244 - acc: 0.866 - ETA: 0s - loss: 0.4237 - acc: 0.866 - ETA: 0s - loss: 0.4236 - acc: 0.866 - ETA: 0s - loss: 0.4234 - acc: 0.866 - ETA: 0s - loss: 0.4238 - acc: 0.865 - ETA: 0s - loss: 0.4253 - acc: 0.865 - ETA: 0s - loss: 0.4280 - acc: 0.864 - ETA: 0s - loss: 0.4264 - acc: 0.865 - ETA: 0s - loss: 0.4262 - acc: 0.865 - ETA: 0s - loss: 0.4249 - acc: 0.865 - ETA: 0s - loss: 0.4238 - acc: 0.865 - ETA: 0s - loss: 0.4232 - acc: 0.865 - ETA: 0s - loss: 0.4245 - acc: 0.865 - ETA: 0s - loss: 0.4238 - acc: 0.865 - ETA: 0s - loss: 0.4246 - acc: 0.865 - ETA: 0s - loss: 0.4231 - acc: 0.8656Epoch 00014: val_loss improved from 0.41440 to 0.41015, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.4238 - acc: 0.8654 - val_loss: 0.4102 - val_acc: 0.8623\n",
      "Epoch 16/20\n",
      "6640/6680 [============================>.] - ETA: 8s - loss: 0.1062 - acc: 0.950 - ETA: 8s - loss: 0.2095 - acc: 0.950 - ETA: 8s - loss: 0.2771 - acc: 0.910 - ETA: 8s - loss: 0.3745 - acc: 0.885 - ETA: 8s - loss: 0.3678 - acc: 0.877 - ETA: 8s - loss: 0.3580 - acc: 0.872 - ETA: 8s - loss: 0.3642 - acc: 0.871 - ETA: 7s - loss: 0.3356 - acc: 0.885 - ETA: 7s - loss: 0.3345 - acc: 0.886 - ETA: 7s - loss: 0.3682 - acc: 0.876 - ETA: 7s - loss: 0.3582 - acc: 0.880 - ETA: 7s - loss: 0.3755 - acc: 0.878 - ETA: 7s - loss: 0.3656 - acc: 0.881 - ETA: 7s - loss: 0.3663 - acc: 0.881 - ETA: 7s - loss: 0.3678 - acc: 0.882 - ETA: 7s - loss: 0.3689 - acc: 0.886 - ETA: 7s - loss: 0.3837 - acc: 0.881 - ETA: 7s - loss: 0.3879 - acc: 0.880 - ETA: 7s - loss: 0.3831 - acc: 0.881 - ETA: 7s - loss: 0.3868 - acc: 0.878 - ETA: 7s - loss: 0.3915 - acc: 0.877 - ETA: 7s - loss: 0.4105 - acc: 0.875 - ETA: 6s - loss: 0.4202 - acc: 0.873 - ETA: 6s - loss: 0.4254 - acc: 0.871 - ETA: 6s - loss: 0.4231 - acc: 0.871 - ETA: 6s - loss: 0.4196 - acc: 0.871 - ETA: 6s - loss: 0.4258 - acc: 0.871 - ETA: 6s - loss: 0.4274 - acc: 0.869 - ETA: 6s - loss: 0.4253 - acc: 0.871 - ETA: 6s - loss: 0.4175 - acc: 0.874 - ETA: 6s - loss: 0.4184 - acc: 0.873 - ETA: 6s - loss: 0.4236 - acc: 0.874 - ETA: 6s - loss: 0.4262 - acc: 0.876 - ETA: 6s - loss: 0.4209 - acc: 0.876 - ETA: 6s - loss: 0.4208 - acc: 0.876 - ETA: 6s - loss: 0.4192 - acc: 0.877 - ETA: 6s - loss: 0.4136 - acc: 0.878 - ETA: 5s - loss: 0.4144 - acc: 0.876 - ETA: 5s - loss: 0.4161 - acc: 0.876 - ETA: 5s - loss: 0.4181 - acc: 0.874 - ETA: 5s - loss: 0.4136 - acc: 0.874 - ETA: 5s - loss: 0.4118 - acc: 0.875 - ETA: 5s - loss: 0.4126 - acc: 0.876 - ETA: 5s - loss: 0.4139 - acc: 0.875 - ETA: 5s - loss: 0.4149 - acc: 0.874 - ETA: 5s - loss: 0.4152 - acc: 0.873 - ETA: 5s - loss: 0.4199 - acc: 0.872 - ETA: 5s - loss: 0.4168 - acc: 0.873 - ETA: 5s - loss: 0.4158 - acc: 0.874 - ETA: 5s - loss: 0.4160 - acc: 0.874 - ETA: 5s - loss: 0.4201 - acc: 0.873 - ETA: 5s - loss: 0.4190 - acc: 0.873 - ETA: 5s - loss: 0.4235 - acc: 0.872 - ETA: 5s - loss: 0.4260 - acc: 0.870 - ETA: 4s - loss: 0.4279 - acc: 0.868 - ETA: 4s - loss: 0.4240 - acc: 0.870 - ETA: 4s - loss: 0.4249 - acc: 0.870 - ETA: 4s - loss: 0.4301 - acc: 0.870 - ETA: 4s - loss: 0.4289 - acc: 0.870 - ETA: 4s - loss: 0.4313 - acc: 0.869 - ETA: 4s - loss: 0.4333 - acc: 0.868 - ETA: 4s - loss: 0.4328 - acc: 0.869 - ETA: 4s - loss: 0.4335 - acc: 0.869 - ETA: 4s - loss: 0.4324 - acc: 0.870 - ETA: 4s - loss: 0.4310 - acc: 0.871 - ETA: 4s - loss: 0.4301 - acc: 0.870 - ETA: 4s - loss: 0.4295 - acc: 0.869 - ETA: 4s - loss: 0.4336 - acc: 0.868 - ETA: 4s - loss: 0.4307 - acc: 0.869 - ETA: 4s - loss: 0.4324 - acc: 0.869 - ETA: 3s - loss: 0.4337 - acc: 0.868 - ETA: 3s - loss: 0.4348 - acc: 0.868 - ETA: 3s - loss: 0.4339 - acc: 0.868 - ETA: 3s - loss: 0.4331 - acc: 0.867 - ETA: 3s - loss: 0.4378 - acc: 0.865 - ETA: 3s - loss: 0.4357 - acc: 0.866 - ETA: 3s - loss: 0.4358 - acc: 0.866 - ETA: 3s - loss: 0.4372 - acc: 0.865 - ETA: 3s - loss: 0.4377 - acc: 0.864 - ETA: 3s - loss: 0.4376 - acc: 0.864 - ETA: 3s - loss: 0.4393 - acc: 0.864 - ETA: 3s - loss: 0.4382 - acc: 0.865 - ETA: 3s - loss: 0.4413 - acc: 0.863 - ETA: 3s - loss: 0.4418 - acc: 0.862 - ETA: 3s - loss: 0.4429 - acc: 0.862 - ETA: 3s - loss: 0.4425 - acc: 0.862 - ETA: 3s - loss: 0.4419 - acc: 0.863 - ETA: 2s - loss: 0.4405 - acc: 0.863 - ETA: 2s - loss: 0.4405 - acc: 0.863 - ETA: 2s - loss: 0.4391 - acc: 0.863 - ETA: 2s - loss: 0.4365 - acc: 0.864 - ETA: 2s - loss: 0.4357 - acc: 0.864 - ETA: 2s - loss: 0.4355 - acc: 0.864 - ETA: 2s - loss: 0.4344 - acc: 0.864 - ETA: 2s - loss: 0.4339 - acc: 0.864 - ETA: 2s - loss: 0.4323 - acc: 0.865 - ETA: 2s - loss: 0.4313 - acc: 0.865 - ETA: 2s - loss: 0.4313 - acc: 0.865 - ETA: 2s - loss: 0.4306 - acc: 0.865 - ETA: 2s - loss: 0.4295 - acc: 0.866 - ETA: 2s - loss: 0.4297 - acc: 0.866 - ETA: 2s - loss: 0.4281 - acc: 0.866 - ETA: 1s - loss: 0.4281 - acc: 0.866 - ETA: 1s - loss: 0.4279 - acc: 0.866 - ETA: 1s - loss: 0.4280 - acc: 0.866 - ETA: 1s - loss: 0.4298 - acc: 0.866 - ETA: 1s - loss: 0.4289 - acc: 0.866 - ETA: 1s - loss: 0.4276 - acc: 0.866 - ETA: 1s - loss: 0.4261 - acc: 0.867 - ETA: 1s - loss: 0.4247 - acc: 0.867 - ETA: 1s - loss: 0.4260 - acc: 0.866 - ETA: 1s - loss: 0.4264 - acc: 0.866 - ETA: 1s - loss: 0.4255 - acc: 0.867 - ETA: 1s - loss: 0.4267 - acc: 0.866 - ETA: 1s - loss: 0.4290 - acc: 0.866 - ETA: 1s - loss: 0.4288 - acc: 0.865 - ETA: 1s - loss: 0.4299 - acc: 0.865 - ETA: 1s - loss: 0.4299 - acc: 0.865 - ETA: 0s - loss: 0.4306 - acc: 0.865 - ETA: 0s - loss: 0.4282 - acc: 0.866 - ETA: 0s - loss: 0.4283 - acc: 0.866 - ETA: 0s - loss: 0.4284 - acc: 0.865 - ETA: 0s - loss: 0.4279 - acc: 0.866 - ETA: 0s - loss: 0.4287 - acc: 0.866 - ETA: 0s - loss: 0.4283 - acc: 0.866 - ETA: 0s - loss: 0.4268 - acc: 0.866 - ETA: 0s - loss: 0.4271 - acc: 0.865 - ETA: 0s - loss: 0.4257 - acc: 0.866 - ETA: 0s - loss: 0.4260 - acc: 0.866 - ETA: 0s - loss: 0.4254 - acc: 0.866 - ETA: 0s - loss: 0.4251 - acc: 0.866 - ETA: 0s - loss: 0.4238 - acc: 0.866 - ETA: 0s - loss: 0.4228 - acc: 0.867 - ETA: 0s - loss: 0.4239 - acc: 0.866 - ETA: 0s - loss: 0.4232 - acc: 0.866 - ETA: 0s - loss: 0.4219 - acc: 0.867 - ETA: 0s - loss: 0.4244 - acc: 0.8670Epoch 00015: val_loss improved from 0.41015 to 0.40445, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.4238 - acc: 0.8672 - val_loss: 0.4045 - val_acc: 0.8695\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 9s - loss: 0.2355 - acc: 0.850 - ETA: 9s - loss: 0.5121 - acc: 0.850 - ETA: 9s - loss: 0.4036 - acc: 0.880 - ETA: 8s - loss: 0.4154 - acc: 0.878 - ETA: 8s - loss: 0.3691 - acc: 0.883 - ETA: 8s - loss: 0.3689 - acc: 0.881 - ETA: 8s - loss: 0.3734 - acc: 0.880 - ETA: 8s - loss: 0.3825 - acc: 0.880 - ETA: 8s - loss: 0.4044 - acc: 0.870 - ETA: 8s - loss: 0.4086 - acc: 0.865 - ETA: 7s - loss: 0.3809 - acc: 0.875 - ETA: 7s - loss: 0.3971 - acc: 0.872 - ETA: 7s - loss: 0.3989 - acc: 0.873 - ETA: 7s - loss: 0.3924 - acc: 0.874 - ETA: 7s - loss: 0.3880 - acc: 0.878 - ETA: 7s - loss: 0.3889 - acc: 0.876 - ETA: 7s - loss: 0.3874 - acc: 0.879 - ETA: 7s - loss: 0.3864 - acc: 0.881 - ETA: 7s - loss: 0.3872 - acc: 0.881 - ETA: 7s - loss: 0.3791 - acc: 0.882 - ETA: 7s - loss: 0.3715 - acc: 0.885 - ETA: 7s - loss: 0.3706 - acc: 0.884 - ETA: 7s - loss: 0.3721 - acc: 0.885 - ETA: 7s - loss: 0.3757 - acc: 0.883 - ETA: 6s - loss: 0.3787 - acc: 0.879 - ETA: 6s - loss: 0.3800 - acc: 0.879 - ETA: 6s - loss: 0.3816 - acc: 0.879 - ETA: 6s - loss: 0.3758 - acc: 0.880 - ETA: 6s - loss: 0.3735 - acc: 0.880 - ETA: 6s - loss: 0.3887 - acc: 0.876 - ETA: 6s - loss: 0.3863 - acc: 0.877 - ETA: 6s - loss: 0.3844 - acc: 0.877 - ETA: 6s - loss: 0.3926 - acc: 0.875 - ETA: 6s - loss: 0.3985 - acc: 0.874 - ETA: 6s - loss: 0.3997 - acc: 0.873 - ETA: 6s - loss: 0.4055 - acc: 0.872 - ETA: 5s - loss: 0.4038 - acc: 0.872 - ETA: 5s - loss: 0.4015 - acc: 0.874 - ETA: 5s - loss: 0.4025 - acc: 0.873 - ETA: 5s - loss: 0.4029 - acc: 0.874 - ETA: 5s - loss: 0.4031 - acc: 0.874 - ETA: 5s - loss: 0.3981 - acc: 0.875 - ETA: 5s - loss: 0.3959 - acc: 0.876 - ETA: 5s - loss: 0.3980 - acc: 0.875 - ETA: 5s - loss: 0.3928 - acc: 0.876 - ETA: 5s - loss: 0.3908 - acc: 0.878 - ETA: 5s - loss: 0.3955 - acc: 0.878 - ETA: 5s - loss: 0.3942 - acc: 0.878 - ETA: 5s - loss: 0.3945 - acc: 0.878 - ETA: 5s - loss: 0.3907 - acc: 0.880 - ETA: 5s - loss: 0.3906 - acc: 0.880 - ETA: 5s - loss: 0.3899 - acc: 0.880 - ETA: 5s - loss: 0.3916 - acc: 0.878 - ETA: 5s - loss: 0.3879 - acc: 0.879 - ETA: 4s - loss: 0.3846 - acc: 0.881 - ETA: 4s - loss: 0.3824 - acc: 0.881 - ETA: 4s - loss: 0.3796 - acc: 0.881 - ETA: 4s - loss: 0.3797 - acc: 0.881 - ETA: 4s - loss: 0.3780 - acc: 0.880 - ETA: 4s - loss: 0.3795 - acc: 0.881 - ETA: 4s - loss: 0.3778 - acc: 0.881 - ETA: 4s - loss: 0.3779 - acc: 0.881 - ETA: 4s - loss: 0.3825 - acc: 0.880 - ETA: 4s - loss: 0.3831 - acc: 0.879 - ETA: 4s - loss: 0.3861 - acc: 0.879 - ETA: 4s - loss: 0.3861 - acc: 0.879 - ETA: 4s - loss: 0.3876 - acc: 0.878 - ETA: 4s - loss: 0.3852 - acc: 0.879 - ETA: 4s - loss: 0.3835 - acc: 0.880 - ETA: 4s - loss: 0.3842 - acc: 0.880 - ETA: 4s - loss: 0.3865 - acc: 0.879 - ETA: 4s - loss: 0.3859 - acc: 0.879 - ETA: 3s - loss: 0.3865 - acc: 0.879 - ETA: 3s - loss: 0.3854 - acc: 0.879 - ETA: 3s - loss: 0.3836 - acc: 0.878 - ETA: 3s - loss: 0.3869 - acc: 0.879 - ETA: 3s - loss: 0.3860 - acc: 0.878 - ETA: 3s - loss: 0.3845 - acc: 0.878 - ETA: 3s - loss: 0.3856 - acc: 0.878 - ETA: 3s - loss: 0.3882 - acc: 0.878 - ETA: 3s - loss: 0.3898 - acc: 0.878 - ETA: 3s - loss: 0.3895 - acc: 0.878 - ETA: 3s - loss: 0.3906 - acc: 0.877 - ETA: 3s - loss: 0.3899 - acc: 0.878 - ETA: 3s - loss: 0.3890 - acc: 0.878 - ETA: 3s - loss: 0.3871 - acc: 0.878 - ETA: 2s - loss: 0.3876 - acc: 0.878 - ETA: 2s - loss: 0.3869 - acc: 0.878 - ETA: 2s - loss: 0.3879 - acc: 0.877 - ETA: 2s - loss: 0.3904 - acc: 0.876 - ETA: 2s - loss: 0.3949 - acc: 0.875 - ETA: 2s - loss: 0.3931 - acc: 0.876 - ETA: 2s - loss: 0.3925 - acc: 0.876 - ETA: 2s - loss: 0.3940 - acc: 0.876 - ETA: 2s - loss: 0.3981 - acc: 0.874 - ETA: 2s - loss: 0.3979 - acc: 0.874 - ETA: 2s - loss: 0.3997 - acc: 0.873 - ETA: 2s - loss: 0.4016 - acc: 0.873 - ETA: 2s - loss: 0.4010 - acc: 0.873 - ETA: 2s - loss: 0.4002 - acc: 0.873 - ETA: 2s - loss: 0.4001 - acc: 0.872 - ETA: 2s - loss: 0.4012 - acc: 0.872 - ETA: 2s - loss: 0.4023 - acc: 0.871 - ETA: 2s - loss: 0.4013 - acc: 0.871 - ETA: 1s - loss: 0.4037 - acc: 0.870 - ETA: 1s - loss: 0.4026 - acc: 0.870 - ETA: 1s - loss: 0.4031 - acc: 0.870 - ETA: 1s - loss: 0.4052 - acc: 0.868 - ETA: 1s - loss: 0.4040 - acc: 0.869 - ETA: 1s - loss: 0.4051 - acc: 0.868 - ETA: 1s - loss: 0.4070 - acc: 0.868 - ETA: 1s - loss: 0.4075 - acc: 0.867 - ETA: 1s - loss: 0.4073 - acc: 0.867 - ETA: 1s - loss: 0.4078 - acc: 0.867 - ETA: 1s - loss: 0.4082 - acc: 0.868 - ETA: 1s - loss: 0.4096 - acc: 0.867 - ETA: 1s - loss: 0.4092 - acc: 0.867 - ETA: 1s - loss: 0.4094 - acc: 0.867 - ETA: 1s - loss: 0.4091 - acc: 0.866 - ETA: 1s - loss: 0.4098 - acc: 0.867 - ETA: 0s - loss: 0.4071 - acc: 0.868 - ETA: 0s - loss: 0.4067 - acc: 0.868 - ETA: 0s - loss: 0.4067 - acc: 0.867 - ETA: 0s - loss: 0.4064 - acc: 0.867 - ETA: 0s - loss: 0.4088 - acc: 0.866 - ETA: 0s - loss: 0.4104 - acc: 0.866 - ETA: 0s - loss: 0.4119 - acc: 0.866 - ETA: 0s - loss: 0.4104 - acc: 0.866 - ETA: 0s - loss: 0.4102 - acc: 0.866 - ETA: 0s - loss: 0.4092 - acc: 0.867 - ETA: 0s - loss: 0.4086 - acc: 0.867 - ETA: 0s - loss: 0.4088 - acc: 0.867 - ETA: 0s - loss: 0.4078 - acc: 0.867 - ETA: 0s - loss: 0.4067 - acc: 0.868 - ETA: 0s - loss: 0.4073 - acc: 0.867 - ETA: 0s - loss: 0.4067 - acc: 0.868 - ETA: 0s - loss: 0.4078 - acc: 0.868 - ETA: 0s - loss: 0.4084 - acc: 0.8682Epoch 00016: val_loss improved from 0.40445 to 0.38925, saving model to saved_models/weights.best.Xception_Resnet50.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 0.4098 - acc: 0.8677 - val_loss: 0.3892 - val_acc: 0.8814\n",
      "Epoch 18/20\n",
      "6660/6680 [============================>.] - ETA: 9s - loss: 0.1166 - acc: 0.950 - ETA: 9s - loss: 0.2785 - acc: 0.933 - ETA: 9s - loss: 0.2300 - acc: 0.950 - ETA: 9s - loss: 0.3200 - acc: 0.921 - ETA: 8s - loss: 0.3304 - acc: 0.905 - ETA: 8s - loss: 0.3238 - acc: 0.900 - ETA: 8s - loss: 0.3377 - acc: 0.896 - ETA: 7s - loss: 0.3562 - acc: 0.891 - ETA: 7s - loss: 0.3628 - acc: 0.890 - ETA: 7s - loss: 0.3541 - acc: 0.886 - ETA: 7s - loss: 0.3546 - acc: 0.884 - ETA: 7s - loss: 0.3520 - acc: 0.885 - ETA: 7s - loss: 0.3404 - acc: 0.890 - ETA: 7s - loss: 0.3403 - acc: 0.890 - ETA: 7s - loss: 0.3506 - acc: 0.890 - ETA: 7s - loss: 0.3470 - acc: 0.891 - ETA: 7s - loss: 0.3451 - acc: 0.892 - ETA: 7s - loss: 0.3438 - acc: 0.893 - ETA: 7s - loss: 0.3419 - acc: 0.894 - ETA: 7s - loss: 0.3509 - acc: 0.893 - ETA: 7s - loss: 0.3475 - acc: 0.894 - ETA: 7s - loss: 0.3519 - acc: 0.893 - ETA: 7s - loss: 0.3538 - acc: 0.889 - ETA: 7s - loss: 0.3655 - acc: 0.888 - ETA: 6s - loss: 0.3612 - acc: 0.888 - ETA: 6s - loss: 0.3575 - acc: 0.890 - ETA: 6s - loss: 0.3620 - acc: 0.890 - ETA: 6s - loss: 0.3671 - acc: 0.889 - ETA: 6s - loss: 0.3666 - acc: 0.888 - ETA: 6s - loss: 0.3648 - acc: 0.889 - ETA: 6s - loss: 0.3665 - acc: 0.889 - ETA: 6s - loss: 0.3643 - acc: 0.888 - ETA: 6s - loss: 0.3643 - acc: 0.887 - ETA: 6s - loss: 0.3614 - acc: 0.888 - ETA: 6s - loss: 0.3569 - acc: 0.890 - ETA: 6s - loss: 0.3562 - acc: 0.887 - ETA: 6s - loss: 0.3548 - acc: 0.887 - ETA: 5s - loss: 0.3610 - acc: 0.886 - ETA: 5s - loss: 0.3679 - acc: 0.883 - ETA: 5s - loss: 0.3721 - acc: 0.881 - ETA: 5s - loss: 0.3718 - acc: 0.882 - ETA: 5s - loss: 0.3669 - acc: 0.883 - ETA: 5s - loss: 0.3739 - acc: 0.883 - ETA: 5s - loss: 0.3782 - acc: 0.882 - ETA: 5s - loss: 0.3757 - acc: 0.883 - ETA: 5s - loss: 0.3772 - acc: 0.883 - ETA: 5s - loss: 0.3762 - acc: 0.883 - ETA: 5s - loss: 0.3781 - acc: 0.883 - ETA: 5s - loss: 0.3837 - acc: 0.882 - ETA: 5s - loss: 0.3883 - acc: 0.882 - ETA: 5s - loss: 0.3911 - acc: 0.881 - ETA: 5s - loss: 0.3957 - acc: 0.878 - ETA: 5s - loss: 0.3971 - acc: 0.877 - ETA: 4s - loss: 0.3965 - acc: 0.878 - ETA: 4s - loss: 0.3971 - acc: 0.878 - ETA: 4s - loss: 0.3977 - acc: 0.877 - ETA: 4s - loss: 0.3998 - acc: 0.877 - ETA: 4s - loss: 0.3995 - acc: 0.876 - ETA: 4s - loss: 0.4031 - acc: 0.875 - ETA: 4s - loss: 0.4024 - acc: 0.875 - ETA: 4s - loss: 0.4012 - acc: 0.875 - ETA: 4s - loss: 0.4017 - acc: 0.875 - ETA: 4s - loss: 0.3987 - acc: 0.875 - ETA: 4s - loss: 0.4000 - acc: 0.875 - ETA: 4s - loss: 0.4012 - acc: 0.875 - ETA: 4s - loss: 0.4012 - acc: 0.875 - ETA: 4s - loss: 0.4005 - acc: 0.875 - ETA: 4s - loss: 0.4014 - acc: 0.875 - ETA: 4s - loss: 0.4018 - acc: 0.875 - ETA: 4s - loss: 0.4027 - acc: 0.874 - ETA: 3s - loss: 0.4039 - acc: 0.873 - ETA: 3s - loss: 0.4024 - acc: 0.873 - ETA: 3s - loss: 0.4005 - acc: 0.874 - ETA: 3s - loss: 0.3983 - acc: 0.875 - ETA: 3s - loss: 0.3970 - acc: 0.875 - ETA: 3s - loss: 0.3995 - acc: 0.875 - ETA: 3s - loss: 0.4006 - acc: 0.874 - ETA: 3s - loss: 0.4020 - acc: 0.874 - ETA: 3s - loss: 0.3998 - acc: 0.875 - ETA: 3s - loss: 0.3982 - acc: 0.876 - ETA: 3s - loss: 0.3966 - acc: 0.876 - ETA: 3s - loss: 0.3945 - acc: 0.876 - ETA: 3s - loss: 0.3938 - acc: 0.876 - ETA: 3s - loss: 0.3933 - acc: 0.876 - ETA: 3s - loss: 0.3921 - acc: 0.876 - ETA: 3s - loss: 0.3934 - acc: 0.875 - ETA: 3s - loss: 0.3911 - acc: 0.875 - ETA: 3s - loss: 0.3926 - acc: 0.875 - ETA: 2s - loss: 0.3940 - acc: 0.875 - ETA: 2s - loss: 0.3969 - acc: 0.875 - ETA: 2s - loss: 0.3967 - acc: 0.875 - ETA: 2s - loss: 0.3957 - acc: 0.875 - ETA: 2s - loss: 0.3972 - acc: 0.875 - ETA: 2s - loss: 0.3988 - acc: 0.874 - ETA: 2s - loss: 0.3975 - acc: 0.874 - ETA: 2s - loss: 0.3977 - acc: 0.874 - ETA: 2s - loss: 0.3979 - acc: 0.874 - ETA: 2s - loss: 0.3963 - acc: 0.874 - ETA: 2s - loss: 0.3964 - acc: 0.874 - ETA: 2s - loss: 0.3960 - acc: 0.874 - ETA: 2s - loss: 0.3956 - acc: 0.874 - ETA: 2s - loss: 0.3967 - acc: 0.874 - ETA: 2s - loss: 0.3969 - acc: 0.874 - ETA: 1s - loss: 0.3950 - acc: 0.875 - ETA: 1s - loss: 0.3962 - acc: 0.875 - ETA: 1s - loss: 0.3947 - acc: 0.875 - ETA: 1s - loss: 0.3946 - acc: 0.875 - ETA: 1s - loss: 0.3951 - acc: 0.876 - ETA: 1s - loss: 0.3940 - acc: 0.876 - ETA: 1s - loss: 0.3929 - acc: 0.876 - ETA: 1s - loss: 0.3954 - acc: 0.875 - ETA: 1s - loss: 0.3954 - acc: 0.875 - ETA: 1s - loss: 0.3970 - acc: 0.874 - ETA: 1s - loss: 0.3968 - acc: 0.874 - ETA: 1s - loss: 0.3962 - acc: 0.875 - ETA: 1s - loss: 0.3952 - acc: 0.875 - ETA: 1s - loss: 0.3952 - acc: 0.875 - ETA: 1s - loss: 0.3956 - acc: 0.875 - ETA: 1s - loss: 0.3965 - acc: 0.875 - ETA: 1s - loss: 0.3971 - acc: 0.875 - ETA: 1s - loss: 0.3966 - acc: 0.875 - ETA: 0s - loss: 0.3971 - acc: 0.875 - ETA: 0s - loss: 0.3976 - acc: 0.875 - ETA: 0s - loss: 0.3980 - acc: 0.875 - ETA: 0s - loss: 0.3969 - acc: 0.875 - ETA: 0s - loss: 0.3980 - acc: 0.875 - ETA: 0s - loss: 0.3997 - acc: 0.875 - ETA: 0s - loss: 0.3990 - acc: 0.875 - ETA: 0s - loss: 0.3988 - acc: 0.875 - ETA: 0s - loss: 0.3979 - acc: 0.875 - ETA: 0s - loss: 0.3981 - acc: 0.875 - ETA: 0s - loss: 0.3988 - acc: 0.875 - ETA: 0s - loss: 0.3992 - acc: 0.875 - ETA: 0s - loss: 0.3974 - acc: 0.875 - ETA: 0s - loss: 0.3967 - acc: 0.875 - ETA: 0s - loss: 0.3972 - acc: 0.875 - ETA: 0s - loss: 0.3965 - acc: 0.8757Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 0.3955 - acc: 0.8760 - val_loss: 0.4056 - val_acc: 0.8754\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.1122 - acc: 0.950 - ETA: 8s - loss: 0.1797 - acc: 0.933 - ETA: 8s - loss: 0.2268 - acc: 0.920 - ETA: 8s - loss: 0.2909 - acc: 0.892 - ETA: 8s - loss: 0.3245 - acc: 0.872 - ETA: 8s - loss: 0.3683 - acc: 0.863 - ETA: 8s - loss: 0.3458 - acc: 0.875 - ETA: 7s - loss: 0.3342 - acc: 0.885 - ETA: 7s - loss: 0.3350 - acc: 0.892 - ETA: 7s - loss: 0.3413 - acc: 0.887 - ETA: 7s - loss: 0.3272 - acc: 0.896 - ETA: 7s - loss: 0.3375 - acc: 0.894 - ETA: 7s - loss: 0.3445 - acc: 0.887 - ETA: 7s - loss: 0.3353 - acc: 0.891 - ETA: 7s - loss: 0.3332 - acc: 0.895 - ETA: 6s - loss: 0.3330 - acc: 0.896 - ETA: 6s - loss: 0.3312 - acc: 0.895 - ETA: 6s - loss: 0.3341 - acc: 0.891 - ETA: 6s - loss: 0.3330 - acc: 0.891 - ETA: 6s - loss: 0.3425 - acc: 0.886 - ETA: 6s - loss: 0.3398 - acc: 0.888 - ETA: 6s - loss: 0.3554 - acc: 0.886 - ETA: 6s - loss: 0.3501 - acc: 0.887 - ETA: 6s - loss: 0.3505 - acc: 0.886 - ETA: 6s - loss: 0.3518 - acc: 0.885 - ETA: 6s - loss: 0.3535 - acc: 0.883 - ETA: 6s - loss: 0.3639 - acc: 0.883 - ETA: 6s - loss: 0.3572 - acc: 0.885 - ETA: 6s - loss: 0.3589 - acc: 0.885 - ETA: 6s - loss: 0.3520 - acc: 0.888 - ETA: 6s - loss: 0.3548 - acc: 0.887 - ETA: 6s - loss: 0.3499 - acc: 0.888 - ETA: 6s - loss: 0.3528 - acc: 0.887 - ETA: 6s - loss: 0.3501 - acc: 0.888 - ETA: 6s - loss: 0.3558 - acc: 0.886 - ETA: 6s - loss: 0.3653 - acc: 0.883 - ETA: 6s - loss: 0.3640 - acc: 0.883 - ETA: 5s - loss: 0.3628 - acc: 0.884 - ETA: 5s - loss: 0.3640 - acc: 0.882 - ETA: 5s - loss: 0.3723 - acc: 0.878 - ETA: 5s - loss: 0.3743 - acc: 0.878 - ETA: 5s - loss: 0.3746 - acc: 0.878 - ETA: 5s - loss: 0.3746 - acc: 0.878 - ETA: 5s - loss: 0.3788 - acc: 0.877 - ETA: 5s - loss: 0.3798 - acc: 0.877 - ETA: 5s - loss: 0.3829 - acc: 0.875 - ETA: 5s - loss: 0.3797 - acc: 0.876 - ETA: 5s - loss: 0.3795 - acc: 0.876 - ETA: 5s - loss: 0.3778 - acc: 0.876 - ETA: 5s - loss: 0.3781 - acc: 0.876 - ETA: 5s - loss: 0.3763 - acc: 0.876 - ETA: 5s - loss: 0.3742 - acc: 0.877 - ETA: 5s - loss: 0.3733 - acc: 0.878 - ETA: 5s - loss: 0.3727 - acc: 0.878 - ETA: 5s - loss: 0.3740 - acc: 0.877 - ETA: 5s - loss: 0.3739 - acc: 0.877 - ETA: 4s - loss: 0.3772 - acc: 0.877 - ETA: 4s - loss: 0.3747 - acc: 0.877 - ETA: 4s - loss: 0.3753 - acc: 0.878 - ETA: 4s - loss: 0.3744 - acc: 0.878 - ETA: 4s - loss: 0.3734 - acc: 0.879 - ETA: 4s - loss: 0.3747 - acc: 0.877 - ETA: 4s - loss: 0.3736 - acc: 0.877 - ETA: 4s - loss: 0.3755 - acc: 0.877 - ETA: 4s - loss: 0.3754 - acc: 0.877 - ETA: 4s - loss: 0.3752 - acc: 0.877 - ETA: 4s - loss: 0.3745 - acc: 0.877 - ETA: 4s - loss: 0.3761 - acc: 0.876 - ETA: 4s - loss: 0.3737 - acc: 0.877 - ETA: 4s - loss: 0.3758 - acc: 0.877 - ETA: 4s - loss: 0.3772 - acc: 0.876 - ETA: 4s - loss: 0.3767 - acc: 0.876 - ETA: 3s - loss: 0.3789 - acc: 0.875 - ETA: 3s - loss: 0.3790 - acc: 0.875 - ETA: 3s - loss: 0.3772 - acc: 0.875 - ETA: 3s - loss: 0.3775 - acc: 0.875 - ETA: 3s - loss: 0.3815 - acc: 0.874 - ETA: 3s - loss: 0.3816 - acc: 0.875 - ETA: 3s - loss: 0.3800 - acc: 0.875 - ETA: 3s - loss: 0.3775 - acc: 0.876 - ETA: 3s - loss: 0.3774 - acc: 0.875 - ETA: 3s - loss: 0.3779 - acc: 0.875 - ETA: 3s - loss: 0.3817 - acc: 0.875 - ETA: 3s - loss: 0.3823 - acc: 0.875 - ETA: 3s - loss: 0.3837 - acc: 0.874 - ETA: 3s - loss: 0.3852 - acc: 0.873 - ETA: 3s - loss: 0.3850 - acc: 0.874 - ETA: 3s - loss: 0.3858 - acc: 0.873 - ETA: 3s - loss: 0.3852 - acc: 0.873 - ETA: 3s - loss: 0.3844 - acc: 0.874 - ETA: 2s - loss: 0.3839 - acc: 0.874 - ETA: 2s - loss: 0.3850 - acc: 0.874 - ETA: 2s - loss: 0.3858 - acc: 0.874 - ETA: 2s - loss: 0.3864 - acc: 0.874 - ETA: 2s - loss: 0.3854 - acc: 0.874 - ETA: 2s - loss: 0.3843 - acc: 0.874 - ETA: 2s - loss: 0.3844 - acc: 0.875 - ETA: 2s - loss: 0.3837 - acc: 0.876 - ETA: 2s - loss: 0.3847 - acc: 0.875 - ETA: 2s - loss: 0.3843 - acc: 0.876 - ETA: 2s - loss: 0.3849 - acc: 0.876 - ETA: 2s - loss: 0.3849 - acc: 0.876 - ETA: 2s - loss: 0.3848 - acc: 0.876 - ETA: 2s - loss: 0.3859 - acc: 0.875 - ETA: 2s - loss: 0.3857 - acc: 0.876 - ETA: 2s - loss: 0.3864 - acc: 0.875 - ETA: 2s - loss: 0.3871 - acc: 0.875 - ETA: 2s - loss: 0.3869 - acc: 0.875 - ETA: 2s - loss: 0.3875 - acc: 0.875 - ETA: 1s - loss: 0.3871 - acc: 0.875 - ETA: 1s - loss: 0.3869 - acc: 0.875 - ETA: 1s - loss: 0.3879 - acc: 0.875 - ETA: 1s - loss: 0.3895 - acc: 0.874 - ETA: 1s - loss: 0.3912 - acc: 0.874 - ETA: 1s - loss: 0.3907 - acc: 0.874 - ETA: 1s - loss: 0.3918 - acc: 0.873 - ETA: 1s - loss: 0.3929 - acc: 0.873 - ETA: 1s - loss: 0.3916 - acc: 0.873 - ETA: 1s - loss: 0.3929 - acc: 0.873 - ETA: 1s - loss: 0.3945 - acc: 0.872 - ETA: 1s - loss: 0.3933 - acc: 0.873 - ETA: 1s - loss: 0.3933 - acc: 0.873 - ETA: 1s - loss: 0.3930 - acc: 0.873 - ETA: 1s - loss: 0.3921 - acc: 0.873 - ETA: 1s - loss: 0.3918 - acc: 0.873 - ETA: 1s - loss: 0.3921 - acc: 0.873 - ETA: 1s - loss: 0.3907 - acc: 0.873 - ETA: 0s - loss: 0.3896 - acc: 0.873 - ETA: 0s - loss: 0.3905 - acc: 0.873 - ETA: 0s - loss: 0.3889 - acc: 0.873 - ETA: 0s - loss: 0.3890 - acc: 0.873 - ETA: 0s - loss: 0.3901 - acc: 0.872 - ETA: 0s - loss: 0.3899 - acc: 0.872 - ETA: 0s - loss: 0.3910 - acc: 0.872 - ETA: 0s - loss: 0.3907 - acc: 0.872 - ETA: 0s - loss: 0.3901 - acc: 0.872 - ETA: 0s - loss: 0.3908 - acc: 0.872 - ETA: 0s - loss: 0.3903 - acc: 0.872 - ETA: 0s - loss: 0.3912 - acc: 0.871 - ETA: 0s - loss: 0.3904 - acc: 0.872 - ETA: 0s - loss: 0.3904 - acc: 0.872 - ETA: 0s - loss: 0.3909 - acc: 0.872 - ETA: 0s - loss: 0.3895 - acc: 0.872 - ETA: 0s - loss: 0.3893 - acc: 0.872 - ETA: 0s - loss: 0.3891 - acc: 0.8723Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 0.3878 - acc: 0.8728 - val_loss: 0.4010 - val_acc: 0.8802\n",
      "Epoch 20/20\n",
      "6640/6680 [============================>.] - ETA: 7s - loss: 0.4348 - acc: 0.850 - ETA: 8s - loss: 0.3992 - acc: 0.850 - ETA: 7s - loss: 0.4065 - acc: 0.850 - ETA: 7s - loss: 0.3811 - acc: 0.855 - ETA: 7s - loss: 0.3378 - acc: 0.879 - ETA: 7s - loss: 0.3371 - acc: 0.885 - ETA: 7s - loss: 0.3466 - acc: 0.881 - ETA: 7s - loss: 0.3453 - acc: 0.880 - ETA: 7s - loss: 0.3456 - acc: 0.880 - ETA: 7s - loss: 0.3494 - acc: 0.879 - ETA: 7s - loss: 0.3417 - acc: 0.879 - ETA: 7s - loss: 0.3415 - acc: 0.882 - ETA: 7s - loss: 0.3508 - acc: 0.882 - ETA: 7s - loss: 0.3496 - acc: 0.882 - ETA: 7s - loss: 0.3549 - acc: 0.878 - ETA: 7s - loss: 0.3546 - acc: 0.876 - ETA: 7s - loss: 0.3570 - acc: 0.877 - ETA: 7s - loss: 0.3739 - acc: 0.872 - ETA: 7s - loss: 0.3902 - acc: 0.865 - ETA: 6s - loss: 0.3754 - acc: 0.871 - ETA: 6s - loss: 0.3718 - acc: 0.870 - ETA: 6s - loss: 0.3716 - acc: 0.871 - ETA: 6s - loss: 0.3730 - acc: 0.873 - ETA: 6s - loss: 0.3715 - acc: 0.872 - ETA: 6s - loss: 0.3774 - acc: 0.869 - ETA: 6s - loss: 0.3730 - acc: 0.871 - ETA: 6s - loss: 0.3720 - acc: 0.871 - ETA: 6s - loss: 0.3665 - acc: 0.874 - ETA: 6s - loss: 0.3618 - acc: 0.875 - ETA: 6s - loss: 0.3602 - acc: 0.873 - ETA: 6s - loss: 0.3718 - acc: 0.872 - ETA: 6s - loss: 0.3728 - acc: 0.870 - ETA: 6s - loss: 0.3747 - acc: 0.871 - ETA: 6s - loss: 0.3770 - acc: 0.870 - ETA: 6s - loss: 0.3755 - acc: 0.871 - ETA: 6s - loss: 0.3713 - acc: 0.872 - ETA: 5s - loss: 0.3675 - acc: 0.873 - ETA: 5s - loss: 0.3644 - acc: 0.875 - ETA: 5s - loss: 0.3640 - acc: 0.875 - ETA: 5s - loss: 0.3609 - acc: 0.876 - ETA: 5s - loss: 0.3583 - acc: 0.877 - ETA: 5s - loss: 0.3558 - acc: 0.878 - ETA: 5s - loss: 0.3489 - acc: 0.881 - ETA: 5s - loss: 0.3484 - acc: 0.882 - ETA: 5s - loss: 0.3509 - acc: 0.882 - ETA: 5s - loss: 0.3485 - acc: 0.883 - ETA: 5s - loss: 0.3509 - acc: 0.881 - ETA: 5s - loss: 0.3524 - acc: 0.881 - ETA: 5s - loss: 0.3499 - acc: 0.881 - ETA: 5s - loss: 0.3499 - acc: 0.882 - ETA: 5s - loss: 0.3468 - acc: 0.883 - ETA: 5s - loss: 0.3501 - acc: 0.883 - ETA: 4s - loss: 0.3506 - acc: 0.883 - ETA: 4s - loss: 0.3527 - acc: 0.882 - ETA: 4s - loss: 0.3521 - acc: 0.883 - ETA: 4s - loss: 0.3519 - acc: 0.884 - ETA: 4s - loss: 0.3568 - acc: 0.881 - ETA: 4s - loss: 0.3658 - acc: 0.879 - ETA: 4s - loss: 0.3694 - acc: 0.877 - ETA: 4s - loss: 0.3694 - acc: 0.877 - ETA: 4s - loss: 0.3685 - acc: 0.878 - ETA: 4s - loss: 0.3678 - acc: 0.878 - ETA: 4s - loss: 0.3643 - acc: 0.879 - ETA: 4s - loss: 0.3708 - acc: 0.878 - ETA: 4s - loss: 0.3701 - acc: 0.879 - ETA: 4s - loss: 0.3687 - acc: 0.879 - ETA: 4s - loss: 0.3690 - acc: 0.879 - ETA: 3s - loss: 0.3693 - acc: 0.879 - ETA: 3s - loss: 0.3680 - acc: 0.880 - ETA: 3s - loss: 0.3739 - acc: 0.878 - ETA: 3s - loss: 0.3727 - acc: 0.878 - ETA: 3s - loss: 0.3725 - acc: 0.878 - ETA: 3s - loss: 0.3733 - acc: 0.878 - ETA: 3s - loss: 0.3768 - acc: 0.878 - ETA: 3s - loss: 0.3752 - acc: 0.878 - ETA: 3s - loss: 0.3759 - acc: 0.877 - ETA: 3s - loss: 0.3747 - acc: 0.877 - ETA: 3s - loss: 0.3724 - acc: 0.879 - ETA: 3s - loss: 0.3708 - acc: 0.879 - ETA: 3s - loss: 0.3710 - acc: 0.879 - ETA: 3s - loss: 0.3710 - acc: 0.879 - ETA: 3s - loss: 0.3699 - acc: 0.879 - ETA: 3s - loss: 0.3708 - acc: 0.879 - ETA: 3s - loss: 0.3717 - acc: 0.878 - ETA: 3s - loss: 0.3713 - acc: 0.879 - ETA: 3s - loss: 0.3712 - acc: 0.879 - ETA: 2s - loss: 0.3707 - acc: 0.879 - ETA: 2s - loss: 0.3704 - acc: 0.879 - ETA: 2s - loss: 0.3699 - acc: 0.879 - ETA: 2s - loss: 0.3684 - acc: 0.879 - ETA: 2s - loss: 0.3665 - acc: 0.880 - ETA: 2s - loss: 0.3664 - acc: 0.880 - ETA: 2s - loss: 0.3657 - acc: 0.880 - ETA: 2s - loss: 0.3673 - acc: 0.880 - ETA: 2s - loss: 0.3668 - acc: 0.880 - ETA: 2s - loss: 0.3649 - acc: 0.880 - ETA: 2s - loss: 0.3647 - acc: 0.880 - ETA: 2s - loss: 0.3638 - acc: 0.880 - ETA: 2s - loss: 0.3641 - acc: 0.880 - ETA: 2s - loss: 0.3637 - acc: 0.880 - ETA: 2s - loss: 0.3632 - acc: 0.880 - ETA: 2s - loss: 0.3627 - acc: 0.881 - ETA: 1s - loss: 0.3632 - acc: 0.880 - ETA: 1s - loss: 0.3629 - acc: 0.880 - ETA: 1s - loss: 0.3618 - acc: 0.881 - ETA: 1s - loss: 0.3613 - acc: 0.881 - ETA: 1s - loss: 0.3597 - acc: 0.881 - ETA: 1s - loss: 0.3582 - acc: 0.881 - ETA: 1s - loss: 0.3590 - acc: 0.882 - ETA: 1s - loss: 0.3589 - acc: 0.881 - ETA: 1s - loss: 0.3588 - acc: 0.881 - ETA: 1s - loss: 0.3583 - acc: 0.881 - ETA: 1s - loss: 0.3610 - acc: 0.880 - ETA: 1s - loss: 0.3611 - acc: 0.880 - ETA: 1s - loss: 0.3614 - acc: 0.880 - ETA: 1s - loss: 0.3605 - acc: 0.880 - ETA: 1s - loss: 0.3598 - acc: 0.881 - ETA: 1s - loss: 0.3601 - acc: 0.881 - ETA: 1s - loss: 0.3612 - acc: 0.880 - ETA: 1s - loss: 0.3605 - acc: 0.881 - ETA: 0s - loss: 0.3622 - acc: 0.880 - ETA: 0s - loss: 0.3606 - acc: 0.881 - ETA: 0s - loss: 0.3603 - acc: 0.881 - ETA: 0s - loss: 0.3601 - acc: 0.881 - ETA: 0s - loss: 0.3604 - acc: 0.881 - ETA: 0s - loss: 0.3613 - acc: 0.880 - ETA: 0s - loss: 0.3638 - acc: 0.880 - ETA: 0s - loss: 0.3632 - acc: 0.880 - ETA: 0s - loss: 0.3659 - acc: 0.880 - ETA: 0s - loss: 0.3644 - acc: 0.880 - ETA: 0s - loss: 0.3635 - acc: 0.881 - ETA: 0s - loss: 0.3632 - acc: 0.880 - ETA: 0s - loss: 0.3644 - acc: 0.880 - ETA: 0s - loss: 0.3648 - acc: 0.880 - ETA: 0s - loss: 0.3652 - acc: 0.881 - ETA: 0s - loss: 0.3660 - acc: 0.880 - ETA: 0s - loss: 0.3642 - acc: 0.881 - ETA: 0s - loss: 0.3634 - acc: 0.881 - ETA: 0s - loss: 0.3633 - acc: 0.8813Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 0.3629 - acc: 0.8814 - val_loss: 0.3939 - val_acc: 0.8814\n"
     ]
    }
   ],
   "source": [
    "merged_model = model1('Xception', 'Resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86.961722488038276, <keras.models.Sequential object at 0x0000021BA5487EF0>)\n"
     ]
    }
   ],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-79e2f9131b61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names)\u001b[0m\n\u001b[0;32m     98\u001b[0m                \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                show_layer_names=True):\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     18\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(merged_model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "def model1(network1):\n",
    "    ### Obtain Bottleneck Features\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network1))\n",
    "    train_VGG16 = bottleneck_features['train']\n",
    "    valid_VGG16 = bottleneck_features['valid']\n",
    "    test_VGG16 = bottleneck_features['test']\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    ### Compile the Model\n",
    "\n",
    "    # merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    ### Train the Model\n",
    "    model_file = 'saved_models/weights.best.{}.hdf5'.format(network1)\n",
    "    checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    model.fit(train_VGG16, train_targets, \n",
    "              validation_data=(valid_VGG16, valid_targets),\n",
    "              epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    ### Load the Model with the Best Validation Loss\n",
    "\n",
    "    model.load_weights(model_file)\n",
    "\n",
    "    ### Test the Model\n",
    "\n",
    "    # get index of predicted dog breed for each image in test set\n",
    "    merged_predictions = [np.argmax(model.predict(np.expand_dims(feature[0], axis=0), np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "    return (test_accuracy, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_13  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model_xception = model1(\"Xception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "print(model_xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Define your architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "def predict(image_path):\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(image_path))\n",
    "    #                    extract_Resnet50(path_to_tensor(image_path))]\n",
    "    predicted_prob = merged_model[-1].predict(bottleneck_feature)\n",
    "    print(predicted_prob.shape)\n",
    "    print(predicted_prob)\n",
    "    predicted_idx = np.argmax(predicted_prob[0])\n",
    "    print(predicted_idx)\n",
    "    return dog_names[predicted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "my_image_paths = [\"test_images/dog4.jpg\",\n",
    "                \"test_images/philippe_jordan.jpg\", \n",
    "                  \"test_images/eric_clapton.jpg\",\n",
    "                  \"test_images/shibainu.jpg\", \n",
    "                  \"test_images/pag.jpg\", \n",
    "                 \"test_images/dog2.jpg\",\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048)\n",
      "[[ 0.34740329  0.          0.         ...,  0.00185373  0.          0.        ]]\n",
      "160\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-6cfc26d91344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmy_image_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-150-e563d41e0d49>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpredicted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdog_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for image_path in my_image_paths:\n",
    "    print(predict(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Affenpinscher', 'Afghan_hound', 'Airedale_terrier', 'Akita', 'Alaskan_malamute', 'American_eskimo_dog', 'American_foxhound', 'American_staffordshire_terrier', 'American_water_spaniel', 'Anatolian_shepherd_dog', 'Australian_cattle_dog', 'Australian_shepherd', 'Australian_terrier', 'Basenji', 'Basset_hound', 'Beagle', 'Bearded_collie', 'Beauceron', 'Bedlington_terrier', 'Belgian_malinois', 'Belgian_sheepdog', 'Belgian_tervuren', 'Bernese_mountain_dog', 'Bichon_frise', 'Black_and_tan_coonhound', 'Black_russian_terrier', 'Bloodhound', 'Bluetick_coonhound', 'Border_collie', 'Border_terrier', 'Borzoi', 'Boston_terrier', 'Bouvier_des_flandres', 'Boxer', 'Boykin_spaniel', 'Briard', 'Brittany', 'Brussels_griffon', 'Bull_terrier', 'Bulldog', 'Bullmastiff', 'Cairn_terrier', 'Canaan_dog', 'Cane_corso', 'Cardigan_welsh_corgi', 'Cavalier_king_charles_spaniel', 'Chesapeake_bay_retriever', 'Chihuahua', 'Chinese_crested', 'Chinese_shar-pei', 'Chow_chow', 'Clumber_spaniel', 'Cocker_spaniel', 'Collie', 'Curly-coated_retriever', 'Dachshund', 'Dalmatian', 'Dandie_dinmont_terrier', 'Doberman_pinscher', 'Dogue_de_bordeaux', 'English_cocker_spaniel', 'English_setter', 'English_springer_spaniel', 'English_toy_spaniel', 'Entlebucher_mountain_dog', 'Field_spaniel', 'Finnish_spitz', 'Flat-coated_retriever', 'French_bulldog', 'German_pinscher', 'German_shepherd_dog', 'German_shorthaired_pointer', 'German_wirehaired_pointer', 'Giant_schnauzer', 'Glen_of_imaal_terrier', 'Golden_retriever', 'Gordon_setter', 'Great_dane', 'Great_pyrenees', 'Greater_swiss_mountain_dog', 'Greyhound', 'Havanese', 'Ibizan_hound', 'Icelandic_sheepdog', 'Irish_red_and_white_setter', 'Irish_setter', 'Irish_terrier', 'Irish_water_spaniel', 'Irish_wolfhound', 'Italian_greyhound', 'Japanese_chin', 'Keeshond', 'Kerry_blue_terrier', 'Komondor', 'Kuvasz', 'Labrador_retriever', 'Lakeland_terrier', 'Leonberger', 'Lhasa_apso', 'Lowchen', 'Maltese', 'Manchester_terrier', 'Mastiff', 'Miniature_schnauzer', 'Neapolitan_mastiff', 'Newfoundland', 'Norfolk_terrier', 'Norwegian_buhund', 'Norwegian_elkhound', 'Norwegian_lundehund', 'Norwich_terrier', 'Nova_scotia_duck_tolling_retriever', 'Old_english_sheepdog', 'Otterhound', 'Papillon', 'Parson_russell_terrier', 'Pekingese', 'Pembroke_welsh_corgi', 'Petit_basset_griffon_vendeen', 'Pharaoh_hound', 'Plott', 'Pointer', 'Pomeranian', 'Poodle', 'Portuguese_water_dog', 'Saint_bernard', 'Silky_terrier', 'Smooth_fox_terrier', 'Tibetan_mastiff', 'Welsh_springer_spaniel', 'Wirehaired_pointing_griffon', 'Xoloitzcuintli', 'Yorkshire_terrier']\n"
     ]
    }
   ],
   "source": [
    "print(dog_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dog_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
