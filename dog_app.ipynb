{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n56NJygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba166c59e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "- 99 percentage of the first 100 images in 'human_files' have a detected human face.\n",
    "- 11 percentage of the first 100 images in 'dog_files have a detected human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of the images hava a detected face in human_files_short: 99.0 %\n",
      "percentage of the images hava a detected face in dog_files_short: 11.0 %\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "def calc_face_detected_proportion(img_paths):\n",
    "    vectorized_detector = np.vectorize(face_detector)\n",
    "    return len(img_paths[vectorized_detector(img_paths)]) / len(img_paths)\n",
    "\n",
    "\n",
    "print(\"percentage of the images hava a detected face in human_files_short: {} %\".format(calc_face_detected_proportion(human_files_short) * 100) )\n",
    "print(\"percentage of the images hava a detected face in dog_files_short: {} %\".format(calc_face_detected_proportion(dog_files_short) * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "I think it depends on the business purpose. However, supposed the API is designed for wide usage, it should not pose the limitations on the user because it is possible the service of the user can't get the images with human faces. To detect human without his or her face, CNN can be used if it is trained with the images without human faces.  \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ \n",
    "- 0 percentage of the images in 'human_files_short' have a detected dog.\n",
    "- 100 percentage of the images in 'dog_files_short' have a detected dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of the images hava a detected dog in human_files_short: 1.0 %\n",
      "percentage of the images hava a detected dog in dog_files_short: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "def calc_dog_detected_proportion(img_paths):\n",
    "    vectorized_doc_detector = np.vectorize(dog_detector)\n",
    "    num_dog_detected = len(img_paths[vectorized_doc_detector(img_paths)])\n",
    "    return num_dog_detected / len(img_paths)\n",
    "\n",
    "print(\"percentage of the images hava a detected dog in human_files_short: {} %\".format(calc_dog_detected_proportion(human_files_short) * 100))\n",
    "print(\"percentage of the images hava a detected dog in dog_files_short: {} %\".format(calc_dog_detected_proportion(dog_files_short) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [01:29<00:00, 74.71it/s]\n",
      "100%|| 835/835 [00:12<00:00, 67.30it/s]\n",
      "100%|| 836/836 [00:11<00:00, 74.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_1 (Separabl (None, 224, 224, 128)     396       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 128)     512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 112, 112, 256)     33280     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 256)     1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 56, 56, 512)       132096    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 512)       2048      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 334,865\n",
      "Trainable params: 333,073\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "x = SeparableConv2D(filters=128, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = MaxPooling2D(128, strides=2, padding=\"same\")(x)\n",
    "\n",
    "x = SeparableConv2D(filters=256, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = MaxPooling2D(256, strides=2, padding=\"same\")(x)\n",
    "\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=2, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = MaxPooling2D(512, strides=2, padding=\"same\")(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_87 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCon (None, 224, 224, 512) 1563        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 224, 224, 512) 0           separable_conv2d_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 224, 224, 512) 1536        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 224, 224, 512) 2048        dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 224, 224, 512) 0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 224, 224, 512) 2048        dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)   (None, 224, 224, 512) 0           dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 224, 224, 512) 0           max_pooling2d_8[0][0]            \n",
      "                                                                   dropout_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_87 (Glo (None, 512)           0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           global_average_pooling2d_87[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 75,424\n",
      "Trainable params: 73,376\n",
      "Non-trainable params: 2,048\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "residual = Conv2D(filters=512, kernel_size=1, padding='same', use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "residual = Dropout(0.7)(residual)\n",
    "residual = BatchNormalization()(residual)\n",
    "residual = Dropout(0.7)(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=3, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_87 (InputLayer)            (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCon (None, 224, 224, 512) 1563        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 224, 224, 512) 0           separable_conv2d_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 224, 224, 512) 1536        input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 224, 224, 512) 2048        dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 224, 224, 512) 0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 224, 224, 512) 2048        dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)   (None, 224, 224, 512) 0           dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 224, 224, 512) 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 224, 224, 512) 0           max_pooling2d_8[0][0]            \n",
      "                                                                   dropout_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_87 (Glo (None, 512)           0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 512)           0           global_average_pooling2d_87[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 133)           68229       dropout_63[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 75,424\n",
      "Trainable params: 73,376\n",
      "Non-trainable params: 2,048\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "DOG_CLASSES = 133\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "img_input = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = img_input\n",
    "\n",
    "residual = Conv2D(filters=512, kernel_size=1, padding='same', use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "residual = Dropout(0.7)(residual)\n",
    "residual = BatchNormalization()(residual)\n",
    "residual = Dropout(0.7)(residual)\n",
    "\n",
    "x = SeparableConv2D(filters=512, kernel_size=3, padding='same', kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "x = Dropout(0.7)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.7)(x)\n",
    "\n",
    "x = MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(DOG_CLASSES, activation='softmax', kernel_initializer=\"he_normal\")(x)\n",
    "model = Model(img_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n",
      "1560/6680 [======>.......................] - ETA: 8838s - loss: 4.9013 - acc: 0.0077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6d9c5a199cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m history = model.fit(train_tensors, train_targets, \n\u001b[1;32m     48\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           epochs=epochs, batch_size=20, callbacks=[checkpointer, history_storer], verbose=1)\n\u001b[0m",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History, BaseLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "RANDOM_SEED=10\n",
    "epochs = 5\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "\n",
    "valid_generator = ImageDataGenerator(\n",
    "                            rotation_range=45,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True,\n",
    "                            fill_mode='nearest'\n",
    "                        )\n",
    "\n",
    "train_generator.fit(train_tensors, seed=RANDOM_SEED)\n",
    "valid_generator.fit(valid_tensors, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "history_storer = History()\n",
    "base_logger = BaseLogger()\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "batch_size=40\n",
    "#history = model.fit_generator(train_generator.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "#                    steps_per_epoch=train_tensors.shape[0],\n",
    "#                  validation_data=valid_generator.flow(valid_tensors, valid_targets, batch_size=batch_size),\n",
    "#                  epochs=epochs, validation_steps=valid_tensors.shape[0], callbacks=[checkpointer, history_storer], verbose=1)\n",
    "\n",
    "history = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer, history_storer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100 * np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1)) / len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6580/6680 [============================>.] - ETA: 148s - loss: 14.4393 - acc: 0.05 - ETA: 20s - loss: 14.5490 - acc: 0.0188 - ETA: 11s - loss: 14.4552 - acc: 0.023 - ETA: 9s - loss: 14.5233 - acc: 0.023 - ETA: 7s - loss: 14.2980 - acc: 0.03 - ETA: 6s - loss: 14.2529 - acc: 0.03 - ETA: 5s - loss: 14.2113 - acc: 0.03 - ETA: 4s - loss: 14.1709 - acc: 0.03 - ETA: 4s - loss: 14.0686 - acc: 0.04 - ETA: 4s - loss: 13.9183 - acc: 0.04 - ETA: 3s - loss: 13.8582 - acc: 0.04 - ETA: 3s - loss: 13.7835 - acc: 0.05 - ETA: 3s - loss: 13.7094 - acc: 0.05 - ETA: 3s - loss: 13.6460 - acc: 0.05 - ETA: 3s - loss: 13.6196 - acc: 0.05 - ETA: 2s - loss: 13.5635 - acc: 0.05 - ETA: 2s - loss: 13.5402 - acc: 0.05 - ETA: 2s - loss: 13.4613 - acc: 0.06 - ETA: 2s - loss: 13.4469 - acc: 0.06 - ETA: 2s - loss: 13.3886 - acc: 0.06 - ETA: 2s - loss: 13.3197 - acc: 0.06 - ETA: 2s - loss: 13.2838 - acc: 0.07 - ETA: 2s - loss: 13.2448 - acc: 0.07 - ETA: 1s - loss: 13.1960 - acc: 0.07 - ETA: 1s - loss: 13.0754 - acc: 0.07 - ETA: 1s - loss: 13.0476 - acc: 0.07 - ETA: 1s - loss: 12.9930 - acc: 0.08 - ETA: 1s - loss: 12.9255 - acc: 0.08 - ETA: 1s - loss: 12.8542 - acc: 0.08 - ETA: 1s - loss: 12.8286 - acc: 0.08 - ETA: 1s - loss: 12.7661 - acc: 0.09 - ETA: 1s - loss: 12.7339 - acc: 0.09 - ETA: 1s - loss: 12.6976 - acc: 0.09 - ETA: 1s - loss: 12.6459 - acc: 0.09 - ETA: 1s - loss: 12.5538 - acc: 0.10 - ETA: 1s - loss: 12.5098 - acc: 0.10 - ETA: 0s - loss: 12.4834 - acc: 0.10 - ETA: 0s - loss: 12.4200 - acc: 0.10 - ETA: 0s - loss: 12.3607 - acc: 0.10 - ETA: 0s - loss: 12.3212 - acc: 0.11 - ETA: 0s - loss: 12.2731 - acc: 0.11 - ETA: 0s - loss: 12.2582 - acc: 0.11 - ETA: 0s - loss: 12.2195 - acc: 0.11 - ETA: 0s - loss: 12.1752 - acc: 0.11 - ETA: 0s - loss: 12.1475 - acc: 0.12 - ETA: 0s - loss: 12.1003 - acc: 0.12 - ETA: 0s - loss: 12.0741 - acc: 0.12 - ETA: 0s - loss: 12.0473 - acc: 0.12 - ETA: 0s - loss: 11.9993 - acc: 0.12 - ETA: 0s - loss: 11.9650 - acc: 0.13 - ETA: 0s - loss: 11.9343 - acc: 0.1334Epoch 00000: val_loss improved from inf to 10.46817, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 11.9010 - acc: 0.1355 - val_loss: 10.4682 - val_acc: 0.2263\n",
      "Epoch 2/20\n",
      "6600/6680 [============================>.] - ETA: 2s - loss: 11.3439 - acc: 0.15 - ETA: 3s - loss: 10.2317 - acc: 0.28 - ETA: 3s - loss: 10.1631 - acc: 0.26 - ETA: 3s - loss: 10.3988 - acc: 0.25 - ETA: 2s - loss: 10.0310 - acc: 0.26 - ETA: 2s - loss: 10.0438 - acc: 0.27 - ETA: 2s - loss: 10.0230 - acc: 0.27 - ETA: 2s - loss: 9.9733 - acc: 0.2756 - ETA: 2s - loss: 9.9857 - acc: 0.279 - ETA: 2s - loss: 9.9475 - acc: 0.281 - ETA: 2s - loss: 10.0310 - acc: 0.27 - ETA: 2s - loss: 9.9808 - acc: 0.2750 - ETA: 2s - loss: 9.8892 - acc: 0.277 - ETA: 2s - loss: 9.9210 - acc: 0.275 - ETA: 2s - loss: 9.8995 - acc: 0.278 - ETA: 2s - loss: 9.8839 - acc: 0.276 - ETA: 2s - loss: 9.8819 - acc: 0.278 - ETA: 2s - loss: 9.8265 - acc: 0.282 - ETA: 1s - loss: 9.7613 - acc: 0.286 - ETA: 1s - loss: 9.7479 - acc: 0.287 - ETA: 1s - loss: 9.7510 - acc: 0.286 - ETA: 1s - loss: 9.7538 - acc: 0.287 - ETA: 1s - loss: 9.7245 - acc: 0.289 - ETA: 1s - loss: 9.6594 - acc: 0.292 - ETA: 1s - loss: 9.6864 - acc: 0.290 - ETA: 1s - loss: 9.6931 - acc: 0.290 - ETA: 1s - loss: 9.7054 - acc: 0.289 - ETA: 1s - loss: 9.7082 - acc: 0.290 - ETA: 1s - loss: 9.6784 - acc: 0.293 - ETA: 1s - loss: 9.6798 - acc: 0.294 - ETA: 1s - loss: 9.6853 - acc: 0.294 - ETA: 1s - loss: 9.6935 - acc: 0.294 - ETA: 1s - loss: 9.6809 - acc: 0.296 - ETA: 1s - loss: 9.6417 - acc: 0.298 - ETA: 1s - loss: 9.6017 - acc: 0.299 - ETA: 0s - loss: 9.5979 - acc: 0.300 - ETA: 0s - loss: 9.5987 - acc: 0.300 - ETA: 0s - loss: 9.5970 - acc: 0.300 - ETA: 0s - loss: 9.6143 - acc: 0.300 - ETA: 0s - loss: 9.5941 - acc: 0.302 - ETA: 0s - loss: 9.5886 - acc: 0.303 - ETA: 0s - loss: 9.5462 - acc: 0.305 - ETA: 0s - loss: 9.5473 - acc: 0.305 - ETA: 0s - loss: 9.5513 - acc: 0.305 - ETA: 0s - loss: 9.5505 - acc: 0.306 - ETA: 0s - loss: 9.5326 - acc: 0.306 - ETA: 0s - loss: 9.5150 - acc: 0.306 - ETA: 0s - loss: 9.4905 - acc: 0.308 - ETA: 0s - loss: 9.4825 - acc: 0.308 - ETA: 0s - loss: 9.4832 - acc: 0.309 - ETA: 0s - loss: 9.5028 - acc: 0.308 - ETA: 0s - loss: 9.4826 - acc: 0.309 - ETA: 0s - loss: 9.5016 - acc: 0.309 - ETA: 0s - loss: 9.4969 - acc: 0.3103Epoch 00001: val_loss improved from 10.46817 to 9.37262, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 9.4935 - acc: 0.3105 - val_loss: 9.3726 - val_acc: 0.3269\n",
      "Epoch 3/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 8.9605 - acc: 0.400 - ETA: 3s - loss: 8.8353 - acc: 0.378 - ETA: 2s - loss: 9.4159 - acc: 0.361 - ETA: 2s - loss: 9.1783 - acc: 0.373 - ETA: 2s - loss: 9.0468 - acc: 0.384 - ETA: 2s - loss: 9.2068 - acc: 0.374 - ETA: 2s - loss: 9.1361 - acc: 0.379 - ETA: 2s - loss: 9.1137 - acc: 0.373 - ETA: 2s - loss: 9.0287 - acc: 0.377 - ETA: 2s - loss: 9.1241 - acc: 0.373 - ETA: 2s - loss: 9.0334 - acc: 0.378 - ETA: 2s - loss: 8.9112 - acc: 0.381 - ETA: 2s - loss: 8.9210 - acc: 0.378 - ETA: 2s - loss: 8.9963 - acc: 0.375 - ETA: 2s - loss: 9.0427 - acc: 0.369 - ETA: 2s - loss: 8.9661 - acc: 0.372 - ETA: 2s - loss: 8.9375 - acc: 0.373 - ETA: 2s - loss: 8.8885 - acc: 0.376 - ETA: 1s - loss: 8.8451 - acc: 0.378 - ETA: 1s - loss: 8.8216 - acc: 0.380 - ETA: 1s - loss: 8.8661 - acc: 0.379 - ETA: 1s - loss: 8.8478 - acc: 0.380 - ETA: 1s - loss: 8.8229 - acc: 0.383 - ETA: 1s - loss: 8.8608 - acc: 0.379 - ETA: 1s - loss: 8.8577 - acc: 0.380 - ETA: 1s - loss: 8.8589 - acc: 0.381 - ETA: 1s - loss: 8.8738 - acc: 0.378 - ETA: 1s - loss: 8.8724 - acc: 0.379 - ETA: 1s - loss: 8.8711 - acc: 0.380 - ETA: 1s - loss: 8.9108 - acc: 0.378 - ETA: 1s - loss: 8.8954 - acc: 0.378 - ETA: 1s - loss: 8.8411 - acc: 0.381 - ETA: 1s - loss: 8.8492 - acc: 0.379 - ETA: 1s - loss: 8.8443 - acc: 0.380 - ETA: 1s - loss: 8.8353 - acc: 0.381 - ETA: 1s - loss: 8.8410 - acc: 0.381 - ETA: 0s - loss: 8.8586 - acc: 0.379 - ETA: 0s - loss: 8.8255 - acc: 0.381 - ETA: 0s - loss: 8.8362 - acc: 0.381 - ETA: 0s - loss: 8.8675 - acc: 0.380 - ETA: 0s - loss: 8.8812 - acc: 0.379 - ETA: 0s - loss: 8.8733 - acc: 0.379 - ETA: 0s - loss: 8.8501 - acc: 0.381 - ETA: 0s - loss: 8.8464 - acc: 0.381 - ETA: 0s - loss: 8.8665 - acc: 0.380 - ETA: 0s - loss: 8.8626 - acc: 0.380 - ETA: 0s - loss: 8.8726 - acc: 0.380 - ETA: 0s - loss: 8.8891 - acc: 0.379 - ETA: 0s - loss: 8.8843 - acc: 0.379 - ETA: 0s - loss: 8.8623 - acc: 0.380 - ETA: 0s - loss: 8.8573 - acc: 0.380 - ETA: 0s - loss: 8.8402 - acc: 0.381 - ETA: 0s - loss: 8.8329 - acc: 0.381 - ETA: 0s - loss: 8.8208 - acc: 0.381 - ETA: 0s - loss: 8.8063 - acc: 0.3821Epoch 00002: val_loss improved from 9.37262 to 9.01960, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.8262 - acc: 0.3813 - val_loss: 9.0196 - val_acc: 0.3521\n",
      "Epoch 4/20\n",
      "6560/6680 [============================>.] - ETA: 3s - loss: 5.3237 - acc: 0.600 - ETA: 3s - loss: 7.1220 - acc: 0.507 - ETA: 2s - loss: 7.3717 - acc: 0.496 - ETA: 2s - loss: 8.4736 - acc: 0.431 - ETA: 2s - loss: 8.7189 - acc: 0.414 - ETA: 2s - loss: 8.8358 - acc: 0.408 - ETA: 2s - loss: 8.5364 - acc: 0.423 - ETA: 2s - loss: 8.4849 - acc: 0.421 - ETA: 2s - loss: 8.3415 - acc: 0.431 - ETA: 2s - loss: 8.3891 - acc: 0.428 - ETA: 2s - loss: 8.2625 - acc: 0.438 - ETA: 2s - loss: 8.2255 - acc: 0.439 - ETA: 2s - loss: 8.1427 - acc: 0.443 - ETA: 2s - loss: 8.0838 - acc: 0.444 - ETA: 2s - loss: 8.1855 - acc: 0.438 - ETA: 2s - loss: 8.2372 - acc: 0.436 - ETA: 1s - loss: 8.2657 - acc: 0.435 - ETA: 1s - loss: 8.2471 - acc: 0.435 - ETA: 1s - loss: 8.2455 - acc: 0.434 - ETA: 1s - loss: 8.2371 - acc: 0.432 - ETA: 1s - loss: 8.2651 - acc: 0.430 - ETA: 1s - loss: 8.3193 - acc: 0.426 - ETA: 1s - loss: 8.3409 - acc: 0.424 - ETA: 1s - loss: 8.4076 - acc: 0.421 - ETA: 1s - loss: 8.4390 - acc: 0.419 - ETA: 1s - loss: 8.4817 - acc: 0.416 - ETA: 1s - loss: 8.4930 - acc: 0.416 - ETA: 1s - loss: 8.4874 - acc: 0.417 - ETA: 1s - loss: 8.4706 - acc: 0.418 - ETA: 1s - loss: 8.4739 - acc: 0.417 - ETA: 1s - loss: 8.4571 - acc: 0.419 - ETA: 1s - loss: 8.4532 - acc: 0.419 - ETA: 1s - loss: 8.4446 - acc: 0.420 - ETA: 1s - loss: 8.4694 - acc: 0.419 - ETA: 1s - loss: 8.4866 - acc: 0.418 - ETA: 1s - loss: 8.4713 - acc: 0.419 - ETA: 0s - loss: 8.4670 - acc: 0.420 - ETA: 0s - loss: 8.4651 - acc: 0.419 - ETA: 0s - loss: 8.4688 - acc: 0.419 - ETA: 0s - loss: 8.4627 - acc: 0.419 - ETA: 0s - loss: 8.4608 - acc: 0.419 - ETA: 0s - loss: 8.4735 - acc: 0.418 - ETA: 0s - loss: 8.4998 - acc: 0.416 - ETA: 0s - loss: 8.4985 - acc: 0.416 - ETA: 0s - loss: 8.5191 - acc: 0.415 - ETA: 0s - loss: 8.4988 - acc: 0.416 - ETA: 0s - loss: 8.4695 - acc: 0.418 - ETA: 0s - loss: 8.4595 - acc: 0.418 - ETA: 0s - loss: 8.4962 - acc: 0.416 - ETA: 0s - loss: 8.5077 - acc: 0.415 - ETA: 0s - loss: 8.5033 - acc: 0.415 - ETA: 0s - loss: 8.5056 - acc: 0.415 - ETA: 0s - loss: 8.4939 - acc: 0.416 - ETA: 0s - loss: 8.5064 - acc: 0.4166Epoch 00003: val_loss improved from 9.01960 to 8.87551, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.5072 - acc: 0.4169 - val_loss: 8.8755 - val_acc: 0.3665\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 9.7305 - acc: 0.350 - ETA: 3s - loss: 8.0499 - acc: 0.466 - ETA: 3s - loss: 8.2876 - acc: 0.445 - ETA: 3s - loss: 8.4324 - acc: 0.429 - ETA: 2s - loss: 8.3660 - acc: 0.439 - ETA: 2s - loss: 8.2938 - acc: 0.446 - ETA: 2s - loss: 8.2847 - acc: 0.445 - ETA: 2s - loss: 8.2611 - acc: 0.447 - ETA: 2s - loss: 8.3435 - acc: 0.442 - ETA: 2s - loss: 8.3644 - acc: 0.438 - ETA: 2s - loss: 8.4946 - acc: 0.432 - ETA: 2s - loss: 8.4310 - acc: 0.437 - ETA: 2s - loss: 8.4968 - acc: 0.434 - ETA: 2s - loss: 8.5064 - acc: 0.431 - ETA: 2s - loss: 8.4344 - acc: 0.436 - ETA: 2s - loss: 8.4922 - acc: 0.432 - ETA: 2s - loss: 8.5164 - acc: 0.431 - ETA: 2s - loss: 8.4297 - acc: 0.436 - ETA: 2s - loss: 8.4066 - acc: 0.437 - ETA: 1s - loss: 8.4033 - acc: 0.438 - ETA: 1s - loss: 8.4564 - acc: 0.436 - ETA: 1s - loss: 8.4379 - acc: 0.437 - ETA: 1s - loss: 8.4295 - acc: 0.437 - ETA: 1s - loss: 8.3866 - acc: 0.440 - ETA: 1s - loss: 8.4043 - acc: 0.440 - ETA: 1s - loss: 8.4229 - acc: 0.440 - ETA: 1s - loss: 8.4047 - acc: 0.441 - ETA: 1s - loss: 8.3823 - acc: 0.442 - ETA: 1s - loss: 8.3570 - acc: 0.444 - ETA: 1s - loss: 8.3383 - acc: 0.445 - ETA: 1s - loss: 8.3757 - acc: 0.443 - ETA: 1s - loss: 8.3778 - acc: 0.443 - ETA: 1s - loss: 8.4256 - acc: 0.441 - ETA: 1s - loss: 8.4048 - acc: 0.442 - ETA: 1s - loss: 8.4423 - acc: 0.440 - ETA: 1s - loss: 8.4278 - acc: 0.440 - ETA: 1s - loss: 8.4177 - acc: 0.441 - ETA: 1s - loss: 8.4707 - acc: 0.439 - ETA: 0s - loss: 8.4689 - acc: 0.438 - ETA: 0s - loss: 8.4752 - acc: 0.437 - ETA: 0s - loss: 8.4786 - acc: 0.437 - ETA: 0s - loss: 8.4920 - acc: 0.436 - ETA: 0s - loss: 8.4651 - acc: 0.437 - ETA: 0s - loss: 8.4421 - acc: 0.439 - ETA: 0s - loss: 8.4364 - acc: 0.439 - ETA: 0s - loss: 8.4233 - acc: 0.439 - ETA: 0s - loss: 8.4089 - acc: 0.440 - ETA: 0s - loss: 8.3937 - acc: 0.441 - ETA: 0s - loss: 8.3927 - acc: 0.442 - ETA: 0s - loss: 8.4207 - acc: 0.440 - ETA: 0s - loss: 8.4118 - acc: 0.441 - ETA: 0s - loss: 8.3988 - acc: 0.441 - ETA: 0s - loss: 8.3896 - acc: 0.442 - ETA: 0s - loss: 8.3831 - acc: 0.442 - ETA: 0s - loss: 8.3800 - acc: 0.442 - ETA: 0s - loss: 8.3790 - acc: 0.4415Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 8.3737 - acc: 0.4415 - val_loss: 8.9443 - val_acc: 0.3689\n",
      "Epoch 6/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 6.7664 - acc: 0.500 - ETA: 2s - loss: 8.0628 - acc: 0.450 - ETA: 2s - loss: 8.5319 - acc: 0.428 - ETA: 2s - loss: 8.5088 - acc: 0.430 - ETA: 2s - loss: 8.5447 - acc: 0.434 - ETA: 2s - loss: 8.6621 - acc: 0.432 - ETA: 2s - loss: 8.3765 - acc: 0.450 - ETA: 2s - loss: 8.3038 - acc: 0.453 - ETA: 2s - loss: 8.2782 - acc: 0.454 - ETA: 2s - loss: 8.3307 - acc: 0.453 - ETA: 2s - loss: 8.3753 - acc: 0.450 - ETA: 2s - loss: 8.3757 - acc: 0.451 - ETA: 2s - loss: 8.3996 - acc: 0.450 - ETA: 2s - loss: 8.3017 - acc: 0.457 - ETA: 2s - loss: 8.3390 - acc: 0.453 - ETA: 1s - loss: 8.3596 - acc: 0.452 - ETA: 1s - loss: 8.3547 - acc: 0.451 - ETA: 1s - loss: 8.3621 - acc: 0.451 - ETA: 1s - loss: 8.2643 - acc: 0.457 - ETA: 1s - loss: 8.2443 - acc: 0.457 - ETA: 1s - loss: 8.3189 - acc: 0.452 - ETA: 1s - loss: 8.3773 - acc: 0.449 - ETA: 1s - loss: 8.3550 - acc: 0.450 - ETA: 1s - loss: 8.3578 - acc: 0.450 - ETA: 1s - loss: 8.3488 - acc: 0.451 - ETA: 1s - loss: 8.3065 - acc: 0.454 - ETA: 1s - loss: 8.3143 - acc: 0.453 - ETA: 1s - loss: 8.3325 - acc: 0.452 - ETA: 1s - loss: 8.3011 - acc: 0.454 - ETA: 1s - loss: 8.3003 - acc: 0.454 - ETA: 1s - loss: 8.3014 - acc: 0.454 - ETA: 1s - loss: 8.3164 - acc: 0.453 - ETA: 1s - loss: 8.3182 - acc: 0.453 - ETA: 1s - loss: 8.3307 - acc: 0.452 - ETA: 1s - loss: 8.3344 - acc: 0.452 - ETA: 1s - loss: 8.3261 - acc: 0.453 - ETA: 0s - loss: 8.3210 - acc: 0.453 - ETA: 0s - loss: 8.3294 - acc: 0.452 - ETA: 0s - loss: 8.3608 - acc: 0.451 - ETA: 0s - loss: 8.3588 - acc: 0.451 - ETA: 0s - loss: 8.3566 - acc: 0.452 - ETA: 0s - loss: 8.3247 - acc: 0.453 - ETA: 0s - loss: 8.3241 - acc: 0.452 - ETA: 0s - loss: 8.3132 - acc: 0.453 - ETA: 0s - loss: 8.3365 - acc: 0.452 - ETA: 0s - loss: 8.3218 - acc: 0.452 - ETA: 0s - loss: 8.2879 - acc: 0.455 - ETA: 0s - loss: 8.2901 - acc: 0.455 - ETA: 0s - loss: 8.2931 - acc: 0.455 - ETA: 0s - loss: 8.2892 - acc: 0.455 - ETA: 0s - loss: 8.2579 - acc: 0.457 - ETA: 0s - loss: 8.2726 - acc: 0.456 - ETA: 0s - loss: 8.2728 - acc: 0.457 - ETA: 0s - loss: 8.2699 - acc: 0.457 - ETA: 0s - loss: 8.2874 - acc: 0.4566Epoch 00005: val_loss improved from 8.87551 to 8.71168, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.2933 - acc: 0.4564 - val_loss: 8.7117 - val_acc: 0.3916\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6680 [============================>.] - ETA: 3s - loss: 7.3177 - acc: 0.500 - ETA: 3s - loss: 8.7902 - acc: 0.421 - ETA: 2s - loss: 9.2973 - acc: 0.388 - ETA: 2s - loss: 8.7896 - acc: 0.425 - ETA: 2s - loss: 8.6150 - acc: 0.441 - ETA: 2s - loss: 8.4869 - acc: 0.451 - ETA: 2s - loss: 8.2471 - acc: 0.464 - ETA: 2s - loss: 8.3114 - acc: 0.459 - ETA: 2s - loss: 8.2859 - acc: 0.463 - ETA: 2s - loss: 8.2576 - acc: 0.464 - ETA: 2s - loss: 8.2291 - acc: 0.465 - ETA: 2s - loss: 8.1707 - acc: 0.469 - ETA: 2s - loss: 8.1631 - acc: 0.469 - ETA: 2s - loss: 8.1207 - acc: 0.472 - ETA: 2s - loss: 8.1269 - acc: 0.472 - ETA: 2s - loss: 8.2007 - acc: 0.467 - ETA: 2s - loss: 8.1342 - acc: 0.470 - ETA: 1s - loss: 8.1475 - acc: 0.470 - ETA: 1s - loss: 8.1266 - acc: 0.471 - ETA: 1s - loss: 8.1363 - acc: 0.471 - ETA: 1s - loss: 8.0957 - acc: 0.474 - ETA: 1s - loss: 8.1484 - acc: 0.470 - ETA: 1s - loss: 8.1751 - acc: 0.468 - ETA: 1s - loss: 8.1707 - acc: 0.467 - ETA: 1s - loss: 8.1790 - acc: 0.467 - ETA: 1s - loss: 8.1472 - acc: 0.469 - ETA: 1s - loss: 8.0897 - acc: 0.473 - ETA: 1s - loss: 8.1017 - acc: 0.472 - ETA: 1s - loss: 8.1085 - acc: 0.472 - ETA: 1s - loss: 8.1156 - acc: 0.471 - ETA: 1s - loss: 8.1155 - acc: 0.472 - ETA: 1s - loss: 8.1199 - acc: 0.472 - ETA: 1s - loss: 8.1228 - acc: 0.472 - ETA: 1s - loss: 8.0876 - acc: 0.474 - ETA: 1s - loss: 8.0900 - acc: 0.474 - ETA: 1s - loss: 8.1104 - acc: 0.473 - ETA: 0s - loss: 8.1243 - acc: 0.472 - ETA: 0s - loss: 8.0906 - acc: 0.473 - ETA: 0s - loss: 8.1229 - acc: 0.471 - ETA: 0s - loss: 8.1124 - acc: 0.471 - ETA: 0s - loss: 8.1142 - acc: 0.471 - ETA: 0s - loss: 8.1242 - acc: 0.470 - ETA: 0s - loss: 8.1325 - acc: 0.470 - ETA: 0s - loss: 8.1348 - acc: 0.469 - ETA: 0s - loss: 8.1567 - acc: 0.468 - ETA: 0s - loss: 8.1599 - acc: 0.468 - ETA: 0s - loss: 8.1638 - acc: 0.468 - ETA: 0s - loss: 8.1636 - acc: 0.468 - ETA: 0s - loss: 8.1317 - acc: 0.470 - ETA: 0s - loss: 8.1503 - acc: 0.468 - ETA: 0s - loss: 8.1421 - acc: 0.468 - ETA: 0s - loss: 8.1326 - acc: 0.468 - ETA: 0s - loss: 8.1473 - acc: 0.4681Epoch 00006: val_loss improved from 8.71168 to 8.69161, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.1615 - acc: 0.4672 - val_loss: 8.6916 - val_acc: 0.3880\n",
      "Epoch 8/20\n",
      "6560/6680 [============================>.] - ETA: 4s - loss: 8.0625 - acc: 0.500 - ETA: 3s - loss: 6.9307 - acc: 0.557 - ETA: 3s - loss: 6.9942 - acc: 0.550 - ETA: 2s - loss: 7.5059 - acc: 0.521 - ETA: 2s - loss: 7.7243 - acc: 0.507 - ETA: 2s - loss: 7.9415 - acc: 0.495 - ETA: 2s - loss: 7.9000 - acc: 0.497 - ETA: 2s - loss: 7.8857 - acc: 0.493 - ETA: 2s - loss: 7.7506 - acc: 0.501 - ETA: 2s - loss: 7.7340 - acc: 0.501 - ETA: 2s - loss: 7.7987 - acc: 0.499 - ETA: 2s - loss: 7.8160 - acc: 0.497 - ETA: 2s - loss: 7.8259 - acc: 0.498 - ETA: 2s - loss: 7.9456 - acc: 0.491 - ETA: 2s - loss: 8.0395 - acc: 0.485 - ETA: 1s - loss: 8.0448 - acc: 0.485 - ETA: 1s - loss: 8.1496 - acc: 0.479 - ETA: 1s - loss: 8.1067 - acc: 0.480 - ETA: 1s - loss: 8.0512 - acc: 0.484 - ETA: 1s - loss: 8.0687 - acc: 0.483 - ETA: 1s - loss: 8.1079 - acc: 0.481 - ETA: 1s - loss: 8.1183 - acc: 0.481 - ETA: 1s - loss: 8.0718 - acc: 0.484 - ETA: 1s - loss: 8.1284 - acc: 0.480 - ETA: 1s - loss: 8.1255 - acc: 0.481 - ETA: 1s - loss: 8.1345 - acc: 0.480 - ETA: 1s - loss: 8.1256 - acc: 0.480 - ETA: 1s - loss: 8.0885 - acc: 0.482 - ETA: 1s - loss: 8.0836 - acc: 0.483 - ETA: 1s - loss: 8.1146 - acc: 0.481 - ETA: 1s - loss: 8.1055 - acc: 0.482 - ETA: 1s - loss: 8.1227 - acc: 0.481 - ETA: 1s - loss: 8.1481 - acc: 0.479 - ETA: 1s - loss: 8.1527 - acc: 0.478 - ETA: 1s - loss: 8.1441 - acc: 0.478 - ETA: 0s - loss: 8.1290 - acc: 0.479 - ETA: 0s - loss: 8.1149 - acc: 0.480 - ETA: 0s - loss: 8.1329 - acc: 0.478 - ETA: 0s - loss: 8.1383 - acc: 0.478 - ETA: 0s - loss: 8.1085 - acc: 0.480 - ETA: 0s - loss: 8.1078 - acc: 0.480 - ETA: 0s - loss: 8.1061 - acc: 0.479 - ETA: 0s - loss: 8.1058 - acc: 0.480 - ETA: 0s - loss: 8.0874 - acc: 0.480 - ETA: 0s - loss: 8.0822 - acc: 0.480 - ETA: 0s - loss: 8.1018 - acc: 0.479 - ETA: 0s - loss: 8.1102 - acc: 0.478 - ETA: 0s - loss: 8.1126 - acc: 0.478 - ETA: 0s - loss: 8.1391 - acc: 0.477 - ETA: 0s - loss: 8.1334 - acc: 0.477 - ETA: 0s - loss: 8.1133 - acc: 0.478 - ETA: 0s - loss: 8.1041 - acc: 0.479 - ETA: 0s - loss: 8.1038 - acc: 0.4791Epoch 00007: val_loss improved from 8.69161 to 8.60822, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 8.0902 - acc: 0.4799 - val_loss: 8.6082 - val_acc: 0.4024\n",
      "Epoch 9/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 4.0661 - acc: 0.700 - ETA: 3s - loss: 7.2257 - acc: 0.535 - ETA: 3s - loss: 7.5119 - acc: 0.525 - ETA: 2s - loss: 7.4731 - acc: 0.530 - ETA: 2s - loss: 7.6414 - acc: 0.516 - ETA: 2s - loss: 7.5592 - acc: 0.520 - ETA: 2s - loss: 7.7839 - acc: 0.506 - ETA: 2s - loss: 7.8440 - acc: 0.502 - ETA: 2s - loss: 7.9425 - acc: 0.495 - ETA: 2s - loss: 7.8985 - acc: 0.498 - ETA: 2s - loss: 7.7897 - acc: 0.505 - ETA: 2s - loss: 7.7552 - acc: 0.508 - ETA: 2s - loss: 7.7702 - acc: 0.506 - ETA: 2s - loss: 7.8182 - acc: 0.502 - ETA: 2s - loss: 7.7519 - acc: 0.506 - ETA: 2s - loss: 7.7147 - acc: 0.507 - ETA: 2s - loss: 7.7126 - acc: 0.506 - ETA: 1s - loss: 7.7690 - acc: 0.502 - ETA: 1s - loss: 7.7842 - acc: 0.502 - ETA: 1s - loss: 7.7606 - acc: 0.504 - ETA: 1s - loss: 7.7861 - acc: 0.502 - ETA: 1s - loss: 7.7662 - acc: 0.504 - ETA: 1s - loss: 7.7111 - acc: 0.506 - ETA: 1s - loss: 7.7150 - acc: 0.507 - ETA: 1s - loss: 7.7148 - acc: 0.507 - ETA: 1s - loss: 7.7530 - acc: 0.504 - ETA: 1s - loss: 7.7746 - acc: 0.504 - ETA: 1s - loss: 7.7976 - acc: 0.502 - ETA: 1s - loss: 7.8645 - acc: 0.498 - ETA: 1s - loss: 7.8706 - acc: 0.497 - ETA: 1s - loss: 7.8980 - acc: 0.495 - ETA: 1s - loss: 7.8786 - acc: 0.496 - ETA: 1s - loss: 7.9065 - acc: 0.494 - ETA: 1s - loss: 7.9359 - acc: 0.492 - ETA: 1s - loss: 7.9589 - acc: 0.491 - ETA: 0s - loss: 7.9287 - acc: 0.494 - ETA: 0s - loss: 7.9187 - acc: 0.494 - ETA: 0s - loss: 7.9169 - acc: 0.494 - ETA: 0s - loss: 7.9008 - acc: 0.495 - ETA: 0s - loss: 7.9155 - acc: 0.494 - ETA: 0s - loss: 7.9045 - acc: 0.494 - ETA: 0s - loss: 7.8947 - acc: 0.495 - ETA: 0s - loss: 7.8603 - acc: 0.497 - ETA: 0s - loss: 7.8929 - acc: 0.495 - ETA: 0s - loss: 7.8665 - acc: 0.497 - ETA: 0s - loss: 7.8477 - acc: 0.498 - ETA: 0s - loss: 7.8540 - acc: 0.498 - ETA: 0s - loss: 7.8417 - acc: 0.499 - ETA: 0s - loss: 7.8549 - acc: 0.498 - ETA: 0s - loss: 7.8628 - acc: 0.497 - ETA: 0s - loss: 7.8908 - acc: 0.495 - ETA: 0s - loss: 7.9032 - acc: 0.495 - ETA: 0s - loss: 7.9311 - acc: 0.493 - ETA: 0s - loss: 7.9516 - acc: 0.492 - ETA: 0s - loss: 7.9685 - acc: 0.4913Epoch 00008: val_loss improved from 8.60822 to 8.60318, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9671 - acc: 0.4913 - val_loss: 8.6032 - val_acc: 0.3940\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 2s - loss: 8.8650 - acc: 0.450 - ETA: 2s - loss: 8.6839 - acc: 0.442 - ETA: 2s - loss: 8.2762 - acc: 0.473 - ETA: 2s - loss: 7.8472 - acc: 0.502 - ETA: 2s - loss: 7.9378 - acc: 0.498 - ETA: 2s - loss: 7.6806 - acc: 0.512 - ETA: 2s - loss: 7.6580 - acc: 0.514 - ETA: 2s - loss: 7.5822 - acc: 0.518 - ETA: 2s - loss: 7.5514 - acc: 0.519 - ETA: 2s - loss: 7.7158 - acc: 0.510 - ETA: 2s - loss: 7.7615 - acc: 0.508 - ETA: 2s - loss: 7.7764 - acc: 0.507 - ETA: 2s - loss: 7.7249 - acc: 0.511 - ETA: 2s - loss: 7.6207 - acc: 0.516 - ETA: 2s - loss: 7.7167 - acc: 0.510 - ETA: 2s - loss: 7.7191 - acc: 0.509 - ETA: 1s - loss: 7.8207 - acc: 0.503 - ETA: 1s - loss: 7.7705 - acc: 0.506 - ETA: 1s - loss: 7.8011 - acc: 0.504 - ETA: 1s - loss: 7.8122 - acc: 0.503 - ETA: 1s - loss: 7.8438 - acc: 0.502 - ETA: 1s - loss: 7.8734 - acc: 0.500 - ETA: 1s - loss: 7.8896 - acc: 0.499 - ETA: 1s - loss: 7.9301 - acc: 0.497 - ETA: 1s - loss: 7.9379 - acc: 0.496 - ETA: 1s - loss: 7.9804 - acc: 0.493 - ETA: 1s - loss: 8.0081 - acc: 0.492 - ETA: 1s - loss: 7.9654 - acc: 0.495 - ETA: 1s - loss: 7.9939 - acc: 0.493 - ETA: 1s - loss: 8.0011 - acc: 0.493 - ETA: 1s - loss: 7.9730 - acc: 0.495 - ETA: 1s - loss: 7.9556 - acc: 0.496 - ETA: 1s - loss: 7.9591 - acc: 0.496 - ETA: 1s - loss: 7.9737 - acc: 0.494 - ETA: 1s - loss: 7.9618 - acc: 0.495 - ETA: 0s - loss: 7.9688 - acc: 0.495 - ETA: 0s - loss: 7.9872 - acc: 0.493 - ETA: 0s - loss: 8.0061 - acc: 0.492 - ETA: 0s - loss: 7.9974 - acc: 0.493 - ETA: 0s - loss: 7.9690 - acc: 0.495 - ETA: 0s - loss: 7.9739 - acc: 0.494 - ETA: 0s - loss: 7.9570 - acc: 0.495 - ETA: 0s - loss: 7.9763 - acc: 0.494 - ETA: 0s - loss: 7.9666 - acc: 0.494 - ETA: 0s - loss: 7.9509 - acc: 0.496 - ETA: 0s - loss: 7.9417 - acc: 0.496 - ETA: 0s - loss: 7.9592 - acc: 0.495 - ETA: 0s - loss: 7.9755 - acc: 0.494 - ETA: 0s - loss: 7.9901 - acc: 0.493 - ETA: 0s - loss: 7.9907 - acc: 0.493 - ETA: 0s - loss: 7.9572 - acc: 0.495 - ETA: 0s - loss: 7.9470 - acc: 0.495 - ETA: 0s - loss: 7.9594 - acc: 0.495 - ETA: 0s - loss: 7.9658 - acc: 0.494 - ETA: 0s - loss: 7.9369 - acc: 0.4961Epoch 00009: val_loss improved from 8.60318 to 8.52814, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.9284 - acc: 0.4967 - val_loss: 8.5281 - val_acc: 0.4036\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 6.4753 - acc: 0.600 - ETA: 2s - loss: 8.0408 - acc: 0.492 - ETA: 2s - loss: 7.7725 - acc: 0.510 - ETA: 2s - loss: 7.7966 - acc: 0.509 - ETA: 2s - loss: 7.8259 - acc: 0.509 - ETA: 2s - loss: 7.6773 - acc: 0.518 - ETA: 2s - loss: 7.8284 - acc: 0.507 - ETA: 2s - loss: 7.7973 - acc: 0.507 - ETA: 2s - loss: 7.8276 - acc: 0.506 - ETA: 2s - loss: 7.7553 - acc: 0.511 - ETA: 2s - loss: 7.6622 - acc: 0.515 - ETA: 2s - loss: 7.7079 - acc: 0.513 - ETA: 2s - loss: 7.8032 - acc: 0.507 - ETA: 2s - loss: 7.8985 - acc: 0.501 - ETA: 2s - loss: 7.9279 - acc: 0.500 - ETA: 1s - loss: 7.8880 - acc: 0.502 - ETA: 1s - loss: 7.9313 - acc: 0.500 - ETA: 1s - loss: 7.9165 - acc: 0.500 - ETA: 1s - loss: 7.8733 - acc: 0.503 - ETA: 1s - loss: 7.9109 - acc: 0.501 - ETA: 1s - loss: 7.8660 - acc: 0.504 - ETA: 1s - loss: 7.8285 - acc: 0.506 - ETA: 1s - loss: 7.8437 - acc: 0.504 - ETA: 1s - loss: 7.8375 - acc: 0.504 - ETA: 1s - loss: 7.8467 - acc: 0.504 - ETA: 1s - loss: 7.8241 - acc: 0.506 - ETA: 1s - loss: 7.8252 - acc: 0.505 - ETA: 1s - loss: 7.8310 - acc: 0.504 - ETA: 1s - loss: 7.8590 - acc: 0.502 - ETA: 1s - loss: 7.9053 - acc: 0.499 - ETA: 1s - loss: 7.8959 - acc: 0.499 - ETA: 1s - loss: 7.8829 - acc: 0.500 - ETA: 1s - loss: 7.8671 - acc: 0.501 - ETA: 1s - loss: 7.8786 - acc: 0.500 - ETA: 1s - loss: 7.8638 - acc: 0.500 - ETA: 0s - loss: 7.8558 - acc: 0.500 - ETA: 0s - loss: 7.8634 - acc: 0.500 - ETA: 0s - loss: 7.9031 - acc: 0.497 - ETA: 0s - loss: 7.9386 - acc: 0.495 - ETA: 0s - loss: 7.8997 - acc: 0.497 - ETA: 0s - loss: 7.8795 - acc: 0.498 - ETA: 0s - loss: 7.8634 - acc: 0.499 - ETA: 0s - loss: 7.8535 - acc: 0.500 - ETA: 0s - loss: 7.8629 - acc: 0.499 - ETA: 0s - loss: 7.8572 - acc: 0.499 - ETA: 0s - loss: 7.8186 - acc: 0.502 - ETA: 0s - loss: 7.8203 - acc: 0.501 - ETA: 0s - loss: 7.8057 - acc: 0.502 - ETA: 0s - loss: 7.8350 - acc: 0.500 - ETA: 0s - loss: 7.8526 - acc: 0.499 - ETA: 0s - loss: 7.8618 - acc: 0.499 - ETA: 0s - loss: 7.8685 - acc: 0.498 - ETA: 0s - loss: 7.8611 - acc: 0.498 - ETA: 0s - loss: 7.8550 - acc: 0.499 - ETA: 0s - loss: 7.8588 - acc: 0.4995Epoch 00010: val_loss improved from 8.52814 to 8.50432, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.8737 - acc: 0.4985 - val_loss: 8.5043 - val_acc: 0.4168\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 7.2532 - acc: 0.550 - ETA: 3s - loss: 6.9818 - acc: 0.557 - ETA: 3s - loss: 7.2974 - acc: 0.541 - ETA: 2s - loss: 7.2627 - acc: 0.541 - ETA: 2s - loss: 7.4991 - acc: 0.527 - ETA: 2s - loss: 7.6433 - acc: 0.515 - ETA: 2s - loss: 7.7138 - acc: 0.508 - ETA: 2s - loss: 7.8006 - acc: 0.504 - ETA: 2s - loss: 7.8403 - acc: 0.503 - ETA: 2s - loss: 7.9016 - acc: 0.499 - ETA: 2s - loss: 7.8951 - acc: 0.499 - ETA: 2s - loss: 7.8934 - acc: 0.499 - ETA: 2s - loss: 7.8347 - acc: 0.502 - ETA: 2s - loss: 7.8716 - acc: 0.499 - ETA: 2s - loss: 7.8028 - acc: 0.503 - ETA: 2s - loss: 7.7847 - acc: 0.505 - ETA: 2s - loss: 7.8757 - acc: 0.500 - ETA: 1s - loss: 7.8864 - acc: 0.500 - ETA: 1s - loss: 7.8538 - acc: 0.501 - ETA: 1s - loss: 7.8369 - acc: 0.503 - ETA: 1s - loss: 7.8250 - acc: 0.504 - ETA: 1s - loss: 7.7755 - acc: 0.507 - ETA: 1s - loss: 7.8052 - acc: 0.505 - ETA: 1s - loss: 7.7237 - acc: 0.509 - ETA: 1s - loss: 7.8014 - acc: 0.504 - ETA: 1s - loss: 7.7964 - acc: 0.505 - ETA: 1s - loss: 7.8336 - acc: 0.503 - ETA: 1s - loss: 7.8378 - acc: 0.502 - ETA: 1s - loss: 7.8016 - acc: 0.505 - ETA: 1s - loss: 7.8259 - acc: 0.503 - ETA: 1s - loss: 7.8122 - acc: 0.504 - ETA: 1s - loss: 7.8266 - acc: 0.503 - ETA: 1s - loss: 7.8547 - acc: 0.501 - ETA: 1s - loss: 7.8914 - acc: 0.499 - ETA: 1s - loss: 7.8980 - acc: 0.498 - ETA: 1s - loss: 7.9066 - acc: 0.497 - ETA: 0s - loss: 7.8722 - acc: 0.500 - ETA: 0s - loss: 7.8738 - acc: 0.500 - ETA: 0s - loss: 7.8863 - acc: 0.499 - ETA: 0s - loss: 7.8636 - acc: 0.500 - ETA: 0s - loss: 7.8474 - acc: 0.501 - ETA: 0s - loss: 7.8617 - acc: 0.500 - ETA: 0s - loss: 7.8493 - acc: 0.501 - ETA: 0s - loss: 7.8417 - acc: 0.501 - ETA: 0s - loss: 7.8272 - acc: 0.502 - ETA: 0s - loss: 7.8287 - acc: 0.502 - ETA: 0s - loss: 7.8426 - acc: 0.501 - ETA: 0s - loss: 7.8256 - acc: 0.502 - ETA: 0s - loss: 7.8117 - acc: 0.503 - ETA: 0s - loss: 7.7925 - acc: 0.504 - ETA: 0s - loss: 7.7978 - acc: 0.504 - ETA: 0s - loss: 7.7867 - acc: 0.505 - ETA: 0s - loss: 7.7990 - acc: 0.504 - ETA: 0s - loss: 7.7733 - acc: 0.506 - ETA: 0s - loss: 7.7650 - acc: 0.506 - ETA: 0s - loss: 7.7795 - acc: 0.506 - ETA: 0s - loss: 7.7720 - acc: 0.5065Epoch 00011: val_loss improved from 8.50432 to 8.46542, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.7925 - acc: 0.5054 - val_loss: 8.4654 - val_acc: 0.4096\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6540/6680 [============================>.] - ETA: 2s - loss: 6.4495 - acc: 0.600 - ETA: 2s - loss: 7.8594 - acc: 0.512 - ETA: 2s - loss: 8.1359 - acc: 0.492 - ETA: 2s - loss: 7.3457 - acc: 0.535 - ETA: 2s - loss: 7.3331 - acc: 0.534 - ETA: 2s - loss: 7.5548 - acc: 0.518 - ETA: 2s - loss: 7.5526 - acc: 0.518 - ETA: 2s - loss: 7.6068 - acc: 0.515 - ETA: 2s - loss: 7.5039 - acc: 0.523 - ETA: 2s - loss: 7.4838 - acc: 0.525 - ETA: 2s - loss: 7.5669 - acc: 0.521 - ETA: 2s - loss: 7.4369 - acc: 0.529 - ETA: 2s - loss: 7.5020 - acc: 0.526 - ETA: 2s - loss: 7.5402 - acc: 0.523 - ETA: 2s - loss: 7.4798 - acc: 0.526 - ETA: 2s - loss: 7.5529 - acc: 0.522 - ETA: 2s - loss: 7.5918 - acc: 0.520 - ETA: 2s - loss: 7.6391 - acc: 0.517 - ETA: 2s - loss: 7.6866 - acc: 0.514 - ETA: 2s - loss: 7.6370 - acc: 0.517 - ETA: 2s - loss: 7.6614 - acc: 0.516 - ETA: 2s - loss: 7.6722 - acc: 0.515 - ETA: 2s - loss: 7.6665 - acc: 0.515 - ETA: 2s - loss: 7.6485 - acc: 0.516 - ETA: 1s - loss: 7.5695 - acc: 0.521 - ETA: 1s - loss: 7.5502 - acc: 0.523 - ETA: 1s - loss: 7.5639 - acc: 0.522 - ETA: 1s - loss: 7.6171 - acc: 0.518 - ETA: 1s - loss: 7.6392 - acc: 0.517 - ETA: 1s - loss: 7.6815 - acc: 0.515 - ETA: 1s - loss: 7.6905 - acc: 0.514 - ETA: 1s - loss: 7.6745 - acc: 0.515 - ETA: 1s - loss: 7.6859 - acc: 0.514 - ETA: 1s - loss: 7.6618 - acc: 0.515 - ETA: 1s - loss: 7.6856 - acc: 0.514 - ETA: 1s - loss: 7.6521 - acc: 0.516 - ETA: 1s - loss: 7.6623 - acc: 0.515 - ETA: 1s - loss: 7.6947 - acc: 0.514 - ETA: 1s - loss: 7.7066 - acc: 0.513 - ETA: 1s - loss: 7.7163 - acc: 0.513 - ETA: 1s - loss: 7.7202 - acc: 0.513 - ETA: 0s - loss: 7.7373 - acc: 0.512 - ETA: 0s - loss: 7.7484 - acc: 0.511 - ETA: 0s - loss: 7.7246 - acc: 0.512 - ETA: 0s - loss: 7.7433 - acc: 0.511 - ETA: 0s - loss: 7.7160 - acc: 0.513 - ETA: 0s - loss: 7.7529 - acc: 0.511 - ETA: 0s - loss: 7.7435 - acc: 0.511 - ETA: 0s - loss: 7.7257 - acc: 0.512 - ETA: 0s - loss: 7.7210 - acc: 0.512 - ETA: 0s - loss: 7.7143 - acc: 0.512 - ETA: 0s - loss: 7.7229 - acc: 0.512 - ETA: 0s - loss: 7.7514 - acc: 0.510 - ETA: 0s - loss: 7.7648 - acc: 0.509 - ETA: 0s - loss: 7.7822 - acc: 0.508 - ETA: 0s - loss: 7.7880 - acc: 0.508 - ETA: 0s - loss: 7.7783 - acc: 0.5093Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.7654 - acc: 0.5100 - val_loss: 8.4743 - val_acc: 0.4036\n",
      "Epoch 14/20\n",
      "6580/6680 [============================>.] - ETA: 3s - loss: 10.4768 - acc: 0.35 - ETA: 2s - loss: 7.3460 - acc: 0.5286 - ETA: 2s - loss: 7.6405 - acc: 0.510 - ETA: 2s - loss: 8.0890 - acc: 0.487 - ETA: 2s - loss: 8.0152 - acc: 0.490 - ETA: 2s - loss: 7.8556 - acc: 0.498 - ETA: 2s - loss: 7.7358 - acc: 0.505 - ETA: 2s - loss: 7.6293 - acc: 0.513 - ETA: 2s - loss: 7.7857 - acc: 0.503 - ETA: 2s - loss: 7.9546 - acc: 0.493 - ETA: 2s - loss: 8.0355 - acc: 0.487 - ETA: 2s - loss: 8.0276 - acc: 0.486 - ETA: 2s - loss: 8.0238 - acc: 0.486 - ETA: 2s - loss: 8.0075 - acc: 0.487 - ETA: 2s - loss: 8.0244 - acc: 0.485 - ETA: 2s - loss: 8.0768 - acc: 0.482 - ETA: 1s - loss: 8.0705 - acc: 0.482 - ETA: 1s - loss: 7.9951 - acc: 0.486 - ETA: 1s - loss: 8.0326 - acc: 0.485 - ETA: 1s - loss: 7.9879 - acc: 0.487 - ETA: 1s - loss: 7.9851 - acc: 0.488 - ETA: 1s - loss: 7.9421 - acc: 0.490 - ETA: 1s - loss: 7.8868 - acc: 0.494 - ETA: 1s - loss: 7.9118 - acc: 0.493 - ETA: 1s - loss: 7.9199 - acc: 0.492 - ETA: 1s - loss: 7.8760 - acc: 0.495 - ETA: 1s - loss: 7.9038 - acc: 0.494 - ETA: 1s - loss: 7.8777 - acc: 0.495 - ETA: 1s - loss: 7.8453 - acc: 0.497 - ETA: 1s - loss: 7.8356 - acc: 0.498 - ETA: 1s - loss: 7.8693 - acc: 0.496 - ETA: 1s - loss: 7.8666 - acc: 0.497 - ETA: 1s - loss: 7.8427 - acc: 0.498 - ETA: 1s - loss: 7.8304 - acc: 0.499 - ETA: 1s - loss: 7.7840 - acc: 0.502 - ETA: 1s - loss: 7.7776 - acc: 0.503 - ETA: 0s - loss: 7.7484 - acc: 0.504 - ETA: 0s - loss: 7.7135 - acc: 0.506 - ETA: 0s - loss: 7.7423 - acc: 0.505 - ETA: 0s - loss: 7.7297 - acc: 0.505 - ETA: 0s - loss: 7.7079 - acc: 0.507 - ETA: 0s - loss: 7.6878 - acc: 0.508 - ETA: 0s - loss: 7.6600 - acc: 0.510 - ETA: 0s - loss: 7.6465 - acc: 0.511 - ETA: 0s - loss: 7.6526 - acc: 0.510 - ETA: 0s - loss: 7.6627 - acc: 0.510 - ETA: 0s - loss: 7.6488 - acc: 0.511 - ETA: 0s - loss: 7.6228 - acc: 0.513 - ETA: 0s - loss: 7.6111 - acc: 0.513 - ETA: 0s - loss: 7.6070 - acc: 0.514 - ETA: 0s - loss: 7.6190 - acc: 0.513 - ETA: 0s - loss: 7.6214 - acc: 0.513 - ETA: 0s - loss: 7.6466 - acc: 0.512 - ETA: 0s - loss: 7.6692 - acc: 0.5111Epoch 00013: val_loss improved from 8.46542 to 8.36962, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6757 - acc: 0.5108 - val_loss: 8.3696 - val_acc: 0.4168\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 8.1191 - acc: 0.450 - ETA: 3s - loss: 8.0702 - acc: 0.491 - ETA: 3s - loss: 7.9357 - acc: 0.500 - ETA: 3s - loss: 7.9247 - acc: 0.502 - ETA: 3s - loss: 7.8120 - acc: 0.509 - ETA: 3s - loss: 7.6424 - acc: 0.519 - ETA: 3s - loss: 7.4859 - acc: 0.530 - ETA: 2s - loss: 7.6784 - acc: 0.519 - ETA: 2s - loss: 7.6886 - acc: 0.518 - ETA: 2s - loss: 7.7069 - acc: 0.517 - ETA: 2s - loss: 7.7120 - acc: 0.516 - ETA: 2s - loss: 7.8228 - acc: 0.509 - ETA: 2s - loss: 7.7544 - acc: 0.513 - ETA: 2s - loss: 7.7470 - acc: 0.514 - ETA: 2s - loss: 7.6705 - acc: 0.518 - ETA: 2s - loss: 7.6413 - acc: 0.520 - ETA: 2s - loss: 7.5957 - acc: 0.521 - ETA: 2s - loss: 7.5690 - acc: 0.523 - ETA: 2s - loss: 7.4859 - acc: 0.529 - ETA: 2s - loss: 7.4043 - acc: 0.534 - ETA: 2s - loss: 7.4210 - acc: 0.533 - ETA: 2s - loss: 7.4082 - acc: 0.534 - ETA: 2s - loss: 7.4216 - acc: 0.533 - ETA: 1s - loss: 7.4345 - acc: 0.532 - ETA: 1s - loss: 7.3897 - acc: 0.535 - ETA: 1s - loss: 7.4044 - acc: 0.534 - ETA: 1s - loss: 7.3905 - acc: 0.534 - ETA: 1s - loss: 7.3873 - acc: 0.534 - ETA: 1s - loss: 7.3781 - acc: 0.534 - ETA: 1s - loss: 7.3924 - acc: 0.533 - ETA: 1s - loss: 7.4233 - acc: 0.531 - ETA: 1s - loss: 7.4619 - acc: 0.528 - ETA: 1s - loss: 7.4428 - acc: 0.529 - ETA: 1s - loss: 7.4667 - acc: 0.527 - ETA: 1s - loss: 7.4746 - acc: 0.526 - ETA: 1s - loss: 7.4823 - acc: 0.525 - ETA: 1s - loss: 7.4716 - acc: 0.526 - ETA: 1s - loss: 7.4985 - acc: 0.524 - ETA: 1s - loss: 7.4770 - acc: 0.526 - ETA: 1s - loss: 7.4648 - acc: 0.527 - ETA: 1s - loss: 7.5160 - acc: 0.524 - ETA: 0s - loss: 7.5405 - acc: 0.523 - ETA: 0s - loss: 7.5298 - acc: 0.523 - ETA: 0s - loss: 7.5401 - acc: 0.522 - ETA: 0s - loss: 7.5246 - acc: 0.523 - ETA: 0s - loss: 7.5507 - acc: 0.522 - ETA: 0s - loss: 7.5677 - acc: 0.521 - ETA: 0s - loss: 7.5770 - acc: 0.520 - ETA: 0s - loss: 7.5592 - acc: 0.521 - ETA: 0s - loss: 7.5814 - acc: 0.520 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6050 - acc: 0.519 - ETA: 0s - loss: 7.6259 - acc: 0.518 - ETA: 0s - loss: 7.6211 - acc: 0.518 - ETA: 0s - loss: 7.6039 - acc: 0.519 - ETA: 0s - loss: 7.6143 - acc: 0.518 - ETA: 0s - loss: 7.6179 - acc: 0.518 - ETA: 0s - loss: 7.6215 - acc: 0.5186Epoch 00014: val_loss improved from 8.36962 to 8.29462, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.6171 - acc: 0.5190 - val_loss: 8.2946 - val_acc: 0.4287\n",
      "Epoch 16/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 7.2635 - acc: 0.550 - ETA: 2s - loss: 7.8818 - acc: 0.500 - ETA: 2s - loss: 7.6538 - acc: 0.519 - ETA: 2s - loss: 7.7301 - acc: 0.513 - ETA: 2s - loss: 7.6270 - acc: 0.521 - ETA: 2s - loss: 7.8571 - acc: 0.506 - ETA: 2s - loss: 7.7768 - acc: 0.512 - ETA: 2s - loss: 7.7399 - acc: 0.515 - ETA: 2s - loss: 7.7312 - acc: 0.514 - ETA: 2s - loss: 7.6309 - acc: 0.521 - ETA: 2s - loss: 7.6504 - acc: 0.520 - ETA: 2s - loss: 7.5978 - acc: 0.521 - ETA: 2s - loss: 7.5918 - acc: 0.522 - ETA: 2s - loss: 7.5499 - acc: 0.524 - ETA: 2s - loss: 7.6451 - acc: 0.519 - ETA: 2s - loss: 7.7274 - acc: 0.514 - ETA: 2s - loss: 7.7723 - acc: 0.511 - ETA: 2s - loss: 7.7591 - acc: 0.512 - ETA: 2s - loss: 7.7080 - acc: 0.516 - ETA: 2s - loss: 7.7203 - acc: 0.515 - ETA: 1s - loss: 7.7586 - acc: 0.513 - ETA: 1s - loss: 7.7348 - acc: 0.514 - ETA: 1s - loss: 7.7444 - acc: 0.513 - ETA: 1s - loss: 7.7465 - acc: 0.513 - ETA: 1s - loss: 7.7508 - acc: 0.513 - ETA: 1s - loss: 7.7711 - acc: 0.511 - ETA: 1s - loss: 7.7875 - acc: 0.511 - ETA: 1s - loss: 7.7927 - acc: 0.510 - ETA: 1s - loss: 7.7901 - acc: 0.510 - ETA: 1s - loss: 7.8511 - acc: 0.507 - ETA: 1s - loss: 7.8544 - acc: 0.506 - ETA: 1s - loss: 7.8357 - acc: 0.507 - ETA: 1s - loss: 7.8195 - acc: 0.508 - ETA: 1s - loss: 7.7948 - acc: 0.510 - ETA: 1s - loss: 7.7749 - acc: 0.511 - ETA: 1s - loss: 7.7451 - acc: 0.513 - ETA: 1s - loss: 7.7392 - acc: 0.514 - ETA: 1s - loss: 7.7333 - acc: 0.514 - ETA: 0s - loss: 7.7100 - acc: 0.516 - ETA: 0s - loss: 7.6758 - acc: 0.518 - ETA: 0s - loss: 7.6740 - acc: 0.518 - ETA: 0s - loss: 7.6735 - acc: 0.518 - ETA: 0s - loss: 7.6920 - acc: 0.516 - ETA: 0s - loss: 7.6697 - acc: 0.518 - ETA: 0s - loss: 7.6695 - acc: 0.518 - ETA: 0s - loss: 7.6542 - acc: 0.519 - ETA: 0s - loss: 7.6551 - acc: 0.519 - ETA: 0s - loss: 7.6409 - acc: 0.520 - ETA: 0s - loss: 7.6396 - acc: 0.519 - ETA: 0s - loss: 7.6266 - acc: 0.520 - ETA: 0s - loss: 7.6219 - acc: 0.521 - ETA: 0s - loss: 7.6389 - acc: 0.520 - ETA: 0s - loss: 7.6367 - acc: 0.519 - ETA: 0s - loss: 7.6329 - acc: 0.520 - ETA: 0s - loss: 7.6234 - acc: 0.520 - ETA: 0s - loss: 7.6242 - acc: 0.5207Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.6061 - acc: 0.5217 - val_loss: 8.3678 - val_acc: 0.4180\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 7.2603 - acc: 0.550 - ETA: 3s - loss: 7.3919 - acc: 0.541 - ETA: 3s - loss: 7.2556 - acc: 0.550 - ETA: 3s - loss: 7.4446 - acc: 0.538 - ETA: 3s - loss: 7.7097 - acc: 0.521 - ETA: 2s - loss: 7.4213 - acc: 0.539 - ETA: 2s - loss: 7.6229 - acc: 0.527 - ETA: 2s - loss: 7.6053 - acc: 0.528 - ETA: 2s - loss: 7.5328 - acc: 0.530 - ETA: 2s - loss: 7.5453 - acc: 0.527 - ETA: 2s - loss: 7.4494 - acc: 0.532 - ETA: 2s - loss: 7.4969 - acc: 0.527 - ETA: 2s - loss: 7.4867 - acc: 0.528 - ETA: 2s - loss: 7.5227 - acc: 0.526 - ETA: 2s - loss: 7.4745 - acc: 0.530 - ETA: 2s - loss: 7.4288 - acc: 0.532 - ETA: 2s - loss: 7.4533 - acc: 0.529 - ETA: 2s - loss: 7.4586 - acc: 0.528 - ETA: 2s - loss: 7.4895 - acc: 0.526 - ETA: 1s - loss: 7.4453 - acc: 0.529 - ETA: 1s - loss: 7.3830 - acc: 0.533 - ETA: 1s - loss: 7.3776 - acc: 0.533 - ETA: 1s - loss: 7.3054 - acc: 0.538 - ETA: 1s - loss: 7.3171 - acc: 0.537 - ETA: 1s - loss: 7.3206 - acc: 0.537 - ETA: 1s - loss: 7.3628 - acc: 0.535 - ETA: 1s - loss: 7.3986 - acc: 0.532 - ETA: 1s - loss: 7.3544 - acc: 0.534 - ETA: 1s - loss: 7.3540 - acc: 0.534 - ETA: 1s - loss: 7.3563 - acc: 0.534 - ETA: 1s - loss: 7.3580 - acc: 0.533 - ETA: 1s - loss: 7.4031 - acc: 0.530 - ETA: 1s - loss: 7.4518 - acc: 0.527 - ETA: 1s - loss: 7.4404 - acc: 0.528 - ETA: 1s - loss: 7.4320 - acc: 0.528 - ETA: 1s - loss: 7.4322 - acc: 0.528 - ETA: 1s - loss: 7.4380 - acc: 0.527 - ETA: 1s - loss: 7.4436 - acc: 0.527 - ETA: 0s - loss: 7.4739 - acc: 0.525 - ETA: 0s - loss: 7.5068 - acc: 0.524 - ETA: 0s - loss: 7.4910 - acc: 0.525 - ETA: 0s - loss: 7.4882 - acc: 0.525 - ETA: 0s - loss: 7.4994 - acc: 0.524 - ETA: 0s - loss: 7.4936 - acc: 0.524 - ETA: 0s - loss: 7.4997 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4998 - acc: 0.524 - ETA: 0s - loss: 7.5014 - acc: 0.524 - ETA: 0s - loss: 7.4827 - acc: 0.525 - ETA: 0s - loss: 7.4958 - acc: 0.524 - ETA: 0s - loss: 7.4885 - acc: 0.524 - ETA: 0s - loss: 7.4867 - acc: 0.524 - ETA: 0s - loss: 7.5016 - acc: 0.523 - ETA: 0s - loss: 7.4819 - acc: 0.524 - ETA: 0s - loss: 7.4759 - acc: 0.525 - ETA: 0s - loss: 7.4787 - acc: 0.524 - ETA: 0s - loss: 7.4916 - acc: 0.5240Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s - loss: 7.4977 - acc: 0.5237 - val_loss: 8.3458 - val_acc: 0.4180\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 8.0591 - acc: 0.500 - ETA: 2s - loss: 7.0893 - acc: 0.550 - ETA: 2s - loss: 6.9683 - acc: 0.560 - ETA: 2s - loss: 6.9400 - acc: 0.561 - ETA: 2s - loss: 7.1489 - acc: 0.546 - ETA: 2s - loss: 7.1231 - acc: 0.548 - ETA: 2s - loss: 7.2517 - acc: 0.541 - ETA: 2s - loss: 7.2871 - acc: 0.538 - ETA: 2s - loss: 7.3151 - acc: 0.538 - ETA: 2s - loss: 7.2947 - acc: 0.540 - ETA: 2s - loss: 7.2529 - acc: 0.543 - ETA: 2s - loss: 7.2548 - acc: 0.543 - ETA: 2s - loss: 7.2656 - acc: 0.541 - ETA: 2s - loss: 7.2673 - acc: 0.540 - ETA: 2s - loss: 7.2426 - acc: 0.540 - ETA: 2s - loss: 7.2729 - acc: 0.538 - ETA: 2s - loss: 7.3583 - acc: 0.532 - ETA: 1s - loss: 7.2495 - acc: 0.537 - ETA: 1s - loss: 7.2414 - acc: 0.537 - ETA: 1s - loss: 7.2802 - acc: 0.534 - ETA: 1s - loss: 7.2668 - acc: 0.535 - ETA: 1s - loss: 7.3609 - acc: 0.529 - ETA: 1s - loss: 7.3710 - acc: 0.528 - ETA: 1s - loss: 7.4021 - acc: 0.526 - ETA: 1s - loss: 7.3636 - acc: 0.529 - ETA: 1s - loss: 7.3848 - acc: 0.527 - ETA: 1s - loss: 7.4198 - acc: 0.524 - ETA: 1s - loss: 7.3947 - acc: 0.526 - ETA: 1s - loss: 7.3927 - acc: 0.527 - ETA: 1s - loss: 7.3394 - acc: 0.530 - ETA: 1s - loss: 7.3555 - acc: 0.529 - ETA: 1s - loss: 7.3621 - acc: 0.528 - ETA: 1s - loss: 7.3426 - acc: 0.529 - ETA: 1s - loss: 7.3620 - acc: 0.527 - ETA: 1s - loss: 7.3478 - acc: 0.528 - ETA: 1s - loss: 7.2966 - acc: 0.531 - ETA: 1s - loss: 7.3112 - acc: 0.530 - ETA: 0s - loss: 7.3256 - acc: 0.529 - ETA: 0s - loss: 7.3207 - acc: 0.530 - ETA: 0s - loss: 7.2994 - acc: 0.531 - ETA: 0s - loss: 7.2911 - acc: 0.532 - ETA: 0s - loss: 7.2815 - acc: 0.533 - ETA: 0s - loss: 7.2866 - acc: 0.533 - ETA: 0s - loss: 7.2518 - acc: 0.535 - ETA: 0s - loss: 7.2667 - acc: 0.534 - ETA: 0s - loss: 7.2647 - acc: 0.534 - ETA: 0s - loss: 7.2813 - acc: 0.533 - ETA: 0s - loss: 7.2608 - acc: 0.534 - ETA: 0s - loss: 7.2649 - acc: 0.534 - ETA: 0s - loss: 7.2799 - acc: 0.533 - ETA: 0s - loss: 7.2986 - acc: 0.532 - ETA: 0s - loss: 7.3243 - acc: 0.530 - ETA: 0s - loss: 7.3390 - acc: 0.529 - ETA: 0s - loss: 7.3387 - acc: 0.529 - ETA: 0s - loss: 7.3281 - acc: 0.529 - ETA: 0s - loss: 7.3360 - acc: 0.5292Epoch 00017: val_loss improved from 8.29462 to 8.08558, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.3317 - acc: 0.5295 - val_loss: 8.0856 - val_acc: 0.4335\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 2s - loss: 6.8885 - acc: 0.550 - ETA: 2s - loss: 7.2228 - acc: 0.535 - ETA: 2s - loss: 7.3910 - acc: 0.526 - ETA: 2s - loss: 7.4907 - acc: 0.521 - ETA: 2s - loss: 7.3426 - acc: 0.532 - ETA: 2s - loss: 7.3179 - acc: 0.532 - ETA: 2s - loss: 7.2918 - acc: 0.533 - ETA: 2s - loss: 7.3430 - acc: 0.531 - ETA: 2s - loss: 7.4151 - acc: 0.527 - ETA: 2s - loss: 7.3154 - acc: 0.532 - ETA: 2s - loss: 7.2280 - acc: 0.537 - ETA: 2s - loss: 7.2360 - acc: 0.537 - ETA: 2s - loss: 7.2641 - acc: 0.536 - ETA: 2s - loss: 7.2337 - acc: 0.538 - ETA: 2s - loss: 7.2446 - acc: 0.536 - ETA: 2s - loss: 7.2642 - acc: 0.535 - ETA: 2s - loss: 7.2639 - acc: 0.535 - ETA: 2s - loss: 7.2385 - acc: 0.537 - ETA: 1s - loss: 7.2326 - acc: 0.538 - ETA: 1s - loss: 7.2576 - acc: 0.537 - ETA: 1s - loss: 7.2939 - acc: 0.533 - ETA: 1s - loss: 7.2757 - acc: 0.533 - ETA: 1s - loss: 7.3289 - acc: 0.529 - ETA: 1s - loss: 7.2758 - acc: 0.532 - ETA: 1s - loss: 7.2195 - acc: 0.536 - ETA: 1s - loss: 7.1786 - acc: 0.539 - ETA: 1s - loss: 7.1907 - acc: 0.538 - ETA: 1s - loss: 7.1671 - acc: 0.540 - ETA: 1s - loss: 7.1686 - acc: 0.540 - ETA: 1s - loss: 7.1838 - acc: 0.539 - ETA: 1s - loss: 7.1874 - acc: 0.539 - ETA: 1s - loss: 7.1951 - acc: 0.539 - ETA: 1s - loss: 7.2130 - acc: 0.537 - ETA: 1s - loss: 7.1998 - acc: 0.538 - ETA: 1s - loss: 7.2027 - acc: 0.538 - ETA: 1s - loss: 7.2407 - acc: 0.535 - ETA: 1s - loss: 7.2058 - acc: 0.537 - ETA: 1s - loss: 7.2381 - acc: 0.535 - ETA: 0s - loss: 7.2452 - acc: 0.535 - ETA: 0s - loss: 7.2500 - acc: 0.535 - ETA: 0s - loss: 7.2371 - acc: 0.536 - ETA: 0s - loss: 7.2252 - acc: 0.536 - ETA: 0s - loss: 7.2330 - acc: 0.536 - ETA: 0s - loss: 7.2559 - acc: 0.535 - ETA: 0s - loss: 7.2780 - acc: 0.534 - ETA: 0s - loss: 7.2628 - acc: 0.535 - ETA: 0s - loss: 7.2629 - acc: 0.535 - ETA: 0s - loss: 7.2444 - acc: 0.536 - ETA: 0s - loss: 7.2302 - acc: 0.537 - ETA: 0s - loss: 7.2377 - acc: 0.537 - ETA: 0s - loss: 7.2305 - acc: 0.537 - ETA: 0s - loss: 7.2353 - acc: 0.537 - ETA: 0s - loss: 7.2317 - acc: 0.537 - ETA: 0s - loss: 7.2283 - acc: 0.537 - ETA: 0s - loss: 7.2171 - acc: 0.538 - ETA: 0s - loss: 7.2086 - acc: 0.5388Epoch 00018: val_loss improved from 8.08558 to 8.07654, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.2070 - acc: 0.5388 - val_loss: 8.0765 - val_acc: 0.4299\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 2s - loss: 8.0597 - acc: 0.500 - ETA: 2s - loss: 8.6445 - acc: 0.450 - ETA: 2s - loss: 7.7636 - acc: 0.510 - ETA: 2s - loss: 7.4556 - acc: 0.530 - ETA: 2s - loss: 7.4778 - acc: 0.528 - ETA: 2s - loss: 7.4375 - acc: 0.531 - ETA: 2s - loss: 7.4656 - acc: 0.530 - ETA: 2s - loss: 7.5119 - acc: 0.527 - ETA: 2s - loss: 7.3846 - acc: 0.536 - ETA: 2s - loss: 7.5269 - acc: 0.525 - ETA: 2s - loss: 7.4774 - acc: 0.528 - ETA: 2s - loss: 7.4726 - acc: 0.528 - ETA: 2s - loss: 7.4169 - acc: 0.531 - ETA: 2s - loss: 7.3852 - acc: 0.533 - ETA: 2s - loss: 7.3618 - acc: 0.534 - ETA: 2s - loss: 7.3575 - acc: 0.534 - ETA: 2s - loss: 7.3638 - acc: 0.533 - ETA: 1s - loss: 7.3835 - acc: 0.531 - ETA: 1s - loss: 7.3778 - acc: 0.531 - ETA: 1s - loss: 7.3376 - acc: 0.534 - ETA: 1s - loss: 7.3365 - acc: 0.534 - ETA: 1s - loss: 7.3058 - acc: 0.535 - ETA: 1s - loss: 7.3157 - acc: 0.535 - ETA: 1s - loss: 7.2926 - acc: 0.536 - ETA: 1s - loss: 7.2798 - acc: 0.538 - ETA: 1s - loss: 7.2591 - acc: 0.539 - ETA: 1s - loss: 7.2075 - acc: 0.542 - ETA: 1s - loss: 7.2032 - acc: 0.542 - ETA: 1s - loss: 7.1955 - acc: 0.543 - ETA: 1s - loss: 7.1500 - acc: 0.545 - ETA: 1s - loss: 7.1603 - acc: 0.544 - ETA: 1s - loss: 7.1489 - acc: 0.545 - ETA: 1s - loss: 7.1580 - acc: 0.544 - ETA: 1s - loss: 7.1693 - acc: 0.544 - ETA: 1s - loss: 7.1396 - acc: 0.546 - ETA: 1s - loss: 7.1629 - acc: 0.545 - ETA: 1s - loss: 7.1425 - acc: 0.546 - ETA: 0s - loss: 7.1432 - acc: 0.545 - ETA: 0s - loss: 7.1497 - acc: 0.545 - ETA: 0s - loss: 7.1651 - acc: 0.544 - ETA: 0s - loss: 7.1317 - acc: 0.546 - ETA: 0s - loss: 7.1152 - acc: 0.547 - ETA: 0s - loss: 7.1272 - acc: 0.546 - ETA: 0s - loss: 7.1364 - acc: 0.546 - ETA: 0s - loss: 7.1539 - acc: 0.545 - ETA: 0s - loss: 7.1809 - acc: 0.543 - ETA: 0s - loss: 7.1697 - acc: 0.544 - ETA: 0s - loss: 7.1756 - acc: 0.543 - ETA: 0s - loss: 7.1549 - acc: 0.545 - ETA: 0s - loss: 7.1626 - acc: 0.544 - ETA: 0s - loss: 7.1839 - acc: 0.543 - ETA: 0s - loss: 7.1940 - acc: 0.542 - ETA: 0s - loss: 7.1900 - acc: 0.542 - ETA: 0s - loss: 7.1768 - acc: 0.543 - ETA: 0s - loss: 7.1672 - acc: 0.543 - ETA: 0s - loss: 7.1493 - acc: 0.5442Epoch 00019: val_loss improved from 8.07654 to 8.01932, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 3s - loss: 7.1519 - acc: 0.5442 - val_loss: 8.0193 - val_acc: 0.4407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b9f5bff98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.5311%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def extract_bottleneck_features(network):\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{}Data.npz'.format(network))\n",
    "    train_network = bottleneck_features['train']\n",
    "    valid_network = bottleneck_features['valid']\n",
    "    test_network = bottleneck_features['test']\n",
    "    \n",
    "    return train_network, valid_network, test_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xception_feature_set = extract_bottleneck_features(\"Xception\")\n",
    "resnet_feature_set = extract_bottleneck_features('Resnet50')\n",
    "inception_feature_set = extract_bottleneck_features('InceptionV3') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__  My model is the merge model which use all of the bottleneck features extracted with Xception, Resnet50 and InceptionV3. I think each network extracts the different features of an image and it is possible some network extracts the good features for some dog breed but not for another one. Therefore I use the all the features from the three networks and adjust each feature's weight by learning.\n",
    "\n",
    "The architecture is as follows.\n",
    "In the input layer, the bottleneck features are extracted.\n",
    "In next layer, each bottleneck feature is pooled with global averaging to unify the dimensions of the features and to reduce the number of parameters.\n",
    "In the merge layer, the features from each network are concatenated. I select concatenation to merge layers to remain the features distinguished.\n",
    "Then, there is a dense layer before the output layer to adjust each feature's weight.\n",
    "To output for classification, the last layer is a dense layer with softmax acttivation. \n",
    "Dropout Layers are inserted to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_118 (InputLayer)           (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_119 (InputLayer)           (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_120 (InputLayer)           (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Glo (None, 2048)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_31 (Glo (None, 2048)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_32 (Glo (None, 2048)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)     (None, 6144)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1024)          6292480                                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_2446 (Activation)     (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 133)           136325                                       \n",
      "====================================================================================================\n",
      "Total params: 6,428,805.0\n",
      "Trainable params: 6,428,805.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, SeparableConv2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, regularizers\n",
    "import numpy as np\n",
    "\n",
    "xception_input_data = Input(shape=xception_feature_set[0].shape[1:])\n",
    "#print(xception_input_data.shape)\n",
    "#xception_x = MaxPooling2D(pool_size=(7, 7), padding=\"same\")(xception_input_data)\n",
    "xception_x = GlobalAveragePooling2D()(xception_input_data)\n",
    "\n",
    "resnet_input_data = Input(shape=resnet_feature_set[0].shape[1:])\n",
    "#print(resnet_input_data.shape)\n",
    "#resnet_x = MaxPooling2D(pool_size=(1, 1), padding=\"same\")(resnet_input_data)\n",
    "resnet_x = GlobalAveragePooling2D()(resnet_input_data)\n",
    "\n",
    "inception_input_data = Input(shape=inception_feature_set[0].shape[1:])\n",
    "#print(inception_input_data.shape)\n",
    "#inception_x = MaxPooling2D(pool_size=(5, 5), padding=\"same\")(inception_input_data)\n",
    "inception_x = GlobalAveragePooling2D()(inception_input_data)\n",
    "\n",
    "modules = [xception_x, resnet_x,  inception_x]\n",
    "\n",
    "merged = layers.concatenate(modules, axis=1)\n",
    "\n",
    "ker1, l2_1, dr1 = 1024, 1e-05, 0.5\n",
    "#ker1, l2_1, dr1 = 512, 1e-05, 0.5\n",
    "#ker1, l2_1, dr1 = 1024, 0, 0.5\n",
    "#x = Dropout(0.7)\n",
    "x = Dense(ker1, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(l2_1))(merged)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(dr1)(x)\n",
    "x = Dense(133, activation='softmax')(x)\n",
    "\n",
    "merged_model = Model([xception_input_data, resnet_input_data, inception_input_data], x)\n",
    "merged_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "merged_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6060/6680 [==========================>...] - ETA: 19189s - loss: 5.5486 - acc: 0.033 - ETA: 14227s - loss: 8.7866 - acc: 0.066 - ETA: 10994s - loss: 10.2667 - acc: 0.07 - ETA: 9005s - loss: 11.4576 - acc: 0.0667 - ETA: 8058s - loss: 11.5278 - acc: 0.073 - ETA: 7355s - loss: 11.9793 - acc: 0.066 - ETA: 6806s - loss: 12.0818 - acc: 0.057 - ETA: 6762s - loss: 11.8408 - acc: 0.058 - ETA: 6674s - loss: 11.7356 - acc: 0.066 - ETA: 6340s - loss: 11.8484 - acc: 0.063 - ETA: 6111s - loss: 11.7342 - acc: 0.063 - ETA: 6008s - loss: 11.7492 - acc: 0.061 - ETA: 5980s - loss: 11.6970 - acc: 0.059 - ETA: 5817s - loss: 11.5221 - acc: 0.064 - ETA: 5661s - loss: 11.3670 - acc: 0.071 - ETA: 5733s - loss: 11.0790 - acc: 0.081 - ETA: 6060s - loss: 10.7858 - acc: 0.084 - ETA: 6173s - loss: 10.6052 - acc: 0.085 - ETA: 6346s - loss: 10.4114 - acc: 0.089 - ETA: 6408s - loss: 10.1133 - acc: 0.103 - ETA: 6394s - loss: 9.8958 - acc: 0.101 - ETA: 6529s - loss: 9.6648 - acc: 0.10 - ETA: 6577s - loss: 9.4253 - acc: 0.11 - ETA: 6521s - loss: 9.2257 - acc: 0.11 - ETA: 6401s - loss: 9.0232 - acc: 0.11 - ETA: 6506s - loss: 8.8331 - acc: 0.11 - ETA: 6562s - loss: 8.6640 - acc: 0.12 - ETA: 6464s - loss: 8.4757 - acc: 0.13 - ETA: 6694s - loss: 8.3135 - acc: 0.13 - ETA: 6891s - loss: 8.1668 - acc: 0.14 - ETA: 6878s - loss: 8.0047 - acc: 0.15 - ETA: 6904s - loss: 7.8489 - acc: 0.15 - ETA: 6948s - loss: 7.7311 - acc: 0.16 - ETA: 6952s - loss: 7.5766 - acc: 0.16 - ETA: 6939s - loss: 7.4647 - acc: 0.17 - ETA: 6977s - loss: 7.3373 - acc: 0.17 - ETA: 7065s - loss: 7.2105 - acc: 0.18 - ETA: 7137s - loss: 7.0886 - acc: 0.19 - ETA: 7256s - loss: 7.0041 - acc: 0.19 - ETA: 7266s - loss: 6.8924 - acc: 0.20 - ETA: 7237s - loss: 6.8129 - acc: 0.20 - ETA: 7131s - loss: 6.7246 - acc: 0.20 - ETA: 6999s - loss: 6.6379 - acc: 0.20 - ETA: 6873s - loss: 6.5664 - acc: 0.20 - ETA: 6766s - loss: 6.5080 - acc: 0.20 - ETA: 6662s - loss: 6.4146 - acc: 0.21 - ETA: 6581s - loss: 6.3337 - acc: 0.21 - ETA: 6471s - loss: 6.2387 - acc: 0.22 - ETA: 6371s - loss: 6.1656 - acc: 0.22 - ETA: 6294s - loss: 6.0878 - acc: 0.23 - ETA: 6229s - loss: 6.0191 - acc: 0.23 - ETA: 6197s - loss: 5.9555 - acc: 0.23 - ETA: 6187s - loss: 5.9073 - acc: 0.23 - ETA: 6187s - loss: 5.8304 - acc: 0.24 - ETA: 6124s - loss: 5.7646 - acc: 0.24 - ETA: 6129s - loss: 5.7080 - acc: 0.25 - ETA: 6115s - loss: 5.6491 - acc: 0.25 - ETA: 6079s - loss: 5.5863 - acc: 0.26 - ETA: 6101s - loss: 5.5262 - acc: 0.26 - ETA: 6089s - loss: 5.4745 - acc: 0.26 - ETA: 6027s - loss: 5.4177 - acc: 0.26 - ETA: 5974s - loss: 5.3640 - acc: 0.27 - ETA: 5943s - loss: 5.2998 - acc: 0.27 - ETA: 5896s - loss: 5.2585 - acc: 0.28 - ETA: 5827s - loss: 5.2092 - acc: 0.28 - ETA: 5758s - loss: 5.1587 - acc: 0.28 - ETA: 5696s - loss: 5.1201 - acc: 0.29 - ETA: 5631s - loss: 5.0759 - acc: 0.29 - ETA: 5564s - loss: 5.0268 - acc: 0.29 - ETA: 5522s - loss: 4.9755 - acc: 0.30 - ETA: 5467s - loss: 4.9358 - acc: 0.30 - ETA: 5399s - loss: 4.8905 - acc: 0.30 - ETA: 5310s - loss: 4.8459 - acc: 0.31 - ETA: 5234s - loss: 4.8010 - acc: 0.31 - ETA: 5156s - loss: 4.7590 - acc: 0.32 - ETA: 5081s - loss: 4.7259 - acc: 0.32 - ETA: 5008s - loss: 4.6838 - acc: 0.32 - ETA: 4939s - loss: 4.6463 - acc: 0.33 - ETA: 4872s - loss: 4.6169 - acc: 0.33 - ETA: 4797s - loss: 4.5834 - acc: 0.33 - ETA: 4719s - loss: 4.5537 - acc: 0.33 - ETA: 4648s - loss: 4.5130 - acc: 0.34 - ETA: 4578s - loss: 4.4816 - acc: 0.34 - ETA: 4510s - loss: 4.4533 - acc: 0.34 - ETA: 4440s - loss: 4.4245 - acc: 0.35 - ETA: 4373s - loss: 4.3918 - acc: 0.35 - ETA: 4318s - loss: 4.3704 - acc: 0.35 - ETA: 4257s - loss: 4.3425 - acc: 0.35 - ETA: 4197s - loss: 4.3250 - acc: 0.35 - ETA: 4132s - loss: 4.2933 - acc: 0.36 - ETA: 4071s - loss: 4.2735 - acc: 0.36 - ETA: 4012s - loss: 4.2429 - acc: 0.36 - ETA: 3954s - loss: 4.2193 - acc: 0.36 - ETA: 3897s - loss: 4.2046 - acc: 0.36 - ETA: 3837s - loss: 4.1762 - acc: 0.37 - ETA: 3784s - loss: 4.1489 - acc: 0.37 - ETA: 3729s - loss: 4.1275 - acc: 0.37 - ETA: 3674s - loss: 4.1114 - acc: 0.37 - ETA: 3619s - loss: 4.0846 - acc: 0.37 - ETA: 3567s - loss: 4.0591 - acc: 0.38 - ETA: 3516s - loss: 4.0377 - acc: 0.38 - ETA: 3473s - loss: 4.0134 - acc: 0.38 - ETA: 3425s - loss: 3.9909 - acc: 0.38 - ETA: 3379s - loss: 3.9706 - acc: 0.38 - ETA: 3330s - loss: 3.9521 - acc: 0.38 - ETA: 3284s - loss: 3.9257 - acc: 0.39 - ETA: 3238s - loss: 3.9002 - acc: 0.39 - ETA: 3191s - loss: 3.8794 - acc: 0.39 - ETA: 3147s - loss: 3.8645 - acc: 0.39 - ETA: 3103s - loss: 3.8496 - acc: 0.39 - ETA: 3061s - loss: 3.8275 - acc: 0.40 - ETA: 3016s - loss: 3.8174 - acc: 0.40 - ETA: 2972s - loss: 3.8001 - acc: 0.40 - ETA: 2930s - loss: 3.7801 - acc: 0.40 - ETA: 2885s - loss: 3.7599 - acc: 0.40 - ETA: 2842s - loss: 3.7398 - acc: 0.40 - ETA: 2800s - loss: 3.7245 - acc: 0.40 - ETA: 2760s - loss: 3.7112 - acc: 0.40 - ETA: 2722s - loss: 3.6941 - acc: 0.40 - ETA: 2685s - loss: 3.6780 - acc: 0.40 - ETA: 2646s - loss: 3.6621 - acc: 0.41 - ETA: 2607s - loss: 3.6447 - acc: 0.41 - ETA: 2568s - loss: 3.6338 - acc: 0.41 - ETA: 2535s - loss: 3.6149 - acc: 0.41 - ETA: 2494s - loss: 3.5983 - acc: 0.41 - ETA: 2453s - loss: 3.5781 - acc: 0.41 - ETA: 2413s - loss: 3.5616 - acc: 0.41 - ETA: 2373s - loss: 3.5464 - acc: 0.42 - ETA: 2334s - loss: 3.5308 - acc: 0.42 - ETA: 2295s - loss: 3.5185 - acc: 0.42 - ETA: 2257s - loss: 3.5015 - acc: 0.42 - ETA: 2219s - loss: 3.4828 - acc: 0.42 - ETA: 2182s - loss: 3.4732 - acc: 0.42 - ETA: 2146s - loss: 3.4585 - acc: 0.42 - ETA: 2113s - loss: 3.4461 - acc: 0.42 - ETA: 2079s - loss: 3.4274 - acc: 0.43 - ETA: 2043s - loss: 3.4125 - acc: 0.43 - ETA: 2008s - loss: 3.3960 - acc: 0.43 - ETA: 1973s - loss: 3.3792 - acc: 0.43 - ETA: 1943s - loss: 3.3732 - acc: 0.43 - ETA: 1912s - loss: 3.3587 - acc: 0.43 - ETA: 1882s - loss: 3.3435 - acc: 0.44 - ETA: 1852s - loss: 3.3280 - acc: 0.44 - ETA: 1822s - loss: 3.3182 - acc: 0.44 - ETA: 1791s - loss: 3.3065 - acc: 0.44 - ETA: 1758s - loss: 3.2964 - acc: 0.44 - ETA: 1726s - loss: 3.2830 - acc: 0.44 - ETA: 1694s - loss: 3.2713 - acc: 0.44 - ETA: 1663s - loss: 3.2626 - acc: 0.44 - ETA: 1631s - loss: 3.2539 - acc: 0.44 - ETA: 1600s - loss: 3.2425 - acc: 0.45 - ETA: 1569s - loss: 3.2310 - acc: 0.45 - ETA: 1539s - loss: 3.2226 - acc: 0.45 - ETA: 1509s - loss: 3.2088 - acc: 0.45 - ETA: 1480s - loss: 3.1937 - acc: 0.45 - ETA: 1452s - loss: 3.1809 - acc: 0.45 - ETA: 1430s - loss: 3.1745 - acc: 0.46 - ETA: 1401s - loss: 3.1606 - acc: 0.46 - ETA: 1374s - loss: 3.1557 - acc: 0.46 - ETA: 1346s - loss: 3.1492 - acc: 0.46 - ETA: 1319s - loss: 3.1376 - acc: 0.46 - ETA: 1291s - loss: 3.1288 - acc: 0.46 - ETA: 1264s - loss: 3.1232 - acc: 0.46 - ETA: 1237s - loss: 3.1119 - acc: 0.46 - ETA: 1210s - loss: 3.1030 - acc: 0.46 - ETA: 1183s - loss: 3.0930 - acc: 0.46 - ETA: 1157s - loss: 3.0776 - acc: 0.46 - ETA: 1131s - loss: 3.0697 - acc: 0.47 - ETA: 1104s - loss: 3.0600 - acc: 0.47 - ETA: 1079s - loss: 3.0503 - acc: 0.47 - ETA: 1053s - loss: 3.0396 - acc: 0.47 - ETA: 1028s - loss: 3.0332 - acc: 0.47 - ETA: 1003s - loss: 3.0209 - acc: 0.47 - ETA: 978s - loss: 3.0078 - acc: 0.4776 - ETA: 954s - loss: 2.9989 - acc: 0.478 - ETA: 930s - loss: 2.9938 - acc: 0.479 - ETA: 906s - loss: 2.9844 - acc: 0.480 - ETA: 883s - loss: 2.9757 - acc: 0.481 - ETA: 859s - loss: 2.9646 - acc: 0.483 - ETA: 836s - loss: 2.9550 - acc: 0.484 - ETA: 812s - loss: 2.9466 - acc: 0.485 - ETA: 789s - loss: 2.9353 - acc: 0.487 - ETA: 766s - loss: 2.9302 - acc: 0.488 - ETA: 743s - loss: 2.9229 - acc: 0.488 - ETA: 721s - loss: 2.9155 - acc: 0.489 - ETA: 699s - loss: 2.9065 - acc: 0.490 - ETA: 677s - loss: 2.8995 - acc: 0.491 - ETA: 655s - loss: 2.8894 - acc: 0.492 - ETA: 633s - loss: 2.8814 - acc: 0.493 - ETA: 611s - loss: 2.8758 - acc: 0.493 - ETA: 590s - loss: 2.8706 - acc: 0.493 - ETA: 569s - loss: 2.8616 - acc: 0.495 - ETA: 548s - loss: 2.8538 - acc: 0.496 - ETA: 527s - loss: 2.8468 - acc: 0.496 - ETA: 506s - loss: 2.8394 - acc: 0.497 - ETA: 486s - loss: 2.8340 - acc: 0.498 - ETA: 465s - loss: 2.8270 - acc: 0.498 - ETA: 445s - loss: 2.8211 - acc: 0.499 - ETA: 425s - loss: 2.8170 - acc: 0.499 - ETA: 405s - loss: 2.8122 - acc: 0.500 - ETA: 386s - loss: 2.8069 - acc: 0.500 - ETA: 366s - loss: 2.8023 - acc: 0.5015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 347s - loss: 2.7945 - acc: 0.502 - ETA: 328s - loss: 2.7884 - acc: 0.503 - ETA: 309s - loss: 2.7842 - acc: 0.504 - ETA: 290s - loss: 2.7821 - acc: 0.504 - ETA: 271s - loss: 2.7773 - acc: 0.505 - ETA: 253s - loss: 2.7724 - acc: 0.505 - ETA: 235s - loss: 2.7676 - acc: 0.506 - ETA: 217s - loss: 2.7632 - acc: 0.506 - ETA: 199s - loss: 2.7584 - acc: 0.507 - ETA: 181s - loss: 2.7513 - acc: 0.508 - ETA: 163s - loss: 2.7454 - acc: 0.508 - ETA: 146s - loss: 2.7396 - acc: 0.509 - ETA: 128s - loss: 2.7314 - acc: 0.510 - ETA: 111s - loss: 2.7275 - acc: 0.511 - ETA: 94s - loss: 2.7231 - acc: 0.511 - ETA: 77s - loss: 2.7163 - acc: 0.51 - ETA: 60s - loss: 2.7120 - acc: 0.51 - ETA: 43s - loss: 2.7082 - acc: 0.51 - ETA: 27s - loss: 2.7062 - acc: 0.51 - ETA: 10s - loss: 2.7029 - acc: 0.5143Epoch 00000: val_loss improved from inf to 0.90548, saving model to saved_models/weights.best.Xception_Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 3687s - loss: 2.6981 - acc: 0.5150 - val_loss: 0.9055 - val_acc: 0.7689\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6210/6680 [==========================>...] - ETA: 481s - loss: 0.8115 - acc: 0.700 - ETA: 297s - loss: 1.3571 - acc: 0.616 - ETA: 264s - loss: 1.3856 - acc: 0.633 - ETA: 241s - loss: 1.2016 - acc: 0.675 - ETA: 228s - loss: 1.2408 - acc: 0.686 - ETA: 209s - loss: 1.3723 - acc: 0.672 - ETA: 235s - loss: 1.3318 - acc: 0.685 - ETA: 224s - loss: 1.3133 - acc: 0.695 - ETA: 218s - loss: 1.3038 - acc: 0.696 - ETA: 232s - loss: 1.3169 - acc: 0.700 - ETA: 236s - loss: 1.3376 - acc: 0.690 - ETA: 242s - loss: 1.3536 - acc: 0.688 - ETA: 242s - loss: 1.4011 - acc: 0.684 - ETA: 235s - loss: 1.3624 - acc: 0.688 - ETA: 243s - loss: 1.3223 - acc: 0.693 - ETA: 264s - loss: 1.3370 - acc: 0.693 - ETA: 255s - loss: 1.3235 - acc: 0.700 - ETA: 263s - loss: 1.3391 - acc: 0.701 - ETA: 260s - loss: 1.3071 - acc: 0.707 - ETA: 262s - loss: 1.2655 - acc: 0.716 - ETA: 257s - loss: 1.2426 - acc: 0.720 - ETA: 250s - loss: 1.2221 - acc: 0.724 - ETA: 245s - loss: 1.2520 - acc: 0.723 - ETA: 248s - loss: 1.2436 - acc: 0.723 - ETA: 242s - loss: 1.2776 - acc: 0.722 - ETA: 249s - loss: 1.3080 - acc: 0.719 - ETA: 247s - loss: 1.3011 - acc: 0.717 - ETA: 241s - loss: 1.2887 - acc: 0.720 - ETA: 241s - loss: 1.2756 - acc: 0.720 - ETA: 244s - loss: 1.2604 - acc: 0.723 - ETA: 240s - loss: 1.2522 - acc: 0.721 - ETA: 237s - loss: 1.2320 - acc: 0.724 - ETA: 233s - loss: 1.2384 - acc: 0.723 - ETA: 232s - loss: 1.2610 - acc: 0.718 - ETA: 237s - loss: 1.2662 - acc: 0.717 - ETA: 235s - loss: 1.2742 - acc: 0.717 - ETA: 233s - loss: 1.2631 - acc: 0.718 - ETA: 233s - loss: 1.2634 - acc: 0.717 - ETA: 236s - loss: 1.2568 - acc: 0.718 - ETA: 232s - loss: 1.2692 - acc: 0.719 - ETA: 234s - loss: 1.2694 - acc: 0.719 - ETA: 232s - loss: 1.2634 - acc: 0.719 - ETA: 233s - loss: 1.2535 - acc: 0.718 - ETA: 233s - loss: 1.2520 - acc: 0.718 - ETA: 231s - loss: 1.2549 - acc: 0.718 - ETA: 227s - loss: 1.2479 - acc: 0.718 - ETA: 225s - loss: 1.2550 - acc: 0.716 - ETA: 226s - loss: 1.2606 - acc: 0.716 - ETA: 226s - loss: 1.2730 - acc: 0.715 - ETA: 224s - loss: 1.2703 - acc: 0.715 - ETA: 223s - loss: 1.2637 - acc: 0.715 - ETA: 224s - loss: 1.2671 - acc: 0.712 - ETA: 222s - loss: 1.2651 - acc: 0.713 - ETA: 222s - loss: 1.2591 - acc: 0.714 - ETA: 220s - loss: 1.2642 - acc: 0.713 - ETA: 222s - loss: 1.2664 - acc: 0.713 - ETA: 220s - loss: 1.2548 - acc: 0.716 - ETA: 217s - loss: 1.2479 - acc: 0.717 - ETA: 215s - loss: 1.2463 - acc: 0.717 - ETA: 213s - loss: 1.2462 - acc: 0.716 - ETA: 214s - loss: 1.2337 - acc: 0.718 - ETA: 212s - loss: 1.2327 - acc: 0.718 - ETA: 210s - loss: 1.2234 - acc: 0.720 - ETA: 209s - loss: 1.2181 - acc: 0.722 - ETA: 209s - loss: 1.2159 - acc: 0.723 - ETA: 207s - loss: 1.2227 - acc: 0.721 - ETA: 205s - loss: 1.2207 - acc: 0.721 - ETA: 205s - loss: 1.2162 - acc: 0.723 - ETA: 207s - loss: 1.2258 - acc: 0.722 - ETA: 206s - loss: 1.2231 - acc: 0.721 - ETA: 206s - loss: 1.2327 - acc: 0.718 - ETA: 207s - loss: 1.2309 - acc: 0.718 - ETA: 205s - loss: 1.2303 - acc: 0.719 - ETA: 204s - loss: 1.2259 - acc: 0.718 - ETA: 205s - loss: 1.2283 - acc: 0.718 - ETA: 202s - loss: 1.2262 - acc: 0.717 - ETA: 201s - loss: 1.2298 - acc: 0.716 - ETA: 199s - loss: 1.2405 - acc: 0.715 - ETA: 196s - loss: 1.2357 - acc: 0.718 - ETA: 197s - loss: 1.2296 - acc: 0.718 - ETA: 195s - loss: 1.2297 - acc: 0.718 - ETA: 193s - loss: 1.2252 - acc: 0.719 - ETA: 191s - loss: 1.2176 - acc: 0.720 - ETA: 192s - loss: 1.2089 - acc: 0.722 - ETA: 191s - loss: 1.2013 - acc: 0.723 - ETA: 188s - loss: 1.2020 - acc: 0.723 - ETA: 186s - loss: 1.1968 - acc: 0.723 - ETA: 185s - loss: 1.2100 - acc: 0.720 - ETA: 185s - loss: 1.2028 - acc: 0.721 - ETA: 183s - loss: 1.2085 - acc: 0.720 - ETA: 183s - loss: 1.2090 - acc: 0.720 - ETA: 182s - loss: 1.2041 - acc: 0.721 - ETA: 181s - loss: 1.1967 - acc: 0.722 - ETA: 179s - loss: 1.1976 - acc: 0.722 - ETA: 177s - loss: 1.1881 - acc: 0.724 - ETA: 175s - loss: 1.1927 - acc: 0.723 - ETA: 173s - loss: 1.1925 - acc: 0.723 - ETA: 171s - loss: 1.1986 - acc: 0.722 - ETA: 170s - loss: 1.1969 - acc: 0.722 - ETA: 171s - loss: 1.1919 - acc: 0.722 - ETA: 170s - loss: 1.1948 - acc: 0.721 - ETA: 169s - loss: 1.1929 - acc: 0.722 - ETA: 168s - loss: 1.1886 - acc: 0.723 - ETA: 167s - loss: 1.1863 - acc: 0.724 - ETA: 165s - loss: 1.1806 - acc: 0.725 - ETA: 164s - loss: 1.1846 - acc: 0.724 - ETA: 162s - loss: 1.1847 - acc: 0.724 - ETA: 160s - loss: 1.1834 - acc: 0.725 - ETA: 159s - loss: 1.1775 - acc: 0.726 - ETA: 157s - loss: 1.1784 - acc: 0.725 - ETA: 156s - loss: 1.1729 - acc: 0.725 - ETA: 155s - loss: 1.1810 - acc: 0.725 - ETA: 153s - loss: 1.1818 - acc: 0.724 - ETA: 152s - loss: 1.1861 - acc: 0.723 - ETA: 151s - loss: 1.1854 - acc: 0.723 - ETA: 149s - loss: 1.1881 - acc: 0.723 - ETA: 148s - loss: 1.1871 - acc: 0.723 - ETA: 147s - loss: 1.1864 - acc: 0.723 - ETA: 146s - loss: 1.1824 - acc: 0.723 - ETA: 144s - loss: 1.1847 - acc: 0.722 - ETA: 143s - loss: 1.1886 - acc: 0.722 - ETA: 141s - loss: 1.1855 - acc: 0.722 - ETA: 141s - loss: 1.1871 - acc: 0.722 - ETA: 140s - loss: 1.1915 - acc: 0.722 - ETA: 138s - loss: 1.1950 - acc: 0.721 - ETA: 137s - loss: 1.1955 - acc: 0.722 - ETA: 136s - loss: 1.1883 - acc: 0.723 - ETA: 135s - loss: 1.1959 - acc: 0.722 - ETA: 134s - loss: 1.1937 - acc: 0.722 - ETA: 133s - loss: 1.1946 - acc: 0.721 - ETA: 132s - loss: 1.1965 - acc: 0.721 - ETA: 131s - loss: 1.2032 - acc: 0.720 - ETA: 129s - loss: 1.2030 - acc: 0.720 - ETA: 128s - loss: 1.1992 - acc: 0.720 - ETA: 127s - loss: 1.1994 - acc: 0.721 - ETA: 126s - loss: 1.2008 - acc: 0.721 - ETA: 124s - loss: 1.1944 - acc: 0.722 - ETA: 122s - loss: 1.1924 - acc: 0.722 - ETA: 122s - loss: 1.1858 - acc: 0.723 - ETA: 120s - loss: 1.1846 - acc: 0.723 - ETA: 118s - loss: 1.1878 - acc: 0.723 - ETA: 117s - loss: 1.1898 - acc: 0.723 - ETA: 116s - loss: 1.1921 - acc: 0.723 - ETA: 114s - loss: 1.1907 - acc: 0.723 - ETA: 112s - loss: 1.1863 - acc: 0.723 - ETA: 111s - loss: 1.1845 - acc: 0.724 - ETA: 110s - loss: 1.1893 - acc: 0.723 - ETA: 108s - loss: 1.1905 - acc: 0.724 - ETA: 106s - loss: 1.1939 - acc: 0.723 - ETA: 105s - loss: 1.1954 - acc: 0.723 - ETA: 103s - loss: 1.1962 - acc: 0.723 - ETA: 102s - loss: 1.1955 - acc: 0.723 - ETA: 100s - loss: 1.1953 - acc: 0.724 - ETA: 99s - loss: 1.1902 - acc: 0.724 - ETA: 98s - loss: 1.1858 - acc: 0.72 - ETA: 97s - loss: 1.1855 - acc: 0.72 - ETA: 95s - loss: 1.1832 - acc: 0.72 - ETA: 94s - loss: 1.1831 - acc: 0.72 - ETA: 93s - loss: 1.1789 - acc: 0.72 - ETA: 91s - loss: 1.1777 - acc: 0.72 - ETA: 89s - loss: 1.1790 - acc: 0.72 - ETA: 88s - loss: 1.1789 - acc: 0.72 - ETA: 87s - loss: 1.1822 - acc: 0.72 - ETA: 85s - loss: 1.1776 - acc: 0.72 - ETA: 84s - loss: 1.1766 - acc: 0.72 - ETA: 82s - loss: 1.1766 - acc: 0.72 - ETA: 81s - loss: 1.1748 - acc: 0.72 - ETA: 80s - loss: 1.1714 - acc: 0.72 - ETA: 78s - loss: 1.1701 - acc: 0.72 - ETA: 77s - loss: 1.1720 - acc: 0.72 - ETA: 75s - loss: 1.1759 - acc: 0.72 - ETA: 74s - loss: 1.1748 - acc: 0.72 - ETA: 72s - loss: 1.1761 - acc: 0.72 - ETA: 70s - loss: 1.1770 - acc: 0.72 - ETA: 69s - loss: 1.1767 - acc: 0.72 - ETA: 67s - loss: 1.1753 - acc: 0.72 - ETA: 66s - loss: 1.1759 - acc: 0.72 - ETA: 65s - loss: 1.1739 - acc: 0.72 - ETA: 63s - loss: 1.1789 - acc: 0.72 - ETA: 62s - loss: 1.1807 - acc: 0.72 - ETA: 60s - loss: 1.1770 - acc: 0.72 - ETA: 59s - loss: 1.1787 - acc: 0.72 - ETA: 57s - loss: 1.1759 - acc: 0.72 - ETA: 56s - loss: 1.1773 - acc: 0.72 - ETA: 54s - loss: 1.1784 - acc: 0.72 - ETA: 53s - loss: 1.1792 - acc: 0.72 - ETA: 51s - loss: 1.1790 - acc: 0.72 - ETA: 50s - loss: 1.1767 - acc: 0.72 - ETA: 48s - loss: 1.1730 - acc: 0.73 - ETA: 47s - loss: 1.1725 - acc: 0.73 - ETA: 45s - loss: 1.1701 - acc: 0.73 - ETA: 44s - loss: 1.1685 - acc: 0.73 - ETA: 42s - loss: 1.1751 - acc: 0.72 - ETA: 41s - loss: 1.1769 - acc: 0.72 - ETA: 40s - loss: 1.1813 - acc: 0.72 - ETA: 38s - loss: 1.1843 - acc: 0.72 - ETA: 37s - loss: 1.1827 - acc: 0.72 - ETA: 35s - loss: 1.1815 - acc: 0.72 - ETA: 34s - loss: 1.1806 - acc: 0.72 - ETA: 32s - loss: 1.1762 - acc: 0.72 - ETA: 31s - loss: 1.1790 - acc: 0.72 - ETA: 30s - loss: 1.1792 - acc: 0.72 - ETA: 28s - loss: 1.1834 - acc: 0.72 - ETA: 27s - loss: 1.1891 - acc: 0.72 - ETA: 25s - loss: 1.1890 - acc: 0.72 - ETA: 24s - loss: 1.1880 - acc: 0.72 - ETA: 22s - loss: 1.1870 - acc: 0.7283"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 21s - loss: 1.1843 - acc: 0.72 - ETA: 19s - loss: 1.1871 - acc: 0.72 - ETA: 18s - loss: 1.1876 - acc: 0.72 - ETA: 16s - loss: 1.1901 - acc: 0.72 - ETA: 15s - loss: 1.1880 - acc: 0.72 - ETA: 14s - loss: 1.1883 - acc: 0.72 - ETA: 12s - loss: 1.1880 - acc: 0.72 - ETA: 11s - loss: 1.1873 - acc: 0.72 - ETA: 9s - loss: 1.1861 - acc: 0.7296 - ETA: 8s - loss: 1.1839 - acc: 0.730 - ETA: 6s - loss: 1.1814 - acc: 0.730 - ETA: 5s - loss: 1.1791 - acc: 0.731 - ETA: 3s - loss: 1.1792 - acc: 0.731 - ETA: 2s - loss: 1.1775 - acc: 0.731 - ETA: 0s - loss: 1.1762 - acc: 0.7312Epoch 00001: val_loss improved from 0.90548 to 0.73564, saving model to saved_models/weights.best.Xception_Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 335s - loss: 1.1754 - acc: 0.7311 - val_loss: 0.7356 - val_acc: 0.8084\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6270/6680 [===========================>..] - ETA: 186s - loss: 1.0239 - acc: 0.733 - ETA: 182s - loss: 0.9756 - acc: 0.766 - ETA: 176s - loss: 0.9668 - acc: 0.788 - ETA: 179s - loss: 1.0441 - acc: 0.775 - ETA: 181s - loss: 1.1165 - acc: 0.773 - ETA: 182s - loss: 1.1040 - acc: 0.761 - ETA: 184s - loss: 1.2268 - acc: 0.757 - ETA: 184s - loss: 1.1991 - acc: 0.762 - ETA: 183s - loss: 1.1345 - acc: 0.770 - ETA: 184s - loss: 1.1606 - acc: 0.770 - ETA: 184s - loss: 1.0928 - acc: 0.772 - ETA: 183s - loss: 1.0503 - acc: 0.775 - ETA: 183s - loss: 0.9925 - acc: 0.787 - ETA: 183s - loss: 1.0280 - acc: 0.785 - ETA: 181s - loss: 1.0338 - acc: 0.782 - ETA: 181s - loss: 1.0346 - acc: 0.781 - ETA: 180s - loss: 1.0603 - acc: 0.772 - ETA: 180s - loss: 1.0755 - acc: 0.770 - ETA: 180s - loss: 1.0882 - acc: 0.763 - ETA: 179s - loss: 1.0898 - acc: 0.763 - ETA: 178s - loss: 1.0881 - acc: 0.760 - ETA: 177s - loss: 1.1179 - acc: 0.754 - ETA: 177s - loss: 1.1143 - acc: 0.755 - ETA: 176s - loss: 1.1248 - acc: 0.752 - ETA: 176s - loss: 1.1055 - acc: 0.756 - ETA: 175s - loss: 1.1041 - acc: 0.756 - ETA: 174s - loss: 1.0767 - acc: 0.761 - ETA: 173s - loss: 1.0734 - acc: 0.765 - ETA: 173s - loss: 1.0887 - acc: 0.764 - ETA: 172s - loss: 1.0693 - acc: 0.767 - ETA: 171s - loss: 1.0788 - acc: 0.763 - ETA: 171s - loss: 1.0705 - acc: 0.764 - ETA: 169s - loss: 1.0574 - acc: 0.765 - ETA: 169s - loss: 1.0620 - acc: 0.764 - ETA: 168s - loss: 1.0448 - acc: 0.766 - ETA: 167s - loss: 1.0554 - acc: 0.763 - ETA: 167s - loss: 1.0610 - acc: 0.761 - ETA: 166s - loss: 1.0736 - acc: 0.760 - ETA: 165s - loss: 1.0785 - acc: 0.761 - ETA: 164s - loss: 1.0671 - acc: 0.764 - ETA: 163s - loss: 1.0505 - acc: 0.766 - ETA: 163s - loss: 1.0437 - acc: 0.769 - ETA: 162s - loss: 1.0455 - acc: 0.769 - ETA: 161s - loss: 1.0575 - acc: 0.768 - ETA: 160s - loss: 1.0557 - acc: 0.768 - ETA: 159s - loss: 1.0507 - acc: 0.769 - ETA: 158s - loss: 1.0448 - acc: 0.769 - ETA: 158s - loss: 1.0430 - acc: 0.768 - ETA: 157s - loss: 1.0420 - acc: 0.767 - ETA: 156s - loss: 1.0387 - acc: 0.767 - ETA: 155s - loss: 1.0320 - acc: 0.768 - ETA: 154s - loss: 1.0216 - acc: 0.769 - ETA: 154s - loss: 1.0284 - acc: 0.767 - ETA: 153s - loss: 1.0163 - acc: 0.770 - ETA: 152s - loss: 1.0266 - acc: 0.768 - ETA: 151s - loss: 1.0204 - acc: 0.769 - ETA: 151s - loss: 1.0295 - acc: 0.768 - ETA: 150s - loss: 1.0321 - acc: 0.769 - ETA: 149s - loss: 1.0349 - acc: 0.768 - ETA: 148s - loss: 1.0236 - acc: 0.769 - ETA: 147s - loss: 1.0223 - acc: 0.769 - ETA: 146s - loss: 1.0162 - acc: 0.770 - ETA: 146s - loss: 1.0108 - acc: 0.770 - ETA: 145s - loss: 1.0069 - acc: 0.770 - ETA: 144s - loss: 1.0104 - acc: 0.770 - ETA: 143s - loss: 1.0226 - acc: 0.769 - ETA: 142s - loss: 1.0271 - acc: 0.770 - ETA: 141s - loss: 1.0263 - acc: 0.770 - ETA: 141s - loss: 1.0340 - acc: 0.769 - ETA: 140s - loss: 1.0257 - acc: 0.770 - ETA: 139s - loss: 1.0297 - acc: 0.769 - ETA: 138s - loss: 1.0238 - acc: 0.769 - ETA: 137s - loss: 1.0196 - acc: 0.769 - ETA: 137s - loss: 1.0189 - acc: 0.770 - ETA: 136s - loss: 1.0176 - acc: 0.770 - ETA: 135s - loss: 1.0173 - acc: 0.769 - ETA: 134s - loss: 1.0174 - acc: 0.770 - ETA: 133s - loss: 1.0171 - acc: 0.770 - ETA: 132s - loss: 1.0077 - acc: 0.771 - ETA: 131s - loss: 1.0069 - acc: 0.772 - ETA: 131s - loss: 1.0009 - acc: 0.773 - ETA: 130s - loss: 1.0125 - acc: 0.771 - ETA: 129s - loss: 1.0143 - acc: 0.771 - ETA: 128s - loss: 1.0095 - acc: 0.773 - ETA: 127s - loss: 1.0037 - acc: 0.774 - ETA: 126s - loss: 0.9962 - acc: 0.776 - ETA: 126s - loss: 0.9938 - acc: 0.777 - ETA: 125s - loss: 0.9867 - acc: 0.779 - ETA: 124s - loss: 0.9860 - acc: 0.779 - ETA: 123s - loss: 0.9860 - acc: 0.779 - ETA: 122s - loss: 0.9927 - acc: 0.778 - ETA: 121s - loss: 0.9943 - acc: 0.776 - ETA: 121s - loss: 0.9922 - acc: 0.777 - ETA: 120s - loss: 0.9938 - acc: 0.777 - ETA: 119s - loss: 1.0014 - acc: 0.775 - ETA: 118s - loss: 1.0001 - acc: 0.775 - ETA: 117s - loss: 0.9970 - acc: 0.775 - ETA: 116s - loss: 0.9990 - acc: 0.774 - ETA: 115s - loss: 1.0000 - acc: 0.775 - ETA: 114s - loss: 0.9970 - acc: 0.776 - ETA: 114s - loss: 0.9928 - acc: 0.777 - ETA: 113s - loss: 0.9991 - acc: 0.777 - ETA: 112s - loss: 1.0023 - acc: 0.777 - ETA: 111s - loss: 0.9998 - acc: 0.777 - ETA: 110s - loss: 1.0004 - acc: 0.777 - ETA: 109s - loss: 0.9967 - acc: 0.778 - ETA: 108s - loss: 0.9962 - acc: 0.777 - ETA: 107s - loss: 0.9992 - acc: 0.776 - ETA: 107s - loss: 1.0029 - acc: 0.776 - ETA: 106s - loss: 0.9984 - acc: 0.777 - ETA: 105s - loss: 0.9975 - acc: 0.777 - ETA: 104s - loss: 1.0014 - acc: 0.777 - ETA: 103s - loss: 0.9970 - acc: 0.777 - ETA: 102s - loss: 0.9939 - acc: 0.778 - ETA: 101s - loss: 0.9874 - acc: 0.779 - ETA: 100s - loss: 0.9821 - acc: 0.779 - ETA: 99s - loss: 0.9784 - acc: 0.781 - ETA: 99s - loss: 0.9788 - acc: 0.78 - ETA: 98s - loss: 0.9768 - acc: 0.78 - ETA: 97s - loss: 0.9756 - acc: 0.78 - ETA: 96s - loss: 0.9701 - acc: 0.78 - ETA: 95s - loss: 0.9707 - acc: 0.78 - ETA: 94s - loss: 0.9724 - acc: 0.78 - ETA: 93s - loss: 0.9693 - acc: 0.78 - ETA: 92s - loss: 0.9720 - acc: 0.78 - ETA: 91s - loss: 0.9779 - acc: 0.78 - ETA: 91s - loss: 0.9721 - acc: 0.78 - ETA: 90s - loss: 0.9770 - acc: 0.78 - ETA: 89s - loss: 0.9766 - acc: 0.78 - ETA: 88s - loss: 0.9790 - acc: 0.78 - ETA: 87s - loss: 0.9744 - acc: 0.78 - ETA: 86s - loss: 0.9736 - acc: 0.78 - ETA: 85s - loss: 0.9721 - acc: 0.78 - ETA: 84s - loss: 0.9696 - acc: 0.78 - ETA: 83s - loss: 0.9680 - acc: 0.78 - ETA: 82s - loss: 0.9689 - acc: 0.78 - ETA: 81s - loss: 0.9720 - acc: 0.78 - ETA: 80s - loss: 0.9725 - acc: 0.78 - ETA: 80s - loss: 0.9721 - acc: 0.78 - ETA: 79s - loss: 0.9756 - acc: 0.78 - ETA: 78s - loss: 0.9771 - acc: 0.78 - ETA: 77s - loss: 0.9728 - acc: 0.78 - ETA: 76s - loss: 0.9694 - acc: 0.78 - ETA: 75s - loss: 0.9719 - acc: 0.78 - ETA: 74s - loss: 0.9722 - acc: 0.78 - ETA: 73s - loss: 0.9713 - acc: 0.78 - ETA: 72s - loss: 0.9707 - acc: 0.78 - ETA: 71s - loss: 0.9752 - acc: 0.78 - ETA: 70s - loss: 0.9799 - acc: 0.78 - ETA: 69s - loss: 0.9751 - acc: 0.78 - ETA: 68s - loss: 0.9747 - acc: 0.78 - ETA: 67s - loss: 0.9722 - acc: 0.78 - ETA: 67s - loss: 0.9773 - acc: 0.78 - ETA: 66s - loss: 0.9828 - acc: 0.78 - ETA: 65s - loss: 0.9799 - acc: 0.78 - ETA: 64s - loss: 0.9788 - acc: 0.78 - ETA: 63s - loss: 0.9781 - acc: 0.78 - ETA: 62s - loss: 0.9752 - acc: 0.78 - ETA: 61s - loss: 0.9752 - acc: 0.78 - ETA: 60s - loss: 0.9722 - acc: 0.78 - ETA: 59s - loss: 0.9752 - acc: 0.78 - ETA: 58s - loss: 0.9757 - acc: 0.78 - ETA: 57s - loss: 0.9765 - acc: 0.78 - ETA: 56s - loss: 0.9754 - acc: 0.78 - ETA: 55s - loss: 0.9745 - acc: 0.78 - ETA: 54s - loss: 0.9767 - acc: 0.78 - ETA: 53s - loss: 0.9775 - acc: 0.78 - ETA: 52s - loss: 0.9850 - acc: 0.78 - ETA: 51s - loss: 0.9845 - acc: 0.78 - ETA: 50s - loss: 0.9841 - acc: 0.78 - ETA: 50s - loss: 0.9875 - acc: 0.78 - ETA: 49s - loss: 0.9872 - acc: 0.78 - ETA: 48s - loss: 0.9831 - acc: 0.78 - ETA: 47s - loss: 0.9846 - acc: 0.78 - ETA: 46s - loss: 0.9821 - acc: 0.78 - ETA: 45s - loss: 0.9800 - acc: 0.78 - ETA: 44s - loss: 0.9781 - acc: 0.78 - ETA: 43s - loss: 0.9788 - acc: 0.78 - ETA: 42s - loss: 0.9785 - acc: 0.78 - ETA: 41s - loss: 0.9766 - acc: 0.78 - ETA: 40s - loss: 0.9775 - acc: 0.78 - ETA: 39s - loss: 0.9792 - acc: 0.78 - ETA: 38s - loss: 0.9799 - acc: 0.78 - ETA: 37s - loss: 0.9768 - acc: 0.78 - ETA: 36s - loss: 0.9755 - acc: 0.78 - ETA: 35s - loss: 0.9808 - acc: 0.78 - ETA: 34s - loss: 0.9810 - acc: 0.78 - ETA: 33s - loss: 0.9826 - acc: 0.78 - ETA: 32s - loss: 0.9789 - acc: 0.78 - ETA: 31s - loss: 0.9816 - acc: 0.78 - ETA: 30s - loss: 0.9845 - acc: 0.78 - ETA: 29s - loss: 0.9844 - acc: 0.78 - ETA: 28s - loss: 0.9842 - acc: 0.78 - ETA: 27s - loss: 0.9836 - acc: 0.78 - ETA: 26s - loss: 0.9847 - acc: 0.78 - ETA: 25s - loss: 0.9831 - acc: 0.78 - ETA: 24s - loss: 0.9827 - acc: 0.78 - ETA: 24s - loss: 0.9803 - acc: 0.78 - ETA: 23s - loss: 0.9805 - acc: 0.78 - ETA: 22s - loss: 0.9824 - acc: 0.78 - ETA: 21s - loss: 0.9802 - acc: 0.78 - ETA: 20s - loss: 0.9768 - acc: 0.78 - ETA: 19s - loss: 0.9782 - acc: 0.78 - ETA: 18s - loss: 0.9773 - acc: 0.78 - ETA: 17s - loss: 0.9767 - acc: 0.78 - ETA: 16s - loss: 0.9778 - acc: 0.78 - ETA: 15s - loss: 0.9753 - acc: 0.78 - ETA: 14s - loss: 0.9723 - acc: 0.78 - ETA: 13s - loss: 0.9720 - acc: 0.7818"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 12s - loss: 0.9685 - acc: 0.78 - ETA: 11s - loss: 0.9680 - acc: 0.78 - ETA: 10s - loss: 0.9681 - acc: 0.78 - ETA: 9s - loss: 0.9669 - acc: 0.7833 - ETA: 8s - loss: 0.9705 - acc: 0.782 - ETA: 7s - loss: 0.9747 - acc: 0.782 - ETA: 6s - loss: 0.9736 - acc: 0.782 - ETA: 5s - loss: 0.9736 - acc: 0.782 - ETA: 4s - loss: 0.9741 - acc: 0.782 - ETA: 3s - loss: 0.9762 - acc: 0.781 - ETA: 2s - loss: 0.9740 - acc: 0.782 - ETA: 1s - loss: 0.9710 - acc: 0.782 - ETA: 0s - loss: 0.9695 - acc: 0.7827Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 230s - loss: 0.9708 - acc: 0.7825 - val_loss: 0.8274 - val_acc: 0.8168\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6270/6680 [===========================>..] - ETA: 198s - loss: 0.9567 - acc: 0.833 - ETA: 200s - loss: 1.3373 - acc: 0.733 - ETA: 200s - loss: 1.1018 - acc: 0.733 - ETA: 199s - loss: 0.9539 - acc: 0.766 - ETA: 199s - loss: 0.9217 - acc: 0.780 - ETA: 199s - loss: 0.9017 - acc: 0.777 - ETA: 198s - loss: 0.9632 - acc: 0.771 - ETA: 198s - loss: 0.9907 - acc: 0.758 - ETA: 197s - loss: 1.0211 - acc: 0.766 - ETA: 196s - loss: 0.9935 - acc: 0.770 - ETA: 195s - loss: 0.9961 - acc: 0.772 - ETA: 195s - loss: 0.9774 - acc: 0.769 - ETA: 194s - loss: 0.9635 - acc: 0.771 - ETA: 194s - loss: 0.9582 - acc: 0.771 - ETA: 193s - loss: 0.9302 - acc: 0.777 - ETA: 193s - loss: 0.9082 - acc: 0.785 - ETA: 192s - loss: 0.9029 - acc: 0.790 - ETA: 191s - loss: 0.8827 - acc: 0.796 - ETA: 190s - loss: 0.8480 - acc: 0.805 - ETA: 190s - loss: 0.8490 - acc: 0.805 - ETA: 190s - loss: 0.8520 - acc: 0.801 - ETA: 189s - loss: 0.8446 - acc: 0.806 - ETA: 188s - loss: 0.8580 - acc: 0.805 - ETA: 187s - loss: 0.8460 - acc: 0.806 - ETA: 185s - loss: 0.8272 - acc: 0.810 - ETA: 184s - loss: 0.8637 - acc: 0.803 - ETA: 183s - loss: 0.8853 - acc: 0.801 - ETA: 183s - loss: 0.8726 - acc: 0.806 - ETA: 183s - loss: 0.8807 - acc: 0.808 - ETA: 182s - loss: 0.8621 - acc: 0.812 - ETA: 181s - loss: 0.8737 - acc: 0.809 - ETA: 180s - loss: 0.8811 - acc: 0.806 - ETA: 179s - loss: 0.8905 - acc: 0.803 - ETA: 179s - loss: 0.8843 - acc: 0.804 - ETA: 180s - loss: 0.8680 - acc: 0.807 - ETA: 180s - loss: 0.8539 - acc: 0.808 - ETA: 179s - loss: 0.8539 - acc: 0.808 - ETA: 178s - loss: 0.8522 - acc: 0.808 - ETA: 177s - loss: 0.8407 - acc: 0.810 - ETA: 176s - loss: 0.8310 - acc: 0.812 - ETA: 175s - loss: 0.8353 - acc: 0.811 - ETA: 174s - loss: 0.8247 - acc: 0.812 - ETA: 172s - loss: 0.8202 - acc: 0.811 - ETA: 171s - loss: 0.8138 - acc: 0.812 - ETA: 170s - loss: 0.8161 - acc: 0.811 - ETA: 169s - loss: 0.8107 - acc: 0.812 - ETA: 168s - loss: 0.8176 - acc: 0.812 - ETA: 167s - loss: 0.8110 - acc: 0.812 - ETA: 167s - loss: 0.8121 - acc: 0.812 - ETA: 166s - loss: 0.8114 - acc: 0.812 - ETA: 165s - loss: 0.8086 - acc: 0.811 - ETA: 164s - loss: 0.8139 - acc: 0.811 - ETA: 163s - loss: 0.8108 - acc: 0.811 - ETA: 162s - loss: 0.8217 - acc: 0.811 - ETA: 161s - loss: 0.8336 - acc: 0.810 - ETA: 160s - loss: 0.8249 - acc: 0.812 - ETA: 159s - loss: 0.8184 - acc: 0.814 - ETA: 158s - loss: 0.8089 - acc: 0.814 - ETA: 157s - loss: 0.8013 - acc: 0.816 - ETA: 156s - loss: 0.8082 - acc: 0.814 - ETA: 155s - loss: 0.7974 - acc: 0.816 - ETA: 154s - loss: 0.7958 - acc: 0.816 - ETA: 153s - loss: 0.7978 - acc: 0.815 - ETA: 152s - loss: 0.7929 - acc: 0.815 - ETA: 151s - loss: 0.7892 - acc: 0.815 - ETA: 150s - loss: 0.7889 - acc: 0.816 - ETA: 149s - loss: 0.7902 - acc: 0.816 - ETA: 148s - loss: 0.7904 - acc: 0.817 - ETA: 147s - loss: 0.8020 - acc: 0.815 - ETA: 146s - loss: 0.8035 - acc: 0.813 - ETA: 145s - loss: 0.7985 - acc: 0.815 - ETA: 144s - loss: 0.7980 - acc: 0.814 - ETA: 143s - loss: 0.7930 - acc: 0.816 - ETA: 142s - loss: 0.7955 - acc: 0.815 - ETA: 141s - loss: 0.7978 - acc: 0.816 - ETA: 141s - loss: 0.7963 - acc: 0.816 - ETA: 140s - loss: 0.7997 - acc: 0.816 - ETA: 139s - loss: 0.8057 - acc: 0.815 - ETA: 138s - loss: 0.7972 - acc: 0.817 - ETA: 137s - loss: 0.7955 - acc: 0.817 - ETA: 136s - loss: 0.7952 - acc: 0.817 - ETA: 135s - loss: 0.7925 - acc: 0.817 - ETA: 134s - loss: 0.7888 - acc: 0.818 - ETA: 133s - loss: 0.7898 - acc: 0.818 - ETA: 132s - loss: 0.7948 - acc: 0.818 - ETA: 131s - loss: 0.7996 - acc: 0.816 - ETA: 130s - loss: 0.7947 - acc: 0.817 - ETA: 129s - loss: 0.7957 - acc: 0.817 - ETA: 129s - loss: 0.8033 - acc: 0.816 - ETA: 128s - loss: 0.8040 - acc: 0.815 - ETA: 127s - loss: 0.8034 - acc: 0.816 - ETA: 126s - loss: 0.7976 - acc: 0.817 - ETA: 125s - loss: 0.7901 - acc: 0.819 - ETA: 124s - loss: 0.7904 - acc: 0.818 - ETA: 123s - loss: 0.7899 - acc: 0.819 - ETA: 122s - loss: 0.7831 - acc: 0.820 - ETA: 121s - loss: 0.7846 - acc: 0.821 - ETA: 120s - loss: 0.7889 - acc: 0.821 - ETA: 119s - loss: 0.8052 - acc: 0.818 - ETA: 118s - loss: 0.8006 - acc: 0.819 - ETA: 117s - loss: 0.8053 - acc: 0.817 - ETA: 116s - loss: 0.8146 - acc: 0.817 - ETA: 115s - loss: 0.8174 - acc: 0.817 - ETA: 114s - loss: 0.8127 - acc: 0.818 - ETA: 113s - loss: 0.8137 - acc: 0.817 - ETA: 112s - loss: 0.8231 - acc: 0.815 - ETA: 111s - loss: 0.8185 - acc: 0.816 - ETA: 110s - loss: 0.8200 - acc: 0.816 - ETA: 109s - loss: 0.8240 - acc: 0.815 - ETA: 108s - loss: 0.8221 - acc: 0.815 - ETA: 107s - loss: 0.8223 - acc: 0.815 - ETA: 107s - loss: 0.8190 - acc: 0.815 - ETA: 106s - loss: 0.8165 - acc: 0.815 - ETA: 105s - loss: 0.8175 - acc: 0.815 - ETA: 104s - loss: 0.8164 - acc: 0.815 - ETA: 103s - loss: 0.8163 - acc: 0.816 - ETA: 102s - loss: 0.8146 - acc: 0.816 - ETA: 102s - loss: 0.8133 - acc: 0.817 - ETA: 101s - loss: 0.8175 - acc: 0.816 - ETA: 100s - loss: 0.8180 - acc: 0.816 - ETA: 99s - loss: 0.8210 - acc: 0.815 - ETA: 98s - loss: 0.8189 - acc: 0.81 - ETA: 97s - loss: 0.8211 - acc: 0.81 - ETA: 96s - loss: 0.8200 - acc: 0.81 - ETA: 95s - loss: 0.8213 - acc: 0.81 - ETA: 95s - loss: 0.8248 - acc: 0.81 - ETA: 94s - loss: 0.8297 - acc: 0.81 - ETA: 93s - loss: 0.8304 - acc: 0.81 - ETA: 92s - loss: 0.8375 - acc: 0.81 - ETA: 91s - loss: 0.8404 - acc: 0.81 - ETA: 90s - loss: 0.8371 - acc: 0.81 - ETA: 89s - loss: 0.8384 - acc: 0.81 - ETA: 88s - loss: 0.8356 - acc: 0.81 - ETA: 87s - loss: 0.8390 - acc: 0.81 - ETA: 86s - loss: 0.8379 - acc: 0.81 - ETA: 85s - loss: 0.8377 - acc: 0.81 - ETA: 84s - loss: 0.8370 - acc: 0.81 - ETA: 84s - loss: 0.8370 - acc: 0.81 - ETA: 83s - loss: 0.8353 - acc: 0.81 - ETA: 82s - loss: 0.8376 - acc: 0.81 - ETA: 81s - loss: 0.8342 - acc: 0.81 - ETA: 80s - loss: 0.8410 - acc: 0.81 - ETA: 79s - loss: 0.8362 - acc: 0.81 - ETA: 78s - loss: 0.8409 - acc: 0.81 - ETA: 77s - loss: 0.8376 - acc: 0.81 - ETA: 76s - loss: 0.8361 - acc: 0.81 - ETA: 75s - loss: 0.8323 - acc: 0.81 - ETA: 74s - loss: 0.8357 - acc: 0.81 - ETA: 73s - loss: 0.8345 - acc: 0.81 - ETA: 72s - loss: 0.8347 - acc: 0.81 - ETA: 71s - loss: 0.8323 - acc: 0.81 - ETA: 70s - loss: 0.8297 - acc: 0.81 - ETA: 69s - loss: 0.8281 - acc: 0.81 - ETA: 68s - loss: 0.8294 - acc: 0.81 - ETA: 67s - loss: 0.8311 - acc: 0.81 - ETA: 67s - loss: 0.8298 - acc: 0.81 - ETA: 66s - loss: 0.8273 - acc: 0.81 - ETA: 65s - loss: 0.8331 - acc: 0.81 - ETA: 64s - loss: 0.8303 - acc: 0.81 - ETA: 63s - loss: 0.8342 - acc: 0.81 - ETA: 62s - loss: 0.8370 - acc: 0.81 - ETA: 61s - loss: 0.8391 - acc: 0.81 - ETA: 60s - loss: 0.8375 - acc: 0.81 - ETA: 59s - loss: 0.8355 - acc: 0.81 - ETA: 58s - loss: 0.8362 - acc: 0.81 - ETA: 57s - loss: 0.8339 - acc: 0.81 - ETA: 56s - loss: 0.8372 - acc: 0.81 - ETA: 55s - loss: 0.8349 - acc: 0.81 - ETA: 54s - loss: 0.8333 - acc: 0.81 - ETA: 53s - loss: 0.8349 - acc: 0.81 - ETA: 52s - loss: 0.8359 - acc: 0.81 - ETA: 51s - loss: 0.8368 - acc: 0.81 - ETA: 50s - loss: 0.8368 - acc: 0.81 - ETA: 49s - loss: 0.8355 - acc: 0.81 - ETA: 48s - loss: 0.8374 - acc: 0.81 - ETA: 47s - loss: 0.8343 - acc: 0.81 - ETA: 46s - loss: 0.8349 - acc: 0.81 - ETA: 45s - loss: 0.8338 - acc: 0.81 - ETA: 44s - loss: 0.8373 - acc: 0.81 - ETA: 43s - loss: 0.8347 - acc: 0.81 - ETA: 42s - loss: 0.8339 - acc: 0.81 - ETA: 41s - loss: 0.8326 - acc: 0.81 - ETA: 40s - loss: 0.8330 - acc: 0.81 - ETA: 39s - loss: 0.8309 - acc: 0.81 - ETA: 38s - loss: 0.8298 - acc: 0.81 - ETA: 37s - loss: 0.8310 - acc: 0.81 - ETA: 36s - loss: 0.8349 - acc: 0.81 - ETA: 35s - loss: 0.8370 - acc: 0.81 - ETA: 34s - loss: 0.8360 - acc: 0.81 - ETA: 33s - loss: 0.8348 - acc: 0.81 - ETA: 32s - loss: 0.8353 - acc: 0.81 - ETA: 31s - loss: 0.8335 - acc: 0.81 - ETA: 30s - loss: 0.8333 - acc: 0.81 - ETA: 29s - loss: 0.8369 - acc: 0.81 - ETA: 28s - loss: 0.8352 - acc: 0.81 - ETA: 27s - loss: 0.8358 - acc: 0.81 - ETA: 26s - loss: 0.8348 - acc: 0.81 - ETA: 25s - loss: 0.8344 - acc: 0.81 - ETA: 24s - loss: 0.8334 - acc: 0.81 - ETA: 23s - loss: 0.8315 - acc: 0.81 - ETA: 22s - loss: 0.8327 - acc: 0.81 - ETA: 21s - loss: 0.8308 - acc: 0.81 - ETA: 20s - loss: 0.8335 - acc: 0.81 - ETA: 19s - loss: 0.8307 - acc: 0.81 - ETA: 18s - loss: 0.8319 - acc: 0.81 - ETA: 17s - loss: 0.8315 - acc: 0.81 - ETA: 16s - loss: 0.8303 - acc: 0.81 - ETA: 15s - loss: 0.8300 - acc: 0.81 - ETA: 14s - loss: 0.8327 - acc: 0.8171"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 13s - loss: 0.8328 - acc: 0.81 - ETA: 12s - loss: 0.8318 - acc: 0.81 - ETA: 11s - loss: 0.8298 - acc: 0.81 - ETA: 10s - loss: 0.8284 - acc: 0.81 - ETA: 9s - loss: 0.8295 - acc: 0.8171 - ETA: 7s - loss: 0.8286 - acc: 0.817 - ETA: 6s - loss: 0.8271 - acc: 0.817 - ETA: 5s - loss: 0.8255 - acc: 0.817 - ETA: 4s - loss: 0.8240 - acc: 0.817 - ETA: 3s - loss: 0.8243 - acc: 0.817 - ETA: 2s - loss: 0.8240 - acc: 0.817 - ETA: 1s - loss: 0.8231 - acc: 0.817 - ETA: 0s - loss: 0.8213 - acc: 0.8179Epoch 00003: val_loss improved from 0.73564 to 0.69103, saving model to saved_models/weights.best.Xception_Resnet50_InceptionV3.hdf5\n",
      "6680/6680 [==============================] - 250s - loss: 0.8208 - acc: 0.8180 - val_loss: 0.6910 - val_acc: 0.8551\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6240/6680 [===========================>..] - ETA: 412s - loss: 0.5327 - acc: 0.933 - ETA: 322s - loss: 0.5988 - acc: 0.916 - ETA: 293s - loss: 0.5582 - acc: 0.922 - ETA: 281s - loss: 0.7661 - acc: 0.891 - ETA: 272s - loss: 0.7286 - acc: 0.886 - ETA: 265s - loss: 0.7238 - acc: 0.866 - ETA: 260s - loss: 0.7036 - acc: 0.842 - ETA: 257s - loss: 0.7269 - acc: 0.833 - ETA: 254s - loss: 0.6592 - acc: 0.848 - ETA: 252s - loss: 0.6676 - acc: 0.843 - ETA: 250s - loss: 0.6554 - acc: 0.842 - ETA: 247s - loss: 0.6885 - acc: 0.841 - ETA: 244s - loss: 0.6488 - acc: 0.851 - ETA: 243s - loss: 0.6159 - acc: 0.857 - ETA: 241s - loss: 0.6223 - acc: 0.853 - ETA: 240s - loss: 0.5923 - acc: 0.858 - ETA: 238s - loss: 0.5891 - acc: 0.860 - ETA: 237s - loss: 0.5850 - acc: 0.861 - ETA: 236s - loss: 0.5757 - acc: 0.861 - ETA: 235s - loss: 0.5968 - acc: 0.861 - ETA: 234s - loss: 0.5792 - acc: 0.865 - ETA: 232s - loss: 0.5932 - acc: 0.862 - ETA: 231s - loss: 0.6003 - acc: 0.860 - ETA: 229s - loss: 0.6446 - acc: 0.858 - ETA: 228s - loss: 0.6252 - acc: 0.860 - ETA: 227s - loss: 0.6146 - acc: 0.859 - ETA: 225s - loss: 0.6200 - acc: 0.853 - ETA: 224s - loss: 0.6183 - acc: 0.854 - ETA: 223s - loss: 0.6326 - acc: 0.851 - ETA: 222s - loss: 0.6390 - acc: 0.850 - ETA: 221s - loss: 0.6351 - acc: 0.852 - ETA: 220s - loss: 0.6623 - acc: 0.850 - ETA: 219s - loss: 0.6564 - acc: 0.850 - ETA: 217s - loss: 0.6569 - acc: 0.850 - ETA: 216s - loss: 0.6543 - acc: 0.849 - ETA: 215s - loss: 0.6461 - acc: 0.851 - ETA: 214s - loss: 0.6568 - acc: 0.850 - ETA: 213s - loss: 0.6518 - acc: 0.850 - ETA: 212s - loss: 0.6504 - acc: 0.849 - ETA: 210s - loss: 0.6547 - acc: 0.847 - ETA: 209s - loss: 0.6528 - acc: 0.846 - ETA: 208s - loss: 0.6495 - acc: 0.846 - ETA: 207s - loss: 0.6416 - acc: 0.848 - ETA: 206s - loss: 0.6333 - acc: 0.850 - ETA: 205s - loss: 0.6450 - acc: 0.846 - ETA: 203s - loss: 0.6446 - acc: 0.847 - ETA: 202s - loss: 0.6495 - acc: 0.847 - ETA: 201s - loss: 0.6611 - acc: 0.847 - ETA: 200s - loss: 0.6582 - acc: 0.848 - ETA: 199s - loss: 0.6671 - acc: 0.846 - ETA: 198s - loss: 0.6576 - acc: 0.847 - ETA: 196s - loss: 0.6636 - acc: 0.848 - ETA: 195s - loss: 0.6615 - acc: 0.845 - ETA: 194s - loss: 0.6561 - acc: 0.846 - ETA: 193s - loss: 0.6728 - acc: 0.844 - ETA: 192s - loss: 0.6707 - acc: 0.844 - ETA: 191s - loss: 0.6811 - acc: 0.843 - ETA: 190s - loss: 0.6872 - acc: 0.842 - ETA: 189s - loss: 0.6937 - acc: 0.842 - ETA: 188s - loss: 0.6958 - acc: 0.842 - ETA: 187s - loss: 0.7026 - acc: 0.840 - ETA: 186s - loss: 0.7128 - acc: 0.839 - ETA: 185s - loss: 0.7100 - acc: 0.840 - ETA: 184s - loss: 0.7053 - acc: 0.841 - ETA: 183s - loss: 0.7023 - acc: 0.842 - ETA: 182s - loss: 0.6970 - acc: 0.843 - ETA: 180s - loss: 0.6947 - acc: 0.843 - ETA: 179s - loss: 0.6999 - acc: 0.844 - ETA: 178s - loss: 0.6995 - acc: 0.844 - ETA: 176s - loss: 0.6952 - acc: 0.844 - ETA: 175s - loss: 0.6928 - acc: 0.845 - ETA: 174s - loss: 0.6944 - acc: 0.845 - ETA: 172s - loss: 0.6938 - acc: 0.845 - ETA: 171s - loss: 0.6954 - acc: 0.845 - ETA: 170s - loss: 0.6938 - acc: 0.845 - ETA: 168s - loss: 0.6875 - acc: 0.846 - ETA: 167s - loss: 0.6921 - acc: 0.845 - ETA: 166s - loss: 0.6911 - acc: 0.846 - ETA: 165s - loss: 0.6935 - acc: 0.845 - ETA: 163s - loss: 0.6904 - acc: 0.846 - ETA: 162s - loss: 0.6869 - acc: 0.846 - ETA: 161s - loss: 0.6874 - acc: 0.846 - ETA: 159s - loss: 0.6838 - acc: 0.846 - ETA: 158s - loss: 0.6798 - acc: 0.847 - ETA: 157s - loss: 0.6840 - acc: 0.846 - ETA: 156s - loss: 0.6825 - acc: 0.846 - ETA: 154s - loss: 0.6790 - acc: 0.847 - ETA: 153s - loss: 0.6857 - acc: 0.846 - ETA: 152s - loss: 0.6924 - acc: 0.844 - ETA: 151s - loss: 0.6945 - acc: 0.844 - ETA: 150s - loss: 0.6907 - acc: 0.845 - ETA: 148s - loss: 0.6945 - acc: 0.845 - ETA: 147s - loss: 0.6991 - acc: 0.845 - ETA: 146s - loss: 0.6967 - acc: 0.846 - ETA: 145s - loss: 0.6938 - acc: 0.846 - ETA: 144s - loss: 0.6960 - acc: 0.845 - ETA: 143s - loss: 0.6971 - acc: 0.845 - ETA: 141s - loss: 0.6962 - acc: 0.846 - ETA: 140s - loss: 0.7035 - acc: 0.845 - ETA: 139s - loss: 0.7022 - acc: 0.845 - ETA: 138s - loss: 0.7031 - acc: 0.845 - ETA: 137s - loss: 0.7021 - acc: 0.845 - ETA: 136s - loss: 0.7010 - acc: 0.845 - ETA: 135s - loss: 0.6984 - acc: 0.845 - ETA: 134s - loss: 0.6933 - acc: 0.846 - ETA: 133s - loss: 0.6996 - acc: 0.846 - ETA: 132s - loss: 0.6988 - acc: 0.845 - ETA: 131s - loss: 0.6964 - acc: 0.846 - ETA: 130s - loss: 0.7007 - acc: 0.845 - ETA: 129s - loss: 0.7010 - acc: 0.845 - ETA: 128s - loss: 0.7068 - acc: 0.844 - ETA: 127s - loss: 0.7064 - acc: 0.844 - ETA: 126s - loss: 0.7096 - acc: 0.843 - ETA: 125s - loss: 0.7144 - acc: 0.842 - ETA: 124s - loss: 0.7108 - acc: 0.842 - ETA: 122s - loss: 0.7103 - acc: 0.842 - ETA: 121s - loss: 0.7092 - acc: 0.843 - ETA: 120s - loss: 0.7076 - acc: 0.842 - ETA: 119s - loss: 0.7058 - acc: 0.842 - ETA: 118s - loss: 0.7028 - acc: 0.842 - ETA: 117s - loss: 0.6976 - acc: 0.843 - ETA: 115s - loss: 0.6977 - acc: 0.842 - ETA: 114s - loss: 0.7030 - acc: 0.842 - ETA: 113s - loss: 0.7039 - acc: 0.842 - ETA: 112s - loss: 0.7012 - acc: 0.842 - ETA: 111s - loss: 0.7003 - acc: 0.842 - ETA: 109s - loss: 0.7037 - acc: 0.841 - ETA: 108s - loss: 0.7028 - acc: 0.840 - ETA: 107s - loss: 0.7023 - acc: 0.841 - ETA: 106s - loss: 0.6986 - acc: 0.842 - ETA: 105s - loss: 0.6964 - acc: 0.842 - ETA: 104s - loss: 0.6941 - acc: 0.842 - ETA: 102s - loss: 0.6936 - acc: 0.843 - ETA: 101s - loss: 0.6904 - acc: 0.843 - ETA: 100s - loss: 0.6900 - acc: 0.843 - ETA: 99s - loss: 0.6927 - acc: 0.843 - ETA: 98s - loss: 0.6916 - acc: 0.84 - ETA: 96s - loss: 0.6889 - acc: 0.84 - ETA: 95s - loss: 0.6892 - acc: 0.84 - ETA: 94s - loss: 0.6868 - acc: 0.84 - ETA: 93s - loss: 0.6842 - acc: 0.84 - ETA: 92s - loss: 0.6846 - acc: 0.84 - ETA: 90s - loss: 0.6833 - acc: 0.84 - ETA: 89s - loss: 0.6810 - acc: 0.84 - ETA: 88s - loss: 0.6838 - acc: 0.84 - ETA: 87s - loss: 0.6835 - acc: 0.84 - ETA: 86s - loss: 0.6791 - acc: 0.84 - ETA: 85s - loss: 0.6777 - acc: 0.84 - ETA: 84s - loss: 0.6780 - acc: 0.84 - ETA: 82s - loss: 0.6874 - acc: 0.84 - ETA: 81s - loss: 0.6869 - acc: 0.84 - ETA: 80s - loss: 0.6896 - acc: 0.84 - ETA: 79s - loss: 0.6868 - acc: 0.84 - ETA: 78s - loss: 0.6843 - acc: 0.84 - ETA: 77s - loss: 0.6868 - acc: 0.84 - ETA: 76s - loss: 0.6837 - acc: 0.84 - ETA: 74s - loss: 0.6845 - acc: 0.84 - ETA: 73s - loss: 0.6840 - acc: 0.84 - ETA: 72s - loss: 0.6835 - acc: 0.84 - ETA: 71s - loss: 0.6837 - acc: 0.84 - ETA: 70s - loss: 0.6836 - acc: 0.84 - ETA: 69s - loss: 0.6824 - acc: 0.84 - ETA: 68s - loss: 0.6846 - acc: 0.84 - ETA: 67s - loss: 0.6859 - acc: 0.84 - ETA: 65s - loss: 0.6867 - acc: 0.84 - ETA: 64s - loss: 0.6890 - acc: 0.84 - ETA: 63s - loss: 0.6907 - acc: 0.84 - ETA: 62s - loss: 0.6895 - acc: 0.84 - ETA: 61s - loss: 0.6911 - acc: 0.84 - ETA: 60s - loss: 0.6893 - acc: 0.84 - ETA: 58s - loss: 0.6916 - acc: 0.84 - ETA: 57s - loss: 0.6935 - acc: 0.84 - ETA: 56s - loss: 0.6936 - acc: 0.84 - ETA: 55s - loss: 0.6979 - acc: 0.84 - ETA: 54s - loss: 0.6995 - acc: 0.84 - ETA: 53s - loss: 0.7000 - acc: 0.84 - ETA: 52s - loss: 0.6986 - acc: 0.84 - ETA: 50s - loss: 0.6976 - acc: 0.84 - ETA: 49s - loss: 0.7006 - acc: 0.84 - ETA: 48s - loss: 0.7038 - acc: 0.84 - ETA: 47s - loss: 0.7022 - acc: 0.84 - ETA: 46s - loss: 0.7017 - acc: 0.84 - ETA: 45s - loss: 0.6991 - acc: 0.84 - ETA: 43s - loss: 0.7006 - acc: 0.84 - ETA: 42s - loss: 0.6999 - acc: 0.84 - ETA: 41s - loss: 0.6997 - acc: 0.84 - ETA: 40s - loss: 0.7007 - acc: 0.84 - ETA: 39s - loss: 0.7004 - acc: 0.84 - ETA: 38s - loss: 0.6982 - acc: 0.84 - ETA: 36s - loss: 0.7004 - acc: 0.84 - ETA: 35s - loss: 0.6977 - acc: 0.84 - ETA: 34s - loss: 0.7009 - acc: 0.84 - ETA: 33s - loss: 0.7002 - acc: 0.84 - ETA: 32s - loss: 0.6976 - acc: 0.84 - ETA: 31s - loss: 0.6991 - acc: 0.84 - ETA: 30s - loss: 0.6995 - acc: 0.84 - ETA: 29s - loss: 0.7006 - acc: 0.84 - ETA: 27s - loss: 0.7005 - acc: 0.84 - ETA: 26s - loss: 0.7010 - acc: 0.84 - ETA: 25s - loss: 0.6999 - acc: 0.84 - ETA: 24s - loss: 0.7012 - acc: 0.84 - ETA: 23s - loss: 0.7018 - acc: 0.84 - ETA: 22s - loss: 0.6992 - acc: 0.84 - ETA: 21s - loss: 0.6999 - acc: 0.84 - ETA: 19s - loss: 0.6980 - acc: 0.84 - ETA: 18s - loss: 0.6975 - acc: 0.84 - ETA: 17s - loss: 0.6965 - acc: 0.84 - ETA: 16s - loss: 0.6954 - acc: 0.8446"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6680 [============================>.] - ETA: 15s - loss: 0.6924 - acc: 0.84 - ETA: 14s - loss: 0.6928 - acc: 0.84 - ETA: 13s - loss: 0.6913 - acc: 0.84 - ETA: 12s - loss: 0.6925 - acc: 0.84 - ETA: 10s - loss: 0.6917 - acc: 0.84 - ETA: 9s - loss: 0.6949 - acc: 0.8445 - ETA: 8s - loss: 0.6940 - acc: 0.844 - ETA: 7s - loss: 0.6947 - acc: 0.844 - ETA: 6s - loss: 0.6952 - acc: 0.844 - ETA: 5s - loss: 0.6953 - acc: 0.843 - ETA: 4s - loss: 0.6940 - acc: 0.844 - ETA: 3s - loss: 0.6981 - acc: 0.843 - ETA: 1s - loss: 0.6986 - acc: 0.843 - ETA: 0s - loss: 0.6985 - acc: 0.8431Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 268s - loss: 0.6979 - acc: 0.8433 - val_loss: 0.7270 - val_acc: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c02473eeb8>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "epochs = 5\n",
    "batch_size = 30\n",
    "model_file = 'saved_models/weights.best.{}.hdf5'.format(\"_\".join([\"Xception\", \"Resnet50\", \"InceptionV3\"]))\n",
    "checkpointer = ModelCheckpoint(filepath=model_file, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "merged_model.fit([xception_feature_set[0], resnet_feature_set[0], inception_feature_set[0]], train_targets, \n",
    "              validation_data=([xception_feature_set[1], resnet_feature_set[1], inception_feature_set[1]], valid_targets),\n",
    "              epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "merged_model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 84.21052631578948 %\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "test_input = zip(xception_feature_set[2], resnet_feature_set[2], inception_feature_set[2]) \n",
    "to_test_feature = lambda features: [np.expand_dims(feature, axis=0) for feature in features]\n",
    "merged_predictions = [np.argmax(merged_model.predict(to_test_feature(features))) for features in test_input]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(merged_predictions)==np.argmax(test_targets, axis=1))/len(merged_predictions)\n",
    "print(\"test accuracy : {} %\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_var = [0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_var = [0.8, 0.5, 0.3, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ker_var = [4096, 2048, 1024, 512, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_prouct = list(itertools.product(ker_var, l2_var, dr_var, ker_var, l2_var, dr_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ver_product = [var_prouct[np.random.randint(len(var_prouct))] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1024, 1e-05, 0.8, 1024, 0.01, 0.1), (1024, 1e-05, 0.1, 2048, 0.01, 0.1), (2048, 0.01, 0.01, 1024, 1e-05, 0.3), (2048, 0.0001, 0.8, 1024, 1e-05, 0.5), (1024, 0.001, 0.8, 2048, 0.01, 0.5), (256, 0.0001, 0.1, 512, 0.0001, 0.3), (256, 1e-05, 0.5, 4096, 0.01, 0.01), (256, 0.0001, 0.3, 2048, 0.001, 0.8), (1024, 0.01, 0.5, 1024, 0.001, 0.01), (512, 0.01, 0.3, 1024, 0.01, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "print(rand_ver_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 1e-05 Dropout1 0.8 \n",
      " filters2 1024 l2 0.01 Dropout2 0.1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_49 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_50 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_51 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_51 (Glo (None, 2048)          0           input_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_52 (Glo (None, 2048)          0           input_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_53 (Glo (None, 2048)          0           input_51[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)     (None, 6144)          0           global_average_pooling2d_51[0][0]\n",
      "                                                                   global_average_pooling2d_52[0][0]\n",
      "                                                                   global_average_pooling2d_53[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (None, 1024)          6292480     concatenate_18[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 1024)          0           dense_48[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 1024)          0           activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_49 (Dense)                 (None, 133)           136325      dropout_31[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 7.3577 - acc: 0.1716Epoch 00000: val_loss improved from inf to 1.74129, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 7.3330 - acc: 0.1722 - val_loss: 1.7413 - val_acc: 0.5605\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5635 - acc: 0.4477Epoch 00001: val_loss improved from 1.74129 to 0.77915, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.5583 - acc: 0.4488 - val_loss: 0.7792 - val_acc: 0.7808\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.9991 - acc: 0.5548Epoch 00002: val_loss improved from 0.77915 to 0.71690, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.0009 - acc: 0.5543 - val_loss: 0.7169 - val_acc: 0.7749\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.7910 - acc: 0.5938Epoch 00003: val_loss improved from 0.71690 to 0.58400, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.7877 - acc: 0.5942 - val_loss: 0.5840 - val_acc: 0.8251\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.6430 - acc: 0.6371Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.6427 - acc: 0.6364 - val_loss: 0.5879 - val_acc: 0.8323\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5126 - acc: 0.6597Epoch 00005: val_loss improved from 0.58400 to 0.56279, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.5132 - acc: 0.6593 - val_loss: 0.5628 - val_acc: 0.8467\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.5124 - acc: 0.6680Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.5153 - acc: 0.6678 - val_loss: 0.5753 - val_acc: 0.8515\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4658 - acc: 0.6843Epoch 00007: val_loss improved from 0.56279 to 0.54858, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.4686 - acc: 0.6835 - val_loss: 0.5486 - val_acc: 0.8539\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4191 - acc: 0.6991Epoch 00008: val_loss improved from 0.54858 to 0.51689, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 1.4231 - acc: 0.6985 - val_loss: 0.5169 - val_acc: 0.8479\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4131 - acc: 0.6941Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.4137 - acc: 0.6942 - val_loss: 0.5463 - val_acc: 0.8611\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.4041 - acc: 0.7074Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.4083 - acc: 0.7070 - val_loss: 0.5749 - val_acc: 0.8575\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3958 - acc: 0.7098Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3985 - acc: 0.7091 - val_loss: 0.6152 - val_acc: 0.8575\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3719 - acc: 0.7186Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3678 - acc: 0.7189 - val_loss: 0.5321 - val_acc: 0.8743\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.2932 - acc: 0.7296Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.2929 - acc: 0.7292 - val_loss: 0.5870 - val_acc: 0.8623\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.3335 - acc: 0.7264Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 1.3326 - acc: 0.7269 - val_loss: 0.6241 - val_acc: 0.8635\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 1e-05 Dropout1 0.1 \n",
      " filters2 2048 l2 0.01 Dropout2 0.1\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_52 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_53 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_54 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_54 (Glo (None, 2048)          0           input_52[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_55 (Glo (None, 2048)          0           input_53[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_56 (Glo (None, 2048)          0           input_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)     (None, 6144)          0           global_average_pooling2d_54[0][0]\n",
      "                                                                   global_average_pooling2d_55[0][0]\n",
      "                                                                   global_average_pooling2d_56[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 (None, 1024)          6292480     concatenate_19[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 1024)          0           dense_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 1024)          0           activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 (None, 133)           136325      dropout_32[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.9148 - acc: 0.5317Epoch 00000: val_loss improved from inf to 1.65565, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.8970 - acc: 0.5328 - val_loss: 1.6557 - val_acc: 0.6419\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7956Epoch 00001: val_loss improved from 1.65565 to 1.01342, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.8359 - acc: 0.7958 - val_loss: 1.0134 - val_acc: 0.7725\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8637Epoch 00002: val_loss improved from 1.01342 to 0.93692, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.5774 - acc: 0.8642 - val_loss: 0.9369 - val_acc: 0.8120\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8897Epoch 00003: val_loss improved from 0.93692 to 0.84680, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 0.4576 - acc: 0.8898 - val_loss: 0.8468 - val_acc: 0.8419\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.9155Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.3586 - acc: 0.9157 - val_loss: 0.9428 - val_acc: 0.8491\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9333Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2875 - acc: 0.9335 - val_loss: 0.9826 - val_acc: 0.8383\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9425Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2624 - acc: 0.9424 - val_loss: 1.0347 - val_acc: 0.8443\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9471Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2395 - acc: 0.9472 - val_loss: 0.9884 - val_acc: 0.8527\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9585Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1984 - acc: 0.9585 - val_loss: 1.0780 - val_acc: 0.8527\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9611Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.2046 - acc: 0.9612 - val_loss: 1.1641 - val_acc: 0.8443\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9635Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1755 - acc: 0.9635 - val_loss: 1.2208 - val_acc: 0.8503\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9683Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1642 - acc: 0.9681 - val_loss: 1.1722 - val_acc: 0.8491\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9659Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1750 - acc: 0.9657 - val_loss: 1.2688 - val_acc: 0.8323\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9665Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1852 - acc: 0.9665 - val_loss: 1.1957 - val_acc: 0.8419\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9733Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 0.1435 - acc: 0.9729 - val_loss: 1.1655 - val_acc: 0.8395\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 2048 l2 0.01 Dropout1 0.01 \n",
      " filters2 1024 l2 1e-05 Dropout2 0.3\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_55 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_56 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_57 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_57 (Glo (None, 2048)          0           input_55[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_58 (Glo (None, 2048)          0           input_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_59 (Glo (None, 2048)          0           input_57[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 6144)          0           global_average_pooling2d_57[0][0]\n",
      "                                                                   global_average_pooling2d_58[0][0]\n",
      "                                                                   global_average_pooling2d_59[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_52 (Dense)                 (None, 2048)          12584960    concatenate_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 2048)          0           dense_52[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 2048)          0           activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_53 (Dense)                 (None, 133)           272517      dropout_33[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 12,857,477\n",
      "Trainable params: 12,857,477\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.4318 - acc: 0.3190Epoch 00000: val_loss improved from inf to 4.43211, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 10.3925 - acc: 0.3204 - val_loss: 4.4321 - val_acc: 0.6335\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.9697 - acc: 0.6024Epoch 00001: val_loss improved from 4.43211 to 3.86219, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 3.9710 - acc: 0.6024 - val_loss: 3.8622 - val_acc: 0.5952\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.4520 - acc: 0.6389Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 3.4491 - acc: 0.6394 - val_loss: 4.1148 - val_acc: 0.5725\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.1445 - acc: 0.6665Epoch 00003: val_loss improved from 3.86219 to 3.39953, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 8s - loss: 3.1489 - acc: 0.6653 - val_loss: 3.3995 - val_acc: 0.6419\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0811 - acc: 0.6775- ETA: 0s - loss: 3.0843Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 3.0917 - acc: 0.6763 - val_loss: 4.1267 - val_acc: 0.5317\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.9026 - acc: 0.6973- ETA: 1s - loEpoch 00005: val_loss improved from 3.39953 to 2.87478, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.9006 - acc: 0.6978 - val_loss: 2.8748 - val_acc: 0.6790\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7580 - acc: 0.6991Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 2.7571 - acc: 0.6991 - val_loss: 2.9062 - val_acc: 0.6934\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6513 - acc: 0.7161Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 2.6508 - acc: 0.7159 - val_loss: 3.1201 - val_acc: 0.6323\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5738 - acc: 0.7181Epoch 00008: val_loss improved from 2.87478 to 2.81971, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.5795 - acc: 0.7171 - val_loss: 2.8197 - val_acc: 0.6802\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4761 - acc: 0.7336Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 2.4704 - acc: 0.7346 - val_loss: 2.8879 - val_acc: 0.6802\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3877 - acc: 0.7351Epoch 00010: val_loss improved from 2.81971 to 2.63747, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.3822 - acc: 0.7358 - val_loss: 2.6375 - val_acc: 0.6874\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3093 - acc: 0.7428- ETA: 1s -Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 2.3091 - acc: 0.7425 - val_loss: 3.2330 - val_acc: 0.6108\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1266 - acc: 0.7535Epoch 00012: val_loss improved from 2.63747 to 2.46404, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 2.1257 - acc: 0.7543 - val_loss: 2.4640 - val_acc: 0.7138\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.0035 - acc: 0.7667Epoch 00013: val_loss improved from 2.46404 to 2.23329, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.9991 - acc: 0.7671 - val_loss: 2.2333 - val_acc: 0.7150\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 1.9172 - acc: 0.7665Epoch 00014: val_loss improved from 2.23329 to 2.06659, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 1.9220 - acc: 0.7659 - val_loss: 2.0666 - val_acc: 0.7473\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 2048 l2 0.0001 Dropout1 0.8 \n",
      " filters2 1024 l2 1e-05 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_58 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_59 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_60 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_60 (Glo (None, 2048)          0           input_58[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_61 (Glo (None, 2048)          0           input_59[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_62 (Glo (None, 2048)          0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)     (None, 6144)          0           global_average_pooling2d_60[0][0]\n",
      "                                                                   global_average_pooling2d_61[0][0]\n",
      "                                                                   global_average_pooling2d_62[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_54 (Dense)                 (None, 2048)          12584960    concatenate_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 2048)          0           dense_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)             (None, 2048)          0           activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 133)           272517      dropout_34[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 12,857,477\n",
      "Trainable params: 12,857,477\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 14.5424 - acc: 0.0673Epoch 00000: val_loss improved from inf to 12.52506, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 9s - loss: 14.5358 - acc: 0.0675 - val_loss: 12.5251 - val_acc: 0.2060\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 13.2256 - acc: 0.1569Epoch 00001: val_loss improved from 12.52506 to 11.02990, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 13.2244 - acc: 0.1570 - val_loss: 11.0299 - val_acc: 0.3042\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 12.2135 - acc: 0.2261Epoch 00002: val_loss improved from 11.02990 to 10.81168, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 12.1977 - acc: 0.2269 - val_loss: 10.8117 - val_acc: 0.3305\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 11.4813 - acc: 0.2735Epoch 00003: val_loss improved from 10.81168 to 10.21103, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 11.4785 - acc: 0.2737 - val_loss: 10.2110 - val_acc: 0.3701\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 11.2255 - acc: 0.2968Epoch 00004: val_loss improved from 10.21103 to 10.01307, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 11.2111 - acc: 0.2976 - val_loss: 10.0131 - val_acc: 0.3880\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.9829 - acc: 0.3106Epoch 00005: val_loss improved from 10.01307 to 9.86549, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.9749 - acc: 0.3112 - val_loss: 9.8655 - val_acc: 0.3952\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.9587 - acc: 0.3139Epoch 00006: val_loss improved from 9.86549 to 9.68901, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.9754 - acc: 0.3130 - val_loss: 9.6890 - val_acc: 0.4048\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.7662 - acc: 0.3300Epoch 00007: val_loss improved from 9.68901 to 9.65490, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.7667 - acc: 0.3301 - val_loss: 9.6549 - val_acc: 0.4120\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.7091 - acc: 0.3297Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 10.7069 - acc: 0.3298 - val_loss: 9.8334 - val_acc: 0.3964\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.6412 - acc: 0.3391Epoch 00009: val_loss improved from 9.65490 to 9.65273, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.6256 - acc: 0.3401 - val_loss: 9.6527 - val_acc: 0.4132\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.6582 - acc: 0.3359 ETA: 0s - loss: 10.6790 - acc - ETA: 0s - loss: 10.6750 - accEpoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.6604 - acc: 0.3358 - val_loss: 9.6695 - val_acc: 0.4096\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5729 - acc: 0.3410Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 8s - loss: 10.5765 - acc: 0.3409 - val_loss: 9.7754 - val_acc: 0.4012\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5763 - acc: 0.3424Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.5587 - acc: 0.3436 - val_loss: 9.6575 - val_acc: 0.4108\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.4717 - acc: 0.3484Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 7s - loss: 10.4687 - acc: 0.3485 - val_loss: 9.9933 - val_acc: 0.3892\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 10.5567 - acc: 0.3428Epoch 00014: val_loss improved from 9.65273 to 9.62009, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 10.5548 - acc: 0.3430 - val_loss: 9.6201 - val_acc: 0.4120\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 0.001 Dropout1 0.8 \n",
      " filters2 2048 l2 0.01 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_61 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_62 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_63 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_63 (Glo (None, 2048)          0           input_61[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_64 (Glo (None, 2048)          0           input_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_65 (Glo (None, 2048)          0           input_63[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 6144)          0           global_average_pooling2d_63[0][0]\n",
      "                                                                   global_average_pooling2d_64[0][0]\n",
      "                                                                   global_average_pooling2d_65[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 1024)          6292480     concatenate_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 1024)          0           dense_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)             (None, 1024)          0           activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_57 (Dense)                 (None, 133)           136325      dropout_35[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 7.2409 - acc: 0.1732Epoch 00000: val_loss improved from inf to 1.94713, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 7.2164 - acc: 0.1744 - val_loss: 1.9471 - val_acc: 0.6395\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.3619 - acc: 0.4196Epoch 00001: val_loss improved from 1.94713 to 1.40740, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.3596 - acc: 0.4201 - val_loss: 1.4074 - val_acc: 0.7353\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0168 - acc: 0.5006Epoch 00002: val_loss improved from 1.40740 to 1.19659, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.0142 - acc: 0.5006 - val_loss: 1.1966 - val_acc: 0.8048\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8972 - acc: 0.5371Epoch 00003: val_loss improved from 1.19659 to 1.19115, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8965 - acc: 0.5371 - val_loss: 1.1911 - val_acc: 0.8204\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8250 - acc: 0.5626Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.8213 - acc: 0.5629 - val_loss: 1.2372 - val_acc: 0.8012\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8470 - acc: 0.5745Epoch 00005: val_loss improved from 1.19115 to 1.17878, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8481 - acc: 0.5746 - val_loss: 1.1788 - val_acc: 0.8156\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7223 - acc: 0.5872Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7277 - acc: 0.5858 - val_loss: 1.1798 - val_acc: 0.8323\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7698 - acc: 0.5952Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7723 - acc: 0.5955 - val_loss: 1.2908 - val_acc: 0.8311\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6397 - acc: 0.6145Epoch 00008: val_loss improved from 1.17878 to 1.16438, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6316 - acc: 0.6156 - val_loss: 1.1644 - val_acc: 0.8395\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7203 - acc: 0.6017Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7275 - acc: 0.6009 - val_loss: 1.2538 - val_acc: 0.8419\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.7256 - acc: 0.6179Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.7249 - acc: 0.6187 - val_loss: 1.2336 - val_acc: 0.8359\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6399 - acc: 0.6107Epoch 00011: val_loss improved from 1.16438 to 1.13106, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6356 - acc: 0.6109 - val_loss: 1.1311 - val_acc: 0.8455\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5943 - acc: 0.6222Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5987 - acc: 0.6219 - val_loss: 1.2598 - val_acc: 0.8180\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6367 - acc: 0.6228Epoch 00013: val_loss improved from 1.13106 to 1.11448, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.6382 - acc: 0.6231 - val_loss: 1.1145 - val_acc: 0.8479\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5406 - acc: 0.6249Epoch 00014: val_loss improved from 1.11448 to 1.07862, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.5373 - acc: 0.6250 - val_loss: 1.0786 - val_acc: 0.8527\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 0.0001 Dropout1 0.1 \n",
      " filters2 512 l2 0.0001 Dropout2 0.3\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_64 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_65 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_66 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_66 (Glo (None, 2048)          0           input_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_67 (Glo (None, 2048)          0           input_65[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_68 (Glo (None, 2048)          0           input_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 6144)          0           global_average_pooling2d_66[0][0]\n",
      "                                                                   global_average_pooling2d_67[0][0]\n",
      "                                                                   global_average_pooling2d_68[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_58 (Dense)                 (None, 256)           1573120     concatenate_23[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 256)           0           dense_58[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)             (None, 256)           0           activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_59 (Dense)                 (None, 133)           34181       dropout_36[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7181 - acc: 0.6077Epoch 00000: val_loss improved from inf to 0.74406, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7145 - acc: 0.6079 - val_loss: 0.7441 - val_acc: 0.7856\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.8221Epoch 00001: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.6429 - acc: 0.8225 - val_loss: 0.7455 - val_acc: 0.7964\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8727Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4776 - acc: 0.8729 - val_loss: 0.8240 - val_acc: 0.8120\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8970Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3862 - acc: 0.8972 - val_loss: 0.7558 - val_acc: 0.8168\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.9168Epoch 00004: val_loss improved from 0.74406 to 0.69155, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.3324 - acc: 0.9169 - val_loss: 0.6915 - val_acc: 0.8599\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9239Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.3169 - acc: 0.9235 - val_loss: 0.8649 - val_acc: 0.8419\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9435Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2784 - acc: 0.9425 - val_loss: 0.9183 - val_acc: 0.8419\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9503Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2606 - acc: 0.9503 - val_loss: 0.8732 - val_acc: 0.8527\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9508Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2593 - acc: 0.9513 - val_loss: 0.9081 - val_acc: 0.8503\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9555Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2399 - acc: 0.9552 - val_loss: 0.9621 - val_acc: 0.8431\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9583Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2385 - acc: 0.9582 - val_loss: 0.9588 - val_acc: 0.8479\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9594Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2465 - acc: 0.9596 - val_loss: 0.9935 - val_acc: 0.8383\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9617Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2392 - acc: 0.9618 - val_loss: 1.0856 - val_acc: 0.8359\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9636Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2265 - acc: 0.9639 - val_loss: 1.0267 - val_acc: 0.8575\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9626Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.2390 - acc: 0.9621 - val_loss: 1.2573 - val_acc: 0.8287\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 1e-05 Dropout1 0.5 \n",
      " filters2 4096 l2 0.01 Dropout2 0.01\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_67 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_68 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_69 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_69 (Glo (None, 2048)          0           input_67[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_70 (Glo (None, 2048)          0           input_68[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_71 (Glo (None, 2048)          0           input_69[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)     (None, 6144)          0           global_average_pooling2d_69[0][0]\n",
      "                                                                   global_average_pooling2d_70[0][0]\n",
      "                                                                   global_average_pooling2d_71[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_60 (Dense)                 (None, 256)           1573120     concatenate_24[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 256)           0           dense_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)             (None, 256)           0           activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_61 (Dense)                 (None, 133)           34181       dropout_37[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.7580 - acc: 0.4033Epoch 00000: val_loss improved from inf to 0.76667, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.7435 - acc: 0.4051 - val_loss: 0.7667 - val_acc: 0.7641\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.1685 - acc: 0.6845Epoch 00001: val_loss improved from 0.76667 to 0.59458, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.1644 - acc: 0.6859 - val_loss: 0.5946 - val_acc: 0.8240\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8607 - acc: 0.7653Epoch 00002: val_loss improved from 0.59458 to 0.57576, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 0.8654 - acc: 0.7647 - val_loss: 0.5758 - val_acc: 0.8275\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7964Epoch 00003: val_loss improved from 0.57576 to 0.50511, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.7134 - acc: 0.7964 - val_loss: 0.5051 - val_acc: 0.8695\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6442 - acc: 0.8153Epoch 00004: val_loss improved from 0.50511 to 0.48371, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.6418 - acc: 0.8157 - val_loss: 0.4837 - val_acc: 0.8503\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5418 - acc: 0.8421Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.5457 - acc: 0.8416 - val_loss: 0.5041 - val_acc: 0.8683\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8555Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.5173 - acc: 0.8549 - val_loss: 0.5303 - val_acc: 0.8599\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8638Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4641 - acc: 0.8632 - val_loss: 0.5366 - val_acc: 0.8659\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8712Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4466 - acc: 0.8708 - val_loss: 0.4871 - val_acc: 0.8743\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8841Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4127 - acc: 0.8831 - val_loss: 0.6162 - val_acc: 0.8671\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8848Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4112 - acc: 0.8850 - val_loss: 0.6304 - val_acc: 0.8647\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8920Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3827 - acc: 0.8906 - val_loss: 0.5493 - val_acc: 0.8802\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.9089Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3515 - acc: 0.9081 - val_loss: 0.6849 - val_acc: 0.8539\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.9047Epoch 00013: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 4s - loss: 0.3495 - acc: 0.9045 - val_loss: 0.6030 - val_acc: 0.8731\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.9097Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3475 - acc: 0.9091 - val_loss: 0.6627 - val_acc: 0.8778\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 256 l2 0.0001 Dropout1 0.3 \n",
      " filters2 2048 l2 0.001 Dropout2 0.8\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_70 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_71 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_72 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_72 (Glo (None, 2048)          0           input_70[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_73 (Glo (None, 2048)          0           input_71[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_74 (Glo (None, 2048)          0           input_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)     (None, 6144)          0           global_average_pooling2d_72[0][0]\n",
      "                                                                   global_average_pooling2d_73[0][0]\n",
      "                                                                   global_average_pooling2d_74[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_62 (Dense)                 (None, 256)           1573120     concatenate_25[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 256)           0           dense_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)             (None, 256)           0           activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_63 (Dense)                 (None, 133)           34181       dropout_38[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,607,301\n",
      "Trainable params: 1,607,301\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 2.0120 - acc: 0.5541Epoch 00000: val_loss improved from inf to 0.79505, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 2.0024 - acc: 0.5551 - val_loss: 0.7950 - val_acc: 0.7820\n",
      "Epoch 2/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7779Epoch 00001: val_loss improved from 0.79505 to 0.65269, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.8189 - acc: 0.7777 - val_loss: 0.6527 - val_acc: 0.8084\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.6414 - acc: 0.8303Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.6464 - acc: 0.8296 - val_loss: 0.6995 - val_acc: 0.8323\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8615Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.5306 - acc: 0.8606 - val_loss: 0.8258 - val_acc: 0.8120\n",
      "Epoch 5/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8782Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4796 - acc: 0.8777 - val_loss: 0.6654 - val_acc: 0.8527\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8923Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.4461 - acc: 0.8927 - val_loss: 0.7375 - val_acc: 0.8527\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.9033Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.4148 - acc: 0.9025 - val_loss: 0.6975 - val_acc: 0.8563\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.9130Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3975 - acc: 0.9124 - val_loss: 0.7876 - val_acc: 0.8491\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.9202Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3677 - acc: 0.9198 - val_loss: 0.8019 - val_acc: 0.8491\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9262Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3294 - acc: 0.9254 - val_loss: 0.8139 - val_acc: 0.8515\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.9333Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.3352 - acc: 0.9335 - val_loss: 0.7610 - val_acc: 0.8671\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3465 - acc: 0.9323Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3475 - acc: 0.9320 - val_loss: 0.8896 - val_acc: 0.8503\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9347Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3438 - acc: 0.9343 - val_loss: 0.9100 - val_acc: 0.8503\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9402Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3129 - acc: 0.9395 - val_loss: 0.8023 - val_acc: 0.8671\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.9385Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3242 - acc: 0.9388 - val_loss: 0.9591 - val_acc: 0.8659\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 1024 l2 0.01 Dropout1 0.5 \n",
      " filters2 1024 l2 0.001 Dropout2 0.01\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_73 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_74 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_75 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_75 (Glo (None, 2048)          0           input_73[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_76 (Glo (None, 2048)          0           input_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_77 (Glo (None, 2048)          0           input_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)     (None, 6144)          0           global_average_pooling2d_75[0][0]\n",
      "                                                                   global_average_pooling2d_76[0][0]\n",
      "                                                                   global_average_pooling2d_77[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_64 (Dense)                 (None, 1024)          6292480     concatenate_26[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 1024)          0           dense_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)             (None, 1024)          0           activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_65 (Dense)                 (None, 133)           136325      dropout_39[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,428,805\n",
      "Trainable params: 6,428,805\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 6.8735 - acc: 0.4362Epoch 00000: val_loss improved from inf to 3.08689, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 8s - loss: 6.8457 - acc: 0.4374 - val_loss: 3.0869 - val_acc: 0.6934\n",
      "Epoch 2/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.4593 - acc: 0.5919Epoch 00001: val_loss improved from 3.08689 to 2.69905, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.4579 - acc: 0.5916 - val_loss: 2.6991 - val_acc: 0.6778\n",
      "Epoch 3/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 3.0576 - acc: 0.6258Epoch 00002: val_loss improved from 2.69905 to 2.42455, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 3.0575 - acc: 0.6254 - val_loss: 2.4245 - val_acc: 0.7090\n",
      "Epoch 4/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.9800 - acc: 0.6353Epoch 00003: val_loss improved from 2.42455 to 2.32077, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.9867 - acc: 0.6338 - val_loss: 2.3208 - val_acc: 0.7377\n",
      "Epoch 5/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.8227 - acc: 0.6483Epoch 00004: val_loss improved from 2.32077 to 1.96411, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.8213 - acc: 0.6493 - val_loss: 1.9641 - val_acc: 0.8024\n",
      "Epoch 6/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6496 - acc: 0.6618Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.6531 - acc: 0.6614 - val_loss: 2.0456 - val_acc: 0.7593\n",
      "Epoch 7/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.6377 - acc: 0.6602Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.6407 - acc: 0.6594 - val_loss: 2.1363 - val_acc: 0.7509\n",
      "Epoch 8/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5568 - acc: 0.6674Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5640 - acc: 0.6668 - val_loss: 2.2292 - val_acc: 0.7138\n",
      "Epoch 9/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.5322 - acc: 0.6689Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.5335 - acc: 0.6695 - val_loss: 1.9996 - val_acc: 0.7665\n",
      "Epoch 10/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4412 - acc: 0.6790Epoch 00009: val_loss improved from 1.96411 to 1.93914, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.4392 - acc: 0.6793 - val_loss: 1.9391 - val_acc: 0.7737\n",
      "Epoch 11/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.4002 - acc: 0.6753Epoch 00010: val_loss improved from 1.93914 to 1.91448, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.3990 - acc: 0.6756 - val_loss: 1.9145 - val_acc: 0.7749\n",
      "Epoch 12/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.3052 - acc: 0.6968Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.3086 - acc: 0.6957 - val_loss: 1.9362 - val_acc: 0.7749\n",
      "Epoch 13/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.2366 - acc: 0.6947Epoch 00012: val_loss improved from 1.91448 to 1.81238, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.2347 - acc: 0.6948 - val_loss: 1.8124 - val_acc: 0.7629\n",
      "Epoch 14/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1765 - acc: 0.6902Epoch 00013: val_loss improved from 1.81238 to 1.71082, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 7s - loss: 2.1799 - acc: 0.6900 - val_loss: 1.7108 - val_acc: 0.7916\n",
      "Epoch 15/15\n",
      "6630/6680 [============================>.] - ETA: 0s - loss: 2.1013 - acc: 0.7012Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 6s - loss: 2.0994 - acc: 0.7013 - val_loss: 1.9551 - val_acc: 0.7174\n",
      "==========models with ['Resnet50', 'InceptionV3', 'Xception'] fileters1 512 l2 0.01 Dropout1 0.3 \n",
      " filters2 1024 l2 0.01 Dropout2 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_76 (InputLayer)            (None, 1, 1, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_77 (InputLayer)            (None, 5, 5, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_78 (InputLayer)            (None, 7, 7, 2048)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_78 (Glo (None, 2048)          0           input_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_79 (Glo (None, 2048)          0           input_77[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_80 (Glo (None, 2048)          0           input_78[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)     (None, 6144)          0           global_average_pooling2d_78[0][0]\n",
      "                                                                   global_average_pooling2d_79[0][0]\n",
      "                                                                   global_average_pooling2d_80[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "dense_66 (Dense)                 (None, 512)           3146240     concatenate_27[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 512)           0           dense_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)             (None, 512)           0           activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_67 (Dense)                 (None, 133)           68229       dropout_40[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3,214,469\n",
      "Trainable params: 3,214,469\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 4.4501 - acc: 0.5208Epoch 00000: val_loss improved from inf to 2.21937, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 6s - loss: 4.4302 - acc: 0.5217 - val_loss: 2.2194 - val_acc: 0.6982\n",
      "Epoch 2/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 2.2780 - acc: 0.6829Epoch 00001: val_loss improved from 2.21937 to 2.08154, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 5s - loss: 2.2775 - acc: 0.6828 - val_loss: 2.0815 - val_acc: 0.7102\n",
      "Epoch 3/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9979 - acc: 0.7171Epoch 00002: val_loss improved from 2.08154 to 1.86843, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9974 - acc: 0.7166 - val_loss: 1.8684 - val_acc: 0.7389\n",
      "Epoch 4/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.9117 - acc: 0.7179Epoch 00003: val_loss improved from 1.86843 to 1.70168, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.9144 - acc: 0.7174 - val_loss: 1.7017 - val_acc: 0.7437\n",
      "Epoch 5/15\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.8664 - acc: 0.7263Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.8683 - acc: 0.7260 - val_loss: 1.9851 - val_acc: 0.7269\n",
      "Epoch 6/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7923 - acc: 0.7402Epoch 00005: val_loss improved from 1.70168 to 1.61132, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.7907 - acc: 0.7398 - val_loss: 1.6113 - val_acc: 0.7701\n",
      "Epoch 7/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7556 - acc: 0.7368Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.7554 - acc: 0.7370 - val_loss: 1.7491 - val_acc: 0.7485\n",
      "Epoch 8/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7182 - acc: 0.7373Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.7201 - acc: 0.7367 - val_loss: 1.6669 - val_acc: 0.7353\n",
      "Epoch 9/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.7004 - acc: 0.7500Epoch 00008: val_loss improved from 1.61132 to 1.48000, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6973 - acc: 0.7499 - val_loss: 1.4800 - val_acc: 0.7940\n",
      "Epoch 10/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6313 - acc: 0.7511Epoch 00009: val_loss improved from 1.48000 to 1.39639, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.6291 - acc: 0.7519 - val_loss: 1.3964 - val_acc: 0.7868\n",
      "Epoch 11/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5972 - acc: 0.7502Epoch 00010: val_loss improved from 1.39639 to 1.34935, saving model to saved_models/weights.best.Resnet50_InceptionV3_Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.5965 - acc: 0.7504 - val_loss: 1.3494 - val_acc: 0.8096\n",
      "Epoch 12/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5528 - acc: 0.7645Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5522 - acc: 0.7651 - val_loss: 1.6309 - val_acc: 0.7689\n",
      "Epoch 13/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5133 - acc: 0.7639Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5145 - acc: 0.7641 - val_loss: 1.5043 - val_acc: 0.7760\n",
      "Epoch 14/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.5224 - acc: 0.7623Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.5232 - acc: 0.7629 - val_loss: 1.5055 - val_acc: 0.7689\n",
      "Epoch 15/15\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.4629 - acc: 0.7758Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 1.4650 - acc: 0.7757 - val_loss: 1.3569 - val_acc: 0.7892\n"
     ]
    }
   ],
   "source": [
    "merged_models = [train_merged_model(*var) for var in rand_ver_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 1e-05, 0.8, 1024, 0.01, 0.1) accuracy 83.85167464114832\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 1e-05, 0.1, 2048, 0.01, 0.1) accuracy 83.73205741626795\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(2048, 0.01, 0.01, 1024, 1e-05, 0.3) accuracy 72.8468899521531\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(2048, 0.0001, 0.8, 1024, 1e-05, 0.5) accuracy 41.985645933014354\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 0.001, 0.8, 2048, 0.01, 0.5) accuracy 84.68899521531101\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 0.0001, 0.1, 512, 0.0001, 0.3) accuracy 84.0909090909091\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 1e-05, 0.5, 4096, 0.01, 0.01) accuracy 84.80861244019138\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(256, 0.0001, 0.3, 2048, 0.001, 0.8) accuracy 80.14354066985646\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(1024, 0.01, 0.5, 1024, 0.001, 0.01) accuracy 78.11004784688996\n",
      "ker1, l2_1, dr1, ker2, l2_2, dr2\n",
      "(512, 0.01, 0.3, 1024, 0.01, 0.5) accuracy 80.14354066985646\n"
     ]
    }
   ],
   "source": [
    "for parms, merged_model in zip(rand_ver_product, merged_models):\n",
    "    print(\"ker1, l2_1, dr1, ker2, l2_2, dr2\")\n",
    "    print(\"{} accuracy {}\".format(parms, merged_model[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import extract_InceptionV3, extract_Resnet50, extract_Xception\n",
    "def predict(image_path):\n",
    "    input_data = [extract_Xception(path_to_tensor(image_path)), extract_Resnet50(path_to_tensor(image_path)), \n",
    "                 extract_InceptionV3(path_to_tensor(image_path))]\n",
    "    #inputs= np.array([np.expand_dims(feature1, axis=0), np.expand_dims(feature2, axis=0)])\n",
    "    predicted_prob = merged_model.predict(input_data)\n",
    "    predicted_idx = np.argmax(predicted_prob[0])\n",
    "    return dog_names[predicted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_img(image_path):\n",
    "    absolute_image_path = os.path.abspath(image_path)\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "    img = cv2.imread(absolute_image_path)\n",
    "    ax = fig.add_subplot(1, 1, 1, xticks=range(0, img.shape[1], 100), yticks=range(0, img.shape[0], 100))\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def askDogBreed(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\"Error\\n\\n\\\"{}\\\" doesn't exist.\".format(image_path))\n",
    "        return\n",
    "    \n",
    "    if not os.path.splitext(image_path)[1] == \".jpg\":\n",
    "        print(\"Error\\n\\n \\\"{}\\\" isn't jpeg format.\" )\n",
    "        return\n",
    "        \n",
    "    if dog_detector(image_path):\n",
    "        print(\"hello, dog!\")\n",
    "        visualize_img(image_path)\n",
    "        print(\"You are a {}, aren't you?\".format(predict(image_path)))\n",
    "        \n",
    "    elif face_detector(image_path):\n",
    "        print(\"hello, human!\")\n",
    "        visualize_img(image_path)\n",
    "        print(\"You look like a ...\\n{}\".format(predict(image_path)))\n",
    "    \n",
    "    else:\n",
    "        print(\"Error!\\n\\nYou should request with a image including a human or a dog\")\n",
    "        visualize_img(image_path)\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "my_image_paths = [\"test_images/dog4.jpg\",\n",
    "                \"test_images/philippe_jordan.jpg\", \n",
    "                  \"test_images/eric_clapton.jpg\",\n",
    "                  \"test_images/shibainu.jpg\", \n",
    "                  \"test_images/pag.jpg\", \n",
    "                 \"test_images/dog2.jpg\",\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, dog!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAADGCAYAAADWg+V4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIESSDBIkSADA/wMi6C52rZiDEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bfeb4cbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-ad6bef6e93a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmy_image_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0maskDogBreed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-6bb76e883cae>\u001b[0m in \u001b[0;36maskDogBreed\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hello, dog!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mvisualize_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You are a {}, aren't you?\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-5658f62d1015>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     input_data = [extract_Xception(path_to_tensor(image_path)), extract_Resnet50(path_to_tensor(image_path)), \n\u001b[1;32m----> 6\u001b[1;33m                  extract_InceptionV3(path_to_tensor(image_path))]\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#inputs= np.array([np.expand_dims(feature1, axis=0), np.expand_dims(feature2, axis=0)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpredicted_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ai_nanodegree\\term2\\lesson2\\dog-project\\extract_bottleneck_features.py\u001b[0m in \u001b[0;36mextract_InceptionV3\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_InceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minception_v3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\applications\\inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[0mcache_subdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                 md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'theano'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m   2493\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2495\u001b[1;33m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m   2906\u001b[0m                              ' elements.')\n\u001b[0;32m   2907\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2908\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1996\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1997\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0m_initialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m()\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aind-dog\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for image_path in my_image_paths:\n",
    "    askDogBreed(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:aind-dog]",
   "language": "python",
   "name": "conda-env-aind-dog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
